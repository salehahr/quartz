# Agenda
	  
## To-dos
### Ongoing
* [ ] Training data generation (up to 3k), including adjacency matrix
* [ ] Figure out model checkpoints
* [ ] Adjust model -- add more output channels
* [ ] Start HiWi (Pytorch 3D + PyQT GUI) by the end of the week


### New
* [x] Model summary -- ca. 30M parameters 
* [ ] Make new training data for 256px images (two folders: 256px, 512px)
* [ ] Implement data augmentation


## U-Net
* `DataGenerator`, so far without augmentation
* Based on the class `UNet` in `03_CNN` -- modifications:
	* Branched outputs (to apply different losses/activation functions)
		* Filtered output: linear activation, MSE loss
		* Skeletonised output: sigmoid activation, binary crossentropy loss
	* ~~Alternative: final conv2D layer with 4 filters?~~
* Current results using small dataset (100 photos) and low number of epochs (5 epochs)
	* Training set sample  
	  ![](_img/Pasted%20image%2020211109093723.png)
	* Validation output (Input; Filter GT + Filter predicted; Skel GT + Skel predicted)
	  ![](_img/Pasted%20image%2020211109093753.png)
	  
	  
## To clarify
* [x] SA
* [ ] VGGNet


## Notes
* Workflow established using minimal example with two image outputs.
* Next step: data augmentation, some pre-processing of data.
* After getting model to work with 256px, proceed to 'edge detection' network (finding the adjacency matrix between two nodes), possibly while training the model to work with 512px in the background.