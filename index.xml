<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/</link><description>Recent content on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><lastBuildDate>Mon, 14 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://salehahr.github.io/zettelkasten/index.xml" rel="self" type="application/rss+xml"/><item><title>2022-02-15</title><link>https://salehahr.github.io/zettelkasten/unlisted/2022-02-15/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/2022-02-15/</guid><description>Agenda Currently working on EdgeNN model: VGG16 architecture — training and predicting works in unit tests
Modification of existing skeletonised images to remove edges of length 1? s. previous meeting notes .
orig skel img -&amp;gt; extract nodes -&amp;gt; extract edges, find pixels to remove -&amp;gt; modify skel img -&amp;gt; modify list of nodes/node types New graph extraction w/ new edge attribute matrix? To-do (from last week 2022-02-02 ) 3rd network: Updated DataGenerator Batching method (CombinatorialAdjNN) Train EdgeNN Hyperparameter optimisation of EdgeNN Implement batching/threading logic Adj_vec method (BruteForceAdjNN) Extraction of edge pixels For later/report Get prediction times on model deployment Comparison on test dataset Low prio Programmatic implementation of early stopping Hyperparameter optimisation of second network Train first network (transfer learning)</description></item><item><title>2022-02-08</title><link>https://salehahr.github.io/zettelkasten/unlisted/2022-02-08/</link><pubDate>Thu, 03 Feb 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/2022-02-08/</guid><description>Agenda Visualisation of the dataset for EdgeNN As discussed on Fri, train EdgeNN without the paths as labels for now Refactored edge_extraction function in graph-training repo &amp;ndash; it also detects edges of length one, removes them, slates the corresponding nodes for removal Comparison
Original Pixels to remove To do: new graph extraction?</description></item><item><title>2022-01-25</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-01/2022-01-25/</link><pubDate>Tue, 18 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-01/2022-01-25/</guid><description>Agenda For next week: unit tests demo To-do (from last week 2022-01-18 ) 3rd network: Batching method Adj_vec method Programmatic implementation of early stopping Hyperparameter optimisation &amp;ndash; wanb or similar Get prediction times on model deployment Outcomes New edge attributes (extracted using two local coordinates) &amp;ndash;&amp;gt; resulting edge attribute matrix isn&amp;rsquo;t symmetric anymore.</description></item><item><title>2022-02-02</title><link>https://salehahr.github.io/zettelkasten/unlisted/2022-02-02/</link><pubDate>Tue, 18 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/2022-02-02/</guid><description>Agenda Modified DataGenerator &amp;ndash; makes use of tf.data.Dataset.map functionality Implemented data augmentation for adjacency matrix in graph execution mode
Checks show that it&amp;rsquo;s not correct yet. Done ! Currently working on EdgeExtractionDG to produce data for training EdgeNN Unit tests demo test_file_functions https://docs.python.org/3/library/unittest.html Aufbau: test cases Asserts test_graph CI, ML ops branch with failed test https://mattsegal.dev/devops-academic-research.html Dev Ops: from SW dev &amp;ndash; for shortening development times DVC channel Intro to CI for ML Outcomes For training EdgeNN &amp;ndash; two options: Only output 0 and 1, path extraction in a separate network Output both adjacency and path simultaneously; the idea is that the path extraction helps with the backpropagation Next week: incorporate new graph extraction method and regenerate graph objects [Fri 4.</description></item><item><title>2022-01-18</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-01/2022-01-18/</link><pubDate>Fri, 14 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-01/2022-01-18/</guid><description>Agenda Progress to do: data augmentation for $A$ data pipeline Unit tests demo &amp;ndash;&amp;gt; 2022-02-02 To-do (from last week 2022-01-11 ) Modify U-Net model to be more general 3rd network: Batching method Adj_vec method Programmatic implementation of early stopping Hyperparameter optimisation &amp;ndash; wanb or similar Get prediction times on model deployment</description></item><item><title>2022-01-11</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-01/2022-01-11/</link><pubDate>Tue, 04 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-01/2022-01-11/</guid><description>Agenda Node Extraction NN Unit tests demo &amp;ndash;&amp;gt; next week W&amp;amp;B demo Binary Termine naechste Woche To-do (from last week 2022-01-04 ) Modify U-Net model to be more general 3rd network: NLP zero padding tutorial with RaggedTensor Programmatic implementation of early stopping Hyperparameter optimisation &amp;ndash; wanb or similar Get prediction times on model deployment Notes Cross-validation is not implemented $A$ &amp;ndash;&amp;gt; binary number Network: skel &amp;ndash;&amp;gt; decimal number &amp;ndash;&amp;gt; $A$ Length of binary number:</description></item><item><title>2021-12-14-notes</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-12/2021-12-14-notes/</link><pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-12/2021-12-14-notes/</guid><description>Some test runs with no pretrained weights First run: starts with quite a low loss and high accuracy. Probably due to the zeros of the matrices being matched?
Problem with rotating as a data augmentation method &amp;ndash;&amp;gt; only do flips
Problem with degrees &amp;ndash;&amp;gt; fixed!</description></item><item><title>2022-01-04</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-01/2022-01-04/</link><pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-01/2022-01-04/</guid><description>Notes MA Data Filter parameters for individual videos Preview of processed videos Number of data Total for training + validation: 4416 (real) + 4487 (synth) 1 2 ls */[!t]*/graphs/* | wc -l ls [!t]*/graphs/* | wc -l For testing: 488 (real) + 0 (synth) 1 2 ls **/test_*/graphs/* | wc -l ls test_*/graphs/* | wc -l 1 NN — next steps First network</description></item><item><title>Arrays in C</title><link>https://salehahr.github.io/zettelkasten/embedded/arrays-in-c/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/arrays-in-c/</guid><description>Source: samek-embedded Group of variables of the same type. The variables occupy consecutive memory locations. An array is treated as a pointer (to the beginning of the array). Likewise, every pointer can also be viewed as an array. 1 2 3 4 int counter[2] = {0, 0}; counter[0] = 1; counter[1] = 2; Indexing the array is equivalent to adding the index to the array pointer and getting the value at that address.</description></item><item><title>branch-instructions</title><link>https://salehahr.github.io/zettelkasten/embedded/branch-instructions/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/branch-instructions/</guid><description>Source: samek-embedded Branch instructions B.N (Branch) instruction modifies the PC register so that it skips to a different instruction BLT.N conditional branching only modifies PC if the N bit in the APSR is set. the instruction to jump to is encoded within the instruction itself: 0xFC = -4, so jump back 4 instructions, from 0x8e to 0x8a Branching results in pipeline delays &amp;ndash;&amp;gt; solution e.g. loop unrolling BL (branch and link) BL saves the address of the next instruction into the LR link register .</description></item><item><title>heap</title><link>https://salehahr.github.io/zettelkasten/embedded/heap/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/heap/</guid><description>Source: samek-embedded Region of RAM for dynamic memory allocation using malloc() and free(). Not typically used in real time embedded programming.</description></item><item><title>interrupts</title><link>https://salehahr.github.io/zettelkasten/embedded/interrupts/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/interrupts/</guid><description>Source: samek-embedded Allows processor to abruptly change flow of control Upon interrupting: A hardware in the processor changes the value of the PC (program counter) register The interrupt service routine (ISR) is executed After the ISR ends, the processor resumes execution of the original code</description></item><item><title>stack</title><link>https://salehahr.github.io/zettelkasten/embedded/stack/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/stack/</guid><description>Source: samek-embedded Stack Area of RAM that can grow or shrink from one end. Analogous to a stack of dishes: new data/dishes can only be added to the top, and data/dishes can only be taken away from the top. In ARM , the stack grows towards the lower addresses. Pointed to by the stack pointer . Initial values of the stack are random, therefore it is important, in function calls, to initialise variables correctly.</description></item><item><title>clock gating</title><link>https://salehahr.github.io/zettelkasten/embedded/clock-gating/</link><pubDate>Fri, 31 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/clock-gating/</guid><description>Source: samek-embedded Clock gating Blocks the clock signal to certain parts of the chip Saves power</description></item><item><title>led-example</title><link>https://salehahr.github.io/zettelkasten/embedded/led-example/</link><pubDate>Fri, 31 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/led-example/</guid><description>Source: samek-embedded TM4C123G LED Example LED connection The LEDs are connected to the GPIO port F pins of the microcontroller. To use the LEDs, the values of the GPIO port F need to be set &amp;ndash;&amp;gt; address of GPIO PF needed. Bit-wise setting of the LED pins LED Pin number Binary Hex Red 1 0010 0x2 Blue 2 0100 0x4 Green 3 1000 0x8 All 1, 2, 3 1110 0xE Using bit shift operations , the first, second and third bits can be defined as</description></item><item><title>preprocessor-macros</title><link>https://salehahr.github.io/zettelkasten/embedded/preprocessor-macros/</link><pubDate>Fri, 31 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/preprocessor-macros/</guid><description>Source: samek-embedded Manual definition of macros 1 2 3 4 5 6 7 8 9 10 11 12 #define RCGCGPIO *((unsigned int *)0x400FE608U) #define GPIOF_BASE 0x40025000u #define GPIOF_DIR (*((unsigned int *)( GPIOF_BASE + 0x400U))) #define GPIOF_DEN (*((unsigned int *)( GPIOF_BASE + 0x51CU))) int main() { RCGCGPIO |= 0x20U; GPIODIR |= 0xEU; GPIOF_DEN |= 0xEU; ... } Notes:
Macros can also be partial statements Calculations within the macros don&amp;rsquo;t lead to runtime overheads Header files 1 2 3 4 5 6 #include &amp;lt;stdint.</description></item><item><title>volatile</title><link>https://salehahr.github.io/zettelkasten/embedded/volatile/</link><pubDate>Fri, 31 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/volatile/</guid><description>Source: samek-embedded The volatile keyword indicates that the variable might change spontaneously. For example: variables which depend on user input and are not affected by program instructions It can be useful to define a variable to be volatile when high levels of optimisations are performed by the compiler, e.g. for counter variables which are not used outside of loops.</description></item><item><title>Bin-hex-dec table</title><link>https://salehahr.github.io/zettelkasten/embedded/bin-hex-dec-table/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/bin-hex-dec-table/</guid><description>See also: bits-and-bytes Bin Hex Dec Notes 0000 0x0 0 0001 0x1 1 0010 0x2 2 0011 0x3 3 0100 0x4 4 0101 0x5 5 0110 0x6 6 0111 0x7 7 1000 0x8 8 1001 0x9 9 1010 0xA 10 even 1011 0xB 11 1100 0xC 12 even 1101 0xD 13 1110 0xE 14 even 1111 0xF 15 Note</description></item><item><title>bit-logic</title><link>https://salehahr.github.io/zettelkasten/embedded/bit-logic/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/bit-logic/</guid><description>Source: samek-embedded Logic Description Notes (num &amp;amp; 1) != 0 Odd number Tests the least significant bit (num &amp;amp; 1) == 0 Even number a | b Bit-wise OR a &amp;amp; b Bit-wise AND a ^ b Exclusive OR ~b NOT Bit shift operations Unsigned Uses logical bit shifting.</description></item><item><title>bits-and-bytes</title><link>https://salehahr.github.io/zettelkasten/embedded/bits-and-bytes/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/bits-and-bytes/</guid><description>See also: bin-hex-dec-table Example (bin) Bit Byte Hex Example (hex) Note 0 1 0x0 01 2 0x1 0101 4 0x5 Nibble (half byte) 1111 0xF 0101 0101 8 1 0x55 Half word 1111 1111 0xFF 0011 0101 0011 0101 16 2 1 0x3535 Word 32 4 2 0xDEAD&amp;rsquo;BEEF Note</description></item><item><title>embedded</title><link>https://salehahr.github.io/zettelkasten/embedded/__index/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/__index/</guid><description>Math Bits and bytes Bin-hex-dec table Bit logic Integer overflow Basics Microcontroller Instructions Registers and memory RISC and CISC architectures Clock gating Interrupts Programming General considerations for non-linear control flow Preprocessor macros Stack Variables Local vs.</description></item><item><title>ese-101</title><link>https://salehahr.github.io/zettelkasten/bibliography/ese-101/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/ese-101/</guid><description>Embedded Software Engineering 101 by embedded.fm Microcontroller basics</description></item><item><title>instructions</title><link>https://salehahr.github.io/zettelkasten/embedded/instructions/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/instructions/</guid><description> Instructions are e.g. commands such as STORE, ADD, SUBTRACT, branch instructions . The instructions are stored in memory addresses. The values stored at the corresponding addresses encode the instruction to be carried out.</description></item><item><title>integer-overflow</title><link>https://salehahr.github.io/zettelkasten/embedded/integer-overflow/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/integer-overflow/</guid><description>Source: samek-embedded 8-bit representation Hex Dec Bin Hex Dec Bin 0xFF -1 1111 1111 0x01 1 0000 0001 0xFE -2 1111 1110 0x02 2 0000 0010 0xFD -3 1111 1101 0x03 3 0000 0011 &amp;hellip; 0x81 -127 1000 0001 0x7F 127 0111 1111 0x80 -128 1000 0000 Getting the negative hex representation:</description></item><item><title>local-vs-nonlocal-variables</title><link>https://salehahr.github.io/zettelkasten/embedded/local-vs-nonlocal-variables/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/local-vs-nonlocal-variables/</guid><description>Source: samek-embedded Local variables are stored in the register , while non-local variables can be found in memory .
Local variable 1 2 3 4 5 int main() { int counter = 0; ... } Note:
All local variables on the stack go out of scope (get located outside of stack when the stack shrinks) when the function returns, so they can&amp;rsquo;t be accessed. Therefore, it is a bad idea to return a local variable.</description></item><item><title>microcontroller</title><link>https://salehahr.github.io/zettelkasten/embedded/microcontroller/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/microcontroller/</guid><description>Source: ese-101 Constituents of a microcontroller Instructions Registers Memory</description></item><item><title>nonlinear-control-flow</title><link>https://salehahr.github.io/zettelkasten/embedded/nonlinear-control-flow/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/nonlinear-control-flow/</guid><description>Source: samek-embedded General considerations for non-linear control flow Mostly relevant for time-critical code, such as interrupt processing Loop unrolling can be performed to reduce the nonlinearity of the control flow and thus reduce loop overhead Loop overhead Additional instructions (comparisons, branching) The jumps/branching introduces execution delaysx The instruction pipeline Each row is an instruction In the pipeline, the processor works on multiple instructions at a given clock cycle &amp;ndash;&amp;gt; increased throughput In case of branching, already partially executed instructions need to be discarded e.</description></item><item><title>pointers</title><link>https://salehahr.github.io/zettelkasten/embedded/pointers/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/pointers/</guid><description>Source: samek-embedded Pointers Variables which hold memory addresses. Can also be viewed as arrays . Declaration 1 int *p_int; Assignment Address/pointer assignment
1 2 p_int = &amp;amp;another_variable; p_int = (unsigned int *)0x20000000U; Dereferencing Value assignment
1 *p_int = 0xDEADBEEFU; Shortcut Skips declaration, goes directly to value assignment.
1 *((unsigned int *)0x20000000U) = 0xDEADBEEFU;</description></item><item><title>program-with-loop</title><link>https://salehahr.github.io/zettelkasten/embedded/program-with-loop/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/program-with-loop/</guid><description>Source: samek-embedded See also: General considerations for non-linear control flow Code and instructions Code 1 2 3 4 5 6 7 8 9 10 int main() { int counter = 0; while (counter &amp;lt; 10) { ++counter; } return 0; } Disassembly Comparison Original structure Optimised structure In the compiled code, the execution of the while loop is different than that given by the code.</description></item><item><title>registers-memory</title><link>https://salehahr.github.io/zettelkasten/embedded/registers-memory/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/registers-memory/</guid><description>See also: Local vs non-local variables Register Memory Contains [local] data currently being processed Contains program instructions and data that the whole program requires for execution (data that must survive function calls) Small amount of data Larger amount of data Memory locations that are directly accessible by CPU RAM (primary memory, internal to CPU) and hard drive (secondary memory, external to CPU) Faster access by CPU (one CPU clock cycle) Slower access by CPU Source: samek-embedded Registers Some special registers are described in the following.</description></item><item><title>risc-cisc</title><link>https://salehahr.github.io/zettelkasten/embedded/risc-cisc/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/risc-cisc/</guid><description>Source: samek-embedded Computer architectures or more specifically: instruction set architectures (ISA)
Reduced Instruction Set Computer (RISC) architecture Memory can only be read and stored by using special load/store instructions (LDR.N, STR.N) All data manipulations occur in the registers e.g. ARM Complex Instruction Set Computer (RISC) architecture Operands of the current instruction may be stored in memory e.g. x86</description></item><item><title>samek-embedded</title><link>https://salehahr.github.io/zettelkasten/bibliography/samek-embedded/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/samek-embedded/</guid><description>Modern Embedded Systems Programming Course by Miro Samek
Math Bit logic Integer overflow Basics Registers and memory RISC and CISC architectures Clock gating Interrupts Programming General considerations for non-linear control flow Preprocessor macros Variables Local vs. non-local variables Pointers Arrays in C Volatile Sample programs Using TM4C123G.</description></item><item><title>simple-program</title><link>https://salehahr.github.io/zettelkasten/embedded/simple-program/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/embedded/simple-program/</guid><description>Source: samek-embedded 1 2 3 4 5 6 int main() { int counter = 0; ++counter; ++counter; return 0; } Machine code Instructions Code Instruction Description int counter = 0 MOVS R0, #0 Move value 0 to register R0 ++counter; ADDS R0, R0, #1 Add value 1 to register R0, store it in R0 Registers and memory Register Memory</description></item><item><title>2021-12-14</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-12/2021-12-14/</link><pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-12/2021-12-14/</guid><description>Agenda MA Besprochen letzte Woche: scrap rotations as a data augmentation method modify network architecture skel &amp;ndash;&amp;gt; node attributes, e.g. fewer convolution filters filters between UNet final layer and the degrees and node types outputs architecture that prioritises degrees and node types (because the node positions can be inferred from both) for raw image &amp;ndash;&amp;gt; filt/skel network, use pretrained weights for medical image processing tasks, modifications after the final unet layer First run of model 20211210-102252\train with 64 filters Second run with reduced num.</description></item><item><title>2021-12-07</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-12/2021-12-07/</link><pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-12/2021-12-07/</guid><description>Agenda MA Model now trains till end of epoch with model.fit().
Fix: use dataset.as_numpy_iterator().tolist() instead of dataset Current problem: losses are NaN, resulting predicted matrices are NaN fixed Still looking into it, but according to this thread , the cross entropy loss might be [one of] the cause[s]. https://stackoverflow.com/questions/40192728/cross-entropy-is-nan fix: use softmax activation HiWi Kugel ist fertig
GUI mit Beispielberechnung im separaten Thread ist fertig</description></item><item><title>2021-11-30</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-30/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-30/</guid><description>Agenda HiWi New: light position = camera centre MA Currently: must debug DataGenerator: training stops 1-3 batches before end! To do (from last week) 2021-11-23 HiWi Make light follow camera rotation Enable camera movement parallel to an ongoing rendering calculation (s. example with the sphere to cow rendering) Weihnachtskugel malen MA Train model skeletonised -&amp;gt; node attributes using existing data Debug DataGenerator Preliminaries Data augmentation - debug Data generation Watch more videos and trim away bad sections Adjust filter parameters so that more structures are visible Training data generation up to 3k w/o data augmentation Outcomes Possible outcomes for later: Comparing brute force method (input-output method) to function-/theory-based method (evtl mit PyTorch Funktionen, da muessen die Funktionen ableitbar sein) Try ReLU activation instead of sigmoid Notes World and camera coordinate systems in PyTorch3D Blasenmuster Fake Real</description></item><item><title>Backpropagation</title><link>https://salehahr.github.io/zettelkasten/ma/backpropagation/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/backpropagation/</guid><description>Source: google-ml-course Backpropagation Gradient descent for neural networks Idea Gradience/the need for differentiable functions: things need to be differentiable in order for learning to occur
Problems Vanishing gradients In too deep networks, the signal to noise ratio gets worse further in the model Thus the gradients for the initial layers can approach zero Learning becomes slow Strategies: Limit model depth Use ReLUs Exploding gradients Especially if learning rates are too high, weights too large &amp;ndash;&amp;gt; NaNs in model Gradients in initial layers explode, become too large to converge Strategies: Lower learning rate Batch normalisation ReLU layers can &amp;lsquo;die&amp;rsquo; Due to the cap at zero If values end up being below zero then the gradients can&amp;rsquo;t get backpropagated Strategies: Different initialisation Lower learning rate Tricks Scaling/normalising features If features are roughly on the same scale, this can make convergence faster Dropout</description></item><item><title>bias</title><link>https://salehahr.github.io/zettelkasten/ma/bias/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/bias/</guid><description>Parent: classification Source: google-ml-course Bias Unbiased: average of predictions is equal to the average of observations Zero bias does not mean everything is ok (e.g. could come from a model that was training on too little data) but can be a useful sanity check If there is bias in the model, there is a problem e.g. biased training sample?</description></item><item><title>classification</title><link>https://salehahr.github.io/zettelkasten/ma/classification/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/classification/</guid><description>Source: google-ml-course Classification Logistic regression returns a probability This probability can be converted to a binary value by making use of a classification threshold Classification/decision threshold: the value which splits between true and false
s. confusion matrix , which contains information used for the performance metrics Evaluation metrics One possibility: accuracy (fraction of correct predictions over total predictions) $\dfrac{\text{TP} + \text{TN}}{\text{all predictions}}$ Poor metric, especially when working with datasets with class imbalance (significant difference between number of positive and negative labels) e.</description></item><item><title>confusion-matrix</title><link>https://salehahr.github.io/zettelkasten/ma/confusion-matrix/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/confusion-matrix/</guid><description>Source: google-ml-course Confusion matrix $N \times N$ matrix which shows how successful the classification model&amp;rsquo;s predictions were. $N$: number of classes, e.g. $N=2$ for true/false classes Axes: predicted class/label, actual class/label Predicted True Predicted False Actual True TP FN Actual False FP TN In multi-class classification, the confusion matrix can help to identify mistake patterns—does the model tend to mistakenly predict a certain class for another?</description></item><item><title>data-in-ml</title><link>https://salehahr.github.io/zettelkasten/ma/data-in-ml/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/data-in-ml/</guid><description>Source: google-ml-course Data in ML Sampling data Batching data Data representation One-hot encoding Scaling features Binning Feature crossing Choosing labels and features Use quantifiable, observable data.</description></item><item><title>dropout</title><link>https://salehahr.github.io/zettelkasten/ma/dropout/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/dropout/</guid><description>Parent: regularisation Source: google-ml-course Dropout A form of regularisation for neural networks Randomly not consider a node for a single gradient step Greater dropout &amp;ndash;&amp;gt; stronger regularisation</description></item><item><title>embedding-dimensions</title><link>https://salehahr.github.io/zettelkasten/ma/embedding-dimensions/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/embedding-dimensions/</guid><description>Parent: hyperparameters Source: google-ml-course Embedding dimensions Good starting value: $$D \approx \sqrt[4]{\text{total values}}$$
Higher dimensions
allow more accurate representation of the relationships between the input data can lead to overfitting, slower training</description></item><item><title>embeddings</title><link>https://salehahr.github.io/zettelkasten/ma/embeddings/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/embeddings/</guid><description>Source: google-ml-course Embeddings Translate a high dimensional vector into a lower dimensional space (embedding) e.g. representing words by sparse vectors Ideally, similar inputs are grouped close together in the embedding space (similarity metric) Learning embeddings from data Embedding layer: a hidden layer with $D$ nodes, where $D$ is the dimension of the embedding space Each of these dimensions is called a latent dimension due to it being inferred from the data This hidden layer learns how to cluster the data to optimise the metric we have chosen Weights between the the input and the embedding layer are the coordinate values of the input within the embedding space Hyperparameter: how many embedding dimensions $D$?</description></item><item><title>feature-crossing</title><link>https://salehahr.github.io/zettelkasten/ma/feature-crossing/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/feature-crossing/</guid><description>Parent: data-representation Source: google-ml-course Synthetic features (feature crossing) e.g. generate feature $x_3$ by combining $x_1$ and $x_2$
$$x_3 = x_1 x_2$$ crossing boolean features can result in a very sparse feature set a more sophisticated version of feature crossing is a neural network Advantage Enables learning with nonlinear features while making use of a linear model
&amp;ndash;&amp;gt; nonlinear features scale well with large scale data sets
Disadvantage Crossing sparse features may significantly increase the size of the feature space.</description></item><item><title>hyperparameters</title><link>https://salehahr.github.io/zettelkasten/ma/hyperparameters/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/hyperparameters/</guid><description>Source: Wikipedia Parameters to control the process of learning the model, as opposed to being parameters of the model, which are to be learned.
Subsets:
model hyperparameters (for model selection), e.g. size of the NN topology of the NN algorithm hyperparameters (affect the efficiency of the learning process), e.g. learning rate batch size Source: google-ml-course Learning rate $\alpha$ Regularisation rate $\lambda$ Embedding dimensions</description></item><item><title>l0-regularisation</title><link>https://salehahr.github.io/zettelkasten/ma/l0-regularisation/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/l0-regularisation/</guid><description>Parent: regularisation Source: google-ml-course L$_0$ regularisation Explicitly set some weights to zero Regularisation term: only $\mathbf{w}$ Disadvantages This would penalise non-zero weights Non-convex optimisation Solution &amp;ndash;&amp;gt; relax this to an L$_1$ regularisation</description></item><item><title>l1-regularisation</title><link>https://salehahr.github.io/zettelkasten/ma/l1-regularisation/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/l1-regularisation/</guid><description>Parent: regularisation Source: google-ml-course L$_1$ regularisation Relaxed version of an L$_0$ regularisation Penalises the sum of the absolute values of the weights Regularisation term: $$\left\lvert \mathbf{w} \right\rvert = \left| |w_1| + \dots + |w_n| \right|$$ Convex problem Encourages a sparse model: model will want to drive $w$ to zero c.f. L$_2$ regularisation , which aims to make the weights smaller</description></item><item><title>l2-regularisation</title><link>https://salehahr.github.io/zettelkasten/ma/l2-regularisation/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/l2-regularisation/</guid><description>Parent: regularisation Source: google-ml-course L$_2$ regularisation / Ridge regularisation Model complexity is given by the sum of the squares of the weights1
$$ \begin{align} &amp;amp;\min L(x, y, \text{model}) + \lambda \left\lVert \mathbf{w} \right\rVert_2^2 \end{align} $$
with the L$_2$ regularisation term $$ \left\lVert \mathbf{w} \right\rVert_2^2 = \left( w_1^2 + \dots + w_n^2 \right) $$
and the regularisation rate $\lambda$ determines whether the total loss is more dependent on the training loss or on the model complexity.</description></item><item><title>logistic-regression</title><link>https://salehahr.github.io/zettelkasten/ma/logistic-regression/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/logistic-regression/</guid><description>See also: Linear regression Source: google-ml-course Linear logistic regression For predicting probabilities (value between 0 and 1) As an alternative, it could be possible to cap the predicted data at 1, but this would be introducing bias to the model Therefore the necessity for a different loss function and data prediction model, which outputs the predicted probabilities Sigmoid function Gives a bounded value between zero and one. $$ y = \dfrac{1}{1 + e^{-z}} $$</description></item><item><title>multi-class-classification</title><link>https://salehahr.github.io/zettelkasten/ma/multi-class-classification/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/multi-class-classification/</guid><description>Source: google-ml-course Multi-class classification Building off on binary classification &amp;ndash;&amp;gt; do one-vs-all classification, e.g. for the classes red-blue-yellow: Red vs not red Blue vs not blue Yellow vs not yellow For unique classes: SoftMax can be used, where the sum of the outputs is one For non-unique classes: one-vs-all classification with multiple logistic regressions , the sum of the outputs need not be one</description></item><item><title>neural-networks</title><link>https://salehahr.github.io/zettelkasten/ma/neural-networks/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/neural-networks/</guid><description>Source: google-ml-course Neural networks Goal: we want the model to learn the nonlinear features itself without us having to manually define the nonlinear features More sophisticated version of feature crosses Automatic learning of the feature crosses Linear model Each input (feature) is assigned a weight and the weight combination of all these features result in the output.
Incorporate nonlinearity Add another layer of features?
not linear yet linear combination of linear functions is still linear solution: do a nonlinear transform between layers (activation function) Activation function Sigmoid function $$ F(x) = \dfrac{1}{1 + e^{-x}} $$</description></item><item><title>overfitting</title><link>https://salehahr.github.io/zettelkasten/ma/overfitting/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/overfitting/</guid><description>Parent: generalisation Source: google-ml-course Overfitting Model is overly specific
is only usable to predict a subset of input data (the training data that was used to train the model) model would have been fitted to the specific properties of the dataset used, e.g. if the dataset was noisy, and overfitted model takes into account the noise of this particular dataset not usable on new data &amp;ndash;&amp;gt; not generalisable</description></item><item><title>precision-and-recall</title><link>https://salehahr.github.io/zettelkasten/ma/precision-and-recall/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/precision-and-recall/</guid><description>Parent: classification Source: google-ml-course Precision and Recall Precision true positives / all positive predictions
$$\dfrac{\text{TP}}{\text{TP} + \text{FP}}$$
&amp;ldquo;What fraction of positive predictions were actually correct?&amp;rdquo; — focus on the correctness of the positive predictions or: &amp;ldquo;Did the model predict positive too often?&amp;rdquo; e.g. precision = 0.5, model is correct 50% of the time Recall / True Positive Rate (TPR) true positives / all actual positives
$$\dfrac{\text{TP}}{\text{TP} + \text{FN}}$$</description></item><item><title>pytorch3d-cs</title><link>https://salehahr.github.io/zettelkasten/ma/pytorch3d-cs/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/pytorch3d-cs/</guid><description>Coordinate systems in PyTorch3D To get camera coordinates in world CS:
Camera.get_camera_center()
Reference
https://pytorch3d.org/docs/cameras</description></item><item><title>regularisation-rate</title><link>https://salehahr.github.io/zettelkasten/ma/regularisation-rate/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/regularisation-rate/</guid><description>Parent: Hyperparameters Source: google-ml-course Regularisation rate $\lambda$ If $\lambda$ too high: simpler model, risk of underfitting (not enough training being done) If $\lambda$ too low: model more complex, risk of overfitting Ideal $\lambda$ depends on data &amp;ndash;&amp;gt; needs to be tuned Strong L$_2$ regularisation has similar effect to that of lower learning rate (smaller step size) High regularisation drives weights towards zero Lower learning rates result in lower step sizes, and the steps towards zero (in parameter space) are smaller than steps away Therefore tuning $\alpha$ and $\lambda$ simultaneously could be a bit confusing Ensure that there are a high enough number of iterations (so that effect of early stopping doesn&amp;rsquo;t affect the tuning of $\lambda$)</description></item><item><title>simple-regression</title><link>https://salehahr.github.io/zettelkasten/ma/simple-regression/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/simple-regression/</guid><description>Source: google-ml-course Simple linear regression model $$ \begin{align} f(\mathbf{x}) = y' &amp;amp;= b + w_1 x_1 + \dots + w_n x_n\
&amp;amp;= w_0 + w_1 x_1 + \dots + w_n x_n \end{align} $$</description></item><item><title>SoftMax</title><link>https://salehahr.github.io/zettelkasten/ma/softmax/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/softmax/</guid><description>Source: google-ml-course SoftMax A more general version of logistic regression For multi-class classification with unique labels Additionally constraints all output nodes to sum up to 1.0 Helps convergence Probabilistic interpretation of outputs Options Full SoftMax (brute force): a probability is calculated for every single class Cheap for small number of classes Candidate sampling: calculates probability for all positive labels but only for a random sample of negative labels e.</description></item><item><title>batch</title><link>https://salehahr.github.io/zettelkasten/definitions/batch/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/definitions/batch/</guid><description>See also: batching-in-ml ]
Source: google-ml-course Batch Number of data points used to calculate the gradient in a single iteration</description></item><item><title>batching-in-ml</title><link>https://salehahr.github.io/zettelkasten/ma/batching-in-ml/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/batching-in-ml/</guid><description>Parent: Data in ML See also: Batch Source: google-ml-course Large batches of data
result in a long computation time of a single iteration possibly contain redundant data Hence, this results in variations of the classic gradient descent , which performs calculation over the whole dataset per iteration.</description></item><item><title>binning</title><link>https://salehahr.github.io/zettelkasten/ma/binning/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/binning/</guid><description>Parent: data-representation Source: google-ml-course Binning using histograms &amp;ndash;&amp;gt; then use one-hot encoding ; this is a cheap way of mapping nonlinear data into the model</description></item><item><title>convex-problems</title><link>https://salehahr.github.io/zettelkasten/ma/convex-problems/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/convex-problems/</guid><description>Source: google-ml-course A convex function is shaped like this with respect to its parameters:
Convex problems only have one minimum (where the gradient is zero) &amp;ndash;&amp;gt; global minimum.</description></item><item><title>data-representation</title><link>https://salehahr.github.io/zettelkasten/ma/data-representation/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/data-representation/</guid><description>Parent: Data in ML Source: google-ml-course Good ML relies on good data.
Data representation Feature engineering: extracting features from raw data Features: data that is readable or workable on by the ML algorithm Good features Should appear with non-zero values sufficiently often enough within the dataset (i.e. is actually useful enough to classify a lot of data points in the dataset and not just overly specific to sevveral data points only) Have clear and obvious meaning (e.</description></item><item><title>generalisation</title><link>https://salehahr.github.io/zettelkasten/ma/generalisation/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/generalisation/</guid><description>Source: google-ml-course Generalisation Balancing a model between underfitting and overfitting.
Underfitting Model is not specific enough; fails to accurately predict outputs.
Overfitting Model is overly specific, s. overfitting .
Strategies to improve generalisation, avoid overfitting Split the dataset strategically Regularisation — penalise model complexity</description></item><item><title>Google ML Course</title><link>https://salehahr.github.io/zettelkasten/bibliography/google-ml-course/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/google-ml-course/</guid><description>Link: https://developers.google.com/machine-learning/crash-course/
Main topics What is ML ML terminology Hyperparameters Data in ML Models Linear regression Linear logistic regression Classification Multi-class classification SoftMax Metrics Bias Losses Precision and recall Reducing loss Batching in ML Redundancy Variations of gradient descent Backpropagation Generalisation Overfitting Splitting the dataset Regularisation Neural networks Embeddings Definitions Convex problems Batch</description></item><item><title>gradient-descent</title><link>https://salehahr.github.io/zettelkasten/ma/gradient-descent/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/gradient-descent/</guid><description>Parent: Variations of gradient descent Source: google-ml-course Gradient descent An iterated approach
Labeled data arrives Gradient of the loss function is computed Now the direction for updating the model parameters $\mathbf{w}$ is known (negative gradient). A step is taken in this direction, in the parameter space. The step size is equivalent to the learning rate . Repeat This process tunes all model parameters simultaneously.
Notes Works well for convex problems (loss function w.</description></item><item><title>learning-rate</title><link>https://salehahr.github.io/zettelkasten/ma/learning-rate/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/learning-rate/</guid><description>Parent: Hyperparameters Source: google-ml-course Hyperparameter &amp;lsquo;learning rate&amp;rsquo; $\alpha$ Small learning rate: small steps, long time to reach the minimum Big learning rate: big steps, shorter computation time but with potential to overshoot the minimum Ideal learning rate In 1D: the inverse of the second derivative of the model $$ \begin{align} \alpha = \frac{1}{f(x)''} \end{align} $$
In $\geq$ 2D: inverse of the Hessian</description></item><item><title>losses</title><link>https://salehahr.github.io/zettelkasten/ma/losses/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/losses/</guid><description>Source: google-ml-course Losses Losses according to dataset Training loss Validation loss — this is the one that matters, as this is the measure of the model&amp;rsquo;s ability to predict on new and unseen data Losses according to definition Mean square error Average square loss of the whole dataset. The greater the prediction disparity, the higher the penalisation (squared amplification)
e.g. a disparity of 2 units results in a loss 4x bigger than a disparity of 1 unit Single datapoint L$_2$ loss, squared error $$\left(\mathbf{y} - \mathbf{f}(\mathbf{x})\right)^\text{T} \left(\mathbf{y} - \mathbf{f}(\mathbf{x})\right)$$</description></item><item><title>machine-learning</title><link>https://salehahr.github.io/zettelkasten/ma/what-is-machine-learning/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/what-is-machine-learning/</guid><description>See also: ml-terminology Source: google-ml-course What is machine learning (Supervised) Machine learning in a nutshell Learn the patterns that exist within a set of input-output data Apply the patterns to a set of new input data, so as to be able to predict the outputs</description></item><item><title>mini-batch-gradient-descent</title><link>https://salehahr.github.io/zettelkasten/ma/mini-batch-gradient-descent/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/mini-batch-gradient-descent/</guid><description>Parent: Variations of gradient descent Source: google-ml-course Mini-Batch Gradient Descent Performs gradient and loss calculation on 10 to 1000 data points Less noise compared to SGD Nevertheless more efficient than full batch gradient descent</description></item><item><title>ml-terminology</title><link>https://salehahr.github.io/zettelkasten/ma/ml-terminology/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/ml-terminology/</guid><description>Source: google-ml-course ML Terminology Term Symbol Description Output, label $\mathbf{y}$ Variable to be predicted Features $\left\lbrace \mathbf{x}_1, \dots, \mathbf{x}_n \right\rbrace$ Representation of the (input) data Model $f: \mathbf{x} \rightarrow \mathbf{y}'$ Mapping of the input to the predictions $\mathbf{y}'$ Weights $\mathbf{w}$ Parameters of the model Term Description Training Process of determining the model parameters (weights) that will allow an accurate mapping of the input to the resulting output (prediction) i.</description></item><item><title>one-hot-encoding</title><link>https://salehahr.github.io/zettelkasten/ma/one-hot-encoding/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/one-hot-encoding/</guid><description>Parent: data-representation Source: google-ml-course One-hot encoding We can assign each class to a unique coefficient, i.e. maps a category to a number. The problem with this (using integer coefficients) These coefficients get bundled in with the ML math and the weights learned are specific to the defined coefficient code.
$$ \text{model} = \sum \text{weight} * \text{feature}$$
The weight for the specified feature vector is multiplied with all values of the feature, in this case the coefficients&amp;hellip; what happens if we add a new class and/or change the coefficient code?</description></item><item><title>reducing-loss-ml</title><link>https://salehahr.github.io/zettelkasten/ma/reducing-loss-ml/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/reducing-loss-ml/</guid><description>Source: google-ml-course Reducing loss in ML Aim: compute model parameters (weights) such that loss is minimal
For which parameters does the loss converge? (Reach a minimum) How to get these optimal model parameters? &amp;ndash;&amp;gt; which direction do we go in the parameter space? One possibility: compute the gradient and steer the model parameters based on the gradient &amp;ndash;&amp;gt; gradient descent , variations of gradient descent In neural networks : backpropagation Gradient: derivative of the loss function with respect to the model parameters $$ \begin{align} \dfrac{\partial L}{\partial \mathbf{w}} \end{align} $$</description></item><item><title>redundancy-in-ml</title><link>https://salehahr.github.io/zettelkasten/ma/redundancy-in-ml/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/redundancy-in-ml/</guid><description>See also: Batching in ML Source: google-ml-course Redundancy in ML Redundancy tends to increase with batch size Some redundancy is useful to reduce noise in gradient calculations</description></item><item><title>regularisation</title><link>https://salehahr.github.io/zettelkasten/ma/regularisation/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/regularisation/</guid><description>Parent: Generalisation Source: google-ml-course Regularisation So far: penalisation of wrong predictions [empirical risk minimisation]
$$ \min L(x, y, \text{model})$$ Now: penalise model complexity [structural risk minimisation] to prevent overfitting $$ \min L(x, y, \text{model}) + \text{complexity}(\text{model})$$ Some metrics for model complexity Function of the weights Function of the total number of features with nonzero weights Types Early stopping: stop the training before convergence (while using the training data) L$_0$ regularisation L$_1$ regularisation L$_2$ regularisation Dropout L$_1$ vs.</description></item><item><title>sampling-data</title><link>https://salehahr.github.io/zettelkasten/ma/sampling-data/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/sampling-data/</guid><description>Parent: Data in ML Source: google-ml-course Sampling data in ML Three basic assumptions
Data points are drawn independently and identically (i.i.d.), and at random, from the data distribution, i.e. the data points don&amp;rsquo;t influence each other The data distribution doesn&amp;rsquo;t change over time (stationary) The data is pulled from the same distribution, for both training and validation sets Situations where these assumptions may be violated:
change in user perception, therefore resulting in different labelling of a dataset change in population which result in new demographics or target market</description></item><item><title>scaling-features</title><link>https://salehahr.github.io/zettelkasten/ma/scaling-features/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/scaling-features/</guid><description>Parent: data-representation Source: google-ml-course Scaling/normalising feature values No benefit if only one feature in the feature set Can be beneficial for multifeature set Helps the gradient descent to converge more quickly Helps avoid numbers becoming NaN Makes the model spend the same time learning for each feature vector. If scaling is not done, more learning is done for features which have a bigger range of data. Scaling methods Map [min, max] to [-1, +1] Use Z scores</description></item><item><title>splitting-dataset</title><link>https://salehahr.github.io/zettelkasten/ma/splitting-dataset/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/splitting-dataset/</guid><description>Parent: generalisation Source: google-ml-course Splitting the dataset Strategy 1 A good performance on the training set doesn&amp;rsquo;t necessarily correspond to good performance on another test set The training set must be large enough (to exclude possibility of underfitting) The training set isn&amp;rsquo;t reused for validation (to exclude possibility of overfitting ) Thus, the idea is, if we only have one large data set, to split it into subsets: The training set is used to learn the model weights.</description></item><item><title>stochastic-gradient-descent</title><link>https://salehahr.github.io/zettelkasten/ma/stochastic-gradient-descent/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/stochastic-gradient-descent/</guid><description>Parent: Variations of gradient descent Source: google-ml-course Stochastic Gradient Descent (SGD) Performs gradient calculation on only a single data point Gradient is very noisy &amp;lsquo;Stochastic&amp;rsquo;: random data point is chosen at every iteration</description></item><item><title>variations-gradient-descent</title><link>https://salehahr.github.io/zettelkasten/ma/variations-gradient-descent/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/variations-gradient-descent/</guid><description>See also: Batching Source: google-ml-course Variations of gradient descent The two extremes: Gradient descent Batch = whole dataset Stochastic Gradient Descent (SGD) Batch = one data point The compromise: Mini-Batch Gradient Descent For neural networks : backpropagation</description></item><item><title>differentiable-rendering</title><link>https://salehahr.github.io/zettelkasten/ma/differentiable-rendering/</link><pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/differentiable-rendering/</guid><description>Source: https://pytorch3d.org/docs/renderer
&amp;ldquo;Differentiable rendering is a relatively new and exciting research area in computer vision, bridging the gap between 2D and 3D by allowing 2D image pixels to be related back to 3D properties of a scene.&amp;rdquo;</description></item><item><title>2021-11-16</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-16-static/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-16-static/</guid><description>See also: 2021-11-16-outcomes Agenda Notes MA Scrapped translations as a data augmentation option Implemented function to sort nodes (left to right, up to down) before graph creation, thus ensuring that the adjacency matrix uses sorted node positions Created test suites which use a random photo from the existing dataset [training data] Testing suite in TestGraph (check if node positions are sorted, check if adj matrix matches photo) [tfgraph] Testing suite TestDataGeneration for testing data augmentation, s.</description></item><item><title>2021-11-23</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-23/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-23/</guid><description>Agenda Notes MA Function for classifying between end nodes and border (invalid) nodes
Checks if node coordinates are in border zone
Border: 2px wide Helper node (green) -&amp;gt; reclassified to crossing node (blue) Up till now, saved border nodes in graph with the attribute NodeType.BORDER = 0 &amp;ndash;&amp;gt; change to another value to avoid mixup with the black pixels! Sample training data Data augmentation not yet working as desired, s.</description></item><item><title>verb-aspects</title><link>https://salehahr.github.io/zettelkasten/lang/verb-aspects/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/lang/verb-aspects/</guid><description>Imperfective aspect Perfective aspect First form Second form Past, present and future Past and future only Action in progress/repeated Completed action Note:
There are irregular forms Future perfectives mostly follow the present imperfective Example 1 Tense Imperfective Perfective Past я смотрел я посмотрель Present я смотрю - Future я буду смотреть я посмотрю Example 2 Imperfective Perfective Action in progress Completed action Он писал роман цедый год.</description></item><item><title>2021-11-16-sample-augmented-data</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-16-sample-augmented-data/</link><pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-16-sample-augmented-data/</guid><description>Parent: 2021-11-16-static Sample input Augmented Input Filtered Skeletonised Node positions</description></item><item><title>2021-11-09</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-09/</link><pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-09/</guid><description>Agenda To-dos Ongoing Training data generation (up to 3k), including adjacency matrix Figure out model checkpoints &amp;ndash; still don&amp;rsquo;t understand Adjust model &amp;ndash; add more output channels Start HiWi (Pytorch 3D + PyQT GUI) by the end of the week New Model summary &amp;ndash; ca. 30M parameters Make new training data for 256px images (two folders: 256px, 512px) Implement data augmentation Rotations, translation, flip ok Schwierig oder nicht gut machbar: blurring, stretching, zoom U-Net DataGenerator, so far without augmentation Based on the class UNet in 03_CNN &amp;ndash; modifications: Branched outputs (to apply different losses/activation functions) Filtered output: linear activation, MSE loss Skeletonised output: sigmoid activation, binary crossentropy loss Alternative: final conv2D layer with 4 filters?</description></item><item><title>2021-11-03</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-03/</link><pubDate>Wed, 03 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-03/</guid><description>To-do Old Generate up to 3k training photos [Hiwi] combine Peter&amp;rsquo;s existing PyQT GUI with PyTorch 3D to enable changing the camera view using mouse read Jupyter notebook tutorial go through PyQT stuff New Resize images to be smaller, squared, a square of two, e.g. 512 x 512 px To avoid unnecessary padding in the model Avoids the need for resizing within the model Note: if do end up using tf.</description></item><item><title>2021-11-02</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-02/</link><pubDate>Tue, 02 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-02/</guid><description>Agenda To clarify Training data Zugang zum Rechner Tuebingen Hiwi Training data Did a parameter study for B-COSFIRE filter parameters Automated some functions: naming convention, trimming according to video_data.py Some training data GRK021, GRK008 Goal: ca. 3,000 photos, apply transformations to these to get more training data ~10,000 Questions Which videos are already &amp;lsquo;done&amp;rsquo;, which are more important, in what order to process? &amp;ndash; doesn&amp;rsquo;t matter which, as long as I know which videos I took them from Do the images have to be squares?</description></item><item><title>2021-10-15</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-10/2021-10-15/</link><pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-10/2021-10-15/</guid><description>2021-10-15 Vorstellung Overall localisation project
Part that deals with localisation and mapping (graph stuff) Part that deals with deformation &amp;ndash; using Pytorch 3D Pytorch 3D 3D ground truth models are expensive Idea: generate 3D models (mesh) from 2D data (graph as a texture) Localisation using graphs Deformation problem: end nodes can appear to change position based on light conditions, bifurcation nodes are not affected Use neural network to extract adjacency matrix in real time One problem: Extraction of adjacency matrix is not robust to changes in matrix dimension, therefore do zero padding (as a partial solution to the problem) Idea First NN to extract node positions, node attributes such as polynomial degree, coefficients, etc number of connections Second NN to extract adjacency matrix between two nodes only (prerequisite: node positions already known, number of connections already known) Threading &amp;ndash; a thread can be terminated once the number of adjacencies has been found corresponding to the number of connections</description></item><item><title>2021-10-26</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-10/2021-10-26/</link><pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-10/2021-10-26/</guid><description>2021-10-26 Kick-off meeting Tasks Read Regine&amp;rsquo;s documentation, SA thesis Generate training data with ground truth (labels) using Regine&amp;rsquo;s code Trim videos to show only relevant sections (where blood vessels are visible) &amp;mdash; (differentiating the situations to be implemented in other work) &amp;mdash; creating function in Python to trim Framework that runs all the functions in one go Notes Meetings Tuesday 10 a.m. Previous work by Regine: not real time &amp;ndash;&amp;gt; achieve real time capability by using Unet Existing code does: Filter colour photo to black and white Skeletonises the photo Extracts node position from the photo Extracts node attributes NN input: 256x256x3 photo NN output: 256x256xn, with n channels Channel 1: filtered photo Channel 2: skeletonised photo Channel 3: node position Channel 4: polynomial degree Maybe skip Channel 1 (filtered photo) and go to output 2 (skeletonished photo) directly Channel 3 (node pos) to go to polynomial degree directly, because once a nonzero number is in the matrix, it indicates that a node exists at that position!</description></item><item><title>ma-minutes</title><link>https://salehahr.github.io/zettelkasten/unlisted/ma-minutes/</link><pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/ma-minutes/</guid><description> 2021-10-15 2021-10-26 2021-11-02 2021-11-03 2021-11-09 2021-11-16-static 2021-11-16-outcomes 2021-11-23 2021-11-30 2021-12-07 2021-12-14 node-detection-model-first-run 2022-01-04 2022-01-11 2022-01-18 2022-01-25 2022-02-02 2022-02-08 2022-02-15</description></item><item><title>Bladder cancer surgery procedure</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/bladder-cancer-surgery/</link><pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/bladder-cancer-surgery/</guid><description>See also: related-types-of-surgery BCS procedure (according to my understanding)
Cystocopy to inspect the bladder If tumour is found, do resection &amp;ndash;&amp;gt; cryosection Cancer detected &amp;ndash;&amp;gt; tumour removal</description></item><item><title>Identity of a group</title><link>https://salehahr.github.io/zettelkasten/rotations/identity-of-a-group/</link><pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/identity-of-a-group/</guid><description>Source: https://vladimirbozovic.net/univerzitet/wp-content/uploads/2010/11/group-theory-chapter.pdf
In every group, there is an element $E$, such that $$EA = AE = A$$ for every member $A$ of the group.</description></item><item><title>Quaternion double cover</title><link>https://salehahr.github.io/zettelkasten/rotations/quaternion-double-cover/</link><pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/quaternion-double-cover/</guid><description>Parent: Quaternion index Source: Solà 2017 $$ \mathbf{q} = \left[ \begin{array}{c} q_w\ \mathbf{q}_v \end{array} \right] = \left[ \begin{array}{c} \cos\frac{\phi}{2} \ \mathbf{u} \sin\frac{\phi}{2} \end{array} \right] $$ where $\phi$ is the angle rotated by $\mathbf{q}$ on objects in the 3D space $\mathbb{R}^3$.
Recap $\theta$ is the angle in quaternion space (s. unit quaternions ) $\phi$ as the angle in 3D space $\mathbb{R}^3$ Therefore, the angle is halved in quaternion space. $$\theta = \phi / 2$$</description></item><item><title>(Phil's Lab) Sensor Fusion series</title><link>https://salehahr.github.io/zettelkasten/bibliography/phils-lab-sensor-fusion/</link><pubDate>Wed, 29 Sep 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/phils-lab-sensor-fusion/</guid><description>Source: https://www.youtube.com/watch?v=RZd6XDx5VXo
Sensor fusion Gyroscope Example: UAV attitude estimation Goal:
To estimate roll and pitch angles of aircraft These angles are needed for feedback in autonomous control of an unmanned UAV We have:
3-axis accelerometer [$\text{m/s}^2$] 3-axis gyroscope [$\text{rad/s}$] Note:
Here, angles are measured in body frame and not in fixed/inertial frame!
Measured quantities:
acceleration
$\mathbf{a}_B = \left[\begin{array}{ccc} a_x &amp;amp; a_y &amp;amp; a_z \end{array}\right]^\text{T}$ roll rates from gyrometer (body) $\neq$ derivative of Euler angles (fixed)</description></item><item><title>Gyroscope/Gyrometer</title><link>https://salehahr.github.io/zettelkasten/sensors/gyroscope/</link><pubDate>Wed, 29 Sep 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/gyroscope/</guid><description>Source: Phil&amp;rsquo;s Lab Gyroscope model $$ \begin{aligned} \mathbf{\omega}B &amp;amp;= \mathbf{\omega}\text{true} + \mathbf{\beta}(t) + \mathbf{\eta}(t) \end{aligned} $$
We need to transform these body rates to Euler rates! $$ \begin{aligned} \left[ \begin{array}{c} \dot{\phi} \ \dot{\theta} \end{array} \right] &amp;amp;= \left[ \begin{array}{ccc} 1 &amp;amp; \sin\phi \tan\theta &amp;amp; \cos\phi\tan\theta\
0 &amp;amp; \cos\phi &amp;amp; -\sin\phi \end{array} \right] \left[ \begin{array}{c}p\q\r\end{array} \right] \end{aligned} $$
Problem: $\phi$ and $\theta$ need to be known!
&amp;ndash;&amp;gt; integrate? $$\hat{\phi} = \int_0^T \dot{\phi}(t) ~dt ~?</description></item><item><title>Sensor fusion</title><link>https://salehahr.github.io/zettelkasten/sensors/sensor-fusion/</link><pubDate>Wed, 29 Sep 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/sensor-fusion/</guid><description>Source: Phil&amp;rsquo;s Lab Definition Combine multiple sensor readings to form an improved estimate of a desired variable.
Why sensor fusion? Overcome limitations of individual sensors (noise, uncertainty) Estimate quantities that are not directly measurable
e.g. gyroscope measures angular rates of change but can&amp;rsquo;t directly measure roll and pitch angle Fusion in IMU Accelerometer only: good estimate in non-accelerating conditions Gyroscope only: good estimate over short periods of time (due to integration of bias terms)</description></item><item><title>scipy.optimize</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/scipy.optimize/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/scipy.optimize/</guid><description>http://stackoverflow.com/questions/52438263/scipy-optimize-gets-trapped-in-local-minima-what-can-i-do
Local optims sensitive to initial value. Workarounds:
use your optimization in a loop with random starting points inside your boundaries
use an algorithm that can break free of local minima, I can recommend scipy&amp;rsquo;s basinhopping() It repeats your minimize procedure multiple times and get multiple local minimums. The minimal one is the global minimum.
use a global optimization algorithm and use it&amp;rsquo;s result as initial value for a local algorithm.</description></item><item><title>Rigid cystoscope dimensions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/rigid-cystoscope-dimensions/</link><pubDate>Fri, 20 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/rigid-cystoscope-dimensions/</guid><description> ca 20cm x 65 cm = 0.02m x 0.065 m
Source: http://en.wikipedia.org/wiki/Cystoscopy
The sizes of the sheath of the rigid cystoscope are 17 French gauge (5.7 mm diameter), 19 Fr gauge (6.3 mm diameter), and 22 Fr gauge (7.3 mm diameter).
Source: http://www.karlstorz.com/cps/rde/xbcr/karlstorz_assets/ASSETS/3405020.pdf Camera</description></item><item><title>50.2.3 Kalman filter initial estimates</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.3-kalman-filter-initial-estimates/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.3-kalman-filter-initial-estimates/</guid><description>Source: Schneider 2013 How to not make the EKF fail Initial state estimate $\mathbf{x}_0$, $\mathbf{P}_0$
Filter generally not badly affected by wrong initial state $\mathbf{x}_0$, but convergence will be slow if we are way off
If $\mathbf{P}_0$ too small whereas $\mathbf{x}_0$ is way off
the gain K becomes small filter relies on the model more than on the measurements Thus: important to have a consistent pair $\mathbf{x}_0$, $\mathbf{P}_0$</description></item><item><title>50.2.40 Kalman filter performance metric</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.40-kalman-filter-performance-metric/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.40-kalman-filter-performance-metric/</guid><description>Source: Schneider 2013 $$ \text{MSE} = \frac{1}{MKN_{n_x}} \sum_{m=1}^M \sum_{j=1}^K \sum_{k=0}^{N-1} \left( \mathbf{\hat{x}}^j(t_k^+) - \mathbf{x}^j(t_k) \right)^\text{T} \left( \mathbf{\hat{x}}^j(t_k^+) - \mathbf{x}^j(t_k) \right) $$
symbols description $k$ time / step $j$ how many EKF runs? in tutorial: EKF was ran 1000 times (non-deterministic system due to noise)</description></item><item><title>Baumgarte stabilisation over the SO(3) rotation group for control</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/baumgarte-stabilisation-over-the-so-3-rotation-group-for-control/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/baumgarte-stabilisation-over-the-so-3-rotation-group-for-control/</guid><description>Author: Sebastien Gros</description></item><item><title>Schneider 2013 How to not make the EKF fail</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/schneider-2013-how-to-not-make-the-ekf-fail/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/schneider-2013-how-to-not-make-the-ekf-fail/</guid><description>Authors: Schneider, Georgakis URL: http://www.researchgate.net/publication/263942618_How_To_NOT_Make_the_Extended_Kalman_Filter_Fail/citations DOI 10.1021/ie300415d Measurement noise R, V (landmark) Kalman filter initial estimates Process noise Q and W (odometry) Kalman filter performance metric</description></item><item><title>(Markley 2014) Fundamentals of Spacecraft Attitude Determination and Control</title><link>https://salehahr.github.io/zettelkasten/bibliography/markley-2014/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/markley-2014/</guid><description>Authors: FL Markley, John Crassidis
DOI: 10.1007/978-1-4939-0802-8
Note/Nomenclature: This book interpetes rotations/transformations in the passive/alias sense (I&amp;rsquo;m not a fan) Quaternions in JPL conventions instead of Hamiltonian (not a fan of this either&amp;hellip;) Rotation matrix = attitude matrix Introduction Attitude determination: memoryless approach without using statistics Attitude estimation: approaches with memory, uses statistical info from a series of measurements filter approaches uses a dynamic motion model of the object Quaternions Quaternion conventions Quaternion multiplication Rotations &amp;ldquo;Euler&amp;rsquo;s theorem: any rotation is a rotation about a fixed axis&amp;rdquo;</description></item><item><title>50.4.1 Additive quaternion filtering</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.4.1-additive-quaternion-filtering/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.4.1-additive-quaternion-filtering/</guid><description>Parents: Quaternion index , which orientation parametrisation to-choose? Source: Markley 2014 Additive quaternion filtering Additive quaternion error Methods of enforcing the normalisation Renormalise the estimate by brute force Modify KF update equations to enforce a norm constraint using a Lagrange multiplier
[1] and [2] yield biased estimates of the quaternion
Methods that don&amp;rsquo;t enforce normalisation Define the rotation matrix to be guarantees orthogonality introduces unobservable DOF: the quaternion norm Use the above equation without the ||q||-2 factor &amp;ndash;&amp;gt; no orthogonality</description></item><item><title>50.4.2 Multiplicative quaternion filtering (MEKF)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.4.2-multiplicative-quaternion-filtering-mekf/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.4.2-multiplicative-quaternion-filtering-mekf/</guid><description>See also: Which orientation parametrisation to choose? Source: Markley 2014 Main idea is to use
the quaternion as a global rotation representation
a three component state vector as the local representation of rotation errors $$ \begin{aligned} \mathbf{q}\text{tr} &amp;amp;= \delta\mathbf{q} (\delta\mathbf{\theta}) \otimes \mathbf{\hat{q}}\
\mathbf{R}(\mathbf{q}\text{tr}) &amp;amp;= \mathbf{R} (\delta\mathbf{\theta}) \mathbf{R} (\mathbf{\hat{q}}) \end{aligned}$$
each term $(\mathbf{q}_\text{tr},~\delta\mathbf{q},~ \mathbf{\hat{q}})$ is a normalised unit quaternion
Any of the rotation error representations can be used to calculate delta_theta, which is part of the error state of the MEKF.</description></item><item><title>50.7.2 Calculation of K and P in ESKF update</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.2-calculation-of-k-and-p-in-eskf-update/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.2-calculation-of-k-and-p-in-eskf-update/</guid><description>Parent: 50.3 Error-State Kalman Filter , eskf-update See also: Evaluation of the H Jacobian Source: Solà 2017 Quaternion kinematics for ESKF The filter correction equations are (yields a posteriori estimates)
Notes:
Here, the simplest form of the covariance update is used. This has poor numerical stability, however (no guarantee of symmetricity or positive definiteness) More stable forms are e.g. Joseph form (symmetric and positive) Error correction?</description></item><item><title>Euler axis/angle representation</title><link>https://salehahr.github.io/zettelkasten/rotations/euler-axis-angle-representation/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/euler-axis-angle-representation/</guid><description>Parents: rotations-so3-group-index , orientation-parametrisations See also: Rotation vector representation Source: Markley 2014 3 parameters:
there appears to be 4 parameters: 1 angle, 3-component unit vector (Euler axis, Euler angle of the rotation) however, the vector $\mathbf{e}$ is a unit vector (constrained by $\left\lVert \mathbf{e} \right\rVert = 1$ To rotation matrix The matrix is periodic (period 2*pi) From rotation matrix If -1 &amp;lt; cos theta &amp;lt; 1: If cos theta = 1: axis undefined</description></item><item><title>Gibbs / Rodrigues parameter representation for rotations</title><link>https://salehahr.github.io/zettelkasten/rotations/gibbs-rodrigues-parameter/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/gibbs-rodrigues-parameter/</guid><description>Parent: Orientation parametrisations See also: Rotation error representation Source: Markley 2014 From unit quaternions : From euler-axis-angle-representation : To unit quaternions :   Plane of the figure contains identity quaternion , origin The circle is a cross section of the quaternion sphere S^3 The upper horizontal axis is the 3D Gibbs vector hyperplane (tangent at the identity quaternion) [+] q and -q map to the same Gibbs vector, therefore there is a 1:1 mapping of rotations between quaternions and the Gibbs parameter [-] the Gibbs vector is infinite for 180 degree rotations (q.</description></item><item><title>Intrinsic vs extrinsic rotations</title><link>https://salehahr.github.io/zettelkasten/rotations/intrinsic-vs-extrinsic-rotations/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/intrinsic-vs-extrinsic-rotations/</guid><description>Parent: Rotations/SO(3) Group Index See also: Active/passive or Alibi/alias rotation transformations Source: http://rock-learning.github.io/pytransform3d/transformation_ambiguities.html We want to rotate first by $R_1$, then by $R_2$.
Extrinsic rotation In global coordinates, extrinsic rotation: $R_2 \cdot R_1$
Intrinsic rotation In local coordinates, intrinsic rotation: $R_1 \cdot R_2$
($R_1$ defines new coordinates in which $R_2$ is applied)
Specifying the convention is relevant when dealing with Euler angles!!!</description></item><item><title>Quaternion to rotation matrix</title><link>https://salehahr.github.io/zettelkasten/rotations/quaternion-to-rotation-matrix/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/quaternion-to-rotation-matrix/</guid><description>Parents: quaternion-index , rotations-so3-group-index See also: orientation-parametrisations Source: Markley 2014 Unit quaternion to rotation matrix</description></item><item><title>Rotation error representation</title><link>https://salehahr.github.io/zettelkasten/rotations/rotation-error-representation/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/rotation-error-representation/</guid><description>Parents: rotations-so3-group-index , quaternion-index See also: Orientation parametrisations , which orientation parametrisation to-choose? Source: Markley 2014 Note:
Only for small angle approximations! all these representations are equivalent through second order as In terms of&amp;hellip;
Rotation vector Quaternions Euler angles Use upper sign if {i,j,k} even permutation of {1,2,3}, lower sign otherwise
gibbs-rodrigues-parameter . MRPs. etc.</description></item><item><title>Rotation vector representation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/rotation-vector-representation/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/rotation-vector-representation/</guid><description>Parents: rotations-so3-group-index , orientation-parametrisations Source: Markley 2014 Combine the Euler axis/angle into a three component rotation vector $$ \mathbf{\theta} \equiv \theta \mathbf{e}$$
Convenient for analysis, but not for computation</description></item><item><title>Maley 2013 MEKF for Nonspinning Guided Projectiles</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/maley-2013-mekf-for-nonspinning-guided-projectiles/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/maley-2013-mekf-for-nonspinning-guided-projectiles/</guid><description>Source: http://apps.dtic.mil/sti/citations/ADA588831</description></item><item><title>Markley 2003 Attitude Error Representations for Kalman Filtering</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/markley-2003-attitude-error-representations-for-kalman-filtering/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/markley-2003-attitude-error-representations-for-kalman-filtering/</guid><description>Source: http://scholar.google.com/scholar?cluster=9266330323139560128&amp;amp;hl=en&amp;amp;as_sdt=0,5 Author: FL Markley
Motivation
Quaternion as an attitude representation
Good: lowest dimensionality while being a globally nonsingular representation Not so good: must obey a unit norm constraint In research, various methods for either getting around the norm constraint, or to enforce it
Most successful method employs the global attitude as a unit quaternion with a 3-comp attitude error representation
MEKF doesn&amp;rsquo;t estimate the quaternion state.</description></item><item><title>Rotations / SO(3) group index</title><link>https://salehahr.github.io/zettelkasten/rotations/rotations-so3-group-index/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/rotations-so3-group-index/</guid><description>Group theory SE(3) Special Euclidian Group SO(3) 3D rotation group Lie group, Lie algebra Exponential map Logarithm map Ambiguities in rotation representations Active/passive or Alibi/alias rotation transformations Intrinsic vs extrinsic rotations Rotation representations Orientation parametrisations Rotation error representation Which orientation parametrisation to choose? Linearisation of an orientation in SO(3) As Euler angles Euler angles Rotations as xyz Bryan-Tait angles (Kardanwinkel) Axis/angle Euler axis/angle representation As quaternions Quaternion index Quaternion to rotation matrix Kinematics Kinematics primer Chaining rotation matrices and angular velocities IMU specific IMU preintegration on manifold To read https://rip94550.</description></item><item><title>Active/passive or Alibi/alias rotation transformations</title><link>https://salehahr.github.io/zettelkasten/rotations/active-passive-or-alibi-alias-rotation-transformations/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/active-passive-or-alibi-alias-rotation-transformations/</guid><description>Parent: Rotations / SO(3) group index See also: Intrinsic vs extrinsic rotations Source: http://en.wikipedia.org/wiki/Rotation_matrix%20-%20Ambiguities
Alibi / Active Alias / Passive CS is fixed CS is rotated Point rotates within fixed CS Point remains stationary but is represented within a new CS Counterclockwise rotation by theta mathematics physics, robotics Source: http://rock-learning.</description></item><item><title>Euler angles</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/euler-angles/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/euler-angles/</guid><description>Parents: rotations-so3-group-index , orientation-parametrisations Source: Phil&amp;rsquo;s Lab Three angles that describe the orientation of an object w.r.t. a fixed coordinate system Roll $\phi$, Pitch $\theta$, Yaw $\psi$ Source: http://en.wikipedia.org/wiki/Euler_angles Possible representations Proper Euler angles (e.g. $zxz$) vs Tait-Bryan (e.g. $xyz$, $zyx$) Intrinsic vs. extrinsic rotations Extrinsic rotations (around fixed CS $xyz$) Intrinsic rotations (around body CS $XYZ = x''' y''' z'''$) As a rotation matrix $$R = X(\alpha) Y(\beta) Z(\gamma)$$</description></item><item><title>Which orientation parametrisation to choose?</title><link>https://salehahr.github.io/zettelkasten/rotations/20.4-which-orientation-parametrisation/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/20.4-which-orientation-parametrisation/</guid><description>Source: MKok 2017 Estimation algorithms (filtering, smoothing) usually assume that the unknown states and parameters are represented in Euclidean space
Due to wrapping and gimbal lock, Euclidian addition and subtraction don&amp;rsquo;t work Also generally don&amp;rsquo;t work for rotation matrices and unit quaternions Constraints (unit quaternion norm, rotation matrix orthogonality) are usually hard to implement in estimation algorithms These concerns led to the development of the MEKF .
To deal with this: Linearisation of an orientation in SO(3) Alternative method to estimate orientation:</description></item><item><title>Whampsey MEKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/whampsey-mekf/</link><pubDate>Sat, 14 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/whampsey-mekf/</guid><description>Source: http://matthewhampsey.github.io/blog/2020/07/18/mekf
Motivation Working with noisy IMU measurements IMUs usually provide redundant information that can be used to improve dead-reckoning Uses: Hamilton quaternion convention .
Which orientation parametrisation to choose? 50.5-error-state-kalman-filter</description></item><item><title>ESKF repos</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/eskf-repos/</link><pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/eskf-repos/</guid><description>C++ http://github.com/skrogh/msf_ekf http://github.com/je310/ESKF http://github.com/hobbeshunter/IMU_EKF (only IMU)
Python http://github.com/enginBozkurt/Error-State-Extended-Kalman-Filter http://github.com/uoip/stereo_vio_eskf (unsuccessful) &amp;ndash; uses average IMU readings http://github.com/aipiano/ESEKF_IMU</description></item><item><title>Diagnosis bladder cancer</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/diagnosis-bladder-cancer/</link><pubDate>Fri, 30 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/diagnosis-bladder-cancer/</guid><description>http://www.cancer.org/cancer/bladder-cancer/detection-diagnosis-staging/how-diagnosed.html</description></item><item><title>Converting IMU data to inertial frame</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/converting-imu-data-to-inertial-frame/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/converting-imu-data-to-inertial-frame/</guid><description>Parent: IMU index Source: http://redshiftlabs.com.au/wp-content/uploads/2018/02/an-1005_-_understanding_euler_angles.pdf IMU outputs are in the body frame of the sensor.
Convention used in the article: yaw/psi (z) - pitch/theta (y) - roll/phi (x) around momentary axes Momentary coordinate systems: W -&amp;gt; W' -&amp;gt; W'' -&amp;gt; B Body acceleration to inertial acceleration W_a = R_WB @ B_a
Body angular rate to inertial angular rate
Each angular rate must be converted to the corresponding frame p: gyro_z -&amp;gt; rotated into W: R_w_w' @ R_w'_w'' @ R_w''_B @ q q: gyro_y -&amp;gt; rotated into W': R_w'_w'' @ R_w''_B @ q r: gyro_x -&amp;gt; rotated into W'': R_w''_B @ r with Gimbal lock: pitch approaches +-90, terms divided by cos90</description></item><item><title>Exponential map</title><link>https://salehahr.github.io/zettelkasten/rotations/exponential-map/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/exponential-map/</guid><description>Parents: Quaternion index , Rotations / SO(3) group index Notation Variables $$\begin{alignedat}{3} &amp;amp;\phi &amp;amp;&amp;amp;\in \mathbb{R}^3\
&amp;amp;\phi^\wedge &amp;amp;&amp;amp;\in \mathfrak{so}(3)\
&amp;amp;\mathbf{R} &amp;amp;&amp;amp;\in \text{SO}(3)\
\end{alignedat}$$
Functions $$\begin{alignedat}{3} \text{(skew) } \wedge &amp;amp;:&amp;amp; \mathbb{R}^3 &amp;amp;&amp;amp;\rightarrow \mathfrak{so}(3) &amp;amp;\
\exp &amp;amp;:&amp;amp; ~&amp;amp;&amp;amp;\mathfrak{so}(3) &amp;amp;\rightarrow \text{SO}(3)\
\text{Exp} &amp;amp;:&amp;amp; ~\mathbb{R}^3 &amp;amp;&amp;amp; &amp;amp;\rightarrow \text{SO}(3) \end{alignedat}$$
Thus, $$\begin{alignedat}{3} &amp;amp;\exp(\phi^\wedge) = \text{Exp}(\phi) = \mathbf{R} \end{alignedat}$$
Source: Forster 2017 IMU Preintegration At the identity Maps an element of the Lie algebra ($\phi^\wedge \in \mathfrak{so}(3)$, a skew symmetric matrix) to a rotation First order approximation $$ \exp(\phi^\wedge) \approx \mathbf{I} + \phi^\wedge$$</description></item><item><title>Importing a fork in Python instead of installed package</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/importing-a-fork-in-python-instead-of-installed-package/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/importing-a-fork-in-python-instead-of-installed-package/</guid><description>http://stackoverflow.com/questions/23075397/python-how-to-edit-an-installed-package
Run this in repo that uses the fork (this installs the package as a submodule): python3 -m pip install -e git+[ssh://git@github-feudalism/feudalism/spatialmath-python.git
egg=f-spatialmath](ssh://git@github-feudalism/feudalism/spatialmath-python.git egg=f-spatialmath) &amp;ndash;upgrade Instructions:
Fork the package repo cd to own repo where you want to use the package Install the fork using the above pip install command. This creates ./src/submodule When making changes to fork: make changes in either the submodule folder (for immediate effect), or in the fork subdirectory + push + reinstall</description></item><item><title>Lie group, Lie algebra</title><link>https://salehahr.github.io/zettelkasten/rotations/lie-group-lie-algebra/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/lie-group-lie-algebra/</guid><description>Lie group Parent: Rotations / SO(3) group index Source: http://www.seas.upenn.edu/~meam620/slides/kinematicsI.pdf
A group that is a differentiable (smooth) manifold is called a Lie group.
Lie algebra Source: http://en.wikipedia.org/wiki/3D_rotation_group Lie algebra $\mathfrak{so}(3)$
Every Lie group has an associated Lie algebra Lie algebra: linear space with same dimension as the Lie group Consists of all skew-symmetric 3x3 matrices Elements of the Lie algebra $\mathfrak{so}(3)$ are elements of the tangent space of the manifold SO(3)/Lie group at the identity element .</description></item><item><title>Logarithm map</title><link>https://salehahr.github.io/zettelkasten/rotations/logarithm-map/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/logarithm-map/</guid><description>Parents: Quaternion index , rotations / so(3) group-index Backlinks: Linearisation of an orientation in SO(3) Source: Forster 2017 IMU Preintegration Maps a rotation matrix $R$ in SO(3) to a skew-symmetric matrix ( Lie algebra ) Perturbations, first order approximation S. Forster [2015] suppplementary material for the inverse Jacobian
Source: MKok 2017 Approximations for small perturbations</description></item><item><title>30.1.2.2 Endoscope system components</title><link>https://salehahr.github.io/zettelkasten/permanent/30.1.2.2-endoscope-system-components/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/permanent/30.1.2.2-endoscope-system-components/</guid><description>Source: Leiner Imaging system (rod lens array for rigid endoscopes) within a tube/shaft
Illumination/Light source (it&amp;rsquo;s dark inside the body)
separate from the imaging system in order to reduce glare [low contrast of received image] surrounds the imaging system like a ring light Video camera
Coupling device (camera to imaging system)
Sheath and bridge which contain the telescope, fibre optics, and provides a channel for other stuff like irrigation, forceps, other surgical instruments</description></item><item><title>30.1.2.3 Endoscope specification</title><link>https://salehahr.github.io/zettelkasten/permanent/30.1.2.3-endoscope-specification/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/permanent/30.1.2.3-endoscope-specification/</guid><description>Source: Leiner Field of view: maximum angle that can be viewed Leiner Generally constructed so that the FOV is wide enough so that the &amp;lsquo;head on&amp;rsquo; view is visible even when the DOV is not zero. Leiner This is to reduce the chances of bumping the instrument into anatomy right in front of the shaft. Leiner Direction of view: angular offset of the optical axis from the longitudinal axis of the endoscope shaft</description></item><item><title>Distal and proximal ends</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/distal-and-proximal-ends/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/distal-and-proximal-ends/</guid><description>Source: Leiner distal: far from the surgeon
proximal: near the surgeon</description></item><item><title>Endoscope</title><link>https://salehahr.github.io/zettelkasten/permanent/30.1.2-endoscope/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/permanent/30.1.2-endoscope/</guid><description>Source: NHS The general term for the medical instrument used to perform endoscopy is, correspondingly, the endoscope. It is a device that makes use of optical technology to relay images from one end of the scope to another.
Functions not only for looking inside, but also have additional functionalities such as removing small tissue samples (biopsy) Insertion either through
natural body orifices (e.g. mouth, urethra) small incision in case a keyhole surgery is being performed Subtopics Types of endoscopes Components Specification</description></item><item><title>Endoscopes</title><link>https://salehahr.github.io/zettelkasten/permanent/30-endoscopes-index/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/permanent/30-endoscopes-index/</guid><description> Endoscopy Endoscopes (general) Types of endoscopes Endoscope system components Endoscope specification</description></item><item><title>Endoscopy</title><link>https://salehahr.github.io/zettelkasten/permanent/30.1.1-endoscopy/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/permanent/30.1.1-endoscopy/</guid><description>Source: NHS Backlinks: Endoscopes Endoscopy is a procedure which enables inspection of organs inside the body. The prefix &amp;lsquo;endo&amp;rsquo; comes from the Greek language and means &amp;lsquo;within&amp;rsquo; or &amp;lsquo;inside&amp;rsquo;, cf. the prefix &amp;lsquo;exo&amp;rsquo;/&amp;lsquo;ecto&amp;rsquo; meaning &amp;lsquo;outside&amp;rsquo;. Endo- from Greek ἔνδον (within, inside), cf. exo-/ecto- from έκτός (outside) The instrument: endoscope</description></item><item><title>Smooth polymial trajectory generation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/smooth-polymial-trajectory-generation/</link><pubDate>Mon, 19 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/smooth-polymial-trajectory-generation/</guid><description>Source: FLS handouts</description></item><item><title>Symbolic container for Probe</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/symbolic-container-for-probe/</link><pubDate>Sat, 17 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/symbolic-container-for-probe/</guid><description/></item><item><title>KF kinematics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kf-kinematics/</link><pubDate>Fri, 09 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kf-kinematics/</guid><description>Overview of KF states (true, nominal, error) Nominal state kinematics Error state kinematics Old stuff:</description></item><item><title>Program outline</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/program-outline/</link><pubDate>Wed, 07 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/program-outline/</guid><description>Current assumptions (to take care of later!)
Probe is rigid — DOFs are either 0 or constant — switch to rotating scope later No gravity No bias/offset, no noise in IMU Note: Stuff marked with checkboxes are either to-dos or things I&amp;rsquo;m not sure that I implemented correctly
http://github.com/feudalism/dvi-ekf/tree/eskf; projects generate_data.py Data generation (is called from main.py)
Main objects
Generate camera data (from DefSLAM mono trajectory) Make RigidSimpleProbe (for now, all DOFs are 0 or constant) Make IMU object, generate first (om, acc) values from interpolated camera data ( - should generate it from stereo data instead) Variables</description></item><item><title>Modes of operation of the scope</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/modes-of-operation-of-the-scope/</link><pubDate>Wed, 30 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/modes-of-operation-of-the-scope/</guid><description/></item><item><title>Differentiation in different coordinate systems</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/differentiation-in-different-coordinate-systems/</link><pubDate>Sun, 27 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/differentiation-in-different-coordinate-systems/</guid><description>Source: Woernle Mehrkörpersysteme Backlinks: Kinematics primer</description></item><item><title>Update 2021-07-19</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/update-2021-07-19/</link><pubDate>Sun, 20 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/update-2021-07-19/</guid><description>Agenda
Comparison prop only, prop + update Comparison P+U plots (Rp 1000, Rp .01, Rp 1e-6) Changes to equations (pc, qc, err_pc, err_theta_c) s. KF kinematics Currently: getting probe output as function of DOFs Open tasks s. also dvi-eskf project board debug update stage??? get probe outputs as symbols/functions of DOFs switch from rigid probe to rotating scope - at which point do I compensate for notch rotation?</description></item><item><title>Camera views as seen by SLAM at distal end of probe/scope</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/camera-views-as-seen-by-slam-at-distal-end-of-probe-scope/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/camera-views-as-seen-by-slam-at-distal-end-of-probe-scope/</guid><description/></item><item><title>Endoscope tip</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/endoscope-tip/</link><pubDate>Mon, 14 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/endoscope-tip/</guid><description>Source: &amp;lt;http://www.osapublishing.org/ao/fulltext.cfm?uri=ao-43-1-113&amp;amp;id=78236
F2&amp;gt; Note: this is a mini endoscope, probably not the standard construction</description></item><item><title>Equations for obtaining omega (angular velocity) and acceleration of IMU from camera</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/equations-for-obtaining-omega-angular-velocity-and-acceleration-of-imu-from-camera/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/equations-for-obtaining-omega-angular-velocity-and-acceleration-of-imu-from-camera/</guid><description>Parent: Update 2021-06-11</description></item><item><title>Notch positions due to scope rotation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/notch-positions-due-to-scope-rotation/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/notch-positions-due-to-scope-rotation/</guid><description>Backlinks: Update 2021-06-11</description></item><item><title>Obtaining IMU measurements from camera by forward kinematics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/obtaining-imu-measurements-from-camera-by-forward-kinematics/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/obtaining-imu-measurements-from-camera-by-forward-kinematics/</guid><description>Parent: SA TODO Backlinks: Thesis restructure Done:
reverse-fwkin (scrapped) omega_B symbolic check links in BC and CB config — new diagrams (split up into &amp;gt;=2 bodies?) check om_B = om_C + om_CB (s. [http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters Kinematics](http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters Kinematics), Woernle ) save om_B to container obtain accel. (s. [http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters Kinematics](http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters Kinematics), Kinematics primer ) where (ang. vel of body j w.r.t. body i, expressed in CS k)  Chaining velocities and accelerations: [validation] reconstruct rot_B from om_B, compare with camera debug first reconstruction of IMU traj, if that doesn&amp;rsquo;t work debug the fake data generation - update readme update robot model with simplification around pivot point validate updated model Anhang</description></item><item><title>Update 2021-06-11</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/update-2021-06-11/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/update-2021-06-11/</guid><description>General model for probe (from Forward kinematics IMU to camera )
Using the standard DH convention  Note: I have since changed the axis configuration at the camera part—above diagram is no longer up to date; to be updated!
This has the IMU (B) as the base and the camera (C) as the end effector Using robotics-toolbox-python: http://github.com/petercorke/robotics-toolbox-python Simplified model (from Forward kinematics IMU to camera ) Currently using a simplified model with all degrees of freedom set to 0 or constant Modification to (above) existing robot model Probably need to modify the rotational joints around the pivot s.</description></item><item><title>Rigid cystoscope mechanism</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/rigid-cystoscope-mechanism/</link><pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/rigid-cystoscope-mechanism/</guid><description>Parent: Update 2021-06-11</description></item><item><title>Discussion 2021-06-01</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-06-01/</link><pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-06-01/</guid><description>Notes
IMU data to be generated using kinematic relations, not via numerical differentiation Reduce loss of data, model for prediction, noise propagation Forward kinematics B &amp;ndash;&amp;gt; C (everything in terms of SLAM coordinates), s. Probe forward kinematics For visualisation: IMU data in W coordinates Monday: real probe Python robotics toolboxes for generating of forward kinematics matrices, velocity expressions (symbolic differentiation) Predict step
Kinematics ( equations of motion IMU to camera ) = f(DOF) p_BC = f(l1, l2) v_BC = f(ang_vel) a_BC = f(acc)</description></item><item><title>Equations of motion IMU to camera</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kinematics-equations-of-motion-imu-to-camera/</link><pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kinematics-equations-of-motion-imu-to-camera/</guid><description>Backlinks: Discussion 2021-06-01 R_WB = R_WC * R_CB
Notes: ref http://docs.sympy.org/latest/modules/physics/vector/vectors.html for vector calculus (symbolic)</description></item><item><title>Forward kinematics IMU to camera</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/forward-kinematics-imu-to-camera/</link><pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/forward-kinematics-imu-to-camera/</guid><description>Backlinks: [Discussion 2021-05-21](discussion 2021-05-21.md), discussion-2021-06-01 , update-2021-06-11 Simplified model (rigid)</description></item><item><title>Inverse of a homogeneous transformation matrix</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/inverse-of-a-homogeneous-transformation-matrix/</link><pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/inverse-of-a-homogeneous-transformation-matrix/</guid><description>Parent: Kinematics primer Source: http://mathematica.stackexchange.com/questions/106257/how-do-i-get-the-inverse-of-a-homogeneous-transformation-matrix</description></item><item><title>Jeon 2009 Kinematic Kalman Filter for Robot End-Effector Sensing</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/jeon-2009-kinematic-kalman-filter-for-robot-end-effector-sensing/</link><pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/jeon-2009-kinematic-kalman-filter-for-robot-end-effector-sensing/</guid><description>Backlinks: Discussion 2021-05-25 Authors: Jeon and Tomizuka
Abstract
inaccuracies in estimation of EE motion can come from kinematic error (error in parameters in kinematic equations)
to overcome this: take direct measurements e.g. using vision, but vision has high latency IMUs are used to provide interframe data fuse camera and IMU in a kinematic Kalman filter (KKF) framework. Note: uses ESKF
effect of camera measurement delay, augmenting the KF states to also estimate the time delay</description></item><item><title>Discussion 2021-05-25</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-05-25/</link><pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-05-25/</guid><description>Backlinks: Discussion 2021-05-10 , discussion-2021-05-21 Notes
IMU-rod transformation: rotation part (spherical joint), translation part predict and update equations? maybe change variables in states vector to local coordinates add gravity later Ausblick: Einfluss der IMU auf verbesserte Lokalisierung &amp;ndash;&amp;gt; evtl eine IMU Koordinate weglassen Next:
generate fake imu data (delegated, s. [obtaining imu measurements from camera by forward kinematics](obtaining imu measurements-from-camera-by-forward-kinematics.md)) look for existing literature on IMU fusion/EKF which uses kinematic relations Massenmatrix, Koriolisterme etc.</description></item><item><title>Modified vs original DH</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/modified-vs-original-dh/</link><pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/modified-vs-original-dh/</guid><description>Wikipedia
Modified DH (proximal) Original DH (distal?) a: offset in x (from old origin)alpha: twist of z around old x axisd: offset in z (to next origin)theta: rotation around current z d: offset in z (from prev origin)theta: rotation around prev zr / a: offset in x from prev originalp: twist of z around current x</description></item><item><title>Spherical wrist</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/spherical-wrist/</link><pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/spherical-wrist/</guid><description>http://www1.cs.columbia.edu/~allen/F15/NOTES/forwardspong.pdf
S. also http://www.youtube.com/watch?v=S6TFakW5YcI</description></item><item><title>(Hibbeler) Dynamics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/hibbeler-dynamics/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/hibbeler-dynamics/</guid><description>Author: Russell Hibbeler Contents
Kinematics, kinetics of particle [planar] rigid body [3D] rigid body Vibrations Kinematics primer</description></item><item><title>(Woernle) Mehrkoerpersysteme</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/woernle-mehrkoerpersysteme/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/woernle-mehrkoerpersysteme/</guid><description>Author: Christoph Woernle Contents:
Kinematics, kinetics (Dynamik) Some basics Converting velocity from CS1 to CS0 Chaining rotation matrices and angular velocities [Poisson equation for skew symmetric matrix of angular velocity](poisson equation for skew-symmetric-matrix-of-angular-velocity.md) Differentiation in different coordinate systems Kinematics primer Reversed kinematics relations Rotations as xyz Bryan-Tait angles (Kardanwinkel) Holonomic systems, non-holonomic systems Holonomic constraints</description></item><item><title>Chaining rotation matrices and angular velocities</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/chaining-rotation-matrices-and-angular-velocities/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/chaining-rotation-matrices-and-angular-velocities/</guid><description>Source: Woernle Mehrkörpersysteme Backlinks: [Kinematics primer](kinematics primer.md), [obtaining imu measurements from camera by forward kinematics](obtaining imu measurements-from-camera-by-forward-kinematics.md) See also: Rotations / SO(3) group index Chaining rotation matrices T_02 transforms a point in CS2 to CS0
compare: Chaining homogeneous transformation matrices Chaining angular velocities (in same CS) ang.vel. of 2 rel to 0 = ang.vel. of 1 rel. to 0 + ang.vel. of 2 rel. to 1</description></item><item><title>Converting velocity from CS1 to CS0</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/converting-velocity-from-cs1-to-cs0/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/converting-velocity-from-cs1-to-cs0/</guid><description>Source: Woernle Mehrkörpersysteme Backlinks: [Kinematics primer](kinematics primer.md), [poisson equation for skew symmetric matrix of angular velocity](poisson equation for skew-symmetric-matrix-of-angular-velocity.md)
Linear velocity This is the derivative of r relative to CS0, as depicted in CS0 coordinates
Where the expression in square brackets means: the derivative of r relative to CS0, as depicted in CS1 coordinates
Angular velocity with : ang.vel. of D relative to B, given in E coordinates
Note : </description></item><item><title>Kinematics primer</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kinematics-primer/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kinematics-primer/</guid><description>Source: Hibbeler Dynamics , woernle-mehrkörpersysteme See also: Reversed kinematics relations , denavit-hartenberg-convention Backlinks: [Obtaining IMU measurements from camera by forward kinematics](obtaining imu measurements from camera by forward kinematics.md), rotations / so(3) group-index Prereqs:
Chaining rotation matrices and angular velocities Converting velocity from CS1 to CS0 [Poisson equation for skew symmetric matrix of angular velocity](poisson equation for skew-symmetric-matrix-of-angular-velocity.md) Inverse of a homogeneous transformation matrix Differentiation in different coordinate systems Position (in world coordinates)velocity (in world coordinates)where the skew symmetric matrix is the angular velocity of cs1 relative to cs0,(~~given in cs1 coordinates?</description></item><item><title>Modified Denavit-Hartenberg convention</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/modified-denavit-hartenberg-convention/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/modified-denavit-hartenberg-convention/</guid><description>Source: Craig - Introduction to Robotics Backlinks: Kinematics primer Note: s. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1083.6428&amp;amp;rep=rep1&amp;amp;type=pdf for comparison (Lipkin)
Link-frame attachment
Identify joint axes
For joint axes i and i+1, identify the  common perpendicular + where it meets axis i, or point of intersection and let this be the link-frame origin Let Z_i point along the i-th joint axis
Let X_i
point along common perpendicular, or be normal to the plane containing the two axes Assign Y_i (right hand coordinate system)</description></item><item><title>Poisson equation for skew symmetric matrix of angular velocity</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/poisson-equation-for-skew-symmetric-matrix-of-angular-velocity/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/poisson-equation-for-skew-symmetric-matrix-of-angular-velocity/</guid><description>Source: Woernle Mehrkörpersysteme Backlinks: [Kinematics primer](kinematics primer.md), converting velocity from cs1 to cs0 Skew-symmetric angular velocity: Poisson equation</description></item><item><title>Reversed kinematics relations</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/reversed-kinematics-relations/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/reversed-kinematics-relations/</guid><description>Source: Woernle Mehrkörpersysteme See also: Kinematics primer PositionVelocity(given that omega_00 = 0)(given that v_00 = 0)Accelerationgiven , , </description></item><item><title>Rotations as xyz Bryan-Tait angles (Kardanwinkel)</title><link>https://salehahr.github.io/zettelkasten/rotations/bryan-tait-kardanwinkel/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/bryan-tait-kardanwinkel/</guid><description>Parent: rotations-so3-group-index Source: Woernle Rotation angle nomenclature Euler angles: ZXZ (mitgedrehte Achsen) Kardan-Winkel [de] / Bryan-Tait angles: ZYX (mitgedrehte Achsen) xyz-Kardan-Winkel K3: körperfestes KS K0: Welt-KS
Ausgangslage 1. Drehung um x0 2. Drehung um y1 3. Drehung um z2 Winkelgeschwindigkeit
(either depicted in CS0, or CS3)</description></item><item><title>(Leiner) Digital Endoscope Design</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/leiner/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/leiner/</guid><description>Backlinks: Endoscopes URL: http://www.spiedigitallibrary.org/ebooks/SL/Digital-Endoscope-Design/1/Digital-Endoscope-Design/10.1117/3.2235283.ch1?SSO=1
Notes Insertion of an endoscope Types of endoscopes Endoscope system components Endoscope specification</description></item><item><title>Discussion 2021-05-21</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-05-21/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-05-21/</guid><description>Notes — Before
Got the ESKF implementation (Solà) to work with my fake IMU data [non-noisy IMU] results look ok for low process noise (trust the prediction more) with relatively high measurement noise [noisy IMU] ok? current assumptions/simplifications: fake data assumes IMU is sitting right on top of camera fake data, as of yet, does not take into account: biases, gravity simplified state vector (no scale estimate, no gravity estimate, no bias estimate etc) TBD: modify equations/states to fit the problem, i.</description></item><item><title>Endoscope/cystoscopy pics/videos</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/endoscope-cystoscopy-pics-videos/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/endoscope-cystoscopy-pics-videos/</guid><description>Backlinks: Discussion 2021-05-21 Rigid endoscope for cystoscopy
Source: http://www.ebay.com/itm/113780645426 Source: http://www.researchgate.net/figure/Intraoperative-image-of-the-rigid-cystoscope-entering-the-bladder-through-the-screw-tip_fig2_322897289 Source: http://www.youtube.com/watch?v=1gEpz9wijoY http://www.maestro-portal.eu/procedure/detail/4 Videos: Semi-Rigid Ureteroscopy and Laser Lithotripsy for Ureter Stones
Source: http://www.medicinenet.com/how_painful_is_a_cystoscopy/article.htm</description></item><item><title>IMU on cystoscope</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/diagram-imu-on-cystoscope/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/diagram-imu-on-cystoscope/</guid><description>Backlinks: Discussion 2021-05-21</description></item><item><title>Types of endoscopes</title><link>https://salehahr.github.io/zettelkasten/permanent/30.1.2.1-types-of-endoscopes/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/permanent/30.1.2.1-types-of-endoscopes/</guid><description>Source: Leiner Endoscopes can be classified according to their flexibility, thus resulting in &amp;lsquo;rigid&amp;rsquo;, &amp;lsquo;flexible&amp;rsquo; and &amp;lsquo;semi-rigid&amp;rsquo; variants. Video sensor at distal (&amp;ldquo;away from the surgeon&amp;rdquo;, opposite of proximal) end allows rigid endoscope to be converted to a flexible one solely by mechanical design Note: the term endoscope in hospital environments typically refers to the flexible variant More specialised endoscopes may be referred to by specific names, such as the cystoscope (for scoping bladders).</description></item><item><title>40.1 IMU measurement model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/40.1-imu-measurement-model/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/40.1-imu-measurement-model/</guid><description>Parent: [IMU index](imu index.md), probabilistic models-for-imu Backlinks: [IMU motion model](imu motion model.md), imu kinematic model using euler integration An IMU measures, relative to an inertial frame, acceleration and rotation rate.
The measurements are corrupted by bias and noise (often assumed to be white Gaussian noise ). mkok-2017 Additionally, the acceleration measured is affected by gravity. Note the [assumptions in modelling the true angular velocity in IMUs](assumptions in modelling the-true-angular-velocity-in-imus.</description></item><item><title>40.1.1 Assumptions in modelling the true angular velocity in IMUs</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/40.1.1-assumptions-in-modelling-the-true-angular-velocity-in-imus/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/40.1.1-assumptions-in-modelling-the-true-angular-velocity-in-imus/</guid><description>Parent: [IMU index](imu index.md), imu-measurement-model Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
For angular velocity, the termshould really be with
negligible Earth rotation = 0 stationary navigation frame, = 0</description></item><item><title>50.3 IMU motion model in a Kalman filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.3-modelling-imu-in-kf/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.3-modelling-imu-in-kf/</guid><description>Parent: IMU index Source: Solà 2017 Quaternion kinematics for ESKF Which states do we use for the motion model?
Choice of states for the IMU motion/kinematics model How do we model the IMU motion?
Choice of model for the IMU motion model The kinematics (true state) can be partitioned into a nominal part and an error part, s. variables in ESKF . The corresponding [nominal state dynamics and error state dynamics](nominal state-dynamics-and-error-state-dynamics.</description></item><item><title>50.3.1 Choice of states for the IMU motion/kinematics model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.3.1-states-for-imu-motion-model/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.3.1-states-for-imu-motion-model/</guid><description>Parent: IMU index See also: Choice of model for the IMU motion model According to MKok 2017 , we can either
Use the full state vector [+] knowledge about sensor motion is included in model [-] large state vector Or the partial state vector, where the inputs are the inertial measurements from the IMU [+] process noise intuitively represents IMU noise. This is useful when we have no knowledge about the motion model.</description></item><item><title>50.3.2 Choice of model for the KF using IMU readings</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.3.2-imu-model-for-kf/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.3.2-imu-model-for-kf/</guid><description>Parent: IMU index , 50.3-modelling-imu-in-kf According to MKok 2017 , here are some models that assume either a constant acceleration or a constant angular velocity:
Constant acceleration model Constant angular velocity model (Notation: angular velocity of the body with respect to world (n), expressed in body CS)
If motion is unknown, there is also the option of modelling the states using random walk equations.</description></item><item><title>50.5.1 IMU nominal-state and error-state kinematics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.5.1-imu-nominal-state-and-error-state-kinematics/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.5.1-imu-nominal-state-and-error-state-kinematics/</guid><description>Parents: [IMU index](imu index.md), 50.3 error-state-kalman-filter Note on discretisation Solà 2017 :
Convert the differential equations to difference equations (use integration) Integration methods may vary Closed form solutions Numerical integration Integration is done for: The nominal state The error state (deterministic part): error state dynamics and control The error state (stochastic part): noise and perturbations Nominal state Error state Model without noise and perturbations Continuous Discrete summary:with the jacobians defined in imu eskf-prediction-equations</description></item><item><title>50.5.1.1 States of the ESKF for estimating IMU pose</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.5.1.1-states-of-the-eskf-for-estimating-imu-pose/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.5.1.1-states-of-the-eskf-for-estimating-imu-pose/</guid><description>Parent: IMU index Source: Solà 2017 Quaternion kinematics for ESKF Full state Vector with 19 elements The corresponding kinematics equations/motion model is given in IMU kinematic equations/motion model .
Notes The angular error in 3D space is given by the notation $\delta\mathbf{\theta}$.
(s. rotation-error-representation )
The angular error $\delta\mathbf{\theta}$ is defined locally w.r.t. the nominal orientation (classical approach used in most IMU-integration works).
A more optimal approach may be to use a globally-defined angular error.</description></item><item><title>50.5.1.2 The initial gravity vector/orientation for the IMU ESKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.5.1.2-the-initial-gravity-vector-orientation-for-the-imu-eskf/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.5.1.2-the-initial-gravity-vector-orientation-for-the-imu-eskf/</guid><description>Parent: [IMU index](imu index.md), [choice of model for the imu motion model](choice of model-for-the-imu-motion-model.md)
Notes on the initial gravity vector/orientation for the IMU ESKF Solà 2017 For simplicity, it is assumed that  The gravity vector g is estimated in terms of frame q0
This puts the initial uncertainty on the gravity direction, rather than on the initial orientation.
Doing this improves linearity, because now the equation is linear in g and the initiial rotation R0 has no uncertainty</description></item><item><title>50.6 ESKF prediction equations</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.6-eskf-prediction-equations/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.6-eskf-prediction-equations/</guid><description>Parents: [IMU index](imu index.md), 50.3 error-state-kalman-filter Source: Solà 2017 Quaternion kinematics for ESKF Error state system equation becomes: where (s. IMU nominal-state and error-state kinematics for an overview of the nonlinear kinematics equations)
State propagation (without considering noise) — produces a state estimate (a priori) Note: this always returns zero as the mean of the error initialises to zero!
Covariance propagation (considers noise); a priori estimate with the Jacobians (transition matrix approximated using first order Euler, more precise methods are available)</description></item><item><title>50.7 ESKF update / Fusing IMU with complementary sensory data</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.7-eskf-update-fusing-imu-with-complementary-sensory-data/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.7-eskf-update-fusing-imu-with-complementary-sensory-data/</guid><description>Parent: [IMU index](imu index.md), 50.5-error-state-kalman-filter Source: Solà 2017 Quaternion kinematics for ESKF In the ESKF, the arrival of non-IMU sensor data triggers a correction stage. This correction makes the IMU biases observable , allows correct estimation of the biases The correction stage is three-fold:
observe the error state by way of filter correction &amp;lsquo;add&amp;rsquo; the observed errors to the nominal state to get the supposed &amp;lsquo;true&amp;rsquo; state according to the composition rules in variables in ESKF using IMUs reset the error state Source: Markley 2014 What if several measurements come in without IMU / propagation in between (i.</description></item><item><title>50.7.1 Observation of the error state (filter correction)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.1-observation-of-the-error-state-filter-correction/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.1-observation-of-the-error-state-filter-correction/</guid><description>Parents: 50.3 Error-State Kalman Filter , 50.5 eskf update / fusing imu with complementary sensory data Source: Solà 2017 Quaternion kinematics for ESKF Given is a non-IMU sensor with the measurement function [ Solà , Markley ] $$ \mathbf{y} = h(\mathbf{x}_t) + v $$
where $\mathbf{x}_t$ is the true state and $v$ is a white Gaussian noise $$ v \sim \mathcal{N}\left\lbrace 0, \mathbf{V}\right\rbrace $$
Source: Markley 2014 If the measurements are given in quaternion form:</description></item><item><title>50.7.1.1 H Jacobian matrix in the ESKF filter correction</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.1.1-h-jacobian-matrix-in-the-eskf-filter-correction/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.1.1-h-jacobian-matrix-in-the-eskf-filter-correction/</guid><description>Parent: Filter correction , eskf-update Source: Solà 2017 Quaternion kinematics for ESKF Evaluation of the H Jacobian
In the prediction stage, the filter estimates the error state. Therefore, the Jacobian H needs to be defined w.r.t. the error state , and evaluated at the true state estimate  However, as the error state mean is zero (not yet observed), the true state is approximated to the nominal state  Thus we can use the nominal state as the evaluation point The first Jacobian Depends on the sensor&amp;rsquo;s particular measurement function The second Jacobian with Source: Markley 2014 Measurement sensitivity matrix (Jacobian w.</description></item><item><title>50.7.3 ESKF reset</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.3-eskf-reset/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.3-eskf-reset/</guid><description>Parent: Fusing IMU with complementary sensory data Backlinks: 50.3 Error-State Kalman Filter Source: Markley moves the rotation error to the global rotation this keeps the rotation error small and far from any singularities To update the global state, the reset has to obey The reset has to preserve the quaternion norm, therefore an exact unit norm expression must be used, instead of an approximation. Using the Rodrigues parameter , the reset becomes which leads to a two step update (1.</description></item><item><title>(Science Focus) How can one eye alone provide depth perception</title><link>https://salehahr.github.io/zettelkasten/bibliography/science-focus/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/science-focus/</guid><description>Source: http://www.sciencefocus.com/the-human-body/how-can-one-eye-alone-provide-depth-perception/
Author: Hilary Guite
In humans with normal binocular vision, depth perception is obtained using the parallax in the two overlapping fields of vision (&amp;ldquo;binocular disparity&amp;rdquo;)
Each single field of vision has a slightly different view to the other
If vision in one eye is impaired, depth perception is still obtainable even with only one eye. Some tricks that the brain uses:
We know the real size of things Using perspective, e.</description></item><item><title>Monocular depth perception</title><link>https://salehahr.github.io/zettelkasten/permanent/10-monocular-depth-perception/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/permanent/10-monocular-depth-perception/</guid><description>Depth perception in real life In nature, prey animals typically have eyes on either side of their head to maximise field of view, while most predators have forward-facing eyes with overlapping fields of vision (binocular vision) for maximum depth perception. Humans also have binocular vision. (Some exceptions: fruit bats, killer whales)
We perceive depth, or distance to the objects that we see, based on several visual cues.</description></item><item><title>50.5 Error-State Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.5-error-state-kalman-filter/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.5-error-state-kalman-filter/</guid><description>Source: Markley An EKF propagates the expectation and covariance of the state The MEKF propagates the expectation and the covariance of the error state Source: Whampsey MEKF Previously: orientation is represented by one state Now: orientation is split up into  a large signal q_nom (nominal orientation) and a small signal (perturbation angle alpha) &amp;ndash; parametrises an error quaternion  This reformulates the error in terms of the group operation and so maintains the rotation invariance (rotation preserves the origin, length, angle between two vectors, orientation, etc.</description></item><item><title>Identity quaternion</title><link>https://salehahr.github.io/zettelkasten/rotations/identity-quaternion/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/identity-quaternion/</guid><description>Parent: Quaternion index Source: Solà 2017 $$\mathbf{q}_I = 1 = \left[ \begin{array}{c} 1\ \mathbf{0}_3 \end{array} \right]$$
$$\mathbf{q}_I \otimes \mathbf{q} = \mathbf{q} \otimes \mathbf{q}_I = \mathbf{q}$$</description></item><item><title>Inverse quaternion</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/inverse-quaternion/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/inverse-quaternion/</guid><description>Parent: Quaternion index Source: Solà 2017 Quaternion kinematics for ESKF The inverse is the conjugate in case of unit quaternions</description></item><item><title>Quaternion conventions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-conventions/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-conventions/</guid><description>Parent: Quaternion index Source: Solà 2017 Quaternion kinematics for ESKF Source: [ Wikipedia ], markley-2014 For quaternion multiplication : change the order to transform between conventions Hamilton Shuster Transpose of the Hamiltonian version</description></item><item><title>Quaternion differentiation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-differentiation/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-differentiation/</guid><description>Parent: Quaternion index Source: J. D. Hol — Sensor fusion and calibration of inertial sensors, vision, ultra-wideband and GPS
Using the identities: http://math.stackexchange.com/questions/189185/quaternion-differentiation Numerical differentiation (Euler)
http://math.stackexchange.com/questions/1896379/how-to-use-the-quaternion-derivative
$$ \begin{aligned} q(t+dt) &amp;amp;= q(t) \otimes dq\
\dfrac{dq}{dt} &amp;amp;= \frac{1}{2} \omega \otimes q \quad \text{with } \omega = \left[ \begin{array}{cccc} 0 &amp;amp; \omega_x &amp;amp; \omega_y &amp;amp; \omega_z \end{array} \right]^\text{T} \end{aligned}$$
Integrating this, assuming $\omega=\text{const.}$ from $t_0$ to $t_0 + dt$: $$ \begin{aligned} q(t) &amp;amp;= q(t_0) \exp \left( \frac{1}{2} \omega \cdot \left( t - t_0\right) \right)\</description></item><item><title>Quaternion index</title><link>https://salehahr.github.io/zettelkasten/rotations/quaternion-index/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/quaternion-index/</guid><description>Notation Quaternion conventions Basic math/properties Quaternion multiplication Identity quaternion Quaternion conjugate Quaternion norm Inverse quaternion Unit quaternions Double cover Calculus Quaternion differentiation As rotation rotations-so3-group-index exponential-map logarithm-map orientation-parametrisations Which-orientation-parametrisation linearisation-of-an-orientation-in-so-3 Quaternion to rotation matrix Rotation error representation For filtering Additive-quaternion-filtering Multiplicative-quaternion-filtering-mekf Literature Solà 2017 Quaternion kinematics for ESKF MKok 2017 Using inertial sensors for position and orientation estimation</description></item><item><title>Quaternion multiplication</title><link>https://salehahr.github.io/zettelkasten/rotations/quaternion-multiplication/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/quaternion-multiplication/</guid><description>Parent: Quaternion index Source: Solà 2017 Quaternion kinematics for ESKF Here: Hamiltonian convention, s. Quaternion conventions Non-commutative  Associative  Distributive Multiplication as a matrix product With
the matrices the skew operator (skew symmetric matrix) s. also cross product Source: Markley 2014 with the matrices</description></item><item><title>Quaternion norm</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-norm/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-norm/</guid><description>Parent: Quaternion index Source: Solà 2017 Quaternion kinematics for ESKF With the property</description></item><item><title>Solà 2017 Quaternion kinematics for ESKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sol%C3%A0-2017-quaternion-kinematics-for-eskf/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sol%C3%A0-2017-quaternion-kinematics-for-eskf/</guid><description>Link: http://www.iri.upc.edu/people/jsola/JoanSola/objectes/notes/kinematics.pdf
Author: Joan Solà
Abstract Primer on quaternion/rotation group math Math for error state Kalman filters using IMUs Contents/Chapters Unit quaternions , double cover Rotations, s. also SO(3) 3D rotation group Quaternion conventions Perturbations, derivatives, integrals Error-State Kalman Filter for IMU-driven systems Variables in ESKF IMU measurement model IMU motion model The initial gravity vector/orientation for the IMU ESKF IMU nominal-state and error-state kinematics IMU ESKF prediction equations Fusing IMU + other sensors ESKF using global angular errors</description></item><item><title>Unit quaternions</title><link>https://salehahr.github.io/zettelkasten/rotations/unit-quaternions/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/unit-quaternions/</guid><description>Parent: Quaternion index , orientation-parametrisations See also: quaternion-conventions , quaternion double cover Source: Solà 2017 Properties $$ \begin{aligned} \left\lVert \mathbf{q} \right\rVert &amp;amp;= 1\
\mathbf{q}^{-1} &amp;amp;= \mathbf{q}^* \end{aligned} $$
Can be written in the form $$ \mathbf{q} = \left[ \begin{array}{c} \cos\theta \ \mathbf{u} \sin\theta \end{array} \right] $$
with
$\mathbf{u}$ as a unit vector $\theta$ is the angle between $\mathbf{q}$ and the identity quaternion $\mathbf{q}_I = \left[\begin{array}{cccc}1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{array}\right]^\text{T}$</description></item><item><title>Cyril Stachniss EKF-SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cyril-stachniss-ekf-slam/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cyril-stachniss-ekf-slam/</guid><description>Links:
Course material: http://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/ Lectures: http://www.youtube.com/playlist?list=PLgnQpQtFTOGQrZ4O5QzbIHgl3b1JHimN_&amp;amp;feature=g-list</description></item><item><title>IMU data generation from camera/visual data</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/imu-data-generation-from-camera-visual-data/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/imu-data-generation-from-camera-visual-data/</guid><description>Parent: IMU index Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)</description></item><item><title>IMU to camera coordinate transformations</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/imu-to-camera-coordinate-transformations/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/imu-to-camera-coordinate-transformations/</guid><description>Parent: IMU index Source: Weiss 2011</description></item><item><title>Solà 2014 SLAM with EKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sol%C3%A0-2014-slam-with-ekf/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sol%C3%A0-2014-slam-with-ekf/</guid><description> Notes on EKF-SLAM that uses landmarks MATLAB code Notes on partial landmark initialisation (convariance matrix) Notes on the linearity of the observation function in scale</description></item><item><title>Trocar</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/trocar/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/trocar/</guid><description>Source: https://en.wikipedia.org/wiki/Trocar
Here: surgical trocar
Used in [laparoscopic surgery](laparoscopic surgery.md) A medical device which is used to make small incisions and allows insertion of other surgical instruments into the body cavity Source: [Leiner Digital Endoscope Design](Leiner Digital Endoscope Design.md) In an arthroscopic procedure (knee?):
trocar is inserted into the cannula both are pushed through the skin trocar is replaced by obturator (blunt rod) to open the area up.</description></item><item><title>Discussion 2021-05-10</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-05-10/</link><pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-05-10/</guid><description>Agenda
Change/Reduction of scope of SA (from fusing IMU with camera) to using sensor fusion to determine transformation parameters between IMU and camera Camera and IMU setup involves kinematic modelling (not fixed transformation as previously assumed!) Offline implementation in Python/MATLAB (scripting language) HiWi tasks can include DefSLAM bindings / interface C++ bindings of skrogh EKF implementation? HiWi prioritises Versuchsstand for now Tasks
Find an EKF implementation that works well and can be used with DefSLAM + IMU data implement kinematic model equations in the prediction-step, s.</description></item><item><title>Quaternion conjugate</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-conjugate/</link><pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-conjugate/</guid><description>Parent: Quaternion index Source: http://en.wikipedia.org/wiki/Quaternion
Flip signs of vector part
Source: Solà 2017 Quaternion kinematics for ESKF Multiplying with own conjugate (scalar!)
Conjugate operation on quaternion products</description></item><item><title>Transforming velocities to another frame</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/transforming-velocities-to-another-frame/</link><pubDate>Mon, 03 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/transforming-velocities-to-another-frame/</guid><description>http://physics.stackexchange.com/questions/197009/transform-velocities-from-one-frame-to-an-other-within-a-rigid-body
Transforming velocities to another frame Further reading: http://core.ac.uk/download/pdf/154240607.pdf</description></item><item><title>(Weiss Thesis) Vision based navigation for micro helicopters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/weiss-thesis-vision-based-navigation-for-micro-helicopters/</link><pubDate>Sun, 25 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/weiss-thesis-vision-based-navigation-for-micro-helicopters/</guid><description>Source Backlinks
Authors Stephan Weiss Abstract
Issues that arise during state estimation and sensor self-calibration Application area: large and unknown areas, micro helicopter Vision based method used uses SfM, is compares mapless and map-based methods Statistical and modular sensor fusion strategy recovery of pose and drifts modular: camera as a black box sensor, allows other sensors additionally Observability analysis Literature review
Fusing IMU with monocular vision: given extrinsic parameters , an IMU is able to recover metric scale, as well as help transition across short vision failure period Armesto et al.</description></item><item><title>IMU</title><link>https://salehahr.github.io/zettelkasten/sensors/imu/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/imu/</guid><description>Source: Mur-Artal 2017 measures acceleration (from accelerometer) and angular velocity (from gyrometer) of sensor at regular intervals measurements are affected by sensor noise accelerometer bias gyrometer bias accelerometer is further affected by gravity &amp;ndash;&amp;gt; need to subtract effect of gravity</description></item><item><title>IMU states, dynamics equations</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/imu-states-dynamics-equations/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/imu-states-dynamics-equations/</guid><description>Parent: IMU index Source: Mur-Artal 2017 VI-ORB Evolution of IMU states (world frame to IMU: orientation R, position p, velocity v) between consecutive keyframes
Evolution of IMU states (world frame to IMU: orientation R, position p, velocity v) between consecutive frames
Using the preintegration terms Preintegration (delta) terms and the Jacobians can be computed iteratively as IMU measurements arrive (s. Forster&amp;rsquo;s paper on preintegration)</description></item><item><title>Modelling noise and bias for IMU</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/modelling-noise-and-bias-for-imu/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/modelling-noise-and-bias-for-imu/</guid><description>Parent: [IMU index](imu index.md), imu-measurement-model Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
Modelling the noise The noise not only represents measurement noise, but also model uncertainty.
With proper calibration, the three gyroscope axes are independent: Same for accelerometer — assume diagonal for a properly calibrated sensor
Modelling the biases — two approaches
treat bias as constant (due to short experiment times) pre-calibrate in a separate experiment, or make part of the parameters vector treat as slowly time-varying (due to long experiment times or shorter bias stability) make the bias part of the state vector model the bias as a random walk</description></item><item><title>OpenCV Kalman filter pre/post states</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/opencv-kalman-filter-pre-post-states/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/opencv-kalman-filter-pre-post-states/</guid><description/></item><item><title>Besprechung 2021-04-01</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/besprechung-2021-04-01/</link><pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/besprechung-2021-04-01/</guid><description>Agenda
Recap: last meeting (2021-03-15) Offline Kalman — after this works, do a &amp;lsquo;live&amp;rsquo; implementation on DefSLAM run
IMU measurements as [acc, gyro] readings
With noise, but without considering bias, IMU-cam transformation, gravity Two sets of measurements for filter: IMU measurements, DefSLAM (camera) measurements
KF prediction using random walk
Recap: SLAM +filtering terminology (loose coupling, tight coupling) Literature Original suggestion using random walk model Interface (DefSLAM, Python) read: OK write: to do Offline Kalman as a separate repo Good papers?</description></item><item><title>Discretising a state space equation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/discretising-a-state-space-equation/</link><pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/discretising-a-state-space-equation/</guid><description>Source: [http://en.wikibooks.org/wiki/Control_Systems/State-Space_Equations
Discretization](http://en.wikibooks.org/wiki/Control_Systems/State-Space_Equations Discretization) Discretising a state space equation</description></item><item><title>note quaternions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/note-quaternions/</link><pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/note-quaternions/</guid><description>Converting from quaternion to angular velocity then back to quaternion http://math.stackexchange.com/questions/2282938/converting-from-quaternion-to-angular-velocity-then-back-to-quaternion</description></item><item><title>resource IMU common specifications, error models etc</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/resource-imu-common-specifications-error-models-etc/</link><pubDate>Sat, 27 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/resource-imu-common-specifications-error-models-etc/</guid><description>Parent: IMU Source: http://www.vectornav.com/resources/imu-specifications
IMU common specifications, bias, scale factor, orthogonality errors, and acceleration sensitivity for gyroscopes.
Source: Woodman - An introduction to inertial navigation Source: Quinchia - A Comparison between Different Error Modeling of MEMS Applied to GPS/INS Integrated Systems
3.2. State-Space Representation for Different Bias Models
First order Gauss-Markov (GM) Random walk Autoregressive process</description></item><item><title>Markov assumption</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/markov-assumption/</link><pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/markov-assumption/</guid><description>Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md) Backlinks: [Grisetti 2011 - Tutorial graph-based SLAM](grisetti 2011 - tutorial graph-based slam.md), probabilistic models-for-imu Models with state x which have the Markov property:
all information up till time t is contained in xt enables marginalisation of state xt at time t+1</description></item><item><title>note KF with missing measurements</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/note-kf-with-missing-measurements/</link><pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/note-kf-with-missing-measurements/</guid><description>Sources http://math.stackexchange.com/questions/982982/kalman-filter-with-missing-measurement-inputs http://opencv-users.1802565.n2.nabble.com/Kalman-filters-and-missing-measurements-td2886593.html
For a missing measurement:
use the last state estimate as a measurement set the covariance matrix of the measurement to essentially infinity. This would cause a Kalman filter to essentially ignore the new measurement since the ratio of the variance of the prediction to the measurement is zero. The result will be a new prediction that maintains velocity/acceleration but whose variance will grow according to the process noise.</description></item><item><title>note KF with different sampling rate</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/note-kf-with-different-sampling-rate/</link><pubDate>Wed, 24 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/note-kf-with-different-sampling-rate/</guid><description>Source: http://stackoverflow.com/questions/59566384/kalman-filter-with-different-sampling-rate
Approach 1: KF with variable dt Approach 2: KF with static dt
&amp;lsquo;Sub&amp;rsquo; updates? e.g.
predict() update() with sensor A skip update() for sensor B since no measurement arrived update() with sensor c repeat Generally discouraged:
If not predicting before each update, there is the risk of the filter lagging behind real world dynamics. The update step at t=k compares a measurement zk to the projected (predicted) state xk.</description></item><item><title>IMU motion model (discrete)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/imu-motion-model-discrete/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/imu-motion-model-discrete/</guid><description>Parent: [IMU index](imu index.md), probabilistic models-for-imu Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
Position dynamics Orientation dynamics (either quaternion or rotation matrix representation) with</description></item><item><title>Linearisation of an orientation in SO(3)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/linearisation-of-an-orientation-in-so-3/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/linearisation-of-an-orientation-in-so-3/</guid><description>Parents: Rotations / SO(3) group index , Quaternion index , orientation-parametrisations Source: MKok 2017 Rotation of a vector in SO(3)
The SO(3) group is a Lie group , so there exists an exponential map from a corresponding Lie algebra to the SO(3) group a reverse logarithm map Possible to represent orientations using unit quaternions or rotation matrices in SO(3) — linearisation point orientation deviations $\eta_t$ I think this is a global representation</description></item><item><title>MKok 2017 Using inertial sensors for position and orientation estimation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mkok-2017/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mkok-2017/</guid><description>Source: http://arxiv.org/abs/1704.06053 Authors: M Kok, JD Hol, TB Schön
Abstract
Contents/Chapters Quaternions Probabilistic models for IMU Orientation parametrisations Which orientation parametrisation to choose? Linearisation of an orientation in SO(3) IMU measurement model Modelling noise and bias for IMU IMU motion models IMU prior models</description></item><item><title>Orientation parametrisations</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/orientation-parametrisations/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/orientation-parametrisations/</guid><description>Parents: rotations-so3-group-index , quaternion index , probabilistic models-for-imu See also: Rotation error representation Source: MKok 2017 , markley-2014 Orientation parametrisations
Note: CCW rotation of a vector $x_v$ to $x_u$ corresponds to a CW rotation of the CS $v$ to CS $u$ (see also active/passive transformations ). Rotations are a member of SO(3) rotation matrix unique description of orientation Euler axis/angle Rotation vector not unique, due to wrapping Euler angles not unique, due to wrapping and gimbal lock Unit quaternions not unique, -q and q depict the same orientationProof: http://math.</description></item><item><title>Probabilistic models for IMU</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/probabilistic-models-for-imu/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/probabilistic-models-for-imu/</guid><description>Parent: IMU index Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
Three main components to the probabilistic models
IMU measurement model (infer knowledge about pose from measurements)  Prediction model (how sensor pose changes over time) Models of the initial pose (prior) Knowledge we are interested in: pose of the sensor
time-varying variables: states  constants: parameters  Knowledge available to us: sensor dynamics, available sensor measurements Conditional probability distribution</description></item><item><title>Besprechung 2021-03-15</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/besprechung-2021-03-15/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/besprechung-2021-03-15/</guid><description>Status
Last week: generated noisy IMU data (pose) from stereo trajectory, &amp;lsquo;offline&amp;rsquo; Kalman This week: Kalman + &amp;lsquo;live&amp;rsquo; DefSLAM Still to do: design KF (EKF, or other methods&amp;hellip;) &amp;lsquo;Offline&amp;rsquo; Kalman
To learn how to use openCV&amp;rsquo;s Kalman filter without having to rebuild DefSLAM every time Aim was to figure out the update/correction workflow and implement it in live DefSLAM run
Uses pre-extracted trajectories (mono and stereo)</description></item><item><title>On quaternions and rotation matrices</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/on-quaternions-and-rotation-matrices/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/on-quaternions-and-rotation-matrices/</guid><description>http://stackoverflow.com/questions/8919086/why-are-quaternions-used-for-rotations
It&amp;rsquo;s worth bearing in mind that all the properties related to rotation are not truly properties of Quaternions: they&amp;rsquo;re properties of Euler-Rodrigues Parameterisations, which is the actual 4-element structure used to describe a 3D rotation. Their relationship to Quaternions is purely due to a paper by Cayley, &amp;ldquo;On certain results related to Quaternions&amp;rdquo;, where the author observes the correlation between Quaternion multiplication and combination of Euler-Rodrigues parameterisations. This enabled aspects of Quaternion theory to be applied to the representation of rotations and especially to interpolating between them.</description></item><item><title>vi.cc using kalman for xyz states (what goes on with the map?)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/vi.cc-using-kalman-for-xyz-states/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/vi.cc-using-kalman-for-xyz-states/</guid><description>Offline Kalman Kalman and live DefSLAM</description></item><item><title>Besprechung 2021-03-08</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/besprechung-2021-03-08/</link><pubDate>Mon, 08 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/besprechung-2021-03-08/</guid><description>Agenda
DefSLAM + OS3 Up till DefTracking::MonocularInitialization() on hold, working on Kalman stuff for the time being DefSLAM + Kalman new plot (monocular trajectory without any pose updating) to do: use noisy stereo data, plug into update step while discarding images functions in System.cc: read data, update pose DefSLAM + sockets Meeting notes:
next step: implement the Kalman filter. When that is done, discuss next steps e.</description></item><item><title>System::forceTrajectory</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/system-forcetrajectory/</link><pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/system-forcetrajectory/</guid><description>Parent: DefSLAM branch overview Reference: DefSLAMGT (stereo as ground truth) For testing: DefSLAMVI
Description Force update of DefSLAMVI&amp;rsquo;s current frame pose to that of DefSLAMGT&amp;rsquo;s for the frames 230 to 239
Without System::Reset Frame pose is &amp;lsquo;updated&amp;rsquo; during the interval, but after the interval, the optimisation (which uses frame pose as an estimate and also uses map node positions) makes the system resume it&amp;rsquo;s trajectory before the update
(below: with pure monocular trajectory, without any forced updates) With System::Reset The system is reset after every forced pose update (i.</description></item><item><title>World to camera trafo</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/world-to-camera-trafo/</link><pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/world-to-camera-trafo/</guid><description>Parent: SLAM Index See also: [Pinhole camera model](pinhole camera model.md), pinhole camera-projection-function Source: http://www.cse.psu.edu/~rtc12/CSE486/lecture12.pdf
Camera coordinates (X, Y, Z) World coordinates (U, V, W) Image plane (x, y) / Pixel coordinates (u, v) Forward projection
Representing 2D point as a fictitious 3D point (x', y', z') [for matrix calculations] Convention: Given (x', y', z'), we can recover the 2D point (x, y) as World to camera trafo</description></item><item><title>DefSLAM branch overview</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-branch-overview/</link><pubDate>Fri, 19 Feb 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-branch-overview/</guid><description>Parent: SA TODO Repo http://github.com/feudalism/DefSLAM
Dormant
master sa Deprecated
windows - deprecated, changes made for building on Windows imu - deprecated, has Imu tracking functions but dependencies not resolved obs_tuple - initial attempt to incorporate Atlas, attempt to use OS3&amp;rsquo;s structure for MapPoint observations : &amp;lt;KeyFrame, tuple&amp;lt;int, int&amp;raquo; as opposed to &amp;lt;Keyframe, int&amp;gt; in DefSLAM+OS2 Temporary/Experimental
s. to do list
debugging the segfault that seemingly appears in Surface::getNormalSurfacePoint seems to happen after System reset</description></item><item><title>ORBSLAM2 unofficial documentation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam2-unofficial-documentation/</link><pubDate>Wed, 17 Feb 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam2-unofficial-documentation/</guid><description>Partially done, abandonned: http://github.com/raulmur/ORB_SLAM2/compare/master&amp;hellip;AlejandroSilvestri:master In Spanish: http://alejandrosilvestri.github.io/os1/doc/html/</description></item><item><title>Viewer segfault</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/viewer-segfault/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/viewer-segfault/</guid><description>Error [New Thread 0x7fff86ffd700 (LWP 1117)] NORMALS REESTIMATED : 277 - 277 [Thread 0x7fff86ffd700 (LWP 1117) exited] NORMAL ESTIMATOR OUTPoints potential : 293 70 New template requested Number Of normals 277 0x555566923da0 -0.79956 0.655022 -0.594482POINTS matched:167 Points Scale Error Keyframe : 1 stan dev 0.310974 chi 0.013115 0.01 201 SurfaceRegistration not sucessful (Not enough points to align or chi2 too big
Thread 6 &amp;ldquo;DefSLAM&amp;rdquo; received signal SIGSEGV, Segmentation fault. [Switching to Thread 0x7fffc1996700 (LWP 275)] __memmove_avx_unaligned_erms () at .</description></item><item><title>Segfault in DefTracking (imu branch)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/segfault-in-deftracking-imu-branch/</link><pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/segfault-in-deftracking-imu-branch/</guid><description>/home/user3/slam/datasets/mandala0/images/stereo_im_l_1560936003993.png i: 30 POINTS matched:10 Track lost soon after initialisation, reseting&amp;hellip; /home/user3/slam/datasets/mandala0/images/stereo_im_l_1560936004022.png i: 31 System Reseting NORMAL ESTIMATOR IN - NORMALS REESTIMATED : 0 - 0 NORMAL ESTIMATOR OUTPoints potential : 939 70 New template requested Number Of normals 0 0x5555636b1fb0 Not enough normals Reseting Local Mapper&amp;hellip; done Reseting Loop Closing&amp;hellip; done Reseting Database&amp;hellip; done
Thread 1 &amp;ldquo;DefSLAM&amp;rdquo; received signal SIGSEGV, Segmentation fault. 0x00007ffff78d9fae in cv::Mat::Mat (m=&amp;hellip;, this=0x7ffffffeaea0) at /usr/local/include/opencv4/opencv2/core/mat.inl.hpp:545 545 step[0] = m.</description></item><item><title>DefSLAM and discontinuous areas (classical datasets)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-and-discontinuous-areas-classical-datasets/</link><pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-and-discontinuous-areas-classical-datasets/</guid><description>Parent: Lamarca 2020 DefSLAM Source: http://github.com/UZ-SLAMLab/DefSLAM/issues/1
JoseLamarca: DefSLAM is suitable for rigid areas, proof of that is the abdominal sequence that is kind of rigid. The problem for these sequences is the discontinuous areas. For the monocular case, we are assuming that the surface is smooth that is not usually valid for the classical datasets. Apart from complexity issues that algorithms with RGB-D and stereo cameras could have in those scenes [1] and [2].</description></item><item><title>DefSLAM errors encountered</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-errors-encountered/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-errors-encountered/</guid><description>Rebuilding DefSLAM in Debug mode Error: &amp;ldquo;Virtual memory exhausted: Cannot allocate memory&amp;rdquo; Solution: reduce degree of make -j
Segmentation fault in Defslam debug mode Based on http://stackoverflow.com/questions/19615371/segmentation-fault-due-to-vectors Changed: surfacePoints_[ind] to surfacePoints_.at(ind)
New error surfacePoints_ appears to be NULL? Was it instantiated in another thread? http://stackoverflow.com/questions/11645857/debugging-with-gdb-why-this-0x0
Using core dumps with gdb http://jvns.ca/blog/2018/04/28/debugging-a-segfault-on-linux/</description></item><item><title>extern c++</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/extern-c++/</link><pubDate>Mon, 11 Jan 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/extern-c++/</guid><description>http://en.cppreference.com/w/cpp/language/storage_duration is a storage class specifier that controls storage duration and its linkage.
extern - static or thread storage duration and external linkage
Storage duration
static: storage for the object is allocated when the program begins and deallocated when the program ends. only one instance of the object exists thread storage for the object is allocated when the thread begins and deallocated when the thread ends each thread has its own instance of the object Linkage</description></item><item><title>Pizarro 2016 Schwarps</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/pizarro-2016-schwarps/</link><pubDate>Sun, 20 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/pizarro-2016-schwarps/</guid><description>Author: Daniel Pizarro et al.
Abstract
Warp between two images of a deforming surface: a transformation that depict the geometric deformation between the two &amp;lsquo;maps points between images of a deforming surface&amp;rsquo; Current approach to enforce a warp&amp;rsquo;s smoothness: penalise its second order partial derivatives However this favours locally affine warps Does not capture the local projective component of the image deformation Propose: novel penalty to smooth the warp while capturing the deformation&amp;rsquo;s local projective structure Proposed penalty is based on equivalents to the Schwarzian derivatives Schwarzian derivatives: projective differential invariants exactly preserved by homographies Methodology to derive a set of PDEs with only homographies as the solutions Validation: Schwarps outperform existing warps in modeling and extrapolation power: perform better in deformable reconstruction methods Introduction/Related work</description></item><item><title>DBoW2 weighing and scoring</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/dbow2-weighing-and-scoring/</link><pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/dbow2-weighing-and-scoring/</guid><description>Source: http://github.com/dorian3d/DBow</description></item><item><title>Intro to bladder cancer</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/intro-to-bladder-cancer/</link><pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/intro-to-bladder-cancer/</guid><description>http://www.cancer.net/cancer-types/bladder-cancer/introduction</description></item><item><title>Descriptors in feature detection/extraction</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/descriptors-in-feature-detection-extraction/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/descriptors-in-feature-detection-extraction/</guid><description>Source: http://medium.com/data-breach/introduction-to-feature-detection-and-matching-65e27179885d
Backlinks: Bag of words Descriptors A description of the local appearance around each feature point (keypoint) The descriptor encodes &amp;lsquo;interesting&amp;rsquo; information from the image into numbers and act as an identifier (&amp;lsquo;fingerprint&amp;rsquo;) to differentiate between features The description should ideally be invariant to changes (such as illumination, translation, scale, in-plane rotation) so that the feature can be found again, even if the image is transformed Typically: for each feature point, there is a descriptor vector Classes of descriptors: Local descriptor represents the point&amp;rsquo;s local neighbourhood Global descriptor describes the whole image generally not very robust—changes in parts of the image may cause the descriptor to fail Some algorithms for feature detection/descriptor generation SIFT (scale-invariant feature transform) SURF (speeded up robust feature) BRISK (binary robust invariant scalable keypoints) BRIEF (binary robust independent elementary features) ORB (oriented FAST and rotated BRIEF) Source: http://en.</description></item><item><title>FAST keypoint detector</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/fast-keypoint-detector/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/fast-keypoint-detector/</guid><description>Source: http://medium.com/data-breach/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf Parent: ORB descriptor FAST (Features from Accelerated and Segments Test)
How it works
Given: pixel p, surrounded by other pixels in the image
Take the surrounding pixels that are in a small circle around p If more than half of the surrounding pixels are darker/brighter than p, p is selected as a keypoint
Good for edge detection
Drawbacks</description></item><item><title>Feature matching</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/feature-matching/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/feature-matching/</guid><description>Source: http://medium.com/data-breach/introduction-to-feature-detection-and-matching-65e27179885d Backlinks: Bag of words , sparse/feature-based-vslam For matching between images, i.e. to establish a relationship (&amp;lsquo;correspondence&amp;rsquo;) between two images of the same scene or object.
Basic algorithm
Find/detect a set of identifying (&amp;lsquo;distinctive&amp;rsquo;) keypoints from all images to be matched Define a search region around each keypoint Extract and normalise the region content Compute a local descriptor from the normalised region Match local descriptors between the images Performance of matching methods depend on</description></item><item><title>ORB descriptor</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/orb-descriptor/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/orb-descriptor/</guid><description>Source: http://medium.com/data-breach/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf Backlinks: Descriptors in feature detection/extraction Oriented FAST and Rotated BRIEF, developed 2011 Was developed as an alternative to SIFT and SURF, and ended up being better/faster than both Build on FAST keypoint detector BRIEF descriptor ORB using FAST, but with (partial) scale invariance
Use a multiscale image pyramid
Each level of the pyramid is the same image, but scaled at different resolutions (reduced size as you go higher up) Once the pyramid is computed, FAST is used to detect keypoints  ORB detection</description></item><item><title>Gauss-Newton Method on Manifold</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/gauss-newton-method-on-manifold/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/gauss-newton-method-on-manifold/</guid><description>Source: Forster 2017 IMU Preintegration Standard approach for optimization on manifold
define a retraction to reparametrise the problem (lifting) retraction bijective map map between an element of the tangent space at x and a neighbourhood of x on the manifold i.e. we work in the tangent space (locally like a Euclidian space) and apply standard optimisation techniques for Gauss-Newton specifically: [ts] squared cost around current estimate [ts] solve the quadratic approximation &amp;ndash;&amp;gt; we get vector in tangent space [m] update the current guess on the manifold  Consider: Reparametrised: Retraction for SE(3) The exponential map of SE(3) as a retraction is possible, but may not be convenient (computationally)</description></item><item><title>IMU kinematic model using Euler integration</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/imu-kinematic-model-using-euler-integration/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/imu-kinematic-model-using-euler-integration/</guid><description>Parent: IMU index Source: Forster 2017 IMU Preintegration Backlinks: [IMU preintegration on manifold](imu preintegration on-manifold.md), imu-measurement-model Kinematic model Using Euler integration assuming acc and angVel are constant in the time interval: Using the measurement equations:</description></item><item><title>IMU preintegration on manifold</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/imu-preintegration-on-manifold/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/imu-preintegration-on-manifold/</guid><description>Parent: IMU index Source: Forster 2017 IMU Preintegration Backlinks: IMU model Preintegration on manifold
Summarising all measurements between the keyframes i and j into a single measurement This preintegrated IMU measurement constrains the motion between two consecutive keyframes Assume IMU is synchronised with the camera The above equations already provide the summarised IMU measurements, however, the integration has to be repeated whenever the linearisation point at t=t_i changes i.</description></item><item><title>Manifolds</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/manifolds/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/manifolds/</guid><description>Source: https://www.euclideanspace.com/maths/geometry/space/surfaces/manifold/index.htm
Like a surface in $n$-dimensions (hypersurface) An $n$-dim manifold looks like $\mathbb{R}^n$ locally (locally Euclidian) Circle: 1-dim manifold. If we zoom around a point on the circle, it looks like a line ($\mathbb{R}^1$) Sphere: 2-dim manifold. Zooming onto a point, it looks like a plane ($\mathbb{R}^2$) Source: https://www.seas.upenn.edu/~meam620/slides/kinematicsI.pdf
An $n$-dim manifold is a a set $M$ which is locally homeomorphic to $\mathbb{R}^n$</description></item><item><title>MAP estimation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/map-estimation/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/map-estimation/</guid><description>Source: Forster 2017 IMU Preintegration Factor graph: way of representing the posterior probability of the states given the available measurements and priors The terms in the equation above are called &amp;lsquo;factors&amp;rsquo;
MAP: maximum a posteriori We want to maximise the probability derived above &amp;ndash;&amp;gt; MAP estimate (aka minimum of negative log posterior) The negative log posterior can be written as a sum of squared residuals, assuming zero-mean Gaussian noise residual errors (prior, IMU, camera) covariance matrices How do we define these residuals?</description></item><item><title>SE(3) Special Euclidian Group</title><link>https://salehahr.github.io/zettelkasten/rotations/se3-special-euclidian-group/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/se3-special-euclidian-group/</guid><description>Source: Forster 2017 &amp;ndash; IMU Preintegration Group of rigid motion in 3D.
Consists of
a rotation in SO(3) a translation in $\mathbb{R}^3$ $$\begin{aligned} \text{SE}(3) \dot{=} \left\lbrace \left(\mathbf{R}, \mathbf{p} \right) : \mathbf{R} \in \text{SO}(3), \mathbf{p} \in \mathbb{R}^3 \right\rbrace \end{aligned}$$
$$\begin{aligned} \mathbf{T}_1\mathbf{T}_2 &amp;amp;= \left( \mathbf{R}_1\mathbf{R}_2, \mathbf{R}_1\mathbf{p}_2 + \mathbf{p}_1 \right)\
\mathbf{T}_1^{-1} &amp;amp;= \left( \mathbf{R}_1^\text{T}, -\mathbf{R}_1^{\text{T}}\mathbf{p}_1 \right) \end{aligned}$$</description></item><item><title>SO(3) 3D rotation group</title><link>https://salehahr.github.io/zettelkasten/rotations/so3-3d-rotation-group/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/rotations/so3-3d-rotation-group/</guid><description>Parent: Rotations / SO(3) group index See also: Orientation parametrisations , Linearisation of an orientation , Solà 2017 quaternion kinematics for eskf Source: MKok 2017 All orthogonal matrices with dim 3x3 have the property
$$RR^\text{T} = R^\text{T}R = I_3$$ They are part of the orthogonal group O(3) If, additionally, $\det R = 1$, then the matrix belongs to SO(3) and is a rotation matrix Source: http://en.wikipedia.org/wiki/3D_rotation_group The SO(3) group</description></item><item><title>Spaces in mathematics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/spaces-in-mathematics/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/spaces-in-mathematics/</guid><description>Source: http://upload.wikimedia.org/wikiversity/en/c/cd/Spaces_in_mathematics.pdf Types of spaces in mathematics
Euclidian spaces (3D space, 2D space/Euclidian plane) Linear spaces Topological spaces Hilbert spaces etc. What is a space?
No real definition Made of selected mathematical objects which are treated as points selected relationships between these points Points can be elements of a set functions subspaces Isomorphic spaces are considered identical Isomorphism between two spaces: one-to-one mapping between the points, that preserves the relationships between the points</description></item><item><title>System in a VIN problem with IMU preintegration</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/system-in-a-vin-problem-with-imu-preintegration/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/system-in-a-vin-problem-with-imu-preintegration/</guid><description>Source: Forster 2017 IMU Preintegration State x_i of the system at time i with All keyframes up till time k State of all keyframes camera measurements IMU measurements between KFs i and j (consecutive) Set of measurements up till time k IMU pose: , maps a point in B to W</description></item><item><title>Non-Rigid Guided Matching (b/w KFs) in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/non-rigid-guided-matching-b-w-kfs-in-defslam/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/non-rigid-guided-matching-b-w-kfs-in-defslam/</guid><description>Source: lamarca-2020 Matching between keyframes (used in deformation mapping in DefSLAM) Use an estimated warp as a reference To increase number of matches in the covisible keyframes Process Matches are given by deformation tracking Estimate an initial warp between k and k* (covisible keyframes) how? Using this initial warp, estimate where a point would be seen in k* Define a search region around thesse estimated positions.</description></item><item><title>Surface alignment in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/surface-alignment-in-defslam/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/surface-alignment-in-defslam/</guid><description>Parent: Mapping step-by-step in DefSLAM Source: lamarca-2020 Goal: to scale the up-to-scale surface (output of NRSfM ) to the proper dimensions get an idea of the proper dimensions from the already estimated map i.e. resulting surface must match the scale of the template $\mathcal{T}_{k-1}$ $\mathcal{T}{k-1}$: deformed map generated by the tracker at the instance of KF $=k$ insertion, with shape-at-rest of $\mathcal{S}{k-1}$ generated from KF: $(k-1)$ result: scale-corrected shape-at-rest $\mathcal{S}_k$ Method: alignment of the map points using Sim(3)</description></item><item><title>Template substitution in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/template-substitution-in-defslam/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/template-substitution-in-defslam/</guid><description>Parent: Mapping step-by-step in DefSLAM Source: lamarca-2020 Tracking runs at frame-rate, and mapping at keyframe-rate Tracking processes Nm frames during a whole mapping run Process New keyframe $k$ is made. Now at time $t=k$ At this point, the template in the tracking is still based on the old shape-at-rest, S_(k-1) Mapping thread starts creates surface S_k which is aligned to prev. template T_(k-1) k is set as the reference keyframe from S_k, create template T_k and from now on use this template instead of the old one T_(k-1) At time t=k+Nm, use data from the tracking thread image points at t=k+Nm deform the recently computed template T_k based on these images use SfT but neglecting the temporal term (to allow large deformation, &amp;ldquo;as a lot might have happened in the time span of Nm&amp;rdquo;) so now we get a T_k that is deformed (updated) to the most recent image points we do this extra step instead of passing T_k (from step 1) to the tracker immediately because, due to the new points occurring at t=k+Nm, using the original T_k might lead to data association errors mapper passes the new template T_k (t=k+Nm) to the tracker</description></item><item><title>The making of EndoSLAM dataset</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/the-making-of-endoslam-dataset/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/the-making-of-endoslam-dataset/</guid><description>http://www.youtube.com/watch?v=G_LCe0aWWdQ Github: http://github.com/CapsuleEndoscope/EndoSLAM</description></item><item><title>Camera calibration</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/camera-calibration/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/camera-calibration/</guid><description>Parent: SLAM Index Source: http://de.mathworks.com/help/vision/ug/camera-calibration.html
estimates lens/sensor parameters e.g. to correct lens distortion, determine position, measurement etc there are several camera models, e.g. fisheye, pinhole Camera parameters
intrinsic extrinsic distortion coefficients How to solve for camera parameters?
Need to have 3D world points and the corresponding 2D image points Take multiple images of a calibration pattern to obtain these correspondences With the mapping 3Dp -&amp;gt; 2Dp, solve for camera parameters Evaluate accuracy of estimated camera parameters:</description></item><item><title>Data association in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/data-association-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/data-association-in-defslam/</guid><description>Source: Lamarca 2020 DefSLAM See also: Data association Goal: match keypoints in current frame (newly extracted) with map points (already in map/system) Use the active matching strategy proposed in [Agudo 2015]: “Simultaneous pose and non-rigid shape with particle dynamics,” Steps  ORB points (keypoints) are detected in current frame
Camera pose Tcw is predicted
using camera motion model camera motion model: function of past camera poses Predict where map points (existing in map) would be imaged, based on last estimated template i.</description></item><item><title>DefSLAM framework</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-framework/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-framework/</guid><description>Source: Lamarca 2020 DefSLAM See also: template-substitution-in-defslam &amp;ldquo;Fusion of the methods available for processing non-rigid monocular scenes&amp;rdquo;
Deformation tracking [front end]
estimates/recovers/optimises: camera pose scene deformation / deformation of map points (observations) the map points are then embedded into the template Tk (to compute their position on the surface) operates at frame rate SFT-based ( shape from template ), requires prior geometry (template of scene at rest) for the currently being viewed map Map points are deformed (updated) by solving an optimisation problem min { reprojection error + deformation energy } per frame Deformation mapping [back end]</description></item><item><title>Initialisation of monocular SLAM</title><link>https://salehahr.github.io/zettelkasten/SLAM/initialisation-of-monocular-slam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/initialisation-of-monocular-slam/</guid><description>Source: Lamarca 2019 DefSLAM Depth information has to be generated before localisation can be performed — how?
Capture multiple images which have enough parallax These images with parallax allows depth information to be calculated (this uses motion parallax ) From these images, the map can be generated Localisation can then be carried out with respect to the map (as long as camera doesn&amp;rsquo;t move off to an unexplored region)</description></item><item><title>Mapping step-by-step in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-step-by-step-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-step-by-step-in-defslam/</guid><description>Source: lamarca-2020 Parent: DefSLAM framework Steps Recover warps between k and k* (s. [Non-Rigid Guided Matching (b/w KFs) in DefSLAM](non-rigid guided-matching-(b_w-kfs)-in-defslam.md)) with k: anchor keyframes, i.e. KFs where one of the observed map points was initialised with k*: set of best covisible keyframes warps: transformation between the images Ik to Ik* In DefSLAM, Schwarps (a family of warps using 2D Schwarzian equation regularisers) is used Schwarps has something to do with the infinitesimal planarity assumption of NRSfM [ NRSfM ] Process k* to get estimate of an up-to-scale surface  Input of NRSfM: warps [ Surface alignment ] Up-to-scale surface (\hat{S}_k) is aligned with the whole map in order to obtained the scaled surface Sk w.</description></item><item><title>Non-rigid Surface from Motion</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm/</guid><description>Notes:
Original NRSFM paper?
https://www.cs.dartmouth.edu/~lorenzo/Papers/TorrHertzBreg-pami08.pdf A Phd thesis [Kumar] https://openresearch-repository.anu.edu.au/handle/1885/164278?mode=full Source: Kumar
The problem with dynamic or non-rigid scenes:
if we project a scene point into a camera image plane, there will be several possible 3D configurations! Allowing arbitrary deformations makes the 3D reconstruction an ill posed problem (underconstrained) &amp;ndash;&amp;gt; need to make additional assumptions about the object or scene (make more constraints)! Source: lamarca-2020 See also: nrsfm-in-defslam , sfm NRSfM (non-rigid structure from motion) batch processing of images to recover deformation computationally demanding — slower than SfT Orthographic NRSfM usually fails with very large deformations uses an orthographic camera projection/model (weak approximation to the perspective camera) — a limitation, as many vision-related applications have a significant perspective effect exploits spatial constraints temporal constraints spatiotemporal constraints usually ok for small deformations, but not for very large deformations Perspective NRSfM the perspective camera model is more accurate than the orthographic one also uses the isometry assumption (as in SfT methods), which has produced good results in NRSfM Parashar 2018 &amp;ldquo;Isometric NRSfM&amp;rdquo; [6] local method that handles occlusions and missing data well</description></item><item><title>NRSfM in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm-in-defslam/</guid><description>Parent: Mapping step-by-step in DefSLAM Source: lamarca-2020 See also: NRSfM Assumptions Isometric deformation Infinitesimal planarity [DEF]: any surface can be approximated as a plane at infinitesimal level, all the while maintaining its curvature at a global level Locality The method used here is a local method &amp;ndash;&amp;gt; implies that it handles missing data and occlusions inherently
surface deformation is modelled locally for each point, under the above assumptions Embedding, $\phi_k$ of the scene surface is a parametrisation — transforms an image point to a point on a 3D surface uses the normalised coordinates of the image Ik (xhat, yhat) Procedure A point is matched in more than two keyframes (warps are used in the matching process )</description></item><item><title>ORBSLAM2 mods</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam2-mods/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam2-mods/</guid><description>Patch to work with opencv4 http://github.com/Windfisch/ORB_SLAM2 ORBSLAM2 Python bindings http://github.com/jskinn/ORB_SLAM2-PythonBindings</description></item><item><title>Parallax</title><link>https://salehahr.github.io/zettelkasten/definitions/parallax/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/definitions/parallax/</guid><description>Source: https://en.wikipedia.org/wiki/Parallax
See also: motion parallax Definition: The difference in the apparent position of an object viewed from two different positions
This difference is given by the angle between the two lines of sight Binocular vision uses parallax in the overlapping fields of vision in order to gain depth perception Distance measurement (i.e. depth from the viewer) via parallax is based on the principle of triangulation (uses trigonometry, s. Monocular depth perception in humans )</description></item><item><title>Pinhole camera projection function</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/pinhole-camera-projection-function/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/pinhole-camera-projection-function/</guid><description>Backlinks: Pinhole camera model See also: World to camera trafo Source: Mur-Artal 2017 VI-ORB 3D points Projection function  Transforms 3D points into 2D points on image plane Focal length:  Principal point:  The projection does not consider the distortion due to the lens
therefore when extracting image features, first undistort their coordinates only then match to projected points (existing features which have undergone projection from 3D to 2D) Source: Lamarca 2019 DefSLAM 3D point: Projection function maps</description></item><item><title>Shape from Motion</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sft/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sft/</guid><description>Initial paper?: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010934https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010934
Goal reconstruct the surface of an object
reference 3D shape (template) of the object is available under a specific deformation constraint Source: lamarca-2020 SFT (shape from template) uses only a single image — faster than nrsfm lower computational cost must have a known 3D template (textured model) SfT methods require: 1 monocular image 1 textured shape at rest (template) &amp;ldquo;geometry&amp;rdquo; as the deformation model different definitions of the deformation model analytic, e.</description></item><item><title>Template in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/template-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/template-in-defslam/</guid><description>Source: Lamarca 2019 DefSLAM Template
2D triangular mesh floating in the 3D space consists of a set of 2D triangular facets F a facet has 3 nodes (set V) and 3 edges (set E) map points observed in keyframe k are embedded in the facets Map point coordinates in barycentric coordinates</description></item><item><title>Tracking optimisation in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-optimisation-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-optimisation-in-defslam/</guid><description>Source: lamarca-2020 Optimisation function Minimises reprojection error (in the image) deformation energy (of the template) boundary nodes of the local zone are fixed (i.e. not set as arguments to the optimisation function) this makes the absolute camera pose observable how? in order to constrain the gauge freedoms Initial guess: values from previous optimisation (i.e. previous frame: t-1) Reprojection error robust against outliers due to Huber robust kernel</description></item><item><title>Bag of words</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/bag-of-words/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/bag-of-words/</guid><description>Parent: SLAM Index Source: http://towardsdatascience.com/bag-of-visual-words-in-a-nutshell-9ceea97ce0fb
Has its origins in natural language processing (NLP), information retrieval
A text can be seen as a bag of words, with each word having different frequencies from one another This can be used to compare and classify texts (similar histograms) In vision Instead of words we have features (identifying pattern in an image) An image is represented as a set of features Features consist of Keypoints: points that are invariant to transformation Descriptors : description of the keypoint, for feature representation Construct a frequency histogram of features in the image Workflow Feature detection/extraction &amp;ndash;&amp;gt; build vocabulary/codewords &amp;ndash;&amp;gt; make histogram = BoW</description></item><item><title>DefSLAM dependency/inheritance diagram</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-dependency-inheritance-diagram/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-dependency-inheritance-diagram/</guid><description/></item><item><title>Forster 2017 IMU Preintegration</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/forster-2017-imu-preintegration/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/forster-2017-imu-preintegration/</guid><description>Authors: Forster et al
Abstract:
First contribution: preintegration theory (building up on Lupton&amp;rsquo;s work) what&amp;rsquo;s different from Lupton&amp;rsquo;s: addresses manifold structure of the rotation group, analytic derivation of all Jacobians Lupton&amp;rsquo;s work uses Euler angles Using Euler angles and techniques of Euclidian spaces for state propagation/covariance estimation is not properly invariant under rigid transformations uncertainty propagation, a-posteriori bias correction same as Lupton: integration performed in local frame, eliminating need for reintegrating when linearisation point changes Second contribution: integration of the preintegrated IMU model into a visual-inertial pipeline The system presented uses incremental smoothing for fast computation of the optimal MAP estimate Uses structureless model (3D landmarks are not part of the variables to be estimated) for visual measurements &amp;ndash;&amp;gt; allows eliminating large numbers of variables Motivation:</description></item><item><title>Handling the computational complexity of optimisation-based SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/handling-the-computational-complexity-of-optimisation-based-slam/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/handling-the-computational-complexity-of-optimisation-based-slam/</guid><description>Parent: SLAM Index Source: Forster 2017 IMU Preintegration Complexity of nonlinear batch optimisation
The trajectory and the map, which comprise the states, grow with time The larger the SLAM problem, the less feasible it is to perform the optimisation in real-time Solutions to improve computational efficiency
Keyframe-based methods: discard frames except for a few selected keyframes Run the optimisation parallelly (e.g. tracking and mapping threads) Fixed-lag smoothing: Use of a local map of fixed size, with marginalisation of the old states (summarise the old states into a prior term) Filtering is a special case of this: window of size 1, i.</description></item><item><title>Visual-inertial datasets</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/visual-inertial-datasets/</link><pubDate>Fri, 06 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/visual-inertial-datasets/</guid><description>http://sites.google.com/view/awesome-slam-datasets/home
http://fpv.ifi.uzh.ch/ Aggressive drone racing http://www.lirmm.fr/aqualoc/ Underwater Monochromatic http://vision.in.tum.de/data/datasets/visual-inertial-dataset TUM indoor/urban, slides fisheye cameraUsed in ORBSLAM3 http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets EUROC MAV stereo, monochrUsed in ORBSLAM3</description></item><item><title>Discussion 2020-11-03</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2020-11-03/</link><pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2020-11-03/</guid><description>Current progress
Find out how g2o works, how the DefSLAM implementation of the tracking optimisation works Incorporate IMU data (s. ORBSLAM3 implementation) IMU initialisation IMU preintegration IMU terms in cost function (which function?) Topics
How g2o works IMU preintegration DefSLAM + IMU cost function Implementation in DefSLAM using g2o [tbd] ORBSLAM3&amp;rsquo;s implementation the IMU cost function terms initialisation preintegration Kalman idea for IMU integration Compile + run</description></item><item><title>DefTracking::MonocularInitialization</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-monocularinitialization/</link><pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-monocularinitialization/</guid><description>Parent: DefTracking::Track Initialises
surface points in the surface If num. features in current frame &amp;gt; 100
set frame pose to origin make new KF (GroundTruthKeyFrame) pKFini add the KF to the map mpMap iterate over the N features get feature kp convert kp to 3d point make new DefMapPoint(3dp, pKFini, mpMap) set pointers between DefMapPoint, GroundTruthKeyFrame, DefMap Save surface using bbs Set mLastFrame := mCurrentFrame Local window: Add KF to local KFs vector, add MapPoints to local MP vector, mpLocalMapper Calculate Tcr from Tcw Initialise SLAM: Set reference KF, reference MapPoints set mState to OK</description></item><item><title>ORBSLAM::Frame constructor (monocular)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam-frame-constructor-monocular/</link><pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam-frame-constructor-monocular/</guid><description>Source: Tracking::GrabImageMonocular Set scale level info from ORB extractor Extract ORB features mvKeys (vector of keypoints/features) Set N number of features Make mvpMapPoints (null, but with size N), mvbOutlier (all entries false, size N) If first frame or calibration change: ComputeImageBounds AssignFeaturesToGrid()</description></item><item><title>Dynamic Bayesian Network formulation of SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/dynamic-bayesian-network-formulation-of-slam/</link><pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/dynamic-bayesian-network-formulation-of-slam/</guid><description>Source: Grisetti 2011 - Tutorial graph-based SLAM Dynamic Bayesian Network
Solution of full SLAM problem: Transition model: Observation model:  The observation model is usually multimodal: a single observation may result in multiple edges (in the spatial graph) Therefore, the Gaussian assumption does not hold</description></item><item><title>Grisetti 2011 - Tutorial graph-based SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/grisetti-2011/</link><pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/grisetti-2011/</guid><description>temp
Backlinks: What is SLAM? Abstract
formulate SLAM using a graph nodes: poses of the robot (as well as landmark postiions) at different points in time edges: constraints between poses come from sensor measurements/observations robot movement/control input constraints can contradict each other, due to effect of noise in sensor readings solve the graph, i.e. compute the map: find the spatial configuration of the nodes that best satisfy the constraints/edges tutorial for back-end (optimisation) part of graph-based SLAM :: Navigation task: requires a map and knowledge of current position relative to locations in the map</description></item><item><title>DefOptimizer::DefPoseOptimization</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defoptimizer-defposeoptimization/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defoptimizer-defposeoptimization/</guid><description>Parent: DefTracking::Track As far as I understand it:
Uses g2o library for the optimisation (graph-based SLAM) cost function terms are converted to edges and nodes each cost function term seems to correspond to an edge in the graph in g2o paper/tutorial: an edge is fully characterised by its error function and its information matrix int DefPoseOptimization(Frame *pFrame, Map *mMap, double RegLap, double RegInex, double RegTemp, uint NeighboursLayers) // define optimiser, set solver optimizer = new &amp;hellip; optimizer.</description></item><item><title>Edges in g2o</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/edges-in-g2o/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/edges-in-g2o/</guid><description/></item><item><title>DefOptimizer::poseOptimization</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defoptimizer-poseoptimization/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defoptimizer-poseoptimization/</guid><description>Parent: DefTracking:TrackWithMotionModel() int DefOptimizer::poseOptimization(Frame *pFrame)
// Set estimate of solution to current camera pose g2o::VertexSE3Expmap *vSE3 = new g2o::VertexSE3Expmap(); vSE3-&amp;gt;setEstimate(Converter::toSE3Quat(pFrame-&amp;gt;mTcw)); vSE3-&amp;gt;setId(0); vSE3-&amp;gt;setFixed(false); optimizer.addVertex(vSE3);
// Set MapPoint vertices (num. nodes in opt. graph?) const int N = pFrame-&amp;gt;N;</description></item><item><title>defSLAM::System constructor</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-system-constructor/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-system-constructor/</guid><description>Parent: DefSLAM simple_camera defSLAM::System::System(const string &amp;amp;strVocFile, const string &amp;amp;strSettingsFile, const bool bUseViewer)  mSensor(MONOCULAR), mpLoopCloser(NULL), mpViewer(static_cast&amp;lt;Viewer *&amp;gt;(nullptr)), mbReset(false), mbActivateLocalizationMode(false), mbDeactivateLocalizationMode(false) Constructor // initialise mpVocabulary from file // create mpKeyFrameDatabase from mpVocabulary // create map DefMap() // create drawers for viewer DefFrameDrawer DefMapDrawer // initialise tracking, mapping, viewer threads; loop closing not implemented in DefSLAM mpTracker = new DefTracking(&amp;hellip;); mpLocalMapper = new DefLocalMapping(&amp;hellip;); mpViewer = new DefViewer(&amp;hellip;);
Attributes eSensor mSensor ORBVocabulary *mpVocabulary KeyFrameDatabase *mpKeyFrameDatabase Map *mpMap // stores pointers to all KFs, all MapPoints</description></item><item><title>defSLAM::System::TrackMonocular</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-system-trackmonocular/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-system-trackmonocular/</guid><description>Parent: DefSLAM simple_camera cv::Mat defSLAM::System::TrackMonocular cv::Mat Tcw = mpTracker-&amp;gt; GrabImageMonocular (im, timestamp);
// get information from mpTracker: // get states mTrackingState // get map points mTrackedMapPoints // get key points mTrackedKeyPointsUn
// return camera pose return Tcw;</description></item><item><title>DefTracking::Track</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-track/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-track/</guid><description>Parent: Tracking::GrabImageMonocular void DefTracking::Track // if: not initialised, do: monocular initialisation // elseif: already initialised, do: track frame { // if: tracking and mapping, do: bOK = TrackWithMotionModel ();
// if bOK (there exists camera pose estimate and matching), track local map // if template is updated (keyframe-rate update) set reference KF from new template do DefPoseOptimization (&amp;hellip;); bOK = TrackLocalMap();
// if: bOK, update motion model (update mVelocity); clean VO matches // check if we should insert a new KF, delete outliers for BA</description></item><item><title>DefTracking:TrackWithMotionModel()</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-trackwithmotionmodel-/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-trackwithmotionmodel-/</guid><description>Parent: DefTracking::Track // Initial tracking to locate rigidly the camera and discard outliers. bool DefTracking::TrackWithMotionModel()
// Update last frame relative pose according to its reference keyframe UpdateLastFrame();
// Project points seen in prev frames int th = 15; int nmatches = Defmatcher.SearchByProjection(*mCurrentFrame, mLastFrame, th, mSensor == System::MONOCULAR);
// Optimise frame pose with all matches to initialise camera pose Optimizer:: poseOptimization (mCurrentFrame, myfile);
// Discard outliers
// return: sufficient number of matches?</description></item><item><title>Tracking_GrabImageMonocular</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-grabimagemonocular/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-grabimagemonocular/</guid><description>Parent: defSLAM::System::TrackMonocular cv::Mat ORB_SLAM2::Tracking::GrabImageMonocular
colour conversion make frame using image, timestamp, ORB stuff, calibration data, etc. mCurrentFrame = new Frame (mImGray, timestamp, mpORBextractorLeft, mpORBVocabulary, mK, mDistCoef, mbf, mThDepth, im)
perform tracking: Track (); return camera pose return mCurrentFrame-&amp;gt;mTcw.clone();</description></item><item><title>(Mur-Artal 2017) VI-ORB</title><link>https://salehahr.github.io/zettelkasten/bibliography/mur-artal-2017-vi-orb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/mur-artal-2017-vi-orb/</guid><description>URL: http://ieeexplore.ieee.org/abstract/document/7817784
Authors: Mur-Artal, Tardós
Code: http://paperswithcode.com/paper/visual-inertial-monocular-slam-with-map-reuse
Results (video): http://www.youtube.com/watch?v=JXRCSovuxbA
Abstract current VI odometry approaches: drift accumulates due to lack of loop closure therefore there is a need for tightly-coupled VI-SLAM with loop closure and map reuse here: focus on monocular case, but applicable to other camera configurations builds on ORB-SLAM (from same author) IMU initialisation method (initialises: scale, gravity direction, velocities, gyroscope bias, accelerometer bias) depends on visual monocular initialisation (coupled initialisation) Other works: recent tightly-coupled VIO (both filtering- and optimisation-based) lack loop closure, so drift accumulates</description></item><item><title>Badias 2020 MORPH-DSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/badias-2020-morph-dslam/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/badias-2020-morph-dslam/</guid><description>URL: http://arxiv.org/pdf/2009.00576.pdf Video: http://www.youtube.com/watch?v=P_QN8Nv&amp;ndash;_g To read!</description></item><item><title>IMU index</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/imu-index/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/imu-index/</guid><description>Parent: SLAM Index General
IMU Gyroscope Odometry Why use the visual-inertial sensor combination? IMU to camera coordinate transformations phils-lab-sensor-fusion sensor-fusion Practical
Converting IMU data to inertial frame Modelling
Probabilistic models for IMU [Choice of model for the IMU motion model](choice of model-for-the-imu-motion-model.md) * [Choice of states for the IMU motion/kinematics model](choice of states-for-the-imu-motion_kinematics-model.</description></item><item><title>Loop closing in VIORB</title><link>https://salehahr.github.io/zettelkasten/SLAM/loop-closing-in-viorb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/loop-closing-in-viorb/</guid><description>Source: Mur-Artal 2017 VI-ORB See also: Loop closure detection (general) Overview To reduce drift accumulated during exploration (when returning to an already mapped location) Loop detection: of large loops using place recognition Loop correction: first do lightweight pose-graph optimisation (PGO), then do full BA in a separate thread (in order not to interfere with real-time operations) Implementation After loop detection: do match validation (alignment of points between keyframes) Then pose-graph optimisation to reduce the accumulated error in trajectory (PGO: pose-only, ignores IMU info) IMU info ignored, but velocities are corrected by rotating them according to keyframe orientation &amp;ndash;&amp;gt; suboptimal, but should be accurate enough to allow IMU data to be used right after the PGO in ORBSLAM: PGO is 7-DoF optimisation (due to scale + 3 rot + 3 xyz) in VIORB, 6 DoF (scale is known from initialisation bzw.</description></item><item><title>Mapping in VIORB</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-in-viorb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-in-viorb/</guid><description>Source: Mur-Artal 2017 VI-ORB Mapping in VIORB Previously in ORBSLAM (only poses are optimised): Now in VIORB, more states to optimise: Increase in complexity
more states to optimise (v, b) IMU measurements creates constraints between keyframes Original ORBSLAM discards redundants KFs (poses a problem with IMU constraints!) Workaround: in local BA, only allow discarding of KF if, after discarding, the time between two consecutive KFs is short enough (&amp;lt;= 0.</description></item><item><title>Pinhole camera model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/pinhole-camera-model/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/pinhole-camera-model/</guid><description>Parent: SLAM Index Backlinks: [Camera calibration](camera calibration.md), [pinhole camera projection function](pinhole camera projection function.md), [weiss thesis vision based navigation for micro helicopters](weiss thesis vision-based-navigation-for-micro-helicopters.md) See also: World to camera trafo Source: http://de.mathworks.com/help/vision/ug/camera-calibration.html Does not account for lens distortion (ideal pinhole camera doesn&amp;rsquo;t have a lens) To represent a real camera, the full camera model to be used should include (radial and tangential) lens distortion, (such as the one used in the MATLAB computer vision toolbox)</description></item><item><title>Preintegration of IMU</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/preintegration-of-imu/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/preintegration-of-imu/</guid><description>Parent: IMU Backlinks: IMU states, dynamics equations IMU measurements arrive at a higher frequency (frame rate) compared to camera captures (keyframe rate) IMU measurements constrain consecutive states We want to summarise these &amp;lsquo;in-between&amp;rsquo; IMU measurements into one single relative motion constraint between keyframes</description></item><item><title>Tracking in VIORB</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-in-viorb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-in-viorb/</guid><description>Source: Mur-Artal 2017 VI-ORB Tracking in VIORB
Visual-inertial tracking at frame rate, instead of using an ad-hoc motion model as in the original ORB-SLAM Tracked states: [sensor pose (R, p), velocities v, biases b] Once the camera pose is predicted, map points are projected, then matches with existing features on the frame Then optimise the current frame j, depending on whether the map has just been updated the map is unchanged Here, the optimisation function for tracking (when map unchanged) is:</description></item><item><title>Why use the visual-inertial sensor combination?</title><link>https://salehahr.github.io/zettelkasten/SLAM/why-use-the-visual-inertial-sensor-combination/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/why-use-the-visual-inertial-sensor-combination/</guid><description>See also: Multisensor fusion Source: Mur-Artal 2017 VI-ORB Cheap but also with good potential Cameras provide rich information but are relatively cheap IMU provides self-motion info, helps recover scale in monocular applications enables estimation of the direction of gravity &amp;ndash;&amp;gt; renders pitch and roll observable Source: Forster 2017 IMU Preintegration Visual-inertial fusion for 3D structure and motion estimation Both cameras and IMUs are cheap, easy to find and complement each other well Camera exteroceptive sensor measures, up to a to-be-determined metric scale, appearance and geometrical structure of a 3D scene IMU interoceptive sensor makes metric scale of monocular cameras, as well as the direction of gravity, observable Source: (Wu 2018) Image-based camera localization Cameras provide rich information of a scene IMU provide odometry self-motion information and accurate short-term motion estimates at high frequency Source: Mirzaei 2008 A Kalman Filter-Based Algorithm for IMU-Camera Calibration: Observability Analysis and Performance Evaluation</description></item><item><title>DefSLAM simple_camera</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-simple-camera/</link><pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-simple-camera/</guid><description>Without ground truth
App run // Create defSLAM::system, which initializes all threads (local mapping, loop closing, viewer) defSLAM::System SLAM(orbVocab, calibFile, bUseViewer);
// Timestamp uint timestamp := 0;
// Process frames from video capture while (capture.isOpened()) { // Get the capture as a matrix; // SLAM SLAM. TrackMonocular (img_matrix, timestamp); timestamp++; }</description></item><item><title>(Liu 2020) Learned Descriptor</title><link>https://salehahr.github.io/zettelkasten/bibliography/liu-2020/</link><pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/liu-2020/</guid><description>Note: I&amp;rsquo;m only reading this paper for the into to SLAM/SfM
Abstract Problem: 3D reconstuction has subpar performance when dealing with endoscopic videos, partly due to local descriptors &amp;hellip; Introduction Correspondence estimation: match between 2D points in image and corresponding 3D location (s. registration ) Correspondence estimation is needed by SfM, SLAM, &amp;hellip; SfM + SLAM combination has been shown to be effective for surgical navigation in endoscopy &amp;ndash; simultaneous estimation of sparse 3D structure of the observed scene camera trajectory Complementarity of SfM + SLAM Good camera tracking requires dense 3D reconstruction</description></item><item><title>Structure from Motion</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sfm/</link><pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sfm/</guid><description>Source: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Schonberger_Structure-From-Motion_Revisited_CVPR_2016_paper.pdf
Note:
This paper uses incremental SfM Corresponding paper for COLMAP Structure from motion Reconstruction of 3D structure from a sequence of 2D images of that structure, taken from different viewpoints.
Search for correspondence between images &amp;ndash;&amp;gt; output: scene graph (nodes: images, edges: verified pairs) Feature extraction Feature matching Output: set of image pairs and their associated feature correspondences Verification: do features map to the same scene point?</description></item><item><title>OptiTrack in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/optitrack-in-sofa/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/optitrack-in-sofa/</guid><description> Using OptiTrackNatNet C++ implementation? http://www.sofa-framework.org/community/doc/programming-with-sofa/create-your-scene-in-cpp/ XML scenes? OptiTrack + Python otnn_client = root.addObject(&amp;lsquo;OptiTrackNatNetClient&amp;rsquo;, name=&amp;lsquo;otnnClient&amp;rsquo;) &amp;lt;Sofa.Core.Object&amp;gt; dir(otnn_client) [&amp;lsquo;bbox&amp;rsquo;, &amp;lsquo;clientName&amp;rsquo;, &amp;lsquo;componentState&amp;rsquo;, &amp;lsquo;drawOtherMarkersColor&amp;rsquo;, &amp;lsquo;drawOtherMarkersSize&amp;rsquo;, &amp;lsquo;drawTrackedMarkersColor&amp;rsquo;, &amp;lsquo;drawTrackedMarkersSize&amp;rsquo;, &amp;lsquo;listening&amp;rsquo;, &amp;lsquo;name&amp;rsquo;, &amp;lsquo;otherMarkers&amp;rsquo;, &amp;lsquo;printLog&amp;rsquo;, &amp;lsquo;scale&amp;rsquo;, &amp;lsquo;serverName&amp;rsquo;, &amp;lsquo;tags&amp;rsquo;, &amp;lsquo;trackedMarkers&amp;rsquo;] bold: not in API http://www.sofa-framework.org/api/master/plugins/OptiTrackNatNet/html/class_sofa_opti_track_nat_net_1_1_opti_track_nat_net_client.html difference between client name and server name</description></item><item><title>Cheatsheet</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cheatsheet/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cheatsheet/</guid><description>Parent: [SofaPython Index](SofaPython Index.md)
Imports/Plugins import SofaRuntime from SofaRuntime import PluginRepository PluginRepository.addFirstPath(SOFA_INSTALL_DIR) SofaRuntime.importPlugin(&amp;ldquo;SofaComponentAll&amp;rdquo;) SofaRuntime.importPlugin(&amp;ldquo;SofaPython3&amp;rdquo;) SofaRuntime.importPlugin(&amp;ldquo;SofaOpenglVisual&amp;rdquo;)
From root root = Sofa.Core.Node(&amp;ldquo;root&amp;rdquo;) c = root.createObject(&amp;ldquo;MechanicalObject&amp;rdquo;, name=&amp;ldquo;t&amp;rdquo;, position=[ [0, 0, 0], [1, 1, 1], [2, 2, 2]]) c1 = root.addObject(&amp;ldquo;MechanicalObject&amp;rdquo;, name=&amp;ldquo;c1&amp;rdquo;)
From nonroot nonroot_node = Sofa.Core.Node(&amp;ldquo;a_node&amp;rdquo;) nonroot_node.addObject(&amp;ldquo;MechanicalObject&amp;rdquo;, name=&amp;ldquo;c1&amp;rdquo;) .addData(&amp;ldquo;d&amp;rdquo;, value=&amp;ldquo;coucou&amp;rdquo;, type=&amp;ldquo;string&amp;rdquo;) .addData(&amp;ldquo;data1&amp;rdquo;, value=&amp;quot;@/c1.d&amp;quot;) # @ is root
Add data from relative/absolute paths c4.addData(&amp;lsquo;data1&amp;rsquo;, value=&amp;quot;@/n1/c3.data1&amp;quot;) # absolute path (chained) c4.addData(&amp;lsquo;data2&amp;rsquo;, value=&amp;quot;@../n1/c3.data1&amp;quot;) # relative path (down, chained) c1.</description></item><item><title>Someone's SP3 setup</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/someones-sp3-setup/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/someones-sp3-setup/</guid><description>Parent: SofaPython Index http://gist.github.com/pedroperrusi/9fdd4257db72465c8fb481381f396c51</description></item><item><title>Initialising graph in SP3</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/initialising-graph-in-sp3/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/initialising-graph-in-sp3/</guid><description>Parent: SofaPython Index http://github.com/SofaDefrost/plugin.SofaPython3.deprecated/pull/110
&amp;ldquo;Can you share an example of a scene and a component you have in mind ? Because currently to summary the discussion during sofa-meeting the problem with init/bdwInit/reinit is that it that this is severely ill defined and we are considering to totally remove that from Sofa and use alternatives pattern among which:
have an onSimulationStart / onSimulationStop event to detect when the simulation is on or not avoid using getContext() to fetch other components unless you store them in SingleLink.</description></item><item><title>Covisible keyframes</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/covisible-keyframes/</link><pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/covisible-keyframes/</guid><description>Source: Palafox 2019 ( thesis ) Backlinks: Lamarca 2019 DefSLAM Two keyframes are covisible if they share several common landmarks.</description></item><item><title>Pipenv venv in project folder</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/pipenv-venv-in-project-folder/</link><pubDate>Sun, 06 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/pipenv-venv-in-project-folder/</guid><description>PIPENV_VENV_IN_PROJECT=1
Powershell: $Env:PIPENV_VENV_IN_PROJECT=&amp;ldquo;1&amp;rdquo;</description></item><item><title>50.2.30 Multivariate Kalman filter algorithm</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.30-multivariate-kalman-filter-algorithm/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.30-multivariate-kalman-filter-algorithm/</guid><description>Parent: Multivariate Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Initialisation
Initialise filter state Initialise belief in the state Predict
Propagate state to the next time step using the system model [prediction] Adjust belief to take into account the prediction uncertainty [prior] Update
Obtain measurement and associated belief about its accuracy Calculate residual (prior - measurement) Calculate scaling factor/Kalman gain Set estimated state to be on the residual line based on the scaling factor Update the belief in the state based on measurement certainty Designing the measurement function</description></item><item><title>Error ellipse/Confidence ellipse</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/error-ellipse-confidence-ellipse/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/error-ellipse-confidence-ellipse/</guid><description>Parent: Multivariate Gaussian distributions Source: rlabbe Kalman/Bayesian filters in Python Any slice through a multivariate Gaussian is an ellipse Plots show the slice for 3 standard deviations
Showing correlation using error ellipses</description></item><item><title>Hidden variables in a multivariate Kalman filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/hidden-variables-in-a-multivariate-kalman-filter/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/hidden-variables-in-a-multivariate-kalman-filter/</guid><description>Parent: Multivariate Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Example: Blue error ellipse:
Certainty in position x=0 No idea about the velocity (long in y-axis) We know that position and velocity are correlated, i.e. the next position depends on the current velocity value (red error ellipse — likelihood/prediction for the next step) e.g. if v=5m/s, the next position is 5m +- position uncertainties
We get a position update (new blue error ellipse) The new covariance (posterior) is obtained by multiplying the previous two covariances —&amp;gt; intersection The posterior&amp;rsquo;s tilt implies that there is some correlation between position and velocity Not only are we now more certain about the velocity, but our position certainty also increases (compared to not considering the velocity at all)!</description></item><item><title>Multivariate Kalman filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/multivariate-kalman-filters/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/multivariate-kalman-filters/</guid><description>Parent: rlabbe Kalman/Bayesian filters in Python [Hidden variables in a multivariate Kalman filter](hidden variables-in-a-multivariate-kalman-filter.md)
Here:
Focus is on a subset of problems describable using Newton&amp;rsquo;s equations of motion Discretised continuous-time kinematic filters Multivariate Kalman filter algorithm Designing the filter
State (x, P) Process (F, Q) Measurement (z, R) Measurement function H Control inputs (B, u) Assumptions of the Kalman filter
The sensors and motion model have Gaussian noise Everything is linear If the assumptions are true, then the Kalman filter is optimal in a least squares sense</description></item><item><title>Showing correlation using error ellipses</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/showing-correlation-using-error-ellipses/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/showing-correlation-using-error-ellipses/</guid><description>Parent: Error ellipse/Confidence ellipse Source: rlabbe Kalman/Bayesian filters in Python A slanted ellipse implies correlation
The &amp;lsquo;thinner&amp;rsquo; side isn&amp;rsquo;t necessarily more accurate, it just means that the spread of data is reduced along this dimension (when viewing sensor data, for example) Example First epoch Yellow: prior (very uncertain about position) Green: evidence (more accurate in one of the dimensions than the other; more certainty compared to prior) Blue: posterior via multiplication Posterior retains the shape of the evidence (which has more certainty than the prior)</description></item><item><title>50.2. Bayes' Theorem</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.-bayes-theorem/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.-bayes-theorem/</guid><description>Parent: Bayesian Filter Update Step Source: rlabbe Kalman/Bayesian filters in Python How do we compute the probability of an event given previous information? (s. also Frequentist vs Bayesian statistics )
Formula to compute new information into existing information
Used in the update step of a Bayesian filter (valid for both probabilities as well as probability distributions) where || . || expresses normalisation
B Evidence (sensor measurements z) p(A) Prior p(B|A) Likelihood p(A|B) Posterior In filtering systems, computing p(x|z) is nearly impossible, but computing p(z|x) is fairly straightforward, which then facilitates the computation of p(x|z) via the Bayes' theorem formula.</description></item><item><title>50.2.10 Discrete Bayesian filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10-discrete-bayesian-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10-discrete-bayesian-filter/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python The Kalman filter is a subset of Bayesian filters
Predict and update steps like in the g-h filter Here: error percentages are used to implicitly compute the g and h parameters Steps
[Initialise our belief in the state] The predict step always degrades our knowledge (belief/prior) However, in the update step , we add another measurement. This, will always improve our knowledge regardless of noise, enabling convergence Limitations of the discrete Bayes filter</description></item><item><title>50.2.10.1 Discrete Bayesian Filter Predict Step</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10.1-discrete-bayesian-filter-predict-step/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10.1-discrete-bayesian-filter-predict-step/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python The predict step uses the total probability theorem.
Computes total probability of multiple possible events Uses the system model (propagates the states from prev. time step [posterior] to the next one); prediction Accounts for the uncertainty (kernel) in the prediction: produces a prior Generalise the uncertainty using a kernel (distributes the uncertainty over a range around the prediction) Integrate the kernel into the calculations by using convolution * Convolving the &amp;ldquo;current probabilistic estimate&amp;rdquo; with the &amp;ldquo;probabilistic estimate of how much we think the position has changed&amp;rdquo; (from system model) The prior is a &amp;lsquo;degraded&amp;rsquo; version of the belief i.</description></item><item><title>50.2.10.2 Bayesian Filter Update Step</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10.2-bayesian-filter-update-step/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10.2-bayesian-filter-update-step/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python The update step uses Bayes' Theorem Produces the posterior by using the likelihood and the prior Also incorporates sensor data (measurements), as the measurements go into the likelihood calculation Update algorithm
Get a measurement, and associated belief about its accuracy Compute likelihood from the measurement and the measurement accuracy assumption Update the posterior using the likelihood and the prior</description></item><item><title>50.2.20 1D Kalman filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.20-1d-kalman-filters/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.20-1d-kalman-filters/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python [Deriving Kalman filter from Discrete Bayes using Gaussians](deriving kalman filter-from-discrete-bayes-using-gaussians.md) 1D Kalman filter algorithm Kalman gain using Gaussians Variance of the 1D Kalman filter Factors affecting Kalman filter performance</description></item><item><title>50.2.20.1 1D Kalman filter algorithm</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.20.1-1d-kalman-filter-algorithm/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.20.1-1d-kalman-filter-algorithm/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Initialisation:
Initialise the state of the filter Initialise the belief in the state Predict step:
Gaussian addition prior = predict(x, process_model) Incorporate process variance in order to prevent smug filtering Update step:
Gaussian multiplication likelihood = gaussian(z, sensor_var) x = update(prior, likelihood) The output of both steps is a Gaussian probability distribution N(mean, var)</description></item><item><title>Central Limit Theorem</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/central-limit-theorem/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/central-limit-theorem/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python If we make many measurements, the measurements will be normally distributed. (only applies under certain conditions)</description></item><item><title>Computational properties of Gaussian distributions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/computational-properties-of-gaussian-distributions/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/computational-properties-of-gaussian-distributions/</guid><description>Parent: Gaussian distribution Backlinks: Showing correlation using error ellipses Source: rlabbe Kalman/Bayesian filters in Python g1 + g2 = g3; all are Gaussians g1 * g2 = g3; g3 is not Gaussian, but proportional to a Gaussian Sum of two Gaussians Product of two Gaussians: Product of multidimensional Gaussians:</description></item><item><title>Correlation and independence</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/correlation-and-independence/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/correlation-and-independence/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python Independent variables are uncorrelated. But the reverse is not always true: uncorrelated variables may be dependent on one another e.g. y=x^2 has no [linear] correlation, but y depends on x nonetheless</description></item><item><title>Deriving Kalman filter from Discrete Bayes using Gaussians</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/deriving-kalman-filter-from-discrete-bayes-using-gaussians/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/deriving-kalman-filter-from-discrete-bayes-using-gaussians/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Replacing discrete Bayes with Gaussian distributions where the operators in the circles are as of yet undetermined</description></item><item><title>Empirical rule 68/95/99.7</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/empirical-rule-68-95-99.7/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/empirical-rule-68-95-99.7/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python Emprical rule, a.k.a. 68–95–99.7 rule About 68% of all values lie within one standard deviation of the mean.</description></item><item><title>Factors affecting Kalman filter performance</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/factors-affecting-kalman-filter-performance/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/factors-affecting-kalman-filter-performance/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Difficulties of creating a well-performing Kalman filter: Includes modeling the sensor performance (what variance most accurately represents the reality? Which probability distribution?)
Factors affecting the performance of the Kalman filter
On modelling the process noise/variance Bad initial estimate Filter can recover from this, because we have a certain belief in the sensor measurements Typically the initial value is set to the first sensor measurement Nonlinearity of the system</description></item><item><title>Frequentist vs Bayesian statistics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/frequentist-vs-bayesian-statistics/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/frequentist-vs-bayesian-statistics/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python Frequentist vs Bayesian statistics
Probability of flipping a fair coin infinitely many times is 50% - frequentist Probability of flipping a fair coin one more time (which way do I believe it landed?), single event - Bayesian Bayesian statistics takes past information (prior) into account If finding the prior is tricky, frequentist techniques are sometimes used e.</description></item><item><title>Gaussian distribution</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/gaussian-distribution.1/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/gaussian-distribution.1/</guid><description>Backlinks: Limitations of the discrete Bayes filter Source: rlabbe Kalman/Bayesian filters in Python a.k.a. Normal distribution Unimodal, continuous probability distribution function (pdf)
The probability of a range of measurements is the area under the graph of the probability distribution between the end values of the range &amp;ndash; cumulative distribution function (cdf)
Background statistics Variance, standard deviation, covariances Central Limit Theorem Correlation and independence Types Univariate Gaussian distribution Multivariate Gaussian distributions Computational properties of Gaussian distributions Pros and cons of Gaussian distributions</description></item><item><title>Kalman gain using Gaussians</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-gain-using-gaussians/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-gain-using-gaussians/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Kalman gain in the update step Basically a scaling term that chooses a value between the sensor distr. mean and the posterior distr. mean Gives greater weight to the term with lower variance (we trust this data more!) Mean and variance in terms of the Kalman gain Variance of the filter (i.e., what variance is show by the estimated output/posterior?</description></item><item><title>Kalman vs. nonlinear systems</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-vs.-nonlinear-systems/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-vs.-nonlinear-systems/</guid><description>Parent: Factors affecting Kalman filter performance Source: rlabbe Kalman/Bayesian filters in Python Kalman filter equations are linear Example: approximating a sine-wave signal Explanation:
Back to the basic g-h filter structure: the filter output chooses a value on the residual line The process model in the underlying filter assumes constant velocity (0 acceleration), whereas in the sine example above, the signal is always accelerating</description></item><item><title>Limitations of the discrete Bayes filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/limitations-of-the-discrete-bayes-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/limitations-of-the-discrete-bayes-filter/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python Limitations of the discrete Bayes filter
Scaling Dog tracking example is one-dimensional, but in real life we often want to track more things (e.g. 2D coordinates, velocities) Multidimensional case: store probabilities in a grid 4 tracked variables: O(n^4) per time step High computational cost with high dimensionality Filter is discrete and therefore gives discrete output But a lot of applications require continuous output Discretising a solution space can lead to lots of data (depending on accuracy required) &amp;ndash;&amp;gt; calculations for lots of different probabilities!</description></item><item><title>Multivariate Gaussian distributions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/multivariate-gaussian-distributions/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/multivariate-gaussian-distributions/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python See also: Probability distribution N means for N dimensions Variances are now also combined with covariances (to take into account correlation between different dimensions)
Variance: how does a population vary amongst themselves? Covariance: how much do two variables change relative to each other? The correlation helps prediction!
Here: only linear correlation considered; however nonlinear correlations also exist.
Error ellipse/Confidence ellipse</description></item><item><title>Probability distribution</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/probability-distribution/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/probability-distribution/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python Probability distribution:
collection of all possible probabilities for an event the distribution lists all possible events and the probability of each sum up to 1 Prior probability distribution: probability prior to incorporating any measurements or other information
Joint probability P(x,y):
probability of both events happening the multivariate Gaussian distribution is already already a joint probability distribution Marginal probability:</description></item><item><title>Pros and cons of Gaussian distributions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/pros-and-cons-of-gaussian-distributions/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/pros-and-cons-of-gaussian-distributions/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python Big advantage of using Gaussian distributions (as opposed to discrete ones w/ histogram bins): less data, b/c a Gaussian distribution is represented fully using only two values: the mean and the variance Limitations of using Gaussian distributions to model the world i.e. deviations from the central limit theorem
Not all situations are describable by Gaussian distributions e.g. sensors in the real world have fat tails (kurtosis) — don&amp;rsquo;t extend to infinity and skew Can&amp;rsquo;t depict any arbitrary probability distributions like in e.</description></item><item><title>Random variable</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/random-variable/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/random-variable/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python Combination of values + associated probabilities. &amp;ldquo;Event&amp;rdquo; e.g. die toss, height of students e.g. in a fair die values = {1, 2, &amp;hellip;, 6} (range of values = sample space) probabilities = {1/6} * 6</description></item><item><title>Smug filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/smug-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/smug-filter/</guid><description>Parent: 1D Kalman filter algorithm Source: rlabbe Kalman/Bayesian filters in Python A filter that, once enough measurements are made, becomes very confident in its prediction (P gets smaller with time while the filter becomes more inaccurate!). From then on it will ignore measurements
To avoid this: add a bit of error to the prediction step, e.g. using the process variance</description></item><item><title>Univariate Gaussian distribution</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/univariate-gaussian-distribution/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/univariate-gaussian-distribution/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python If normalised (area under the graph is 1): Gaussian distribution If not normalised: Gaussian function Notation: The random variable X has a Gaussian distribution with mean &amp;hellip; and variance &amp;hellip; .</description></item><item><title>Variance of the 1D Kalman filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/variance-of-the-1d-kalman-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/variance-of-the-1d-kalman-filter/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python i.e., what variance is show by the estimated output/posterior?)
Always converges to a fixed value if the sensor and process variances are constant We can run simulations to determine the value to which the filter variance converges Then hard code this value into the filter (+ with first sensor measurement as initial value, the filter should have good performance) Alternative: instead of using the variance value, use the calculated Kalman gain Example implementation using the Kalman gain However, using the Kalman gain obscures the Bayesian approach</description></item><item><title>Variance, standard deviation, covariances</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/variance-standard-deviation-covariances/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/variance-standard-deviation-covariances/</guid><description>Backlinks: Multivariate Gaussian distributions Source: rlabbe Kalman/Bayesian filters in Python See also: Empirical rule 68/95/99.7 How much do the values vary from the mean?
There are other ways of calculating variance (e.g. by using absolute values of error instead of error squared). The other methods may be better w.r.t. outliers (outliers get magnified in the square term) Process variance: error in the process model Sensor variance: error in each sensor measurement</description></item><item><title>(AtsushiSakai) PythonRobotics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/atsushisakai-pythonrobotics/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/atsushisakai-pythonrobotics/</guid><description>http://nbviewer.jupyter.org/github/AtsushiSakai/PythonRobotics/</description></item><item><title>(rlabbe) Kalman/Bayesian filters in Python</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/rlabbe-kalman-bayesian-filters-in-python/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/rlabbe-kalman-bayesian-filters-in-python/</guid><description>URL: http://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python nbviewer link: http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb Abstract:
Introductory text with Python code Caveat: most of the code is written for didactic purposes, may not be the most efficient solution (nor numerically stable) Recommended other works s. Works of possible interest Chapters
Preface Why Kalman filters? [Aim and main principle of Kalman filters](aim and-main-principle-of-kalman-filters.md) Expected value g-h filter or α-β filter Discrete Bayesian filter Gaussian distribution 1D Kalman filters Multivariate Kalman filters</description></item><item><title>50.1 Why Kalman filters?</title><link>https://salehahr.github.io/zettelkasten/permanent/50.1-why-kalman-filters/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/permanent/50.1-why-kalman-filters/</guid><description>Source: rlabbe We work with 2 sources of data:
Sensor measurements Our own predictions (based on knowledge of system behaviour) Sensors are noisy, don&amp;rsquo;t give perfect information
Simple solution: to average readings However, this doesn&amp;rsquo;t work when the sensor is too noisy data collection not possible The prediction, however, is also susceptible to noise (the world is noisy, outside/unaccounted for influences).
In short: &amp;ldquo;Knowledge is uncertain, and we [must] alter our beliefs based on the strength of the evidence.</description></item><item><title>50.1.1 Aim and main principle of Kalman filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.1.1-aim-and-main-principle-of-kalman-filters/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.1.1-aim-and-main-principle-of-kalman-filters/</guid><description>Source: rlabbe Aim Aim of the Kalman/Bayesian filters: to accumulate (or to somehow blend)
our noisy and limited knowledge (of system behaviour) noisy and limited sensor readings and with these, make the best possible prediction (estimate) of the system state.
Main principles: use past information to make predictions for the future never throw away information predict/propagation step: calculate prediction based on process model and using previous state data (previous estimate) update step: calculate the estimates based on prediction and measurement Prediction step a.</description></item><item><title>Calculating the estimated state in the GH-filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/calculating-the-estimated-state-in-the-gh-filter/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/calculating-the-estimated-state-in-the-gh-filter/</guid><description>Parent: g-h filter or α-β filter Source: rlabbe Kalman/Bayesian filters in Python Using a gain , the estimate therefore always falls between the measurements (circles) and the predictions (in red). The prediction is dependent on the previous filter output (i.e. last estimate). Here it is modelled to increase by 1 from the previous estimate.
The estimates are not a straight line, but definitely closer in shape to the ground truth than the measurements alone.</description></item><item><title>Effects of varying g</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/effects-of-varying-g/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/effects-of-varying-g/</guid><description>Parent: [Calculating the estimated state in the GH-filter](calculating the-estimated-state-in-the-gh-filter.md) Source: rlabbe Kalman/Bayesian filters in Python The greater the g value, the more we follow the measurements rather than rely on our [model-based] predictions.</description></item><item><title>Effects of varying h</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/effects-of-varying-h/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/effects-of-varying-h/</guid><description>Parent Improving the gh-filter by using h Source: rlabbe Kalman/Bayesian filters in Python The greater the h value, the more we trust the rate of change that we can derive from the measurement data.
a larger h enables us to react to transient (initial condition dependent) changes more rapidly. Because if we have a large difference between our chosen IC and the measurement, this results in a huge residual velocity</description></item><item><title>Expected value</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/expected-value/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/expected-value/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python Example: if we take a thousand sensor readings, the readings won&amp;rsquo;t always be the same (due to the inherent noise).
The expected value &amp;lsquo;averages&amp;rsquo; all of the readings into a single value. This can be a mean (probabilities of all values assumed equal) Or if incorporating individual and different probabilities, the expectation isn&amp;rsquo;t the mean of the range of values Proven: the average of a large number of measurements will be very close to the actual weight</description></item><item><title>g-h filter or α-β filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/g-h-filter-or-%CE%B1-%CE%B2-filter/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/g-h-filter-or-%CE%B1-%CE%B2-filter/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python A filter that uses two scaling factors:
g or \alpha for the measurement h or \beta for the rate of change of measurement GH filter algorithm [Calculating the estimated state in the GH-filter](calculating the-estimated-state-in-the-gh-filter.md) Improving the gh-filter by using h Several unwanted effects using gh filters Basis for many other filters, e.g.
Kalman filter Least squares filter Benedict-Bordner filter etc.</description></item><item><title>Gain g of the gh-filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/gain-g-of-the-gh-filter/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/gain-g-of-the-gh-filter/</guid><description>Parent: [Calculating the estimated state based on measurements and predictions](calculating the estimated state-based-on-measurements-and-predictions.md) Backlinks: GH filter algorithm Source: rlabbe Kalman/Bayesian filters in Python Which one do we trust more, the meaasurement z or the prediction x? Applying corresponding weights to both, we obtain the estimate x_est
The prediction is nothing other than a propagated state estimate.
[Me] The prediction is basesd on the model (a priori knowledge) If the model also depends on previous states (which are themselves an output of the filter, i.</description></item><item><title>GH filter algorithm</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/gh-filter-algorithm/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/gh-filter-algorithm/</guid><description>Parent: g-h filter or α-β filter Source: rlabbe Kalman/Bayesian filters in Python Initialisation
Initialise the state of the filter Initialise our belief in the state Prediction
Use system model to propagate the state to the next time step Adjust our belief to account for uncertainty in the prediction Update
Get a measurement and an associated belief about its accuracy Calculate residual = measurement - estimated state Using a certain gain , our updated state estimate is somewhere on the residual line</description></item><item><title>Improving the gh-filter by using h</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/improving-the-gh-filter-by-using-h/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/improving-the-gh-filter-by-using-h/</guid><description>Parent: g-h filter or α-β filter Source: rlabbe Kalman/Bayesian filters in Python Implementing the g value without h We improve the estimation, previously by only predicting the state, by now predicting the rate of change of state. i.e. Also predict the weight gain per day instead of setting it at a constant value. We use the sensor information for this! Even if it&amp;rsquo;s noisy, there&amp;rsquo;s information in there somewhere, and data is always better than a guess.</description></item><item><title>Principle of never throwing away information</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/principle-of-never-throwing-away-information/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/principle-of-never-throwing-away-information/</guid><description>Source: rlabbe &amp;ldquo;Two sensors are better than one, even if one is less accurate than the other.&amp;rdquo;
Example 1 Given:
A: a more accurate sensor B: a less accurate sensor Should we therefore choose A as the estimate and discard B? No, because B can improve our knowledge when combined with A.
Using the measurements from B further narrows the range of estimates (overlap between the error bars).</description></item><item><title>Several unwanted effects using gh filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/several-unwanted-effects-using-gh-filters/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/several-unwanted-effects-using-gh-filters/</guid><description>Parent: g-h filter or α-β filter Source: rlabbe Kalman/Bayesian filters in Python Effect of bad initial conditions: ringing (sinusoidal over- and undershooting before finally settling onto a trajectory) Effect of very noisy data Effect of acceleration (in data) Filter lags behind because it uses a model that assumes constant velocity in each propagation step. Hence, a filter is only as good as the mathematical model used to describe the phenomenon.</description></item><item><title>Cadena 2016 Past, Present, and Future of SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cadena-2016/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cadena-2016/</guid><description>Authors Cadena et al
Abstract
cited by 1.2k people &amp;ldquo;This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM&amp;rdquo; Recommended other works s. Works of possible interest Contents/Chapters Takeaway</description></item><item><title>Conditional independence</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/conditional-independence/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/conditional-independence/</guid><description>Source: http://en.wikipedia.org/wiki/Conditional_independence A and B are conditionally independent given C If and only if, given the knowledge that C occurs, the knowledge of whether A occurs provides no information whatsoever on the likelihood of B occurring, and vice versa
Example Weather and delay
Let the two events be the probabilities of persons A and B getting home in time for dinner
The third event C is the fact that a snow storm hit the city.</description></item><item><title>Durrant-Whyte 2006 SLAM Tutorial Part I</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/durrant-whyte-2006-slam-tutorial-part-i/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/durrant-whyte-2006-slam-tutorial-part-i/</guid><description>Backlinks: Works of possible interest Authors: Bailey, Durrant-Whyte
Abstract:
Contents/Chapters
Takeaway</description></item><item><title>Egomotion (vs odometry)</title><link>https://salehahr.github.io/zettelkasten/definitions/egomotion-vs-odometry/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/definitions/egomotion-vs-odometry/</guid><description>See also: Odometry Source: http://en.wiktionary.org/wiki/egomotion
The three-dimensional movement of a camera relative to its environment
Source: http://answers.ros.org/question/296686/what-is-the-differences-between-ego-motion-and-odometry/
Generally used interchangeably with odometry Possible difference: Egomotion is more about the estimation of twist (lin, rotational velocities) Odometry is more about the estimation of path Examples Wheel odometry: path estimation via time-integration of an estimated twist Visual odometry/Scan matching: direct estimation of pose without time-integration</description></item><item><title>Perceptual aliasing</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/perceptual-aliasing/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/perceptual-aliasing/</guid><description>Source: http://en.wikipedia.org/wiki/Robotic_mapping Two different places are perceived as the same
Source: http://arxiv.org/abs/1810.11692 Modeling Perceptual Aliasing in SLAM via Discrete-Continuous Graphical Models [Lajoie 2018] (from the abstract)
Phenomenon where different places generate a similar visual footprint Leads to spurious measurements being fed into the SLAM estimator Result: incorrect localisation and map</description></item><item><title>Chen 2018 Review of VI SLAM</title><link>https://salehahr.github.io/zettelkasten/bibliography/chen-2018-review/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/chen-2018-review/</guid><description>Source: http://www.mdpi.com/2218-6581/7/3/45
Authors: Chen et. al
Abstract Survey on visual-inertial SLAM over the last 10 years Aspects: filtering vs optimisation based, camera type, sensor fusion type Explains core theory of SLAM, feature extraction, feature tracking, loop closure Experimental comparison of filtering-based and optimisation-based methods Research trends for VI-SLAM Recommended other works s. Works of possible interest Contents/Chapters SLAM SLAM: build a real-time map of the unknown environment based on sensor data, while the sensor (robot) itself is traversing the environment</description></item><item><title>Chen 2018 SLAM-based dense surface reconstruction in MIS with AR</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/chen-2018-mis-slam/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/chen-2018-mis-slam/</guid><description>Authors Chen et al
Abstract Intra-operative dense surface reconstruction framework to provide geometry information from only monocular videos The proposed framework works well with rapid camera movements, however is not suitable for large deformations Only tweaks ORBSLAM to adjust between point density and computational performance Contents/Chapters Problems in medical AR:
tissue surface illumination tissue deformation rapid movements of the medical tool e.g. endoscope (s. also kidnapped robot problem for relocalisation, tracking mus therefore be robust) field of view often very small &amp;ldquo;A typical human uses 14 visual cues to perceive depth, only 3/14 are binocular vision related.</description></item><item><title>In vivo</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/in-vivo/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/in-vivo/</guid><description> Latin for &amp;ldquo;within the living&amp;rdquo; studies in which the effects of various biological entities are tested on whole, living organisms or cells, as opposed to a tissue extract/dead organism</description></item><item><title>Song 2018 MIS-SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/song-2018-mis-slam/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/song-2018-mis-slam/</guid><description>Authors: Song et al
Note: referred to in Lamarca 2020 as a stereovisual deformable SLAM, uses CPU and GPU, nonlinear optimisation Video: http://www.youtube.com/watch?v=2pXokldQBWM
Abstract Uses CPU and GPU CPU for ORBSLAM (initial global position) GPU for deformable tracking and dense mapping Contents/Chapters Poor localisation of scope in MIS, compared with open surgery Related works mentioned don&amp;rsquo;t provide a RT and robust solution for localisation while reconstructing dense deformable surfaces focus on the monocular scope, fail to solve the problem of missing scale Fast movement makes visual odometry unstable causes blurry images worsens registrations ORB-SLAM proven to be suitable for coupling with dense deformable SLAM Initial tracking: ORB-SLAM ORB features and global pose are uploaded to GPU Upload the matched ORB features every time a new observation is made The initial global pose makes the system significantly robuster Deformable tracking and dense mapping Receives initial global pose from the CPU Initialises the model with an estimated depth From model: extract potential visible points Project these points onto 2D depth images Registration: to estimate optimum global pose to estimate non-rigid warping field Deform the model based on this transformation (registration) Fuse the deformed model with the new observation Includes in-vivo validation with deformation (Hamlyn datasets)</description></item><item><title>Filter-based vs optimisation-based SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/filter-based-vs-optimisation-based-slam/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/filter-based-vs-optimisation-based-slam/</guid><description>Parent: SLAM Index Source: Scaradozzi 2018 SLAM application in surgery Main paradigms of SLAM
Filters — Kalman filters , Particle filters Graph-based SLAM Estimate the entire trajectory and the map from the full set of measurements (full SLAM) Which SLAM algorithm to use? Depends on application
map resolution update time (real time or not) type of environment type of sensors available</description></item><item><title>General Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/general-kalman-filter/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/general-kalman-filter/</guid><description>Backlinks: Main paradigms of SLAM Source: Scaradozzi 2018 SLAM application in surgery KF original algorithm assumes linearity (rarely ever the case)
Variations of the Kalman filter: Extended Kalman Filter (EKF) Unscented Kalman Filter (UKF) Information filtering (IF) — dual to KF Combination of EKF and IF: CF-SLAM, with the goal to be more efficient w.r.t. computational complexity</description></item><item><title>Information Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/information-filter/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/information-filter/</guid><description>Parent: General Kalman Filter Source: Scaradozzi 2018 SLAM application in surgery also same assumptions as the EKF main difference: how the Gaussian belief is represented est. cov. — replaced by information matrix (IM) est. state — replaced by information vector (IV) superior to KF in the following ways data is filtered by summing up the IMs and IVs often numerically more stable Dual character of KF and IF</description></item><item><title>Particle filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/particle-filters/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/particle-filters/</guid><description>Parent: Filter localisation methods Source: Wikipedia Lokalisierung Particle filter / Monte Carlo localisation / sequential Monte Carlo methods
allow solution of all three localisation problems POSE represented by a particle cloud Each particle : possible POSE The filter checks the plausibility of each particle Increases and decreases the probabilities of each particle accordingly When a lower probability threshold is exceeded, the particle is not considered any longer Source: Scaradozzi 2018 SLAM application in surgery Particle filters (sequential Monte Carlo)</description></item><item><title>Unscented Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/unscented-kalman-filter/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/unscented-kalman-filter/</guid><description>Parent: General Kalman Filter Source: Scaradozzi 2018 SLAM application in surgery developed to overcome main problems of the EKF like EKF, approximates the state distribution with a Gaussian Random Variable only the representation is different—using alpha points (a minimal set of sample points) capture posterior mean and covariance accurately for any nonlinearity, up to3rd order Taylor</description></item><item><title>Visual sensors for localisation</title><link>https://salehahr.github.io/zettelkasten/sensors/visual-sensors-for-localisation/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/visual-sensors-for-localisation/</guid><description>Source: Wikipedia Visual odometry Process of determining robot POSE by analysing the associated camera images Use sequential camera image to estimate the distance travelled Applications: robotics, computer vision
Source: Cometlabs Types
Monocular cameras Stereo cameras RGB-D cameras Provide rich visual information, but for that, higher computational cost
Source: SLAM for Dummies stereo or triclops sytem to measure distance [+] possibly more intuitive (bc humans use vision), more info.</description></item><item><title>Wikipedia Lokalisierung</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-lokalisierung/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-lokalisierung/</guid><description>Source: http://de.wikipedia.org/wiki/Lokalisierung_(Robotik Localisation Particle filters</description></item><item><title>Collision detection and response</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/collision-detection-and-response/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/collision-detection-and-response/</guid><description>Source: SOFA extended documentation Parent: Simulation algorithms in SOFA split into several phases each phase is scheduled by a CollisionPipeline component an object which can potentially collide is associated with a collision geometry returns pairs of colliding bounding volumes (broad phase component) returns pairs of geometric primitives + contact points (narrow phase component) the returned pairs are passed to the contact manager the contact manager creates contact interactions &amp;hellip; (skimmed)</description></item><item><title>Constraint solvers</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/constraint-solvers/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/constraint-solvers/</guid><description>Source: SOFA extended documentation Parent: Simulation algorithms in SOFA Backlinks: Scene graph in SOFA Lagrange multipliers to handle complex constraints (which aren&amp;rsquo;t handle-able using projection matrices ) May be combined with explicit or implicit integration phi: bilateral interaction laws (attachments, sliding joints, &amp;hellip;) psi: unilateral interaction laws (contact, friction, &amp;hellip;)
The Lagrange multipliers add force terms to the equation A*dv = b The H matrices are stored in the MechanicalState of each node.</description></item><item><title>Filter localisation methods</title><link>https://salehahr.github.io/zettelkasten/SLAM/filter-localisation-methods/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/filter-localisation-methods/</guid><description>Backlinks: Back-end optimisation , what is slam? Filtering vs. optimisation Source: Wu 2018 EKF to propagate and update motion states of visual-inertial sensors
Source: Scaradozzi 2018 Filtering techniques in SLAM
Augment/refine the position estimates and map estimates by incorporating new measurements when they become available Generally online, due to their incremental nature Types Kalman filters Particle filters</description></item><item><title>General EKF implementation (non-SLAM)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/general-ekf-implementation-non-slam/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/general-ekf-implementation-non-slam/</guid><description>Parent: Extended Kalman Filter Source: SLAM for Dummies General (non-SLAM) implementation of EKF:
only state estimation robot is given a perfect map no map update necessary SLAM implementations of EKF requires map update and therefore the matrices are changed.
Source: Scaradozzi 2018 SLAM application in surgery EKF vs KF circumvents linearity assumption uses nonlinear functions to describe the next state probability measurement probability approximates the state distribution with a Gaussian Random Variable</description></item><item><title>Laser scanners</title><link>https://salehahr.github.io/zettelkasten/sensors/laser-scanners/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/laser-scanners/</guid><description>Source: SLAM for Dummies Commonly used [+] Precise, efficient, not much processing work necessary [-] Expensive, bad readings with certain surfaces, bad for underwater applications</description></item><item><title>Linear solvers</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/linear-solvers/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/linear-solvers/</guid><description>Source: SOFA extended documentation Parent: Simulation algorithms in SOFA Conjugate gradient J: first-order mapping of a node to its parent path(i): list of mappings from the independent DOFs to the node the force applies to
Computation using a visitor: Top down visitor: propagates the given displacement, clears force vector Bottom up visitor: accumulates forces, maps them up to the independent DOFs Direct solvers
can be used as preconditioners of the conjugate gradient algorithm can be used to solve the equation system A*dv=b implementations are based external libraries</description></item><item><title>Mesh topology</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mesh-topology/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mesh-topology/</guid><description>Source: SOFA extended documentation Parent: Data structure in SOFA See also: Mesh geometry Mesh topology: how the vertices are connected to each other (using what element?)
Hierarrchy of mesh topology: Topology objects consist of four functional members which creates/modifies/gets topology arrays/geometrical information:
Container Modifier Algorithms Geometry Topological mapping:
Define a new mesh topology from an existing one, using the same DOFs e.g. for subsetting a set of nodes, edges, or to split quads into 2 triangles each these topologies are therefore assigned to the same MechanicalState</description></item><item><title>ODE solvers</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/ode-solvers/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/ode-solvers/</guid><description>Source: SOFA extended documentation Parent: Simulation algorithms in SOFA Backlinks: Linear solvers , constraint-solvers implement animation algorithms at each time step integrate and compute positions and velocities one time step ahead uses state vectors (e.g. for position or force), denoted by symbolic identificators called VecId s this allows the solver to be implemented completely independently of the physical model Each statement in the example above is implemented using a visitor</description></item><item><title>Odometry</title><link>https://salehahr.github.io/zettelkasten/definitions/odometry/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/definitions/odometry/</guid><description>See also: Egomotion (vs odometry) Source: https://en.wikipedia.org/wiki/Dead_reckoning In navigation, dead reckoning is the process of calculating one&amp;rsquo;s current position by using a previously determined position, by using estimations of speed and course over elapsed time
s. Brian Douglas video on sensor fusion Source: Wikipedia Visual odometry Data can be generated from actuator movements, e.g. rotary encoders that measure motor shaft rotations This data can be used to estimate changes in position over time Usually has precision problems, e.</description></item><item><title>Simulation algorithms in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/simulation-algorithms-in-sofa/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/simulation-algorithms-in-sofa/</guid><description>Source: SOFA extended documentation ODE integration ( ODE solvers ) Linear equation solution ( linear solvers ) Complex constraints ( constraint solvers ) Collision detection and response GPU support</description></item><item><title>SofaPython Index</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sofapython-index/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sofapython-index/</guid><description>Building/setup Building SOFA on Windows Someone&amp;rsquo;s SP3 setup Running Running SOFA with Python Using python with existing scene Basic python script in Sofa Initialising graph in SP3 Plugins Possible plugins Install ROSConnector in SOFA STLIB (Sofa Template Library) Registration Communication Sending data using sockets Sockets Errno 10054 External data in SOFA Documentation SofaPython API/Documentation links Cheatsheet</description></item><item><title>VecId</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/vecid/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/vecid/</guid><description>Source: [SOFA extended documentation](SOFA extended documentation.md) Parent: [ODE solvers](ODE solvers.md)
Uniquely identifies state vectors (which are scattered over all MechanicalStates ) Mechanical operations (e.g. allocating a state vector, accumulating forces) are implemented using a specialised visitor parametrised on VecIds</description></item><item><title>B1 Modelling of tissue and sensor, navigation of multimodal sensors</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/b1-modelling-of-tissue-and-sensor-navigation-of-multimodal-sensors/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/b1-modelling-of-tissue-and-sensor-navigation-of-multimodal-sensors/</guid><description>Source: http://www.isys.uni-stuttgart.de/forschung/medizintechnik/intraoperative-multisensorische-gewebedifferenzierung/ Backlinks: [GRK 2543: Intraoperative Multi-sensor Tissue Differentiation in Oncology](grk 2543_ intraoperative-multi-sensor-tissue-differentiation-in-oncology.md)
cross-domain modelling (tissue and sensor) tissue parameters change depending on status (benign/malignant), obtained or derived from sensor signals sensor signal must be synchronised with the current position of the sensor probe on the tissue</description></item><item><title>Barycentric coordinates</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/barycentric-coordinates/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/barycentric-coordinates/</guid><description>Source: SOFA extended documentation Baclinks: [Top-down mapping (master to slave)](top-down mapping (master to slave).md), lamarca-2019-defslam Barycentre: centre of mass A coordinate system, in which the location of point of a simplex (line, triangle, tetrahedron, etc) is specified as the centre of mass of the masses placed at its vertices x_i vertices of a simplex p a point in space The a_i coefficients are the barycentric coordinates of p w.</description></item><item><title>Bottom-up mapping (slave to master)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/bottom-up-mapping-slave-to-master/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/bottom-up-mapping-slave-to-master/</guid><description>Source: SOFA extended documentation Parent: Mappings Mapping of a slave forces to the master forces Newton&amp;rsquo;s law f=Ma applies
Equivalence of power using the kinematic relation using the principle of virtual work</description></item><item><title>Components of the internal model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/components-of-the-internal-model/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/components-of-the-internal-model/</guid><description>Source: SOFA extended documentation Parent: [Internal model as a scene graph in SOFA](internal model as-a-scene-graph-in-sofa.md)
MeshLoader: topology, geometry
MechanicalState TetrahedronSetTopologyContainer
tetrahedral connectivity passed on to other components Forces Mass
DiagonalMass UniformMass: less accurate, but allows faster computation FixedConstraint: P (cancels displacements)
EulerSolver: integration scheme</description></item><item><title>Cryosection</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cryosection/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cryosection/</guid><description>Source: https://en.wikipedia.org/wiki/Frozen_section_procedure aka frozen section procedure allows rapid analysis of a dissected/resected specimen during the course of surgery the specimen is frozen rapidly and brought to a lab for analysis the results are relayed to the surgeon by intercom benign or malignant when operating on a previously confirmed malignant tissue, information on whether residual cancer was found on the [resection margin](resection margin.md) of the tissue the surgeon makes his decision on how to continue the operation based on the results</description></item><item><title>Force classes in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/force-classes-in-sofa/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/force-classes-in-sofa/</guid><description>Source: SOFA extended documentation Parent: Components of the internal model More than 30 classes available in SOFA
FEM
for deformable volumes/surfaces
volume: tetrahedron/hexahedron surface: shell/membrane TetrahedralCorotationalFEMForceField: forces based on FEM
corotational/hyperelastic formulations
wire/tubular objects
Springs
SpringForceField: forces generated by the surface (alternative: TriangleFEMFroceField) ConstantForceField: external forces</description></item><item><title>Internal model as a scene graph in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/internal-model-as-a-scene-graph-in-sofa/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/internal-model-as-a-scene-graph-in-sofa/</guid><description>Source: SOFA extended documentation Parent: Internal model Scene graph of the internal model
Consists of components which are connected to a common scenegraph node (root of the internal model)
Each component is responsible for a set of tasks Examples: solver, mass, constraints, &amp;hellip;
Each component can query its parent node to get access to the its sibling components such as MechanicalState , topology
Components are independent of one another — modularity</description></item><item><title>Mathematical model of the internal model in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mathematical-model-of-the-internal-model-in-sofa/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mathematical-model-of-the-internal-model-in-sofa/</guid><description>Source: SOFA extended documentation Parent: Internal model Backlinks: ODE solvers , constraint-solvers Dynamic/quasi-static system of particles (nodes) Independent DOFs: node coordinates, governed by  f: different force functions, e.g. volume, surface and external forces) M: mass matrix P: constraints (projection matrix) each operator corresponds to a simulation component</description></item><item><title>MechanicalState</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mechanicalstate/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mechanicalstate/</guid><description>Source: [SOFA extended documentation](SOFA extended documentation.md) Parents: [Components of the internal model](Components of the internal model.md), [Internal model as a scene graph in SOFA](Internal model as a scene graph in SOFA.md) Backlinks: VecId , [Scene graph in SOFA](Scene graph in SOFA.md), [Mesh topology](Mesh topology.md)
Contains state vectors of each mesh node Coordinates x Velocities v Net force f n nodes: n entries of the state vector Each entry has the same size of the node type (3 for 3D particles) Nodes of different types belong to different MechanicalStates the other MechanicalStates are attached to other scene graph nodes they might be connected with one another using interaction forces</description></item><item><title>Oncology</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/oncology/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/oncology/</guid><description>Source: https://en.wikipedia.org/wiki/Oncology Backlinks: [GRK 2543: Intraoperative Multi-sensor Tissue Differentiation in Oncology](GRK 2543_ Intraoperative Multi-sensor Tissue Differentiation in Oncology.md)
Prevention, diagnosis and treatment of cancer.</description></item><item><title>Related types of surgery</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/related-types-of-surgery/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/related-types-of-surgery/</guid><description>Source: http://en.wikipedia.org/wiki/Resection_(surgery) By procedure Resection: remove all parts or a key part of an internal organ s. also: resection margin Excision: cut out only a part of an organ/tissue By degree of invasiveness minimally-invasive surgery (-scopy) laparoscopy</description></item><item><title>Resection margin</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/resection-margin/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/resection-margin/</guid><description>Source: http://en.wikipedia.org/wiki/Resection_margin Backlinks: Cryosection , related types-of-surgery Margin on non-cancerous tissue around a tumour that has been removed.
Negative margin: no tumour Microscopic positive: tumour identified microscopically Macroscopic positive: tumour significantly present</description></item><item><title>Scene graph (general)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/scene-graph-general/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/scene-graph-general/</guid><description>Source: http://en.wikipedia.org/wiki/Scene_graph See also: Scene graph in SOFA A general data structure Collection of nodes in a graph/tree</description></item><item><title>Simplex</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/simplex/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/simplex/</guid><description>Source: https://en.wikipedia.org/wiki/Simplex Parent: [Barycentric coordinates](Barycentric coordinates.md)
A triangle in arbitrary dimensions 0-simplex point 1-simplex line 2-simplex triangle 3-simplex tetrahedron</description></item><item><title>Top-down mapping (master to slave)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/top-down-mapping-master-to-slave/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/top-down-mapping-master-to-slave/</guid><description>Source: SOFA extended documentation Parent: Mappings Mapping of a master states to the slave states with the Jacobian (kinematic relation) Linear/nonlinear mappings
In linear mappings, J and J are the same In nonlinear mappings, J is nonlinear w.r.t. x_m, i.e. not a matrix Surfaces
Surfaces embedded in deformable cells: J contains barycentric coordinates Surfaces attached to rigid bodies: each row of J encodes for each vertex</description></item><item><title>Visual model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/visual-model/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/visual-model/</guid><description>Source: SOFA extended documentation Parent: Models in SOFA More detailed geometry than that of the internal model , hence uses different meshes Mappings are used to update the visual model with the deformations taking place Contains rendering parameters Libraries for rendering graphics
OGRE (external) Open Scene Graph (external) SOFA&amp;rsquo;s own library based on openGL</description></item><item><title>Cancer biopsy</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cancer-biopsy/</link><pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cancer-biopsy/</guid><description>Source: http://en.wikipedia.org/wiki/Biopsy
Biopsy type of medical test in which a cell/tissue sample is extracted in order to detect disease
Types of biopsy: excisional biopsy: removal of entire suspicious area to be diagnosed incisional biopsy: removal of only samples of the abnormal tissue needle aspiration biopsy: removal of cells via needle Diagnosing / Pathological examination to determine whether the abnormality is benign or malignant (classification of the cancer) to determine how far it has spread negative margins: no disease found at the edge of specimen positive margins: disease was found at edge, further excision may be in order In bladders: usually done using a cystocopy</description></item><item><title>Cystocopy</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cystocopy/</link><pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cystocopy/</guid><description>Source: https://en.wikipedia.org/wiki/Cystoscopy
Backlinks: [Some questions](Some questions.md)
Endoscopy of the bladder via the urethra Tool involved: cystoscope https://www.youtube.com/watch?v=ybhzlW7ivro
Video of cystocopy and bladder biopsy Modern Latin for bladder: cystis</description></item><item><title>Laparoscopy</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/laparoscopy/</link><pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/laparoscopy/</guid><description>Source: https://en.wikipedia.org/wiki/Laparoscopy Backlinks: [Related types of surgery](Related types of surgery.md), [Some questions](Some questions.md), Trocar minimally invasive surgery (MIS) / keyhole surgery make a small incision in the abdomen area and operate through it</description></item><item><title>Qin 2019 General Optimization-based Framework (Multisensor)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/qin-2019-general-optimization-based-framework-multisensor/</link><pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/qin-2019-general-optimization-based-framework-multisensor/</guid><description>Authors: Qin et al Code: http://github.com/HKUST-Aerial-Robotics/VINS-Fusion (uses ROS)
Abstract:
odometry estimation with multiple sensors, general framework which is optimisation-based demonstrated combinations: stereo cameras monocular cam + IMU stereo cams + IMU sensor = factor in the framework comparison with other state-of-the-art algos Aim:
to create a general algo which supports different multisensor suites also for redundancy: in case of sensor failure, it can be switched out easily Related work:</description></item><item><title>State-of-the-art SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/state-of-the-art-slam/</link><pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/state-of-the-art-slam/</guid><description>Parent: SLAM Index Backlinks: Qin 2019 General Optimization-based Framework (Multisensor) Things that I&amp;rsquo;ve seen mentioned several times so far
ORBSLAM: monocular MonoSLAM: monocular (old?) — Andrew Davison OKVIS: visual inertial, stereovision PTAM: parallel tracking and mapping MSCKF: real-time EKF VINS-mono: visual inertial, monocular http://en.wikipedia.org/wiki/List_of_SLAM_Methods</description></item><item><title>Works of possible interest</title><link>https://salehahr.github.io/zettelkasten/bibliography/works-of-interest/</link><pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/works-of-interest/</guid><description>General SLAM Cadena 2016 &amp;ndash; Past, Present, and Future of SLAM durrant-whyte 2006 slam tutorial part i Prerequisites g2o paper - graph-based SLAM Existing SLAM algorithms MonoSLAM, works by Andrew Davison focusing on fusion instead of vision-only SLAM
Maplab (filtering-based) not looking at filtering-based algos
mentioned in the Chen 2018 Review of VI SLAM paper:
ORB-SLAM paper — ORB features VIORB implementation ORB-SLAM3 (improves on ORBSLAM, incl.</description></item><item><title>(Scaradozzi 2018) SLAM application in surgery</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/scaradozzi-2018/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/scaradozzi-2018/</guid><description>Abstract: SLAM&amp;rsquo;s potential in image-guided surgery assuming static environment Review of main techniques in general robotics SLAM Insight into visual SLAM SLAM in surgery Chapters what-is-slam Filter-based vs optimisation-based SLAM General Kalman Filter General EKF Unscented Kalman Filter Information Filter &amp;hellip;.
Takeaway EKF is popular in surgery SLAM techniques Deformable environment encumbers precise registration and data fusion</description></item><item><title>Distance between landmarks</title><link>https://salehahr.github.io/zettelkasten/SLAM/distance-between-landmarks/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/distance-between-landmarks/</guid><description>Source: SLAM for Dummies Methods for calculating distance between landmarks :
Euclidean distance (suitable for far distances) Mahalanobis distance (better, but more complex)</description></item><item><title>EKF System State</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-system-state/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-system-state/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices , step-2:-re-observation Contains robot POSE and landmark position POSE: (x y theta)_r LM: (x, y)_l1 &amp;hellip; (x,y)_ln; n = num. of landmarks Size: 3+2n rows</description></item><item><title>Kalman gain for EKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-gain-for-ekf/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-gain-for-ekf/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices How much we will trust the observed landmarks
compromise between odometry and landmark correction uses uncertainty of observed landmarks measure of quality of the range measurement device odometry performance Gains for range and brearing (3+2n x 2)</description></item><item><title>Lamarca 2020 DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/lamarca-2020/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/lamarca-2020/</guid><description>URL: http://arxiv.org/abs/1908.08918
Authors: Lamarca et al
Code: http://github.com/UZ-SLAMLab/DefSLAM
Results (video): http://www.youtube.com/watch?v=6mmhD2_t6Gs Summary First monocular SLAM for deformable environments in real-time Most other SLAM implementations assume rigidity Main techniques used (techniques for monocular non-rigid scenes): isometric shape from template ( SfT ) non-rigid structure from motion ( NRSfM ) Main principle: computation in two parallel threads (s. DefSLAM framework) Deformation tracking [front end] Deformation mapping [back end] The map from the mapping thread defines the shape-at-rest template used by deformation tracking Validation: compare with ORBSLAM (rigid) Assumes isometric deformation Future work: relocalisation (s.</description></item><item><title>Landmark extraction</title><link>https://salehahr.github.io/zettelkasten/SLAM/landmark-extraction/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/landmark-extraction/</guid><description>Source: SLAM for Dummies Basic landmark extraction using a laser scanner
Spike algorithm RANSAC ( EKF handles points) Expansion of RANSAC so that EKF handles lines Scan-matching: two successive laser scans are matched Spike and RANSAC are good for indoor environments</description></item><item><title>Nearest Neighbour</title><link>https://salehahr.github.io/zettelkasten/SLAM/nearest-neighbour/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/nearest-neighbour/</guid><description>Source: SLAM for Dummies Nearest neighbour approach
Get a new laser scan &amp;ndash;&amp;gt; ( landmark extraction ) extract all visible landmarks Associate each extracted LM to the closest LM we have seen more than $N$ times Pass each pairs of association (extracted LM, LM in database) through a validation gate If pair passes &amp;ndash;&amp;gt; $n = n + 1$ (num. times seen) If pair fails &amp;ndash;&amp;gt; add new LM to database, with $n := 1$</description></item><item><title>SLAM Index</title><link>https://salehahr.github.io/zettelkasten/SLAM/slam_index/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/slam_index/</guid><description>Definition Localisation What is SLAM? Sensors for SLAM Position acquisition (relative vs. absolute) SLAM hardware Relative Odometry IMU Absolute Sensors (absolute measurements) for measuring distance to landmarks Visual sensors for localisation Monocular depth perception Pinhole camera model Camera calibration World to camera trafo Fusion Multisensor fusion Loose vs Tight coupling Why use the visual-inertial sensor combination?</description></item><item><title>Validation gate</title><link>https://salehahr.github.io/zettelkasten/SLAM/validation-gate/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/validation-gate/</guid><description>Source: SLAM for Dummies An observed landmark is associated to a landmark if the following holds
$$ \begin{aligned} v_i^T S_i^{-1} v_i \leq \lambda \end{aligned} $$
$v$ innovation $S$ innovation covariance The validation gate makes use of the fact that the EKF implementation gives a bound on the uncertainty of an observation of a landmark .
Is an observed LM a LM in the database?</description></item><item><title>Literature management</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/literature-management/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/literature-management/</guid><description> Only focus on one paper per topic at a time Skim through and take notes on only the important chapters Link and backlink Note which topics were skimmed Come back later for further literature review</description></item><item><title>Programmatic implementations of MonoSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/programmatic-implementations-of-monoslam/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/programmatic-implementations-of-monoslam/</guid><description>Parent: SLAM resources Python http://github.com/agnivsen/Py-M-SLAM http://github.com/agnivsen/LibMonoSLAM
MATLAB http://perso.ensta-paris.fr/~filliat/Courses/2011_projets_C10-2/BRUNEAU_DUBRAY_MURGUET/monoSLAM_bruneau_dubray_murguet_en.html</description></item><item><title>Template for a bibliography entry</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/template-for-a-bibliography-entry/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/template-for-a-bibliography-entry/</guid><description>Source Backlinks
Authors Abstract Contents/Chapters Takeaway</description></item><item><title>Back-end optimisation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/back-end-optimisation/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/back-end-optimisation/</guid><description>Parent: Visual SLAM Implementation Framework Source: cometlabs (Camera pose optimisation)
To compensate for drift of pose estimation Traditionally using EKF ( filter-based ) simple implementation therefore good for small scale estimations Alternative: bundle adjustment (graph optimisation) joint optimisation of the camera pose and the 3D structure parameters combines numerical methods and graph theory increasingly favoured over filtering, due to the latter&amp;rsquo;s inherent inconsistency more efficient when combined with sub-mapping</description></item><item><title>Feature-based vs direct SLAM workflow</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/feature-based-vs-direct-slam-workflow/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/feature-based-vs-direct-slam-workflow/</guid><description>Parent: SLAM Index Source: cometlabs Feature-based (aka sparse ) direct (aka dense ) Extraction of features required No abstraction necessary Aims to minimise error between point location estimate (from odometry) and location based on camera Tracks objects by minimising photometric error (intensity differences)</description></item><item><title>Key frames in loop closure detection</title><link>https://salehahr.github.io/zettelkasten/SLAM/keyframes-in-loop-closure-detection/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/keyframes-in-loop-closure-detection/</guid><description>Source: cometlabs Most common method to get candidate key frames: use a place recognition approach
approach based on vocab tree feature descriptors of candidate key frames are quantised one colour in image below corresponds to one feature descriptor/&amp;lsquo;vocabulary&amp;rsquo; each point is a &amp;lsquo;word&amp;rsquo; that belongs to a vocabulary the words can then be counted and put into a frequency histogram the histogram is used to compare similarity of images I think similar images then get filtered out, so we get key frames</description></item><item><title>Loop closure detection</title><link>https://salehahr.github.io/zettelkasten/SLAM/loop-closure-detection/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/loop-closure-detection/</guid><description>Source: cometlabs See also: loop-closing-in-viorb Loop closure Process of observing the same scene by non-adjacent frames and adding a constraint (relationship? association?) between them A long-term data association in the VSLAM Framework (part of front end) Sort of incorporates topological SLAM into metric SLAM Importance Final refinement step (in data association) Important for obtaining a globally consistent SLAM solution, especially when optimising over a long period of time Basic loop closure detection Match the current frame to all previous frames using feature matching</description></item><item><title>Precision recall curve</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/precision-recall-curve/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/precision-recall-curve/</guid><description>Parent: Loop closure detection Source: cometlabs used to better quantify the performance (balance between false positives and false negatives in loop closure detection ) highlights tradeoff between precision and recall precision (absence of false positives) but may lead to the appearance of false negatives recall (prediction power) e.g. tweaking to improve recall increases sensitivity to similarities in the image thus increases possibility of false positives</description></item><item><title>(Wu 2018) Image-based camera localization</title><link>https://salehahr.github.io/zettelkasten/bibliography/wu-2018/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/wu-2018/</guid><description>Authors: Wu, Tang, Li
Abstract/Contents overview (classification) of image-based camera localization classification of image-based camera localization approaches techniques, trends only considers 2D cameras focuses on points as features in images (not lines etc) Chapters Classification of image-based camera localization approaches Multisensor fusion — why use the visual-inertial sensor combination? Loose vs Tight coupling Filter localisation methods Some optimisation-based tightly-coupled multisensor SLAM algorithms Questions What&amp;rsquo;s a metric map &amp;ndash; normal map (with landmarks, normal distances) as opposed to a topological one Takeaway Learning SLAM is gaining in popularity, but geometric SLAM is often the chosen method for most applications, as it is more generalisable and at the same time reasonably accurate For reliability and low cost practical applications, multisensor vision-centred fusion is an effective method.</description></item><item><title>Classification of image-based camera localization approaches</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/classification-of-image-based-camera-localization-approaches/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/classification-of-image-based-camera-localization-approaches/</guid><description>Parent: SLAM Index Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)
Unknown environment (must be reconstructed from image data) Online/real-time mapping (SLAM)
geometric metric SLAM (accurate computations, therefore still widely used in practice) monocular multiocular multi-kind sensor (active) loosely-coupled closely-coupled learning SLAM (active) needs a prior dataset to train NN &amp;ndash; dataset determines performanace of the SLAM low generalisation capability, therefore not as flexible as geom.</description></item><item><title>Cometlabs What You Need to Know About SLAM</title><link>https://salehahr.github.io/zettelkasten/bibliography/cometlabs/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/cometlabs/</guid><description>Source: http://blog.cometlabs.io/teaching-robots-presence-what-you-need-to-know-about-slam-9bf0ca037553
SLAM chicken and egg problem Position acquisition Multisensor fusion Sensors (absolute measurements) for measuring distance to landmarks Mapping representations in robotics Visual SLAM Implementation Framework Feature-based vs direct SLAM workflow Sparse/Feature-based VSLAM Dense/direct VSLAM</description></item><item><title>Dense/direct VSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/dense-direct-vslam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/dense-direct-vslam/</guid><description>Parent: Visual SLAM Implementation Framework , slam_index See also: Feature-based vs direct SLAM workflow Source: cometlabs Front-end part of the Visual SLAM Implementation Framework Use most or all of the pixels in each received frame Provide more information about the environment Many more pixels, require GPUs Feature-based vs direct SLAM workflow Disadvantages: Don&amp;rsquo;t handle outliers very well (outliers will be processed and implemented into the final map) Slower than feature-based variants Aims to minimise photometric error (intensity differences) Semi-dense</description></item><item><title>Feature maps</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/feature-maps/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/feature-maps/</guid><description>Parents: [Mapping representations in robotics](mapping representations-in-robotics.md), sparse/feature-based-vslam Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Uses a limited number of sparse objects to represent a map e.g. points, lines
Low computation cost because of the sparsity Map management solutions are good solutions for current applications What&amp;rsquo;s map management (probably storing maps in databases and recognising an existing map) [-] Sensitivity to false data association</description></item><item><title>Geometric metric SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/geometric-metric-slam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/geometric-metric-slam/</guid><description>Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md) Parent: Classification of image-based camera localization approaches Computes 3D maps with accurate mathematical equations
Classification according to sensors
monocular multiocular (most studies focus on binocular vision) multisensor fusion (e.g. vision and IMU &amp;ndash; vision and IMU fusion gaining in popularity) Classification according to techniques used
Filter-based SLAM Keyframe -based SLAM (active) Feature-based (keyframe-based feature SLAM) / sparse Direct / dense Grid-based SLAM (mainly deals with laser data, deals only a bit with image data) According to [76] keyframe-based can provide more accurate results compared to filter-based</description></item><item><title>Kidnapped robot problem</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kidnapped-robot-problem/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kidnapped-robot-problem/</guid><description>Source: Wikipedia Lokalisierung Backlinks: Lamarca 2020 DefSLAM Position initially known Then robot is repositioned without knowing it Robot has to be able to realise that the initial successful localisation isn&amp;rsquo;t valid any more &amp;ndash; a new global localisation must be carried out Realise this via unplausible sensor measurements (huge contradiction to prev. measurements) Has to do with the measure of robustness of the localisation method carried out</description></item><item><title>Localisation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/localisation/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/localisation/</guid><description>Parent: [SLAM Index](SLAM Index.md)
Source: [Wikipedia Lokalisierung](Wikipedia Lokalisierung.md) The positioning of an autonomous mobile robot relative to its environment
The position of a mobile robot is seldom known exactly An unknown initial position / measurement uncertainties while moving Becomes a SLAM problem when neither the position nor the map is known Goal/Output: POSE
Due to uncertainties etc, it&amp;rsquo;s good to have a POSE representation that also shows these uncertainties e.</description></item><item><title>Loose vs Tight coupling</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/loose-vs-tight-coupling/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/loose-vs-tight-coupling/</guid><description>Parent: SLAM Index Backlinks: Multisensor fusion Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)
In loosely-coupled systems: all sensor states are independently estimated and optimized
easier to process frame and IMU data less accurate/robust compared to tight coupling e.g. Integrated IMU data can be incorporated as independent measurements in stereo vision optimization e.g. Vision-only pose estimates are used to update an EKF so that IMU propagation can be performed In tightly-coupled systems: all sensor states are jointly estimated and optimized</description></item><item><title>Mapping representations in robotics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-representations-in-robotics/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-representations-in-robotics/</guid><description>Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Feature maps Occupancy grids Grids containing occupancy probability information Useful for path planning, exploration Drawback: computational complexity</description></item><item><title>Monocular cameras</title><link>https://salehahr.github.io/zettelkasten/sensors/monocular-cameras/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/monocular-cameras/</guid><description>Source: Cometlabs + Simpler hardware implementation + Smaller and cheapter systems - need complexer algos and software because of lack of direct depth information from a 2D image How is the shape of the map generated? Integrating measurements in the chain of frames over time Use triangulation method As well as camera motion, if camera isn&amp;rsquo;t stationary Depths of points are not oberved directly (s.</description></item><item><title>Multisensor fusion</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/multisensor-fusion/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/multisensor-fusion/</guid><description>Parent: SLAM Index , geometric-metric-slam Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Avoid limitations of using only one sensor Relative measurements: provide precise positioning information constantly At certain times absolute measurements are made to correct potential errors (correct drift) several approaches (for localisation), e.g. merge sensor feeds at the lowest level before being processed homogeneously hierarchical approaches (fuse state estimates derived independently from multiple sensors) s.</description></item><item><title>Position acquisition (relative vs. absolute)</title><link>https://salehahr.github.io/zettelkasten/sensors/position-acquisition/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/position-acquisition/</guid><description>See also: SLAM hardware Source: Cometlabs relative (interoceptive sensors) odometry absolute (exteroceptive sensors) can be used alongside relative measurement sensors in order to correct odometry drift beacons direct measurement instead of integrating, therefore error in position does not grow unbounded e.g. laser ranger finders, wifi (collect signal strength across field), GPS (bad for indoors) Lidar, Ultra Wide Band (UWB), Wireless Fidelity, etc [ Wu ] Compared to these, cameras are flexible and low-cost [ Wu ] (are also passive sensors) [ Cometlabs ]</description></item><item><title>RGB-D cameras</title><link>https://salehahr.github.io/zettelkasten/sensors/rgb-d-cameras/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/rgb-d-cameras/</guid><description>Source: Cometlabs Provide depth information directly Employed by most of the SLAM systems Generate 3D images through structured light or time of flight technology Structured light camera projects a known pattern onto objects Perceives deformation of pattern by an infrared camera This lets depth and surface information of the objects be calculated Time of flight ToF of a light signal between camera and objects is measured &amp;ndash;&amp;gt; from this, depth is obtained Structured light sensors are sensitive to illumination &amp;ndash; not applicable in direct sunlight Limitations Range data for semi-transparent or highly reflective surfaces are not reliable Limited effective range</description></item><item><title>SA TODO</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sa-todo/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sa-todo/</guid><description>Studienarbeit Camera-based localisation
Find a classification of approaches/techniques Briefly describe each See if it applies to the project Look into the most promising approach &amp;ndash; how to implement (DefSLAM) DefSLAM Install DefSLAM library Skim through an existing VI-SLAM (rigid) implementation to see how sensor fusion is done (as an overview for the coming sensor fusion task) VINS-Mono, VIORB paper Prepare dummy data for testing VI-SLAM (eventually VI-DefSLAM) — interpolate data between frames and add noise/bias Go through code Get the executables working VideoCapture OpenCV problem &amp;ndash; reinstall with all FFMMPEG options Figure out g2o Go through the rest of DefSLAM Go through the rest of ORBSLAM3 IMU term in cost function IMU preintegration IMU initialisation implement imu term in optimisation (either using ekf (s.</description></item><item><title>Sensors (absolute measurements) for measuring distance to landmarks</title><link>https://salehahr.github.io/zettelkasten/sensors/sensors-absolute/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/sensors-absolute/</guid><description>Parents: slam-hardware Source: Cometlabs Acoustic (Time of Flight) ToF technique Surfaces need to have good acoustic reflection Lack the ability to use surface properties for localisation examples Sonar Ultrasonic, ultrasound Laser rangerfinders ToF and phase-shift techniques Lack the ability to use surface properties for localisation e.g. Lidar Visual sensors for localisation Source: SLAM for Dummies Laser scanners Sonar, usually polaroid sonar [+] cheaper, good for underwater (s.</description></item><item><title>Some optimisation-based tightly-coupled multisensor SLAM algorithms</title><link>https://salehahr.github.io/zettelkasten/SLAM/algos-optimisation-based/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/algos-optimisation-based/</guid><description>Source: Wu 2018 Uses nonlinear optimization may potentially achieve higher accuracy due to the capability to limit linearization errors through repeated linearization of the inherently nonlinear problem [117] Forster 2017 : preintegration theory [118] OKVIS: a novel approach to tightly integrate visual measurements with IMU optimise a joint nonlinear cost function that integrates an IMU error term with the landmark reprojection error in a fully probabilistic manner real-time operation: old states are marginalized to maintain a bounded-sized optimization window Li et al.</description></item><item><title>Sparse/Feature-based VSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sparse-feature-based-vslam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sparse-feature-based-vslam/</guid><description>Parent: Visual SLAM Implementation Framework , slam_index See also: Feature-based vs direct SLAM workflow Source: cometlabs Front-end part of the Visual SLAM Implementation Framework Use only a small selected subset of the pixels in an image frame Feature maps generated are point clouds &amp;ndash;&amp;gt; used to track the camera pose Requires feature extraction and matching To minimise: reprojection error (difference between a point&amp;rsquo;s tracked location and where it is expected to be given camera pose estimate) Pose estimation based on RANSAC A frame with most of its features concentrated in a small area: bad as the features are more likely to overlap Sparse</description></item><item><title>Stereo cameras</title><link>https://salehahr.github.io/zettelkasten/sensors/stereo-cameras/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/stereo-cameras/</guid><description>Source: Cometlabs two cameras separated by a fixed distance (baseline) observations of the position of the same 3D point in both cameras allows depth to be calculated through triangulation (like humans do) depth measurement limited by baseline and resolution generally, wider baseline &amp;ndash;&amp;gt; better depth estimate (but occupies more physical space)</description></item><item><title>Topological SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/topological-slam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/topological-slam/</guid><description>Parent: Classification of image-based camera localization approaches Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)
does not need accurate computation of 3D maps represents the environment by connectivity or topology e.g. Kuipers [130] used a hierarchical description of the spatial environment
a topological network description mediates between a control and metrical level distinctive places and paths are defined by their properties at the control level serve as nodes and arcs of the topological model Decreasing in popularity</description></item><item><title>Visual SLAM Implementation Framework</title><link>https://salehahr.github.io/zettelkasten/SLAM/vslam-framework/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/vslam-framework/</guid><description>Source: Cometlabs Basic principle: tracking a set of points through successive frames these tracks are used to triangulate the 3D positions of the points to create the map at the same time, using the the est point locations to calculate the pose of the camera, which could have observed them (i.e. calculate real time 3D structure of a scene from the estimated motion of the camera) Architecture Front-end Abstracts sensor data into models (which are good for estimation) / Processing Data association Short term (feature tracking); features in consecutive sensor measurements Either from sparse maps or dense-maps Long term ( loop closure ; associating new measurements to older landmarks Back-end Performs inference on the abstracted data produced by the front end</description></item><item><title>What is SLAM?</title><link>https://salehahr.github.io/zettelkasten/SLAM/what-is-slam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/what-is-slam/</guid><description>Parent: SLAM index See also: slam-hardware Source: Scaradozzi 2018 Process which allows a mobile robot to
construct a map of its environment (assumed to be unknown) compute its location using the map simultaneously Source: Lamarca 2020 Goal is to locate a sensor in an unknown map/environment, which is simultaneously being reconstructed. Typically used in exploratory trajectories (new or changing environments) Source: Wikipedia SLAM Simultaneous localization and mapping (SLAM)</description></item><item><title>50.2.1 Process noise Q and W (odometry)</title><link>https://salehahr.github.io/zettelkasten/SLAM/50.2.1-process-noise-q-and-w-odometry/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/50.2.1-process-noise-q-and-w-odometry/</guid><description>See also: Factors affecting Kalman filter performance Source: Tereshkov 2015 Process noise covariance matrix has no clear physical meaning, cannot be deduced from sensor characteristics Leads to non-intuitive, iterative procedures to tune KFs Which means that KF optimality is rarely achieved in practice Alternative to KF tuning: the use of geometric observers
estimates are expresssed only in terms of quantities with clear geometrical meaning Source: Schneider 2013 If perfect model: $Q$ only describes the covariance of the random process noise Not perfect model, has: parametric errors (-&amp;gt; parameter identification) structural erors (error in model structure) workaround: e.</description></item><item><title>50.2.2 Measurement noise R, V (landmark)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.2-measurement-noise-r-v-landmark/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.2-measurement-noise-r-v-landmark/</guid><description>Parent: Multivariate Kalman filter algorithm Source: rlabbe Kalman/Bayesian filters in Python R models the noise in the sensors as a covariance matrix dim(R) = m x m (m: number of sensors) Possible complications in multisensor systems, the correlation between the sensors might not be clear sensor noise might not be pure Gaussian Source: http://www.linkedin.com/pulse/tuning-extended-kalman-filter-process-noise-training-alex-thompson Ways to obtain R
Using the variances given in the sensor specifications Comopare the measurements against a strong ground truth and derive the variance variable by variable Record the steady state measurements over a long period of time and measure the variance (look at the histogram) Source: [Schneider 2013 How to not make the EKF fail](schneider 2013 how to-not-make-the-ekf-fail.</description></item><item><title>Covariance matrix P</title><link>https://salehahr.github.io/zettelkasten/SLAM/covariance-matrix-p/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/covariance-matrix-p/</guid><description>Source: SLAM for Dummies s. also EKF matrices Covariance matrix P
Covariance: measure of correlation of two variables Correlation: measure of degree of linear dependence A covariance of the robote POSEupdated in Step 1: Odometry update 3x3 B .. C covariance on the first .. nth landmarkStep 3: New landmarks 2x2 D covariance between POSE and first LMupdated in Step 1: Odometry update 2x3 E, etc E = D^T, etcupdated in Step 1: Odometry update 3x2 F=G^T Step 3: New landmarks Initially $P = A$ (robot has not seen any LMs)</description></item><item><title>Measurement model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/measurement-model/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/measurement-model/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices/vectors Estimate of the range and bearing (from landmark) in Step 2: Re-observation x, y, theta - current position estimate lambdax, y - landmark position
Jacobian H w.r.t. x, y, theta (here for regular EKF, not for extended) In SLAM we need additional values for the landmarks here for landmark number two in extended EKF Upper row is for information, not part of matrix First three columns are regular H Landmarks don&amp;rsquo;t have any rotation</description></item><item><title>Prediction model</title><link>https://salehahr.github.io/zettelkasten/SLAM/prediction-model/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/prediction-model/</guid><description>Source: SLAM for Dummies Used in the prediction step .
How to compute an expected position of the robot given the old position and the control input (so basically based on odometry .
Control terms are $\Delta x, \Delta y, \Delta \theta$
$$ \begin{align} f &amp;amp;= \left[ \begin{array}{c} x + \Delta t \cos \theta + q \Delta t \cos \theta \
y + \Delta t \sin \theta + q \Delta t \sin \theta \</description></item><item><title>SLAM-specific jacobians</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/slam-specific-jacobians/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/slam-specific-jacobians/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices/vectors Jxr
Jacobian of the prediction of landmarks, which does not include prediction of theta, w.r.t. robot POSE same as J_prediction model, except without rotation term Jz Jacobian of prediction of landmarks, but w.r.t. [range, bearing]</description></item><item><title>Step 1 Odometry update (Prediction step)</title><link>https://salehahr.github.io/zettelkasten/SLAM/ekf-1-prediction/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/ekf-1-prediction/</guid><description>Parent: Basic EKF for SLAM Source: SLAM for Dummies First step in the three-step EKF Update current state using odometry data Based on the controls given to the robot Calculate estimate of new POSE Update equation: prediction model ($x = x + \Delta x \cdot q$)
Or in a simple model, neglect the error term $q$
State vector gets updated via the prediction model
Jacobian of the prediction model also needs to be updated every iteration (with the controls deltax, &amp;hellip;)</description></item><item><title>Step 2 Re-observation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-2-reobservation/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-2-reobservation/</guid><description>Parent: Basic EKF for SLAM Source: SLAM for Dummies Second step in the three-step EKF — overview
In this step we update the robot position that we got in [step 1]](studienarbeit/step-1-odometry-update-prediction-step.md) Compensate for errors due to odometry pos_est (odometry-based) - pos_actual (LM-based) = Innovation, (based on the LM that the robot can see)
Use this to update robot position
Update the uncertainty of each observed LM to reflect recent changes e.</description></item><item><title>Step 3 New landmarks</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-3-new-landmarks/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-3-new-landmarks/</guid><description>Parent: Basic EKF for SLAM Source: SLAM for dummies Overview
Landmarks that are new are not dealt with until step 3. Delaying the incorporation of new landmarks until the will decrease the computation cost needed for this step the covariance matrix, P, and the system state, X, are smaller by then. Update state vector x and covariance matrix P with new landmarks Add new landmark to state vector X Add new row and column to covariance matrix Covariance for new landmark Robot-landmark covariance</description></item><item><title>Basic EKF for SLAM</title><link>https://salehahr.github.io/zettelkasten/SLAM/basic-ekf-for-slam/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/basic-ekf-for-slam/</guid><description>Source: SLAM for Dummies A basic EKF implementation of SLAM consists of multiple parts:
Landmark extraction Data association After odometry change (due to robot moving), state estimation from odometry Update of the estimated state using re-observed landmark data Update landmark database with new landmarks Note: at any point in the three steps on the left, the EKF will have an estimate of the robots current position</description></item><item><title>Data association</title><link>https://salehahr.github.io/zettelkasten/SLAM/data-association/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/data-association/</guid><description>Source: SLAM for Dummies Matching observed landmarks from different scans (different time steps) with each other. Also called &amp;rsquo;re-observing' landmarks.
Problems that can arise The landmark(s) might not be observed every time step (bad landmark) Something might be observed as a landmark, but it never appears again (bad landmark) Wrong association of a landmark to a previously seen landmark Goal Define a suitable data-association policy to minimise the first two problems</description></item><item><title>EKF matrices/vectors</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-matrices-vectors/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-matrices-vectors/</guid><description>Source: SLAM for Dummies System state X Estimate of POSE Jacobian of prediction model Landmark range and bearing Jacobian of measurement model Covariance matrix P Kalman gain K SLAM-specific jacobians</description></item><item><title>Extended Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/SLAM/extended-kalman-filter/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/extended-kalman-filter/</guid><description>Parent: SLAM Index Backlinks: RANSAC , nearest neighbour , Filtering in localisation Source: SLAM for Dummies keeps track of an estimate of the position uncertainty keeps track of the uncertainty in the features/landmarks seen General EKF implementation (non-SLAM) Basic EKF for SLAM Diagram: Triangle Robot Stars Landmarks Dashed triangle Robot&amp;rsquo;s position based on odometry alone (where it thinks it is) Dotted triangle Robot&amp;rsquo;s position estimate based on EKF Solid line triangle Robot&amp;rsquo;s actual position in real life!</description></item><item><title>Landmarks</title><link>https://salehahr.github.io/zettelkasten/SLAM/landmarks/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/landmarks/</guid><description>Source: SLAM for Dummies Features which can easily be re-observed and distinguished from the environment
Characteristics:
Re-observable from different positions and angles Unique (i.e. no mix-up with other landmarks) Plentiful &amp;ndash; should not be so few that robot gets lost (robot spends extended time w/o enough visible landmarks) Stationary Basic landmark extraction</description></item><item><title>RANSAC</title><link>https://salehahr.github.io/zettelkasten/SLAM/ransac/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/ransac/</guid><description>Source: SLAM for Dummies Random Sampling Consensus to extract lines from a laser scan lines then used as landmarks indoors: straight lines from walls line landmarks are found by randomly taking a sample of laser readings (e.g. sample readings from 12deg to 22deg from within a range of 0 to 180deg) least squares approximation for line of best fit RANSAC then checks how many laser readings lie close to the best fit line initially, all readings are assumed to be unassociated to any lines if the num.</description></item><item><title>Riisgaard SLAM for dummies</title><link>https://salehahr.github.io/zettelkasten/bibliography/riisgaard-slam-for-dummies/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/riisgaard-slam-for-dummies/</guid><description>Authors: Søren Riisgaard and Morten Rufus Blas Parent: SLAM resources Abstract:
Tutorial introduction to SLAM, with minimal prerequisites for the understanding of SLAM as explained here Mostly explains a single approach to the steps involved in SLAM Complete solution for SLAM using EKF (extended Kalman filter) Only considers 2D motion, not 3D Chapters
What is SLAM? Overview of SLAM using EKF Hardware Robot Range measurement device SLAM process Step 1: Odometry update Step 2: Reobservation Step 3: Add new landmarks Laser data Odometry data Landmarks Landmark extraction 1.</description></item><item><title>SLAM hardware</title><link>https://salehahr.github.io/zettelkasten/sensors/slam-hardware/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/slam-hardware/</guid><description>Parent: SLAM index See also: Position acquisition (relative vs. absolute) Source: SLAM for Dummies Robot parameters to consider Ease of use Odometry performance: how well the robot can estimate its own position, just from the rotation of the wheels Max errors: 2cm per meter moved, 2deg per 45deg turned Bad odometry &amp;ndash;&amp;gt; bad estimation of current position &amp;ndash;&amp;gt; hard to implement SLAM Range measurement device options</description></item><item><title>SLAM resources</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/slam-resources/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/slam-resources/</guid><description>Parent: SLAM Index Theory
Wikipedia SLAM http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation Thrun - Probabilistic Robotics SLAM for dummies Andrew Davison research page at the Department of Computing , Imperial College London about SLAM using vision. Paper 2002 on monocular SLAM SLAM lectures on YouTube http://openslam-org.github.io / Tutorials SLAM summer school SS06: http://www.robots.ox.ac.uk/~SSS06/Website/
Programming Programmatic implementations of MonoSLAM</description></item><item><title>Spike landmarks</title><link>https://salehahr.github.io/zettelkasten/SLAM/spike-landmarks/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/spike-landmarks/</guid><description>Source: SLAM for Dummies Uses extrema to find landmarks Find values in the range of a laser scan, where two values differ by more than a certain amount (e.g. 0.5 m) This finds big changes in the laser scan Alternatively, using three values next to each other: $A, B, C$ $(A - B) + (C - B)$ yields a value Better for finding spikes as it finds actual spikes Rely on the landscape changing a lot between two laser beams Algo will fail in smooth environments Suitable for indoor environments, however is not robust against envs w/ people people are picked up as spikes as theoretically they are good landmarks (just not stationary!</description></item><item><title>Wikipedia SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-slam/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-slam/</guid><description>Source: http://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping Parents: SLAM Index , slam-resources Different types of sensors give rise to different SLAM algorithms whose assumptions are most appropriate to the sensors.
At one extreme, visual features provide details of many points within an area &amp;ndash;&amp;gt; rendering SLAM unnecessary shapes in these point clouds can be easily and unambiguously aligned at each step via image registration . At the opposite extreme, tactile sensors are extremely sparse they contain only information about points very close to the agent require strong prior models to compensate in purely tactile SLAM.</description></item><item><title>Wikipedia Visual odometry</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-visual-odometry/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-visual-odometry/</guid><description>Source: http://en.wikipedia.org/wiki/Visual_odometry Odometry Visual sensors for localisation</description></item><item><title>External data in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/external-data-in-sofa/</link><pubDate>Fri, 24 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/external-data-in-sofa/</guid><description>Parent: SofaPython Index http://www.sofa-framework.org/community/forum/topic/how-to-use-external-data-in-sofa/ http://www.sofa-framework.org/community/forum/topic/how-to-send-data-to-sofa-through-socket/ http://www.sofa-framework.org/community/forum/topic/connecting-sofa-to-an-external-data-com-port/</description></item><item><title>http://www.sofa-framework.org/applications/gallery/percutaneous-liver-surgery/</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/https-www.sofa-framework.org-applications-gallery-percutaneous-liver-surgery-/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/https-www.sofa-framework.org-applications-gallery-percutaneous-liver-surgery-/</guid><description>http://www.sofa-framework.org/applications/gallery/percutaneous-liver-surgery/ Constraint-based haptic rendering</description></item><item><title>Registration</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/registration/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/registration/</guid><description> Aligning two points, each in different spaces respectively, together Examples aligning an endoscope coordinate frame to CT data (based on a similarity metric between endoscopic image and CT image) &amp;ndash; from https://pubmed.ncbi.nlm.nih.gov/25991876/ match virtual surface to corresponding endoscopic video &amp;ndash; https://ieeexplore.ieee.org/document/958638 Parent: SofaPython Index allows a matching between deformable surfaces finds spatial transformations to align two point sets or two meshes done based on: either target surfaces (ClosestPointRegistrationForceField , RegistrationContactForceField) or target images (IntensityProfileRegistrationForceField), which requires the use of the image plugin</description></item><item><title>Topological changes during elastic registration</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/topological-changes-during-elastic-registration/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/topological-changes-during-elastic-registration/</guid><description>http://www.sofa-framework.org/applications/gallery/augmented-reality-in-nephrology/
http://www.youtube.com/watch?v=3rfdL3-wWE0</description></item><item><title>Sockets Errno 10054</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sockets-errno-10054/</link><pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sockets-errno-10054/</guid><description>Parent: SofaPython Index WSAECONNRESET 10054 Connection reset by peer. An existing connection was forcibly closed by the remote host. This normally results if the peer application on the remote host is suddenly stopped, the host is rebooted, the host or remote network interface is disabled, or the remote host uses a hard close (see setsockopt for more information on the SO_LINGER option on the remote socket). This error may also result if a connection was broken due to keep-alive activity detecting a failure while one or more operations are in progress.</description></item><item><title>Building SOFA on Windows</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/building-sofa-on-windows/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/building-sofa-on-windows/</guid><description>Parent: SofaPython Index http://www.sofa-framework.org/community/doc/getting-started/build/windows/</description></item><item><title>Install ROSConnector in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/install-rosconnector-in-sofa/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/install-rosconnector-in-sofa/</guid><description>Parent: SofaPython Index http://github.com/sofa-framework/SofaROSConnector Documentation (outdated for current SOFA version 20.06.00)
http://www.sofa-framework.org/community/forum/topic/error-configuring-cmake-sofarosconnector/ Pending answer. Last reply 10th July 2020.
Alternative using SoftRobots: &amp;lt;http://www.sofa-framework.org/community/forum/topic/error-with-plugins-with-sofarosconnector/
post-15665&amp;gt; http://project.inria.fr/softrobot/</description></item><item><title>Sending data using sockets</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sending-data-using-sockets/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sending-data-using-sockets/</guid><description>Parent: SofaPython Index http://github.com/psomers3/PyDataSocket
http://docs.python.org/3/howto/sockets.html Sockets: form of IPC (inter-process communication), for cross-platform communication (Alternatives, for fast IPC: pipes, shared memory)
“client” socket - an endpoint of a conversation e.g. browser, other client applications “server” socket, which is more like a switchboard operator. The client application (your browser, for example) uses e.g. web server (uses both server and client sockets) Roughly, how a socket works (ex: clicking a link on the browser) Client socket (browser) / Receive</description></item><item><title>SofaPython API/Documentation links</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sofapython-api-documentation-links/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sofapython-api-documentation-links/</guid><description>Parent: SofaPython Index SP2
SofaPython pdf http://www.sofa-framework.org/api/master/plugins/SofaPython/html/index.html http://sofacomponents.readthedocs.io/en/latest/index.html SP3
http://sofapython3.readthedocs.io/en/latest/menu/SofaPlugin.html</description></item><item><title>STLIB (Sofa Template Library)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/stlib-sofa-template-library/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/stlib-sofa-template-library/</guid><description>Parent: SofaPython Index http://github.com/SofaDefrost/STLIB
API doc: http://stlib.readthedocs.io/en/latest/index.html
contains sofa scene template common scene template used regularly templates should be compatible with .pyscn and PSL scenes</description></item><item><title>Possible plugins</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/possible-plugins/</link><pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/possible-plugins/</guid><description>Parent: Scope of Studienarbeit , sofapython-index Communication
ZMQCommunication someone&amp;rsquo;s own plugin Optical system
OptiTrackNatNet Mesh geometry/topology
CGALPlugin (computational geometry algorithms) Haptic
Haptics with Geomagic &amp;ndash; requires Geomagic probe, but code/intro may be useful SofaHaptics Haption Flexible - for deformations Sensable Robot arm
SoftRobots ROS Connector  too complicated Scenes
STLIB (Sofa Template Library) Registration</description></item><item><title>SOFA Cataract surgery</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sofa-cataract-surgery/</link><pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sofa-cataract-surgery/</guid><description>http://www.sofa-framework.org/applications/gallery/eye-surgery-simulator-insimo/
SOFA – Cataract Surgery – InSimo www.sofa-framework.orgThe SOFA technology is at the core of a advanced eye surgery simulator developed in the context of the HelpMeSee project. HelpMeSee is an American foundation with a singular mission:… read more →</description></item><item><title>Basic python script in Sofa</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/basic-python-script-in-sofa/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/basic-python-script-in-sofa/</guid><description>Parent: SofaPython Index Imports import Sofa
General functions
createGraph(self, root) reset() onKeyPressed() &amp;hellip; Required in every script: createScene(rootNode)
Create a child from a node node.createChild(&amp;lsquo;Name&amp;rsquo;)
Add an object component to the node: node.createObject(type in string, kwargs**)</description></item><item><title>Collision model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/collision-model/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/collision-model/</guid><description>Source: SOFA extended documentation Parent: Models in SOFA Primitives coming into contact — we need
collision detection collision response Collision detection approaches:
Distances between pairs of geometric primitives Points in distance fields Distances between colliding meshes using ray-tracing Intersection volume using images Collision model
Similar to internal model
Topology/geometry is different (geometry specially for contact model), can be stored in a data structure dedicated to collision detection e.</description></item><item><title>Data structure in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/data-structure-in-sofa/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/data-structure-in-sofa/</guid><description>Source: SOFA extended documentation Three different solutions for three relevant levels [of organisation of simulation data].
Scenegraph ( directed acyclic graphs) s. also: Visitors Component attributes
Component params stored using Data&amp;lt;&amp;hellip;&amp;gt; containers e.g. list of particle indices Data&amp;lt;vector&amp;gt; Engines Create connections between Data instances, for synchronisation of values Compute a value from several others (input/output processor) Are only used to apply straightforward relations between model parameters State update algorithms are implemented using visitors Mesh geometry and mesh-topology</description></item><item><title>Haptic rendering</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/haptic-rendering/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/haptic-rendering/</guid><description>Source: SOFA extended documentation The main interest of interactive simulation is that
the user can modify the course of the computations in real-time when a virtual medical instrument comes into contact with some models of a soft-tissue, instantaneous deformations must be computed This visual feedback of the contact can be enhanced by haptic rendering so that the surgeon can really feel the contact.&amp;quot; Two main issues in SOFA for providing haptics</description></item><item><title>Internal model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/internal-model/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/internal-model/</guid><description>Source: SOFA extended documentation Parent: Models in SOFA For the internal deformable mechanics
Contains the independent DOFs, mass and physical laws Mechanical behaviour modelled e.g. by FEM Geometry of this model is optimised for the computation of internal forces usually by using a reduced number of well-shaped tetrahedra this increases speed and stability however not accurate enough for collision detection nor is it smooth enough for visuals * Boxes: fixed nodes* Arrows: external forces [Mathematical model of the internal model in SOFA](mathematical model of-the-internal-model-in-sofa.</description></item><item><title>Mappings</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mappings/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mappings/</guid><description>Source: [SOFA extended documentation](SOFA extended documentation.md) Backlinks: Models in SOFA , Visual model Enforces consistency between the many model representations of an object, by propagating information (such as positions, velocities, forces) in a top-down and bottom-up approach. Figure: Mappings between liver and grasper models
Master model imposes its displacements to the slave models ([top-down mapping](top-down mapping.md)) Slaves, depending on model type, can also pass information (e.g. forces) back to the master (bottom-up) A mapped model can be master of another model Also used to connect generalised coordinates (e.</description></item><item><title>Mesh geometry</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mesh-geometry/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mesh-geometry/</guid><description>Source: SOFA extended documentation Parent: Data structure in SOFA See also: Mesh topology Mesh geometry: location of vertices in space
Meshes
k-simplices (triangles) k-cubes (quads) --&amp;gt; decomposition into k-cells
1-cell: edges 2-cells: triangles, quads 3-cells: tetrahedron, hexahedron Mesh data:
containers, similar to STL std::vector classes there are as many data structures for mesh data as topological elements , e.g. vertices, edges, triangles, quads, tetras, hexas e.</description></item><item><title>Models in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/models-in-sofa/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/models-in-sofa/</guid><description>Source: SOFA extended documentation Backlinks: SOFA Introduction A simulation object can have several models
Each model is &amp;lsquo;predestined&amp;rsquo; for a certain task Each model is independent of the other Synchronisation of models: via a mapping mechanism Three typical models for a physical object
Internal mechanical model Collision model Visual model One of the models acts as the master
typically the internal model imposes its displacements to slaves using mappings (synchronisation of the models)</description></item><item><title>Running SOFA with Python</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/running-sofa-with-python/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/running-sofa-with-python/</guid><description>Parent: SofaPython Index From command line Add to path environment and then execute runSofa via command line
With a python script runSofa -l SofaPython ./script_name.py
How to make SofaPython loaded by default? In bin/plugin_list.conf? Yes sofa-launcher might be useful With pipenv pipenv run runsofa</description></item><item><title>Scene graph in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/scene-graph-in-sofa/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/scene-graph-in-sofa/</guid><description>Source: SOFA extended documentation Parent: Data structure in SOFA Backlinks: SOFA Introduction See also: Scene graph (general) Pool of simulated objects and algorithms in a hierarchical data structure Scenes can be built procedurally or read from XML files Root node represents whole simulation Graph is processed by using visitors A scene graph node
Gathers components associated with the same DOFs/topology Connections between non-sibling components require explicit references Example: The collision spheres of the rigid object are in a child contact node of their own, because they are not independent DOFs (separate from independent DOFs in MechanicalState ) they are of a different data type Interactions between the rigid and deformable objects are handled by a shared component (ContactSpring) defined as a sibling node to both (coupling) Soft coupling using penalty forces Can be modelled by a constant interaction force (assumption) during each time step Compatible with all explicit time intergration schemes Hard coupling using penalty forces / constraint-based interaction via Lagrange multipliers Stiff interaction forces Implicit integration necessary, for large time steps without any instabilities Generally more efficient to process independent interaction groups using separate solvers</description></item><item><title>SOFA extended documentation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sofa-extended-documentation/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sofa-extended-documentation/</guid><description>Source: http://hal.inria.fr/hal-00681539 Authors: Faure et al Backlinks: Scope of Studienarbeit Abstract
SOFA: open source C++ library mainly for interactive physical/medical simulation modular approach by decomposing simulators into its constituent components (DOF, differential equations, solvers etc), and organising them in a scenegraph data structure multimodel representation of objects (collision model, visual model etc) Chapters
Read
1: introduction 2: multimodel framework 3: data structures 3.1 scenegraph and visitors 3.</description></item><item><title>SOFA Introduction</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sofa-introduction/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sofa-introduction/</guid><description>Source: SOFA extended documentation Goal of SOFA: To provide a highly modular framework for interactive medical simulation, enabling collaboration across different disciplines Concept: scene-graph -based multimodel representatios
How it works:
Simulators are broken down into independent components Component: an aspect of the simulation e.g. DOF, forces, constraints, ODEs/PDEs, solvers, algorithms Components are organised in a scene graph data structure Simulated objects represented via several models</description></item><item><title>Using python with existing scene</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/using-python-with-existing-scene/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/using-python-with-existing-scene/</guid><description>Parent: SofaPython Index In scene graph
Add plugin in the scene using RequiredPlugin Define a PythonScriptController in the scene graph</description></item><item><title>Visitors</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/visitors/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/visitors/</guid><description>Source: [SOFA extended documentation](SOFA extended documentation.md) Parent: [Data structure in SOFA](Data structure in SOFA.md) Backlinks: [Scene graph in SOFA](Scene graph in SOFA.md), [Simulation algorithms in SOFA](Simulation algorithms in SOFA.md)
For processing of data structure: parent to child
Allows decoupling of physical model from simulation algo e.g. Easy to replace a time integrator, which wouldn&amp;rsquo;t be the case in a dataflow graph (coupling of data and algo)</description></item><item><title>(GRK 2543) Intraoperative Multi-sensor Tissue Differentiation in Oncology</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/grk-2543/</link><pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/grk-2543/</guid><description>Sources:
Deutsche Forschungsgemeinschaft project page ISYS project page Background:  Cooperation between Uni Tübingen and Uni Stuttgart Gynelogical and urological application scenarios Aims: Minimise invasiveness and duration of surgical cancer treatment , while at the same time maximising effectiveness of the treatment Minimise damage to surrounding tissue during tumour resection Aid decision-making during surgery (intraoperative) Reliable differentiation between cancerous tissue and the surrounding healthy tissue Decide whether to preserve tissue or continue with surgery Complement existing techniques Histological Imaging Optical (IR, Raman spectroscopy) Current standard of intraoperative tissue identification: frozen section diagnostics (takes about 30 minutes)</description></item><item><title>Difference between haptic feedback and vibration alerting</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/difference-between-haptic-feedback-and-vibration-alerting/</link><pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/difference-between-haptic-feedback-and-vibration-alerting/</guid><description>Source: http://www.precisionmicrodrives.com/haptic-feedback/introduction-to-haptic-feedback/ Parent: Scope of Studienarbeit</description></item><item><title/><link>https://salehahr.github.io/zettelkasten/chaos/chaos-and-determinism/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/chaos/chaos-and-determinism/</guid><description>Chaos and determinism One might think that chaotic systems might have nothing to do with deterministic systems.
Chaotic systems are described to be &amp;lsquo;unpredictable&amp;rsquo;. It is normally assumed that &amp;lsquo;deterministic = predictable&amp;rsquo;. However, in real life, the information/data of a natural system is never completely available—we don&amp;rsquo;t know everything about the system, there are modelling errors, etc.
This is at odds with the assumption that one makes when analysing deterministic systems: that all the data of the system is available.</description></item><item><title/><link>https://salehahr.github.io/zettelkasten/chaos/sensitivity-to-initial-conditions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/chaos/sensitivity-to-initial-conditions/</guid><description>Sensitivity to initial conditions Deterministic systems can exhibit chaotic behaviour if they are highly sensitive to initial conditons.
This sensitivity to initial conditions is the hallmark of a chaotic system.
Thus, if a system is described to be deterministic, but without 100% knowledge of the system parameters, this system may well be very unpredictable if it is also a chaotic deterministic system.</description></item><item><title/><link>https://salehahr.github.io/zettelkasten/chaos/what-is-chaos-theory/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/chaos/what-is-chaos-theory/</guid><description>What is chaos theory? A branch of mathematics that deals with nonlinear dynamical systems.
System: a set of interacting components that form a larger whole Nonlinear: the whole system is something greater than just the additions (superposition) of the effects of the individual parts, due to nonlinear relationships such as feedback multiplicative effects between components &amp;hellip; Dynamical: the system changes over time based on its current state Chaotic systems are a subset of nonlinear dynamical systems.</description></item><item><title/><link>https://salehahr.github.io/zettelkasten/lang/ru-to-do/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/lang/ru-to-do/</guid><description>Ongoing Reading Мы все из Бюллербю To do Skimmed through verbs chapter [A Basic Modern Russian Grammar] Grammar drills &amp;ndash; verbs Done</description></item><item><title/><link>https://salehahr.github.io/zettelkasten/studienarbeit/optical-flow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/optical-flow/</guid><description>https://en.wikipedia.org/wiki/Optical_flow https://nanonets.com/blog/optical-flow/</description></item><item><title/><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-23-data-augmentation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-23-data-augmentation/</guid><description>Data Augmentation To try: try a coordinate transformation of the node positions instead.
The current rotation transformation:
Rotation of an integer matrix results in floats between 0 and 1.</description></item><item><title/><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-12/2021-12-14-node-detection-model-first-run/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-12/2021-12-14-node-detection-model-first-run/</guid><description>First run of model 20211210-102252\train 64 filters in first layer model architecture
Validation results Pred | Truth
Pred | Truth
Pred | Truth
Pred | Truth
Total loss
Partial losses
Node positions converge the fastest Node types converge the slowest</description></item><item><title>Introduction to the Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/introduction-to-the-kalman-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/introduction-to-the-kalman-filter/</guid><description> http://resourcium.org/journey/introduction-kalman-filter</description></item><item><title>Motion parallax</title><link>https://salehahr.github.io/zettelkasten/definitions/motion-parallax/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/definitions/motion-parallax/</guid><description>Motion parallax closer objects visually shift much more than distant ones when we move</description></item></channel></rss>