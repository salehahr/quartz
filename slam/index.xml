<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SLAMs on Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/slam/</link><description>Recent content in SLAMs on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><lastBuildDate>Fri, 20 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://salehahr.github.io/zettelkasten/slam/index.xml" rel="self" type="application/rss+xml"/><item><title>Initialisation of monocular SLAM</title><link>https://salehahr.github.io/zettelkasten/SLAM/initialisation-of-monocular-slam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/initialisation-of-monocular-slam/</guid><description>Source: Lamarca 2019 DefSLAM Depth information has to be generated before localisation can be performed — how?
Capture multiple images which have enough parallax These images with parallax allows depth information to be calculated (this uses motion parallax ) From these images, the map can be generated Localisation can then be carried out with respect to the map (as long as camera doesn&amp;rsquo;t move off to an unexplored region)</description></item><item><title>Loop closing in VIORB</title><link>https://salehahr.github.io/zettelkasten/SLAM/loop-closing-in-viorb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/loop-closing-in-viorb/</guid><description>Source: Mur-Artal 2017 VI-ORB See also: Loop closure detection (general) Overview To reduce drift accumulated during exploration (when returning to an already mapped location) Loop detection: of large loops using place recognition Loop correction: first do lightweight pose-graph optimisation (PGO), then do full BA in a separate thread (in order not to interfere with real-time operations) Implementation After loop detection: do match validation (alignment of points between keyframes) Then pose-graph optimisation to reduce the accumulated error in trajectory (PGO: pose-only, ignores IMU info) IMU info ignored, but velocities are corrected by rotating them according to keyframe orientation &amp;ndash;&amp;gt; suboptimal, but should be accurate enough to allow IMU data to be used right after the PGO in ORBSLAM: PGO is 7-DoF optimisation (due to scale + 3 rot + 3 xyz) in VIORB, 6 DoF (scale is known from initialisation bzw.</description></item><item><title>Why use the visual-inertial sensor combination?</title><link>https://salehahr.github.io/zettelkasten/SLAM/why-use-the-visual-inertial-sensor-combination/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/why-use-the-visual-inertial-sensor-combination/</guid><description>See also: Multisensor fusion Source: Mur-Artal 2017 VI-ORB Cheap but also with good potential Cameras provide rich information but are relatively cheap IMU provides self-motion info, helps recover scale in monocular applications enables estimation of the direction of gravity &amp;ndash;&amp;gt; renders pitch and roll observable Source: Forster 2017 IMU Preintegration Visual-inertial fusion for 3D structure and motion estimation Both cameras and IMUs are cheap, easy to find and complement each other well Camera exteroceptive sensor measures, up to a to-be-determined metric scale, appearance and geometrical structure of a 3D scene IMU interoceptive sensor makes metric scale of monocular cameras, as well as the direction of gravity, observable Source: (Wu 2018) Image-based camera localization Cameras provide rich information of a scene IMU provide odometry self-motion information and accurate short-term motion estimates at high frequency Source: Mirzaei 2008 A Kalman Filter-Based Algorithm for IMU-Camera Calibration: Observability Analysis and Performance Evaluation</description></item><item><title>Filter localisation methods</title><link>https://salehahr.github.io/zettelkasten/SLAM/filter-localisation-methods/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/filter-localisation-methods/</guid><description>Backlinks: Back-end optimisation , what is slam? Filtering vs. optimisation Source: Wu 2018 EKF to propagate and update motion states of visual-inertial sensors
Source: Scaradozzi 2018 Filtering techniques in SLAM
Augment/refine the position estimates and map estimates by incorporating new measurements when they become available Generally online, due to their incremental nature Types Kalman filters Particle filters</description></item><item><title>Distance between landmarks</title><link>https://salehahr.github.io/zettelkasten/SLAM/distance-between-landmarks/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/distance-between-landmarks/</guid><description>Source: SLAM for Dummies Methods:
Euclidean distance (suitable for far distances) Mahalanobis distance (better, but more complex)</description></item><item><title>Landmark extraction</title><link>https://salehahr.github.io/zettelkasten/SLAM/landmark-extraction/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/landmark-extraction/</guid><description>Source: SLAM for Dummies Basic landmark extraction using a laser scanner
Spike algorithm RANSAC ( EKF handles points) Expansion of RANSAC so that EKF handles lines Scan-matching: two successive laser scans are matched Spike and RANSAC are good for indoor environments</description></item><item><title>Nearest Neighbour</title><link>https://salehahr.github.io/zettelkasten/SLAM/nearest-neighbour/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/nearest-neighbour/</guid><description>Source: SLAM for Dummies Nearest neighbour approach
Get a new laser scan &amp;ndash;&amp;gt; ( landmark extraction ) extract all visible landmarks Associate each extracted LM to the closest LM we have seen more than $N$ times Pass each pairs of association (extracted LM, LM in database) through a validation gate If pair passes &amp;ndash;&amp;gt; $n = n + 1$ (num. times seen) If pair fails &amp;ndash;&amp;gt; add new LM to database, with $n := 1$</description></item><item><title>SLAM Index</title><link>https://salehahr.github.io/zettelkasten/SLAM/slam_index/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/slam_index/</guid><description>Definition Localisation What is SLAM? Sensors for SLAM Position acquisition (relative vs. absolute) SLAM hardware Relative Odometry IMU Absolute Sensors (absolute measurements) for measuring distance to landmarks Visual sensors for localisation Monocular depth perception Pinhole camera model Camera calibration World to camera trafo Fusion Multisensor fusion Loose vs Tight coupling Why use the visual-inertial sensor combination?</description></item><item><title>Validation gate</title><link>https://salehahr.github.io/zettelkasten/SLAM/validation-gate/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/validation-gate/</guid><description>Source: SLAM for Dummies An observed landmark is associated to a landmark if the following holds
$$ \begin{aligned} v_i^T S_i^{-1} v_i \leq \lambda \end{aligned} $$
v innovation S innovation covariance The validation gate makes use of the fact that the EKF implementation gives a bound on the uncertainty of an observation of a landmark .
Is an observed LM a LM in the database?</description></item><item><title>Key frames in loop closure detection</title><link>https://salehahr.github.io/zettelkasten/SLAM/key-frames-in-loop-closure-detection/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/key-frames-in-loop-closure-detection/</guid><description>Source: cometlabs Most common method to get candidate key frames: use a place recognition approach
approach based on vocab tree feature descriptors of candidate key frames are quantised one colour in image below corresponds to one feature descriptor/&amp;lsquo;vocabulary&amp;rsquo; each point is a &amp;lsquo;word&amp;rsquo; that belongs to a vocabulary the words can then be counted and put into a frequency histogram the histogram is used to compare similarity of images I think similar images then get filtered out, so we get key frames</description></item><item><title>Loop closure detection</title><link>https://salehahr.github.io/zettelkasten/SLAM/loop-closure-detection/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/loop-closure-detection/</guid><description>Source: cometlabs Loop closure Process of observing the same scene by non-adjacent frames and adding a constraint (relationship? association?) between them A long-term data association in the VSLAM Framework (part of front end) Sort of incorporates topological SLAM into metric SLAM Importance Final refinement step (in data association) Important for obtaining a globally consistent SLAM solution, especially when optimising over a long period of time Basic loop closure detection Match the current frame to all previous frames using feature matching</description></item><item><title>Some optimisation-based tightly-coupled multisensor SLAM algorithms</title><link>https://salehahr.github.io/zettelkasten/SLAM/algos-optimisation-based/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/algos-optimisation-based/</guid><description>Source: Wu 2018 Uses nonlinear optimization may potentially achieve higslamher accuracy due to the capability to limit linearization errors through repeated linearization of the inherently nonlinear problem
[117] Forster: preintegration theory [118] OKVIS: a novel approach to tightly integrate visual measurements with IMU optimise a joint nonlinear cost function that integrates an IMU error term with the landmark reprojection error in a fully probabilistic manner real-time operation: old states are marginalized to maintain a bounded-sized optimization window Li et al.</description></item><item><title>Visual SLAM Implementation Framework</title><link>https://salehahr.github.io/zettelkasten/SLAM/vslam-framework/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/vslam-framework/</guid><description>Source: Cometlabs Basic principle: tracking a set of points through successive frames these tracks are used to triangulate the 3D positions of the points to create the map at the same time, using the the est point locations to calculate the pose of the camera, which could have observed them (i.e. calculate real time 3D structure of a scene from the estimated motion of the camera) Architecture Front-end Abstracts sensor data into models (which are good for estimation) / Processing Data association Short term (feature tracking); features in consecutive sensor measurements Either from sparse maps or dense-maps Long term ( loop closure ; associating new measurements to older landmarks Back-end Performs inference on the abstracted data produced by the front end</description></item><item><title>What is SLAM?</title><link>https://salehahr.github.io/zettelkasten/SLAM/what-is-slam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/what-is-slam/</guid><description>Source: Scaradozzi 2018 Process which allows a mobile robot to
construct a map of its environment (assumed to be unknown) compute its location using the map simultaneously Source: Lamarca 2020 Goal is to locate a sensor in an unknown map/environment, which is simultaneously being reconstructed. Typically used in exploratory trajectories (new or changing environments) Source: Wikipedia SLAM Simultaneous localization and mapping (SLAM)
computational problem construct/update a map of an unknown environment simultaneously keep track of an agent&amp;rsquo;s location within it Source: SLAM for Dummies Goal: to use the environment to update robot position</description></item><item><title>50.2.1 Process noise Q and W (odometry)</title><link>https://salehahr.github.io/zettelkasten/SLAM/50.2.1-process-noise-q-and-w-odometry/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/50.2.1-process-noise-q-and-w-odometry/</guid><description>See also: Factors affecting Kalman filter performance Source: Tereshkov 2015 Process noise covariance matrix has no clear physical meaning, cannot be deduced from sensor characteristics Leads to non-intuitive, iterative procedures to tune KFs Which means that KF optimality is rarely achieved in practice Alternative to KF tuning: the use of geometric observers
estimates are expresssed only in terms of quantities with clear geometrical meaning Source: Schneider 2013 If perfect model: $Q$ only describes the covariance of the random process noise Not perfect model, has: parametric errors (-&amp;gt; parameter identification) structural erors (error in model structure) workaround: e.</description></item><item><title>Covariance matrix P</title><link>https://salehahr.github.io/zettelkasten/SLAM/covariance-matrix-p/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/covariance-matrix-p/</guid><description>Source: SLAM for Dummies s. also EKF matrices Covariance matrix P
Covariance: measure of correlation of two variables Correlation: measure of degree of linear dependence A covariance of the robote POSEupdated in Step 1: Odometry update 3x3 B .. C covariance on the first .. nth landmarkStep 3: New landmarks 2x2 D covariance between POSE and first LMupdated in Step 1: Odometry update 2x3 E, etc E = D^T, etcupdated in Step 1: Odometry update 3x2 F=G^T Step 3: New landmarks Initially $P = A$ (robot has not seen any LMs)</description></item><item><title>Prediction model</title><link>https://salehahr.github.io/zettelkasten/SLAM/prediction-model/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/prediction-model/</guid><description>Source: SLAM for Dummies Used in the prediction step .
How to compute an expected position of the robot given the old position and the control input (so basically based on odometry .
Control terms are $\Delta x, \Delta y, \Delta \theta$)
$$ f = \left[ \begin{array}{c} x + \Delta t \cos \theta + q \Delta t \cos \theta \
y + \Delta t \sin \theta + q \Delta t \sin \theta \</description></item><item><title>Step 1 Odometry update (Prediction step)</title><link>https://salehahr.github.io/zettelkasten/SLAM/step-1-odometry-update-prediction-step/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/step-1-odometry-update-prediction-step/</guid><description>Source: SLAM for Dummies First step in the three-step EKF Update current state using odometry data Based on the controls given to the robot Calculate estimate of new POSE Update equation: prediction model ($x = x + \Delta x \cdot q$)
Or in a simple model, neglect the error term $q$
State vector gets updated via the prediction model
Jacobian of the prediction model also needs to be updated every iteration (with the controls deltax, &amp;hellip;)</description></item><item><title>Basic EKF for SLAM</title><link>https://salehahr.github.io/zettelkasten/SLAM/basic-ekf-for-slam/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/basic-ekf-for-slam/</guid><description>Source: SLAM for Dummies A basic EKF implementation of SLAM consists of multiple parts:
Landmark extraction Data association After odometry change (due to robot moving), state estimation from odometry Update of the estimated state using re-observed landmark data Update landmark database with new landmarks Note: at any point in the three steps on the left, the EKF will have an estimate of the robots current position</description></item><item><title>Data association</title><link>https://salehahr.github.io/zettelkasten/SLAM/data-association/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/data-association/</guid><description>Source: SLAM for Dummies Matching observed landmarks from different scans (different time steps) with each other. Also called &amp;rsquo;re-observing' landmarks.
Problems that can arise:
The landmark(s) might not be observed every time step (bad landmark) Something might be observed as a landmark, but it never appears again (bad landmark) Wrong association of a landmark to a previously seen landmark Goal: define a suitable data-association policy to minimise the first two problems Given: database that stores previously seen landmarks (initially empty) As a rule: a landmark is only considered worthwhile to be used in SLAM once it is seen N times</description></item><item><title>Extended Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/SLAM/extended-kalman-filter/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/extended-kalman-filter/</guid><description>Parent: SLAM Index Backlinks: RANSAC , nearest neighbour , Filtering in localisation Source: SLAM for Dummies keeps track of an estimate of the position uncertainty keeps track of the uncertainty in the features/landmarks seen General EKF implementation (non-SLAM) Basic EKF for SLAM Diagram: Triangle Robot Stars Landmarks Dashed triangle Robot&amp;rsquo;s position based on odometry alone (where it thinks it is) Dotted triangle Robot&amp;rsquo;s position estimate based on EKF Solid line triangle Robot&amp;rsquo;s actual position in real life!</description></item><item><title>Landmarks</title><link>https://salehahr.github.io/zettelkasten/SLAM/landmarks/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/landmarks/</guid><description>Source: SLAM for Dummies Features which can easily be re-observed and distinguished from the environment
Characteristics:
Re-observable from different positions and angles Unique (i.e. no mix-up with other landmarks) Plentiful &amp;ndash; should not be so few that robot gets lost (robot spends extended time w/o enough visible landmarks) Stationary Basic landmark extraction</description></item><item><title>RANSAC</title><link>https://salehahr.github.io/zettelkasten/SLAM/ransac/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/ransac/</guid><description>Source: SLAM for Dummies Random Sampling Consensus
to extract lines from a laser scan lines then used as landmarks indoors: straight lines from walls line landmarks are found by randomly taking a sample of laser readings (e.g. sample readings from 12deg to 22deg from within a range of 0 to 180deg) least squares approximation for line of best fit RANSAC then checks how many laser readings lie close to the best fit line initially, all readings are assumed to be unassociated to any lines if the num.</description></item><item><title>Spike landmarks</title><link>https://salehahr.github.io/zettelkasten/SLAM/spike-landmarks/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/spike-landmarks/</guid><description>Source: SLAM for Dummies Uses extrema to find landmarks Find values in the range of a laser scan, where two values differ by more than a certain amount (e.g. 0.5 m) This finds big changes in the laser scan Alternatively, using three values next to each other: $A, B, C$ $(A - B) + (C - B)$ yields a value Better for finding spikes as it finds actual spikes Rely on the landscape changing a lot between two laser beams Algo will fail in smooth environments Suitable for indoor environments, however is not robust against envs w/ people people are picked up as spikes as theoretically they are good landmarks (just not stationary!</description></item></channel></rss>