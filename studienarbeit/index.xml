<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Studienarbeits on</title><link>https://salehahr.github.io/studienarbeit/</link><description>Recent content in Studienarbeits on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 27 Jul 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://salehahr.github.io/studienarbeit/index.xml" rel="self" type="application/rss+xml"/><item><title>Landmarks</title><link>https://salehahr.github.io/studienarbeit/Landmarks/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Landmarks/</guid><description>Source: [[SLAM for Dummies.md|SLAM for Dummies]] Parent: [[SLAM Index.md|SLAM Index]]
Features which can easily be re-observed and distinguished from the environment
Characteristics:
Re-observable from different positions and angles Unique (i.e. no mix-up with other landmarks) Plentiful &amp;ndash; should not be so few that robot gets lost (robot spends extended time w/o enough visible landmarks) Stationary [[Basic landmark extraction.md|Basic landmark extraction]]
Created at 2020-07-27. Last updated at 2020-08-22.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/10.-Monocular-depth-perception/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/10.-Monocular-depth-perception/</guid><description>10. Monocular depth perception Parent: [[SLAM Index.md|SLAM Index]] Backlinks: [[Thesis.md|Thesis]]
Depth perception in real life
In nature, prey animals typically have eyes on either side of their head to maximise field of view, while most predators have forward-facing eyes with overlapping fields of vision (binocular vision) for maximum depth perception. Humans also have binocular vision. (Some exceptions: fruit bats, killer whales)
We perceive depth, or distance to the objects that we see, based on several visual cues.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/20.4-Which-orientation-parametrisation-to-choose_/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/20.4-Which-orientation-parametrisation-to-choose_/</guid><description>20.4 Which orientation parametrisation to choose? Parent: [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]], [[Quaternion index.md|Quaternion index]], [[Orientation parametrisations.md|Orientation parametrisations]] See also: [[Rotation error representation.md|Rotation error representation]]
Source: [[MKok 2017 Using inertial sensors for position and orientation estimation.md|MKok 2017 Using inertial sensors for position and orientation estimation]]
Estimation algorithms (filtering, smoothing) usually assume that the unknown states and parameters are represented in Euclidean space
Due to wrapping and gimbal lock, Euclidian addition and subtraction don&amp;rsquo;t work Also generally don&amp;rsquo;t work for rotation matrices and unit quaternions Constraints (unit quaternion norm, rotation matrix orthogonality) are usually hard to implement in estimation algorithms these concerns are led to the development of the [[MEKF.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/30.-ENDOSCOPES-/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/30.-ENDOSCOPES-/</guid><description>30. ENDOSCOPES &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; Endoscopy and bladder cancery surgery
Relate endoscopy to bladder cancery surgery &amp;hellip; Procedure:
Cystoscopy to inspect the bladder If tumour is found, do resection &amp;ndash;&amp;gt; cryosection Cancer detected &amp;ndash;&amp;gt; tumour removal [[Endoscopy.md|Endoscopy]] [[Endoscopes (general).md|Endoscopes (general)]] [[Types of endoscopes.md|Types of endoscopes]]
[[Endoscope system components.md|Endoscope system components]] [[Endoscope specification.md|Endoscope specification]]
Created at 2021-07-21. Last updated at 2021-08-17.
Tagged: #to-do #misc #-sa/processing #-master/thesis #-permanent #medical/surgery/endoscope</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/30.1.1-Endoscopy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/30.1.1-Endoscopy/</guid><description>30.1.1 Endoscopy Source: NHS Backlinks: [[Endoscopes.md|Endoscopes]]
Endoscopy is a procedure which enables inspection of organs inside the body. The prefix &amp;lsquo;endo&amp;rsquo; comes from the Greek language and means &amp;lsquo;within&amp;rsquo; or &amp;lsquo;inside&amp;rsquo;, cf. the prefix &amp;lsquo;exo&amp;rsquo;/&amp;lsquo;ecto&amp;rsquo; meaning &amp;lsquo;outside&amp;rsquo;. Endo- from Greek ἔνδον (within, inside), cf. exo-/ecto- from έκτός (outside) The instrument: [[endoscope.md|endoscope]]
Created at 2021-07-21. Last updated at 2021-07-24. Source URL: .
Tagged: #medical/surgery #-definitions #-sa/processed #-permanent</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/30.1.2-Endoscope/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/30.1.2-Endoscope/</guid><description>30.1.2 Endoscope Source: NHS Backlinks: [[Endoscopes.md|Endoscopes]]
The general term for the medical instrument used to perform [[endoscopy.md|endoscopy]] is, correspondingly, the endoscope. It is a device that makes use of optical technology to relay images from one end of the scope to another.
Functions are  not only for looking inside, but also have additional functionalities such as removing small tissue samples (biopsy) Insertion either through
natural body orifices (e.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/30.1.2.1-Types-of-endoscopes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/30.1.2.1-Types-of-endoscopes/</guid><description>30.1.2.1 Types of endoscopes Source: [[Leiner Digital Endoscope Design.md|Leiner Digital Endoscope Design]] Backlinks: [[Endoscopes.md|Endoscopes]]
Types of endoscopes
Endoscopes can be classified according to their flexibility, thus resulting in &amp;lsquo;rigid&amp;rsquo;, &amp;lsquo;flexible&amp;rsquo; and &amp;lsquo;semi-rigid&amp;rsquo; variants.
Video sensor at distal (&amp;ldquo;away from the surgeon&amp;rdquo;, opposite of proximal) end allows rigid endoscope to be converted to a flexible one solely by mechanical design Note: the term endoscope in hospital environments typically refers to the flexible variant In rigid endoscopes, an array of rod lenses (lenses which are much longer than their diameter) within a metal tube — also known as a telescope — are used to transmit the images from the [[distal end to the proximal end.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/30.1.2.2-Endoscope-system-components/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/30.1.2.2-Endoscope-system-components/</guid><description>30.1.2.2 Endoscope system components Source: [[Leiner.md|Leiner]] Backlinks: [[Endoscopes.md|Endoscopes]]
![[./_resources/30.1.2.2_Endoscope_system_components.resources/unknown_filename.png]]
Imaging system (rod lens array for rigid endoscopes) within a tube/shaft
Illumination/Light source (it&amp;rsquo;s dark inside the body)
separate from the imaging system in order to reduce glare [low contrast of received image] surrounds the imaging system like a ring light ![[./_resources/30.1.2.2_Endoscope_system_components.resources/unknown_filename.1.png]] Video camera
Coupling device (camera to imaging system)</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/30.1.2.3-Endoscope-specification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/30.1.2.3-Endoscope-specification/</guid><description>30.1.2.3 Endoscope specification Source: [[Leiner.md|Leiner]] Backlinks: [[Endoscopes.md|Endoscopes]]
Field of view: maximum angle that can be viewed [[Leiner.md|Leiner]]
Generally constructed so that the FOV is wide enough so that the &amp;lsquo;head on&amp;rsquo; view is visible even when the DOV is not zero. [[Leiner.md|Leiner]] This is to reduce the chances of bumping the instrument into anatomy right in front of the shaft. [[Leiner.md|Leiner]] Direction of view: angular offset of the optical axis from the longitudinal axis of the endoscope shaft</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/40.1-IMU-measurement-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/40.1-IMU-measurement-model/</guid><description>40.1 IMU measurement model Parent: [[IMU index.md|IMU index]], [[Probabilistic models for IMU.md|Probabilistic models for IMU]] Backlinks: [[IMU motion model.md|IMU motion model]], [[IMU kinematic model using Euler integration.md|IMU kinematic model using Euler integration]]
An IMU measures, relative to an inertial frame, acceleration and rotation rate.
The measurements are corrupted by bias and noise (often assumed to be white Gaussian noise ![[./_resources/40.1_IMU_measurement_model.resources/unknown_filename.3.png]]). [[MKok 2017.md|MKok 2017]] Additionally, the acceleration measured is affected by gravity.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/40.1.1-Assumptions-in-modelling-the-true-angular-velocity-in-IMUs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/40.1.1-Assumptions-in-modelling-the-true-angular-velocity-in-IMUs/</guid><description>40.1.1 Assumptions in modelling the true angular velocity in IMUs Parent: [[IMU index.md|IMU index]], [[IMU measurement model.md|IMU measurement model]] Source: [[MKok 2017 Using inertial sensors for position and orientation estimation.md|MKok 2017 Using inertial sensors for position and orientation estimation]]
For angular velocity, the term![[./_resources/40.1.1_Assumptions_in_modelling_the_true_angular_velocity_in_IMUs.resources/unknown_filename.png]] should really be ![[./_resources/40.1.1_Assumptions_in_modelling_the_true_angular_velocity_in_IMUs.resources/unknown_filename.2.png]] with
negligible Earth rotation ![[./_resources/40.1.1_Assumptions_in_modelling_the_true_angular_velocity_in_IMUs.resources/unknown_filename.1.png]] = 0 stationary navigation frame, ![[./_resources/40.1.1_Assumptions_in_modelling_the_true_angular_velocity_in_IMUs.resources/unknown_filename.3.png]] = 0 Created at 2021-05-18. Last updated at 2021-08-17.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.1-Why-Kalman-filters_/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.1-Why-Kalman-filters_/</guid><description>50.1 Why Kalman filters? Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
We work with 2 sources of data:
Sensor measurements Our own predictions (based on knowledge of system behaviour) Sensors are noisy, don&amp;rsquo;t give perfect information
Simple solution: to average readings However, this doesn&amp;rsquo;t work when the sensor is too noisy data collection not possible The prediction, however, is also susceptible to noise (the world is noisy, outside/unaccounted for influences)</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.1.1-Aim-and-main-principle-of-Kalman-filters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.1.1-Aim-and-main-principle-of-Kalman-filters/</guid><description>50.1.1 Aim and main principle of Kalman filters Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Aim of the Kalman/Bayesian filters: to accumulate (or to somehow blend)
our noisy and limited knowledge (of system behaviour) noisy and limited sensor readings and with these, make the best possible prediction (estimate) of the system state.
Main principles:
use past information to make predictions for the future [[never throw away information.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.2.-Bayes-Theorem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.-Bayes-Theorem/</guid><description>50.2. Bayes' Theorem Parent: [[Bayesian Filter Update Step.md|Bayesian Filter Update Step]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
How do we compute the probability of an event given previous information? (s. also [[Frequentist vs Bayesian statistics.md|Frequentist vs Bayesian statistics]])
Formula to compute new information into existing information
Used in the update step of a Bayesian filter (valid for both probabilities as well as probability distributions) !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.2.1-Process-noise-Q-and-W-odometry/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.1-Process-noise-Q-and-W-odometry/</guid><description>50.2.1 Process noise Q and W (odometry) Parent: [[Factors affecting Kalman filter performance.md|Factors affecting Kalman filter performance]]
Source: Tereshkov 2015 https://www.researchgate.net/publication/271532640_A_Simple_Observer_for_Gyro_and_Accelerometer_Biases_in_Land_Navigation_Systems Process noise covariance matrix has no clear physical meaning, cannot be deduced from sensor characteristics Leads to non-intuitive, iterative procedures to tune KFs Which means that KF optimality is rarely achieved in practice Alternative to KF tuning: the use of geometric observers
estimates are expresssed only in terms of quantities with clear geometrical meaning Source: [[Schneider 2013 How to not make the EKF fail.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.2.10-Discrete-Bayesian-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.10-Discrete-Bayesian-filter/</guid><description>50.2.10 Discrete Bayesian filter Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
The Kalman filter is a subset of Bayesian filters
Predict and update steps like in the g-h filter Here: error percentages are used to implicitly compute the g and h parameters Steps
[Initialise our belief in the state] The [[predict step.md|predict step]] always degrades our knowledge (belief/prior) However, in the [[update step.md|update step]], we add another measurement.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.2.10.1-Discrete-Bayesian-Filter-Predict-Step/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.10.1-Discrete-Bayesian-Filter-Predict-Step/</guid><description>50.2.10.1 Discrete Bayesian Filter Predict Step Parent: [[Discrete Bayesian filter.md|Discrete Bayesian filter]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
The predict step uses the total probability theorem.
Computes total probability of multiple possible events Uses the system model (propagates the states from prev. time step [posterior] to the next one); prediction Accounts for the uncertainty (kernel) in the prediction: produces a prior Generalise the uncertainty using a kernel (distributes the uncertainty over a range around the prediction) Integrate the kernel into the calculations by using convolution * Convolving the &amp;ldquo;current probabilistic estimate&amp;rdquo; with the &amp;ldquo;probabilistic estimate of how much we think the position has changed&amp;rdquo; (from system model) The prior is a &amp;lsquo;degraded&amp;rsquo; version of the belief i.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.2.10.2-Bayesian-Filter-Update-Step/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.10.2-Bayesian-Filter-Update-Step/</guid><description>50.2.10.2 Bayesian Filter Update Step Parent: [[Discrete Bayesian filter.md|Discrete Bayesian filter]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
The update step uses [[Bayes' Theorem.md|Bayes' Theorem]]
Produces the posterior by using the likelihood and the prior Also incorporates sensor data (measurements), as the measurements go into the likelihood calculation Update algorithm
Get a measurement, and associated belief about its accuracy Compute likelihood from the measurement and the measurement accuracy assumption Update the posterior using the likelihood and the prior Created at 2020-08-31.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.2.2-Measurement-noise-R-V-landmark/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.2-Measurement-noise-R-V-landmark/</guid><description>50.2.2 Measurement noise R, V (landmark) Parent: [[Multivariate Kalman filter algorithm.md|Multivariate Kalman filter algorithm]]
Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
R models the noise in the sensors as a covariance matrix dim(R) = m x m (m: number of sensors) Possible complications in multisensor systems, the correlation between the sensors might not be clear sensor noise might not be pure Gaussian Source: https://www.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.2.20-1D-Kalman-filters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.20-1D-Kalman-filters/</guid><description>50.2.20 1D Kalman filters Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
[[Deriving Kalman filter from Discrete Bayes using Gaussians.md|Deriving Kalman filter from Discrete Bayes using Gaussians]] [[1D Kalman filter algorithm.md|1D Kalman filter algorithm]] [[Kalman gain using Gaussians.md|Kalman gain using Gaussians]] [[Variance of the 1D Kalman filter.md|Variance of the 1D Kalman filter]] [[Factors affecting Kalman filter performance.md|Factors affecting Kalman filter performance]]
Created at 2020-08-31. Last updated at 2021-08-19.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.2.20.1-1D-Kalman-filter-algorithm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.20.1-1D-Kalman-filter-algorithm/</guid><description>50.2.20.1 1D Kalman filter algorithm Parent: [[1D Kalman filters.md|1D Kalman filters]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Initialisation:
Initialise the state of the filter Initialise the belief in the state Predict step:
Gaussian addition prior = predict(x, process_model) Incorporate process variance in order to prevent [[smug filtering.md|smug filtering]] Update step:
Gaussian multiplication likelihood = gaussian(z, sensor_var) x = update(prior, likelihood) The output of both steps is a Gaussian probability distribution N(mean, var)</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.2.3-Kalman-filter-initial-estimates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.3-Kalman-filter-initial-estimates/</guid><description>50.2.3 Kalman filter initial estimates Source: [[Schneider 2013 How to not make the EKF fail.md|Schneider 2013 How to not make the EKF fail]]
Initial state estimate x0, P0
Filter generally not badly affected by wrong initial state x0, but convergence will be slow if we are way off
If P0 too small whereas x0 is way off
the gain K becomes small filter relies on the model more than on the measurements Thus: important to have a consistent pair x0, P0</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.2.30-Multivariate-Kalman-filter-algorithm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.30-Multivariate-Kalman-filter-algorithm/</guid><description>50.2.30 Multivariate Kalman filter algorithm Parent: [[Multivariate Kalman filters.md|Multivariate Kalman filters]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Initialisation
Initialise filter state Initialise belief in the state Predict
Propagate state to the next time step using the system model [prediction] Adjust belief to take into account the prediction uncertainty [prior] ![[./_resources/50.2.30_Multivariate_Kalman_filter_algorithm.resources/unknown_filename.1.png]]
Update
Obtain measurement and associated belief about its accuracy Calculate residual (prior - measurement) Calculate scaling factor/Kalman gain Set estimated state to be on the residual line based on the scaling factor Update the belief in the state based on measurement certainty !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.2.40-Kalman-filter-performance-metric/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.40-Kalman-filter-performance-metric/</guid><description>50.2.40 Kalman filter performance metric Source: [[Schneider 2013 How to not make the EKF fail.md|Schneider 2013 How to not make the EKF fail]]
![[./_resources/50.2.40_Kalman_filter_performance_metric.resources/unknown_filename.png]] k: time / step j: how many EKF runs? in tutorial: EKF was ran 1000 times (non-deterministic system due to noise)
Created at 2021-08-18. Last updated at 2021-08-19.
Tagged: #filters/kalman-filter #-permanent #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.3-IMU-motion-model-in-a-Kalman-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.3-IMU-motion-model-in-a-Kalman-filter/</guid><description>50.3 IMU motion model in a Kalman filter Parent: [[IMU index.md|IMU index]] Source: [[Solà 2017 Quaternion kinematics for ESKF.md|Solà 2017 Quaternion kinematics for ESKF]]
Which states do we use for the motion model? [[Choice of states for the IMU motion_kinematics model.md|Choice of states for the IMU motion/kinematics model]]
How do we model the IMU motion? [[Choice of model for the IMU motion model.md|Choice of model for the IMU motion model]]</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.3.1-Choice-of-states-for-the-IMU-motion_kinematics-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.3.1-Choice-of-states-for-the-IMU-motion_kinematics-model/</guid><description>50.3.1 Choice of states for the IMU motion/kinematics model Parent: [[IMU index.md|IMU index]] See also: [[Choice of model for the IMU motion model.md|Choice of model for the IMU motion model]]
According to [[MKok 2017.md|MKok 2017]], we can either
Use the full state vector ![[./_resources/50.3.1_Choice_of_states_for_the_IMU_motion_kinematics_model.resources/unknown_filename.1.png]] [+] knowledge about sensor motion is included in model [-] large state vector Or the partial state vector, where the inputs are the inertial measurements from the IMU !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.3.2-Choice-of-model-for-the-KF-using-IMU-readings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.3.2-Choice-of-model-for-the-KF-using-IMU-readings/</guid><description>50.3.2 Choice of model for the KF using IMU readings Parent: [[IMU index.md|IMU index]], [[IMU kinematic equations_motion model.md|IMU kinematic equations/motion model]]
According to [[MKok 2017.md|MKok 2017]], here are some models that assume either a constant acceleration or a constant angular velocity:
Constant acceleration model ![[./_resources/50.3.2_Choice_of_model_for_the_KF_using_IMU_readings.resources/unknown_filename.png]]
Constant angular velocity model ![[./_resources/50.3.2_Choice_of_model_for_the_KF_using_IMU_readings.resources/unknown_filename.1.png]] (Notation: angular velocity of the body with respect to world (n), expressed in body CS)</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.4.1-Additive-quaternion-filtering/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.4.1-Additive-quaternion-filtering/</guid><description>50.4.1 Additive quaternion filtering Parents: [[Quaternion index.md|Quaternion index]], [[Which orientation parametrisation to choose_.md|Which orientation parametrisation to choose?]]
Source: [[Markley Fundamentals of Spacecraft Attitude Determination.md|Markley Fundamentals of Spacecraft Attitude Determination]]
Additive quaternion filtering Additive quaternion error ![[./_resources/50.4.1_Additive_quaternion_filtering.resources/unknown_filename.2.png]]
Methods of enforcing the normalisation
Renormalise the estimate by brute force ![[./_resources/50.4.1_Additive_quaternion_filtering.resources/unknown_filename.1.png]]
Modify KF update equations to enforce a norm constraint using a Lagrange multiplier
[1] and [2] yield biased estimates of the quaternion</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.4.2-Multiplicative-quaternion-filtering-MEKF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.4.2-Multiplicative-quaternion-filtering-MEKF/</guid><description>50.4.2 Multiplicative quaternion filtering (MEKF) See also: [[Which orientation parametrisation to choose_.md|Which orientation parametrisation to choose?]] Source: [[Markley Fundamentals of Spacecraft Attitude Determination.md|Markley Fundamentals of Spacecraft Attitude Determination]]
Main idea is to use
the quaternion as a global rotation representation
a three component state vector as the local representation of rotation errors ![[./resources/50.4.2_Multiplicative_quaternion_filtering(MEKF).resources/unknown_filename.png]] ![[./resources/50.4.2_Multiplicative_quaternion_filtering(MEKF).resources/unknown_filename.1.png]]
each term (q_true, delta_q, q_est) is a normalised unit quaternion</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.5-Error-State-Kalman-Filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.5-Error-State-Kalman-Filter/</guid><description>50.5 Error-State Kalman Filter Source: [[Markley.md|Markley]]
An EKF propagates the expectation and covariance of the state The MEKF propagates the expectation and the covariance of the error state Source: [[Whampsey MEKF.md|Whampsey MEKF]]
Previously: orientation is represented by one state Now: orientation is split up into ![[./_resources/50.5_Error-State_Kalman_Filter.resources/unknown_filename.1.png]] a large signal q_nom (nominal orientation) and a small signal (perturbation angle alpha) &amp;ndash; parametrises an error quaternion !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.5.1-IMU-nominal-state-and-error-state-kinematics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.5.1-IMU-nominal-state-and-error-state-kinematics/</guid><description>50.5.1 IMU nominal-state and error-state kinematics Parents: [[IMU index.md|IMU index]], [[50.3 Error-State Kalman Filter.md|50.3 Error-State Kalman Filter]]
Note on discretisation [[Solà 2017.md|Solà 2017]]:
Convert the differential equations to difference equations (use integration) Integration methods may vary Closed form solutions Numerical integration Integration is done for: The nominal state The error state (deterministic part): error state dynamics and control The error state (stochastic part): noise and perturbations Nominal state Error state Model without noise and perturbations Continuous !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.5.1.1-States-of-the-ESKF-for-estimating-IMU-pose/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.5.1.1-States-of-the-ESKF-for-estimating-IMU-pose/</guid><description>50.5.1.1 States of the ESKF for estimating IMU pose Parent: [[IMU index.md|IMU index]] Backlinks: [[Fusing IMU with complementary sensory data.md|Fusing IMU with complementary sensory data]]
Source: [[Solà 2017 Quaternion kinematics for ESKF.md|Solà 2017 Quaternion kinematics for ESKF]]
Full state Vector with 19 elements ![[./_resources/50.5.1.1_States_of_the_ESKF_for_estimating_IMU_pose.resources/unknown_filename.1.png]]
The corresponding kinematics equations/motion model is given in [[IMU kinematic equations_motion model.md|IMU kinematic equations/motion model]].
Notes:
The angular error in 3D space is given by the notation!</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.5.1.2-The-initial-gravity-vector_orientation-for-the-IMU-ESKF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.5.1.2-The-initial-gravity-vector_orientation-for-the-IMU-ESKF/</guid><description>50.5.1.2 The initial gravity vector/orientation for the IMU ESKF Parent: [[IMU index.md|IMU index]], [[Choice of model for the IMU motion model.md|Choice of model for the IMU motion model]]
Notes on the initial gravity vector/orientation for the IMU ESKF [[Solà 2017.md|Solà 2017]]
For simplicity, it is assumed that ![[./_resources/50.5.1.2_The_initial_gravity_vector_orientation_for_the_IMU_ESKF.resources/unknown_filename.1.png]]
The gravity vector g is estimated in terms of frame q0
This puts the initial uncertainty on the gravity direction, rather than on the initial orientation.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.6-ESKF-prediction-equations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.6-ESKF-prediction-equations/</guid><description>50.6 ESKF prediction equations Parents: [[IMU index.md|IMU index]], [[50.3 Error-State Kalman Filter.md|50.3 Error-State Kalman Filter]] Source: [[Solà 2017 Quaternion kinematics for ESKF.md|Solà 2017 Quaternion kinematics for ESKF]]
Error state system equation becomes: ![[./_resources/50.6_ESKF_prediction_equations.resources/Image.png]] where ![[./_resources/50.6_ESKF_prediction_equations.resources/unknown_filename.2.png]] (s. [[IMU nominal-state and error-state kinematics.md|IMU nominal-state and error-state kinematics]] for an overview of the nonlinear kinematics equations)
State propagation (without considering noise) — produces a state estimate (a priori) ![[./_resources/50.6_ESKF_prediction_equations.resources/unknown_filename.png]] Note: this always returns zero as the mean of the error !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.7-ESKF-update-_-Fusing-IMU-with-complementary-sensory-data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.7-ESKF-update-_-Fusing-IMU-with-complementary-sensory-data/</guid><description>50.7 ESKF update / Fusing IMU with complementary sensory data Parent: [[IMU index.md|IMU index]], [[50.3 Error-State Kalman Filter.md|50.3 Error-State Kalman Filter]] Source: [[Solà 2017 Quaternion kinematics for ESKF.md|Solà 2017 Quaternion kinematics for ESKF]]
In the ESKF, the arrival of non-IMU sensor data triggers a correction stage. This correction makes the IMU biases observable , allows correct estimation of the biases The correction stage is three-fold:
[[observe the error state.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.7.1-Observation-of-the-error-state-filter-correction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.7.1-Observation-of-the-error-state-filter-correction/</guid><description>50.7.1 Observation of the error state (filter correction) Parents: [[50.3 Error-State Kalman Filter.md|50.3 Error-State Kalman Filter]], [[50.5 ESKF update _ Fusing IMU with complementary sensory data.md|50.5 ESKF update / Fusing IMU with complementary sensory data]] Source: [[Solà 2017 Quaternion kinematics for ESKF.md|Solà 2017 Quaternion kinematics for ESKF]]
Given is a non-IMU sensor with the measurement function [[[Solà.md|Solà]], [[Markley.md|Markley]]] ![[./resources/50.7.1_Observation_of_the_error_state(filter_correction).resources/unknown_filename.png]]
where x_t is the true state and v is a white Gaussian noise !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.7.1.1-H-Jacobian-matrix-in-the-ESKF-filter-correction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.7.1.1-H-Jacobian-matrix-in-the-ESKF-filter-correction/</guid><description>50.7.1.1 H Jacobian matrix in the ESKF filter correction Parent: [[Filter correction.md|Filter correction]], [[ESKF update.md|ESKF update]] Source: [[Solà 2017 Quaternion kinematics for ESKF.md|Solà 2017 Quaternion kinematics for ESKF]]
Evaluation of the H Jacobian
In the prediction stage, the filter estimates the error state![[./_resources/50.7.1.1_H_Jacobian_matrix_in_the_ESKF_filter_correction.resources/unknown_filename.png]]. Therefore, the Jacobian H needs to be defined w.r.t. the error state ![[./_resources/50.7.1.1_H_Jacobian_matrix_in_the_ESKF_filter_correction.resources/unknown_filename.png]], and evaluated at the true state estimate ![[./_resources/50.7.1.1_H_Jacobian_matrix_in_the_ESKF_filter_correction.resources/unknown_filename.3.png]] However, as the error state mean is zero (not yet observed), the true state is approximated to the nominal state !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.7.2-Calculation-of-K-and-P-in-ESKF-update/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.7.2-Calculation-of-K-and-P-in-ESKF-update/</guid><description>50.7.2 Calculation of K and P in ESKF update Parent: [[50.3 Error-State Kalman Filter.md|50.3 Error-State Kalman Filter]], [[ESKF update.md|ESKF update]] See also:[[Evaluation of the H Jacobian.md|Evaluation of the H Jacobian]] Source: [[Solà 2017 Quaternion kinematics for ESKF.md|Solà 2017 Quaternion kinematics for ESKF]]
The filter correction equations are ![[./_resources/50.7.2_Calculation_of_K_and_P_in_ESKF_update.resources/unknown_filename.6.png]]
(yields a posteriori estimates)
Notes:
Here, the simplest form of the covariance update is used. This has poor numerical stability, however (no guarantee of symmetricity or positive definiteness) More stable forms are e.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/50.7.3-ESKF-reset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.7.3-ESKF-reset/</guid><description>50.7.3 ESKF reset Parent: [[Fusing IMU with complementary sensory data.md|Fusing IMU with complementary sensory data]] Backlinks: [[50.3 Error-State Kalman Filter.md|50.3 Error-State Kalman Filter]]
Source: [[Markley.md|Markley]]
moves the rotation error to the global rotation this keeps the rotation error small and far from any singularities To update the global state, the reset has to obey ![[./_resources/50.7.3_ESKF_reset.resources/unknown_filename.7.png]]
The reset has to preserve the quaternion norm, therefore an exact unit norm expression must be used, instead of an approximation.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/AtsushiSakai-PythonRobotics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/AtsushiSakai-PythonRobotics/</guid><description>[AtsushiSakai] PythonRobotics https://nbviewer.jupyter.org/github/AtsushiSakai/PythonRobotics/
Created at 2020-08-27. Last updated at 2020-09-01.
Tagged: #software/python #-sa/processed #-resources/tutorials</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Badias-2020-MORPH-DSLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Badias-2020-MORPH-DSLAM/</guid><description>[Badias 2020] MORPH-DSLAM URL: https://arxiv.org/pdf/2009.00576.pdf Video: https://www.youtube.com/watch?v=P_QN8Nv&amp;ndash;_g To read!
Created at 2020-10-20. Last updated at 2020-10-20.
Tagged: #-resources/-bibliography #-resources/-bibliography/bib-to-read #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Cadena-2016-Past-Present-and-Future-of-SLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Cadena-2016-Past-Present-and-Future-of-SLAM/</guid><description>[Cadena 2016] Past, Present, and Future of SLAM Authors Cadena et al
Abstract
cited by 1.2k people &amp;ldquo;This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM&amp;rdquo; Recommended other works s. [[Works of possible interest.md|Works of possible interest]]
Contents/Chapters Takeaway
Created at 2020-08-25. Last updated at 2020-09-09.
Tagged: #-resources/-bibliography #SLAM #to-do/go-through-literature-later #-resources/-bibliography/bib-to-read #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Chen-2018-Review-of-VI-SLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Chen-2018-Review-of-VI-SLAM/</guid><description>[Chen 2018] Review of VI SLAM Source: https://www.mdpi.com/2218-6581/7/3/45 Backlinks
Authors: Chen et. al Abstract
Survey on visual-inertial SLAM over the last 10 years Aspects: filtering vs optimisation based, camera type, sensor fusion type Explains core theory of SLAM, feature extraction, feature tracking, loop closure Experimental comparison of filtering-based and optimisation-based methods Research trends for VI-SLAM Recommended other works s. [[Works of possible interest.md|Works of possible interest]]
Contents/Chapters SLAM: build a real-time map of the unknown environment based on sensor data, while the sensor (robot) itself is traversing the environment</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Chen-2018-SLAM-based-dense-surface-reconstruction-in-MIS-with-AR/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Chen-2018-SLAM-based-dense-surface-reconstruction-in-MIS-with-AR/</guid><description>[Chen 2018] SLAM-based dense surface reconstruction in MIS with AR Authors Chen et al
Abstract
Intra-operative dense surface reconstruction framework to provide geometry information from only monocular videos The proposed framework works well with rapid camera movements, however is not suitable for large deformations Only tweaks ORBSLAM to adjust between point density and computational performance Contents/Chapters Problems in medical AR:
tissue surface illumination tissue deformation rapid movements of the medical tool e.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Cometlabs-What-You-Need-to-Know-About-SLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Cometlabs-What-You-Need-to-Know-About-SLAM/</guid><description>[Cometlabs] What You Need to Know About SLAM Source: https://blog.cometlabs.io/teaching-robots-presence-what-you-need-to-know-about-slam-9bf0ca037553
[[SLAM chicken and egg problem.md|SLAM chicken and egg problem]] Position acquisition [[Multisensor fusion.md|Multisensor fusion]] [[Sensors (absolute measurements) for measuring distance to landmarks.md|Sensors (absolute measurements) for measuring distance to landmarks]] [[Mapping representations in robotics.md|Mapping representations in robotics]] [[Visual SLAM Implementation Framework.md|Visual SLAM Implementation Framework]] [[Feature-based vs direct SLAM workflow.md|Feature-based vs direct SLAM workflow]] [[Sparse_Feature-based VSLAM.md|Sparse/Feature-based VSLAM]] [[Dense_direct VSLAM.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Durrant-Whyte-2006-SLAM-Tutorial-Part-I/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Durrant-Whyte-2006-SLAM-Tutorial-Part-I/</guid><description>[Durrant-Whyte 2006] SLAM Tutorial Part I Authors: Bailey, Durrant-Whyte
Abstract:
Contents/Chapters
Takeaway
Created at 2020-08-25. Last updated at 2020-08-25.
Tagged: #-resources/-bibliography #-resources/-bibliography/bib-to-read #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Forster-2017-IMU-Preintegration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Forster-2017-IMU-Preintegration/</guid><description>[Forster 2017] IMU Preintegration Authors: Forster et al
Abstract:
First contribution: preintegration theory (building up on Lupton&amp;rsquo;s work) what&amp;rsquo;s different from Lupton&amp;rsquo;s: addresses manifold structure of the rotation group, analytic derivation of all Jacobians Lupton&amp;rsquo;s work uses Euler angles Using Euler angles and techniques of Euclidian spaces for state propagation/covariance estimation is not properly invariant under rigid transformations uncertainty propagation, a-posteriori bias correction same as Lupton: integration performed in local frame, eliminating need for reintegrating when linearisation point changes Second contribution: integration of the preintegrated IMU model into a visual-inertial pipeline The system presented uses incremental smoothing for fast computation of the optimal MAP estimate Uses structureless model (3D landmarks are not part of the variables to be estimated) for visual measurements &amp;ndash;&amp;gt; allows eliminating large numbers of variables Motivation:</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Hibbeler-Dynamics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Hibbeler-Dynamics/</guid><description>[Hibbeler] Dynamics Author: Russell Hibbeler Contents
Kinematics, kinetics of particle [planar] rigid body [3D] rigid body Vibrations [[Kinematics primer.md|Kinematics primer]]
Created at 2021-05-24. Last updated at 2021-05-24.
Tagged: #-resources/-bibliography #-sa/processed #-resources/-bibliography/bib-skimmed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Jeon-2009-Kinematic-Kalman-Filter-for-Robot-End-Effector-Sensing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Jeon-2009-Kinematic-Kalman-Filter-for-Robot-End-Effector-Sensing/</guid><description>[Jeon 2009] Kinematic Kalman Filter for Robot End-Effector Sensing Backlinks: [[Discussion 2021-05-25.md|Discussion 2021-05-25]]
Authors: Jeon and Tomizuka
Abstract
inaccuracies in estimation of EE motion can come from kinematic error (error in parameters in kinematic equations)
to overcome this: take direct measurements e.g. using vision, but vision has high latency IMUs are used to provide interframe data fuse camera and IMU in a kinematic Kalman filter (KKF) framework.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Lamarca-2020-DefSLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Lamarca-2020-DefSLAM/</guid><description>[Lamarca 2020] DefSLAM URL: https://arxiv.org/abs/1908.08918 Authors: Lamarca et al Code: https://github.com/UZ-SLAMLab/DefSLAM Results (video): https://www.youtube.com/watch?v=6mmhD2_t6Gs Summary
First monocular SLAM for deformable environments in real-time Most other SLAM implementations assume rigidity Main techniques used (techniques for monocular non-rigid scenes): isometric shape from template (SfT) non-rigid structure from motion (NRSfM) Main principle: computation in [[two parallel threads (s. DefSLAM framework).md|two parallel threads (s. DefSLAM framework)]] Deformation tracking [front end] Deformation mapping [back end] The map from the mapping thread defines the shape-at-rest template used by deformation tracking Validation: compare with ORBSLAM (rigid) Assumes isometric deformation Future work: relocalisation (s.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Leiner-Digital-Endoscope-Design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Leiner-Digital-Endoscope-Design/</guid><description>[Leiner] Digital Endoscope Design Backlinks: [[Endoscopes.md|Endoscopes]] URL: https://www.spiedigitallibrary.org/ebooks/SL/Digital-Endoscope-Design/1/Digital-Endoscope-Design/10.1117/3.2235283.ch1?SSO=1
Notes [[Insertion of an endoscope.md|Insertion of an endoscope]] [[Types of endoscopes.md|Types of endoscopes]] [[Endoscope system components.md|Endoscope system components]] [[Endoscope specification.md|Endoscope specification]]
Created at 2021-05-21. Last updated at 2021-07-21.
Tagged: #-resources/-bibliography #-resources/-bibliography/bib-read #-sa/processed #medical/surgery/endoscope</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Maley-2013-MEKF-for-Nonspinning-Guided-Projectiles/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Maley-2013-MEKF-for-Nonspinning-Guided-Projectiles/</guid><description>[Maley 2013] MEKF for Nonspinning Guided Projectiles Source: https://apps.dtic.mil/sti/citations/ADA588831
Created at 2021-08-16. Last updated at 2021-08-18. Source URL: .
Tagged: #-resources/-bibliography #-resources/-bibliography/bib-to-read #filters/MEKF #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Markley-2003-Attitude-Error-Representations-for-Kalman-Filtering/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Markley-2003-Attitude-Error-Representations-for-Kalman-Filtering/</guid><description>[Markley 2003] Attitude Error Representations for Kalman Filtering Source: https://scholar.google.com/scholar?cluster=9266330323139560128&amp;amp;hl=en&amp;amp;as_sdt=0,5 Author: FL Markley
Motivation
Quaternion as an attitude representation
Good: lowest dimensionality while being a globally nonsingular representation Not so good: must obey a unit norm constraint In research, various methods for either getting around the norm constraint, or to enforce it
Most successful method employs the global attitude as a unit quaternion with a 3-comp attitude error representation</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Markley-2014-Fundamentals-of-Spacecraft-Attitude-Determination-and-Control/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Markley-2014-Fundamentals-of-Spacecraft-Attitude-Determination-and-Control/</guid><description>[Markley 2014] Fundamentals of Spacecraft Attitude Determination and Control Authors: FL Markley, John Crassidis DOI: 10.1007/978-1-4939-0802-8
Note/Nomenclature:
This book interpetes rotations/transformations in the [[passive_alias.md|passive/alias]] sense (I&amp;rsquo;m not a fan) Quaternions in JPL [[convention.md|convention]] instead of Hamiltonian (not a fan of this either&amp;hellip;) Rotation matrix = attitude matrix Introduction
Attitude determination: memoryless approach without using statistics Attitude estimation: approaches with memory, uses statistical info from a series of measurements filter approaches uses a dynamic motion model of the object Quaternions [[Quaternion conventions.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/MKok-2017-Using-inertial-sensors-for-position-and-orientation-estimation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/MKok-2017-Using-inertial-sensors-for-position-and-orientation-estimation/</guid><description>[MKok 2017] Using inertial sensors for position and orientation estimation Source: https://arxiv.org/abs/1704.06053 Authors: M Kok, JD Hol, TB Schön
Abstract
Contents/Chapters [[Quaternions.md|Quaternions]] [[Probabilistic models for IMU.md|Probabilistic models for IMU]] [[Orientation parametrisations.md|Orientation parametrisations]] [[Which orientation parametrisation to choose_.md|Which orientation parametrisation to choose?]] [[Linearisation of an orientation in SO(3).md|Linearisation of an orientation in SO(3)]] [[IMU measurement model.md|IMU measurement model]] [[Modelling noise and bias for IMU.md|Modelling noise and bias for IMU]] [[IMU motion models.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Mur-Artal-2017-VI-ORB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Mur-Artal-2017-VI-ORB/</guid><description>[Mur-Artal 2017] VI-ORB Backlinks: [[Chen 2018 Review of VI SLAM.md|Chen 2018 Review of VI SLAM]], [[Keyframe-based tightly-coupled multisensor SLAM.md|Keyframe-based tightly-coupled multisensor SLAM]], [[TODO.md|TODO]], [[Works of possible interest.md|Works of possible interest]]
URL: https://ieeexplore.ieee.org/abstract/document/7817784, Authors: Mur-Artal, Tardós Code: https://paperswithcode.com/paper/visual-inertial-monocular-slam-with-map-reuse Results (video): https://www.youtube.com/watch?v=JXRCSovuxbA
Abstract
current VI odometry approaches: drift accumulates due to lack of loop closure therefore there is a need for tightly-coupled VI-SLAM with loop closure and map reuse here: focus on monocular case, but applicable to other camera configurations builds on ORB-SLAM (from same author) IMU initialisation method (initialises: scale, gravity direction, velocities, gyroscope bias, accelerometer bias) depends on visual monocular initialisation (coupled initialisation) Other works: recent tightly-coupled VIO (both filtering- and optimisation-based) lack loop closure, so drift accumulates</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Pizarro-2016-Schwarps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Pizarro-2016-Schwarps/</guid><description>[Pizarro 2016] Schwarps Author: Daniel Pizarro et al.
Abstract
Warp between two images of a deforming surface: a transformation that depict the geometric deformation between the two &amp;lsquo;maps points between images of a deforming surface&amp;rsquo; Current approach to enforce a warp&amp;rsquo;s smoothness: penalise its second order partial derivatives However this favours locally affine warps Does not capture the local projective component of the image deformation Propose: novel penalty to smooth the warp while capturing the deformation&amp;rsquo;s local projective structure Proposed penalty is based on equivalents to the Schwarzian derivatives Schwarzian derivatives: projective differential invariants exactly preserved by homographies Methodology to derive a set of PDEs with only homographies as the solutions Validation: Schwarps outperform existing warps in modeling and extrapolation power: perform better in deformable reconstruction methods Introduction/Related work</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Qin-2019-General-Optimization-based-Framework-Multisensor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Qin-2019-General-Optimization-based-Framework-Multisensor/</guid><description>[Qin 2019] General Optimization-based Framework (Multisensor) Authors: Qin et al Code: https://github.com/HKUST-Aerial-Robotics/VINS-Fusion (uses ROS)
![[./_resources/[Qin_2019]General_Optimization-based_Framework(Multisensor).resources/unknown_filename.3.png]]
Abstract:
odometry estimation with multiple sensors, general framework which is optimisation-based demonstrated combinations: stereo cameras monocular cam + IMU stereo cams + IMU sensor = factor in the framework comparison with other state-of-the-art algos Aim:
to create a general algo which supports different multisensor suites also for redundancy: in case of sensor failure, it can be switched out easily Related work:</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/resource-IMU-common-specifications-error-models-etc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/resource-IMU-common-specifications-error-models-etc/</guid><description>[resource] IMU common specifications, error models etc Parent: [[IMU.md|IMU]] Source: https://www.vectornav.com/resources/imu-specifications
IMU common specifications, bias, scale factor, orthogonality errors, and acceleration sensitivity for gyroscopes.
Source: Woodman - An introduction to inertial navigation ![[./_resources/[resource]_IMU_common_specifications,_error_models_etc.resources/unknown_filename.png]]
Source: Quinchia - A Comparison between Different Error Modeling of MEMS Applied to GPS/INS Integrated Systems
3.2. State-Space Representation for Different Bias Models
First order Gauss-Markov (GM) ![[./_resources/[resource]_IMU_common_specifications,_error_models_etc.resources/unknown_filename.1.png]] ![[./_resources/[resource]_IMU_common_specifications,_error_models_etc.resources/unknown_filename.2.png]]
Random walk !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Riisgaard-SLAM-for-dummies/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Riisgaard-SLAM-for-dummies/</guid><description>[Riisgaard] SLAM for dummies Authors: Søren Riisgaard and Morten Rufus Blas Parent: [[SLAM resources.md|SLAM resources]]
Abstract:
Tutorial introduction to SLAM, with minimal prerequisites for the understanding of SLAM as explained here Mostly explains a single approach to the steps involved in SLAM Complete solution for SLAM using EKF (extended Kalman filter) Only considers 2D motion, not 3D Chapters
[[What is SLAM_.md|What is SLAM?]] [[Overview of SLAM using EKF.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/rlabbe-Kalman_Bayesian-filters-in-Python/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/rlabbe-Kalman_Bayesian-filters-in-Python/</guid><description>[rlabbe] Kalman/Bayesian filters in Python URL: https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python nbviewer link: http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb Abstract:
Introductory text with Python code Caveat: most of the code is written for didactic purposes, may not be the most efficient solution (nor numerically stable) Recommended other works s. [[Works of possible interest.md|Works of possible interest]]
Chapters
Preface [[Why Kalman filters_.md|Why Kalman filters?]] [[Aim and main principle of Kalman filters.md|Aim and main principle of Kalman filters]] [[Expected value.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Scaradozzi-2018-SLAM-application-in-surgery/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Scaradozzi-2018-SLAM-application-in-surgery/</guid><description>[Scaradozzi 2018] SLAM application in surgery Abstract:
SLAM&amp;rsquo;s potential in image-guided surgery assuming static environment Review of main techniques in general robotics SLAM Insight into visual SLAM SLAM in surgery Chapters [[What is SLAM_.md|What is SLAM?]] [[Filter-based vs optimisation-based SLAM.md|Filter-based vs optimisation-based SLAM]]
[[General Kalman Filter.md|General Kalman Filter]] [[General EKF.md|General EKF]] [[Unscented Kalman Filter.md|Unscented Kalman Filter]] [[Information Filter.md|Information Filter]]
&amp;hellip;.
Takeaway
EKF is popular in surgery SLAM techniques Deformable environment encumbers precise registration and data fusion Created at 2020-08-06.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Schneider-2013-How-to-not-make-the-EKF-fail/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Schneider-2013-How-to-not-make-the-EKF-fail/</guid><description>[Schneider 2013] How to not make the EKF fail Authors: Schneider, Georgakis URL: https://www.researchgate.net/publication/263942618_How_To_NOT_Make_the_Extended_Kalman_Filter_Fail/citations DOI 10.1021/ie300415d [[Measurement noise R, V (landmark).md|Measurement noise R, V (landmark)]] [[Kalman filter initial estimates.md|Kalman filter initial estimates]] [[Process noise Q and W (odometry).md|Process noise Q and W (odometry)]] [[Kalman filter performance metric.md|Kalman filter performance metric]]
Created at 2021-08-18. Last updated at 2021-08-18. Source URL: .
Tagged: #-resources/-bibliography #-resources/-bibliography/bib-read #filters/EKF #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Science-Focus-How-can-one-eye-alone-provide-depth-perception_/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Science-Focus-How-can-one-eye-alone-provide-depth-perception_/</guid><description>[Science Focus] How can one eye alone provide depth perception? Source: https://www.sciencefocus.com/the-human-body/how-can-one-eye-alone-provide-depth-perception/ Author: Hilary Guite
In humans with normal binocular vision, depth perception is obtained using the [[parallax.md|parallax]] in the two overlapping fields of vision (&amp;ldquo;binocular disparity&amp;rdquo;)
Each single field of vision has a slightly different view to the other If vision in one eye is impaired, depth perception is still obtainable even with only one eye.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Sol%C3%A0-2014-SLAM-with-EKF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Sol%C3%A0-2014-SLAM-with-EKF/</guid><description>[Solà 2014] SLAM with EKF Notes on EKF-SLAM that uses landmarks MATLAB code Notes on partial landmark initialisation (convariance matrix) Notes on the linearity of the observation function in scale Created at 2021-05-13. Last updated at 2021-05-13.
Tagged: #-resources #filters/EKF #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Sol%C3%A0-2017-Quaternion-kinematics-for-ESKF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Sol%C3%A0-2017-Quaternion-kinematics-for-ESKF/</guid><description>[Solà 2017] Quaternion kinematics for ESKF Link: http://www.iri.upc.edu/people/jsola/JoanSola/objectes/notes/kinematics.pdf
Author: Joan Solà Abstract:
Primer on quaternion/rotation group math Math for error state Kalman filters using IMUs Contents/Chapters:
[[Quaternions.md|Quaternions]] Rotations, s. also [[SO(3) 3D rotation group.md|SO(3) 3D rotation group]] [[Quaternion conventions.md|Quaternion conventions]] Perturbations, derivatives, integrals [[Error-State Kalman Filter.md|Error-State Kalman Filter]] for IMU-driven systems [[Variables in ESKF.md|Variables in ESKF]] [[IMU measurement model.md|IMU measurement model]] [[IMU motion model.md|IMU motion model]] [[The initial gravity vector_orientation for the IMU ESKF.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Song-2018-MIS-SLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Song-2018-MIS-SLAM/</guid><description>[Song 2018] MIS-SLAM Authors Song et al Backlinks: referred to in [[Lamarca 2019.md|Lamarca 2019]] as a stereovisual deformable SLAM, uses CPU and GPU, nonlinear optimisation Video: https://www.youtube.com/watch?v=2pXokldQBWM
Abstract
Uses CPU and GPU CPU for ORBSLAM (initial global position) GPU for deformable tracking and dense mapping Contents/Chapters
Poor localisation of scope in MIS, compared with open surgery Related works mentioned don&amp;rsquo;t provide a RT and robust solution for localisation while reconstructing dense deformable surfaces focus on the monocular scope, fail to solve the problem of missing scale Fast movement makes visual odometry unstable causes blurry images worsens registrations ORB-SLAM proven to be suitable for coupling with dense deformable SLAM Initial tracking: ORB-SLAM</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Weiss-Thesis-Vision-based-navigation-for-micro-helicopters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Weiss-Thesis-Vision-based-navigation-for-micro-helicopters/</guid><description>[Weiss Thesis] Vision based navigation for micro helicopters Source Backlinks
Authors Stephan Weiss Abstract
Issues that arise during state estimation and sensor self-calibration Application area: large and unknown areas, micro helicopter Vision based method used uses SfM, is compares mapless and map-based methods Statistical and modular sensor fusion strategy recovery of pose and drifts modular: camera as a black box sensor, allows other sensors additionally Observability analysis Literature review</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Whampsey-MEKF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Whampsey-MEKF/</guid><description>[Whampsey] MEKF https://matthewhampsey.github.io/blog/2020/07/18/mekf
Motivation:
Working with noisy IMU measurements IMUs usually provide redundant information that can be used to improve dead-reckoning Uses: Hamilton [[quaternion convention.md|quaternion convention]] [[Which orientation parametrisation to choose_.md|Which orientation parametrisation to choose?]]
[[Error-State Kalman Filter.md|Error-State Kalman Filter]]
Created at 2021-08-14. Last updated at 2021-08-18. Source URL: .
Tagged: #-resources/-bibliography #-resources/-bibliography/bib-read #-sa/processed #filters/kalman-filter #math/quaternions #discussion/2021/2021-08 #filters/MEKF</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Wikipedia-Lokalisierung/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Wikipedia-Lokalisierung/</guid><description>[Wikipedia] Lokalisierung Source: https://de.wikipedia.org/wiki/Lokalisierung_(Robotik [[Localisation.md|Localisation]] [[Particle filters.md|Particle filters]] [[Categories of sensors for localisation.md|Categories of sensors for localisation]]
Created at 2020-08-23. Last updated at 2020-08-23.
Tagged: #-resources/-bibliography #localisation #-resources/-bibliography/bib-read #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Wikipedia-SLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Wikipedia-SLAM/</guid><description>[Wikipedia] SLAM Source: https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping Parents: [[SLAM Index.md|SLAM Index]], [[SLAM resources.md|SLAM resources]]
Different types of sensors give rise to different SLAM algorithms whose assumptions are most appropriate to the sensors.
At one extreme, visual features provide details of many points within an area &amp;ndash;&amp;gt; rendering SLAM unnecessary shapes in these point clouds can be easily and unambiguously aligned at each step via image registration . At the opposite extreme, tactile sensors are extremely sparse they contain only information about points very close to the agent require strong prior models to compensate in purely tactile SLAM.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Wikipedia-Visual-odometry/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Wikipedia-Visual-odometry/</guid><description>[Wikipedia] Visual odometry Source: https://en.wikipedia.org/wiki/Visual_odometry [[Odometry.md|Odometry]] [[Visual sensors for localisation.md|Visual sensors for localisation]]
Created at 2020-07-27. Last updated at 2020-08-23.
Tagged: #-resources/-bibliography #localisation #-resources/-bibliography/bib-read #-sa/processed #localisation/odometry</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Woernle-Mehrk%C3%B6rpersysteme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Woernle-Mehrk%C3%B6rpersysteme/</guid><description>[Woernle] Mehrkörpersysteme Author: Christoph Woernle Contents:
Kinematics, kinetics (Dynamik) Some basics [[Converting velocity from CS1 to CS0.md|Converting velocity from CS1 to CS0]] [[Chaining rotation matrices and angular velocities.md|Chaining rotation matrices and angular velocities]] [[Poisson equation for skew symmetric matrix of angular velocity.md|Poisson equation for skew symmetric matrix of angular velocity]] [[Differentiation in different coordinate systems.md|Differentiation in different coordinate systems]] [[Kinematics primer.md|Kinematics primer]] [[Reversed kinematics relations.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Wu-2018-Image-based-camera-localization_-an-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Wu-2018-Image-based-camera-localization_-an-overview/</guid><description>[Wu 2018] Image-based camera localization: an overview Authors: Wu, Tang, Li
Abstract/Contents
overview (classification) of image-based camera localization classification of image-based camera localization approaches techniques, trends only considers 2D cameras focuses on points as features in images (not lines etc) Chapters [[Classification of image-based camera localization approaches.md|Classification of image-based camera localization approaches]] [[Multisensor fusion.md|Multisensor fusion]] — [[Why use the visual-inertial sensor combination_.md|Why use the visual-inertial sensor combination?]] [[Loose vs Tight coupling.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Active_passive-or-Alibi_alias-rotation-transformations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Active_passive-or-Alibi_alias-rotation-transformations/</guid><description>Active/passive or Alibi/alias rotation transformations Parent: [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]] See also: [[Intrinsic vs extrinsic rotations.md|Intrinsic vs extrinsic rotations]] Backlinks: [[Markley Fundamentals of Spacecraft Attitude Determination.md|Markley Fundamentals of Spacecraft Attitude Determination]]
Source: https://en.wikipedia.org/wiki/Rotation_matrix#Ambiguities Alibi / Active![[./_resources/Active_passive_or_Alibi_alias_rotation_transformations.resources/unknown_filename.4.png]] Alias / Passive![[./_resources/Active_passive_or_Alibi_alias_rotation_transformations.resources/unknown_filename.3.png]] CS is fixed CS is rotated Point rotates within fixed CS Point remains stationary but is represented within a new CS !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Ausblick/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Ausblick/</guid><description>Ausblick Parent: [[Thesis.md|Thesis]]
possible improvements, further work to be done
not real time &amp;ndash;&amp;gt; realtime erweitern auf optim.basierten Ansatz experimental validation using test setup: assume perfect data (ground truth from OptiTrack) &amp;ndash;&amp;gt; real data Einfluss der IMU auf verbesserte Lokalisierung &amp;ndash;&amp;gt; evtl eine IMU Koordinate weglassen Created at 2021-05-26. Last updated at 2021-05-26.
Tagged: #-sa/processed #-master/thesis</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/B1_-Modelling-of-tissue-and-sensor-navigation-of-multimodal-sensors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/B1_-Modelling-of-tissue-and-sensor-navigation-of-multimodal-sensors/</guid><description>B1: Modelling of tissue and sensor, navigation of multimodal sensors Source: https://www.isys.uni-stuttgart.de/forschung/medizintechnik/intraoperative-multisensorische-gewebedifferenzierung/ Backlinks: [[GRK 2543_ Intraoperative Multi-sensor Tissue Differentiation in Oncology.md|GRK 2543: Intraoperative Multi-sensor Tissue Differentiation in Oncology]]
![[./_resources/B1__Modelling_of_tissue_and_sensor,_navigation_of_multimodal_sensors.resources/unknown_filename.png]]
cross-domain modelling (tissue and sensor) tissue parameters change depending on status (benign/malignant), obtained or derived from sensor signals sensor signal must be synchronised with the current position of the sensor probe on the tissue Created at 2020-08-09. Last updated at 2020-08-09.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Back-end-optimisation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Back-end-optimisation/</guid><description>Back-end optimisation Parent: [[Visual SLAM Implementation Framework.md|Visual SLAM Implementation Framework]]
Source: [[cometlabs.md|cometlabs]]
(Camera pose optimisation)
To compensate for drift of pose estimation Traditionally using EKF ( filter-based ) simple implementation therefore good for small scale estimations Alternative: bundle adjustment (graph optimisation) joint optimisation of the camera pose and the 3D structure parameters combines numerical methods and graph theory increasingly favoured over filtering, due to the latter&amp;rsquo;s inherent inconsistency more efficient when combined with sub-mapping Created at 2020-08-03.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Bag-of-words/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Bag-of-words/</guid><description>Bag of words Parent: [[SLAM Index.md|SLAM Index]]
Source: https://towardsdatascience.com/bag-of-visual-words-in-a-nutshell-9ceea97ce0fb
Has its origins in natural language processing (NLP), information retrieval
A text can be seen as a bag of words, with each word having different frequencies from one another This can be used to compare and classify texts (similar histograms) In vision
Instead of words we have features (identifying pattern in an image) An image is represented as a set of features Features consist of Keypoints: points that are invariant to transformation [[Descriptors.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Barycentric-coordinates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Barycentric-coordinates/</guid><description>Barycentric coordinates Source: [[SOFA extended documentation.md|SOFA extended documentation]] Baclinks: [[Top-down mapping (master to slave).md|Top-down mapping (master to slave)]], [[Lamarca 2019 DefSLAM.md|Lamarca 2019 DefSLAM]]
Barycentre: centre of mass A coordinate system, in which the location of point of a [[simplex.md|simplex]] (line, triangle, tetrahedron, etc) is specified as the centre of mass of the masses placed at its vertices ![[./_resources/Barycentric_coordinates.resources/unknown_filename.png]]
x_i vertices of a simplex p a point in space The a_i coefficients are the barycentric coordinates of p w.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Basic-EKF-for-SLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Basic-EKF-for-SLAM/</guid><description>Basic EKF for SLAM Parent: [[Extended Kalman Filter.md|Extended Kalman Filter]], [[SLAM Index.md|SLAM Index]] Backlinks: [[RANSAC.md|RANSAC]] See also: [[What is SLAM_.md|What is SLAM?]]
Source: SLAM for Dummies A basic EKF implementation of SLAM consists of multiple parts:
Landmark extraction Data association After odometry change (due to robot moving), state estimation from odometry Update of the estimated state using re-observed landmark data Update landmark database with new landmarks Note: at any point in the three steps on the left, the EKF will have an estimate of the robots current position</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Basic-python-script-in-Sofa/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Basic-python-script-in-Sofa/</guid><description>Basic python script in Sofa Parent: [[SofaPython Index.md|SofaPython Index]]
Imports import Sofa
General functions
createGraph(self, root) reset() onKeyPressed() &amp;hellip; Required in every script: createScene(rootNode)
Create a child from a node node.createChild(&amp;lsquo;Name&amp;rsquo;)
Add an object component to the node: node.createObject(type in string, kwargs**)
Created at 2020-07-15. Last updated at 2020-08-22.
Tagged: #software/SOFA #software/python #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Baumgarte-stabilisation-over-the-SO3-rotation-group-for-control/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Baumgarte-stabilisation-over-the-SO3-rotation-group-for-control/</guid><description>Baumgarte stabilisation over the SO(3) rotation group for control Author: Sebastien Gros
Created at 2021-08-18. Last updated at 2021-08-18.
Tagged: #-resources/-bibliography #-resources/-bibliography/bib-read #math/rotations #math/quaternions #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Besprechung-2021-03-08/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Besprechung-2021-03-08/</guid><description>Besprechung 2021-03-08 Agenda
DefSLAM + OS3 Up till DefTracking::MonocularInitialization() on hold, working on Kalman stuff for the time being DefSLAM + Kalman new plot (monocular trajectory without any pose updating) to do: use noisy stereo data, plug into update step while discarding images functions in System.cc: read data, update pose DefSLAM + sockets Meeting notes:
next step: implement the Kalman filter.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Besprechung-2021-03-15/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Besprechung-2021-03-15/</guid><description>Besprechung 2021-03-15 Status
Last week: generated noisy IMU data (pose) from stereo trajectory, &amp;lsquo;offline&amp;rsquo; Kalman This week: Kalman + &amp;lsquo;live&amp;rsquo; DefSLAM Still to do: design KF (EKF, or other methods&amp;hellip;) &amp;lsquo;Offline&amp;rsquo; Kalman
To learn how to use openCV&amp;rsquo;s Kalman filter without having to rebuild DefSLAM every time Aim was to figure out the update/correction workflow and implement it in live DefSLAM run
Uses pre-extracted trajectories (mono and stereo)</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Besprechung-2021-04-01/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Besprechung-2021-04-01/</guid><description>Besprechung 2021-04-01 Agenda
Recap: [[last meeting (2021-03-15).md|last meeting (2021-03-15)]] Offline Kalman — after this works, do a &amp;lsquo;live&amp;rsquo; implementation on DefSLAM run
IMU measurements as [acc, gyro] readings
With noise, but without considering bias, IMU-cam transformation, gravity Two sets of measurements for filter: IMU measurements, DefSLAM (camera) measurements
KF prediction using random walk
Recap: SLAM +filtering terminology (loose coupling, tight coupling) Literature Original suggestion using random walk model Interface (DefSLAM, Python) read: OK write: to do Offline Kalman as a separate repo Good papers?</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Bottom-up-mapping-slave-to-master/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Bottom-up-mapping-slave-to-master/</guid><description>Bottom-up mapping (slave to master) Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Mappings.md|Mappings]]
Mapping of a slave forces to the master forces Newton&amp;rsquo;s law f=Ma applies
Equivalence of power ![[./resources/Bottom-up_mapping(slave_to_master).resources/unknown_filename.png]] ![[./resources/Bottom-up_mapping(slave_to_master).resources/unknown_filename.2.png]] using the kinematic relation ![[./resources/Bottom-up_mapping(slave_to_master).resources/unknown_filename.1.png]] ![[./resources/Bottom-up_mapping(slave_to_master).resources/unknown_filename.3.png]]using the principle of virtual work
Created at 2020-08-09. Last updated at 2020-08-09.
Tagged: #software/SOFA/mappings-in-SOFA #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Building-SOFA-on-Windows/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Building-SOFA-on-Windows/</guid><description>Building SOFA on Windows Parent: [[SofaPython Index.md|SofaPython Index]]
https://www.sofa-framework.org/community/doc/getting-started/build/windows/
Created at 2020-07-17. Last updated at 2020-08-22.
Tagged: #software/SOFA #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Calculating-the-estimated-state-in-the-GH-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Calculating-the-estimated-state-in-the-GH-filter/</guid><description>Calculating the estimated state in the GH-filter Parent: [[g-h filter or α-β filter.md|g-h filter or α-β filter]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
![[./_resources/Calculating_the_estimated_state_in_the_GH-filter.resources/unknown_filename.png]] Using a gain , the estimate therefore always falls between the measurements (circles) and the predictions (in red). The prediction is dependent on the previous filter output (i.e. last estimate). Here it is modelled to increase by 1 from the previous estimate.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Camera-calibration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Camera-calibration/</guid><description>Camera calibration Parent: [[SLAM Index.md|SLAM Index]] Source: https://de.mathworks.com/help/vision/ug/camera-calibration.html
estimates lens/sensor parameters e.g. to correct lens distortion, determine position, measurement etc there are several camera models, e.g. fisheye, [[pinhole.md|pinhole]] Camera parameters
[[intrinsic.md|intrinsic]] [[extrinsic.md|extrinsic]] distortion coefficients How to solve for camera parameters?
Need to have 3D world points and the corresponding 2D image points Take multiple images of a calibration pattern to obtain these correspondences With the mapping 3Dp -&amp;gt; 2Dp, solve for camera parameters Evaluate accuracy of estimated camera parameters:</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Camera-views-as-seen-by-SLAM-at-distal-end-of-probe_scope/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Camera-views-as-seen-by-SLAM-at-distal-end-of-probe_scope/</guid><description>Camera views as seen by SLAM at distal end of probe/scope ![[./_resources/Camera_views_as_seen_by_SLAM_at_distal_end_of_probe_scope.resources/unknown_filename.jpeg]]
Created at 2021-06-16. Last updated at 2021-06-16.
Tagged: #to-do/orphan #-sa/processed #medical/surgery/endoscope</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Cancer-biopsy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Cancer-biopsy/</guid><description>Cancer biopsy Source: https://en.wikipedia.org/wiki/Biopsy Backlinks: [[GRK 2543_ Intraoperative Multi-sensor Tissue Differentiation in Oncology.md|GRK 2543: Intraoperative Multi-sensor Tissue Differentiation in Oncology]]
Biopsy: type of medical test in which a cell/tissue sample is extracted in order to detect disease Types of biopsy: excisional biopsy: removal of entire suspicious area to be diagnosed incisional biopsy: removal of only samples of the abnormal tissue needle aspiration biopsy: removal of cells via needle Diagnosing / Pathological examination to determine whether the abnormality is benign or malignant (classification of the cancer) to determine how far it has spread negative margins: no disease found at the edge of specimen positive margins: disease was found at edge, further excision may be in order In bladders: usually done using a [[cystocopy.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Central-Limit-Theorem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Central-Limit-Theorem/</guid><description>Central Limit Theorem Parent: [[Gaussian distribution.md|Gaussian distribution]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
If we make many measurements, the measurements will be normally distributed. (only applies under certain conditions)
Created at 2020-08-31. Last updated at 2020-08-31.
Tagged: #-definitions #-sa/processed #math/statistics</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Chaining-rotation-matrices-and-angular-velocities/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Chaining-rotation-matrices-and-angular-velocities/</guid><description>Chaining rotation matrices and angular velocities Source: [[Woernle Mehrkörpersysteme.md|Woernle Mehrkörpersysteme]] Backlinks: [[Kinematics primer.md|Kinematics primer]], [[Obtaining IMU measurements from camera by forward kinematics.md|Obtaining IMU measurements from camera by forward kinematics]] See also: [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]]
Chaining rotation matrices ![[./_resources/Chaining_rotation_matrices_and_angular_velocities.resources/unknown_filename.png]] T_02 transforms a point in CS2 to CS0
compare: ![[./_resources/Chaining_rotation_matrices_and_angular_velocities.resources/unknown_filename.3.png]]
Chaining homogeneous transformation matrices ![[./_resources/Chaining_rotation_matrices_and_angular_velocities.resources/unknown_filename.4.png]]
Chaining angular velocities (in same CS) ![[./_resources/Chaining_rotation_matrices_and_angular_velocities.resources/unknown_filename.1.png]] ![[./_resources/Chaining_rotation_matrices_and_angular_velocities.resources/unknown_filename.2.png]]
ang.vel. of 2 rel to 0 = ang.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Cheatsheet/</guid><description>Cheatsheet Parent: [[SofaPython Index.md|SofaPython Index]]
Imports/Plugins import SofaRuntime from SofaRuntime import PluginRepository PluginRepository.addFirstPath(SOFA_INSTALL_DIR) SofaRuntime.importPlugin(&amp;ldquo;SofaComponentAll&amp;rdquo;) SofaRuntime.importPlugin(&amp;ldquo;SofaPython3&amp;rdquo;) SofaRuntime.importPlugin(&amp;ldquo;SofaOpenglVisual&amp;rdquo;)
From root root = Sofa.Core.Node(&amp;ldquo;root&amp;rdquo;) c = root.createObject(&amp;ldquo;MechanicalObject&amp;rdquo;, name=&amp;ldquo;t&amp;rdquo;, position=[ [0, 0, 0], [1, 1, 1], [2, 2, 2]]) c1 = root.addObject(&amp;ldquo;MechanicalObject&amp;rdquo;, name=&amp;ldquo;c1&amp;rdquo;)
From nonroot nonroot_node = Sofa.Core.Node(&amp;ldquo;a_node&amp;rdquo;) nonroot_node.addObject(&amp;ldquo;MechanicalObject&amp;rdquo;, name=&amp;ldquo;c1&amp;rdquo;) .addData(&amp;ldquo;d&amp;rdquo;, value=&amp;ldquo;coucou&amp;rdquo;, type=&amp;ldquo;string&amp;rdquo;) .addData(&amp;ldquo;data1&amp;rdquo;, value=&amp;quot;@/c1.d&amp;quot;) # @ is root
Add data from relative/absolute paths c4.addData(&amp;lsquo;data1&amp;rsquo;, value=&amp;quot;@/n1/c3.data1&amp;quot;) # absolute path (chained) c4.addData(&amp;lsquo;data2&amp;rsquo;, value=&amp;quot;@../n1/c3.data1&amp;quot;) # relative path (down, chained) c1.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Classification-of-image-based-camera-localization-approaches/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Classification-of-image-based-camera-localization-approaches/</guid><description>Classification of image-based camera localization approaches Parent: [[SLAM Index.md|SLAM Index]] Source: [[Wu 2018 Image-based camera localization_ an overview.md|Wu 2018 Image-based camera localization: an overview]]
![[./_resources/Classification_of_image-based_camera_localization_approaches.resources/unknown_filename.png]]
Unknown environment (must be reconstructed from image data) Online/real-time mapping (SLAM)
geometric metric SLAM (accurate computations, therefore still widely used in practice) monocular multiocular multi-kind sensor (active) loosely-coupled closely-coupled learning SLAM (active) needs a prior dataset to train NN &amp;ndash; dataset determines performanace of the SLAM low generalisation capability, therefore not as flexible as geom.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Collision-detection-and-response/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Collision-detection-and-response/</guid><description>Collision detection and response Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Simulation algorithms in SOFA.md|Simulation algorithms in SOFA]]
split into several phases each phase is scheduled by a CollisionPipeline component an object which can potentially collide is associated with a collision geometry returns pairs of colliding bounding volumes (broad phase component) returns pairs of geometric primitives + contact points (narrow phase component) the returned pairs are passed to the contact manager the contact manager creates contact interactions &amp;hellip; (skimmed)</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Collision-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Collision-model/</guid><description>Collision model Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Models in SOFA.md|Models in SOFA]]
Primitives coming into contact — we need
collision detection collision response Collision detection approaches:
Distances between pairs of geometric primitives Points in distance fields Distances between colliding meshes using ray-tracing Intersection volume using images Collision model
Similar to internal model
Topology/geometry is different (geometry specially for contact model), can be stored in a data structure dedicated to collision detection e.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Components-of-the-internal-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Components-of-the-internal-model/</guid><description>Components of the internal model Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Internal model as a scene graph in SOFA.md|Internal model as a scene graph in SOFA]]
![[./_resources/Components_of_the_internal_model.resources/Image.png]]
MeshLoader: topology, geometry
[[MechanicalState.md|MechanicalState]]
TetrahedronSetTopologyContainer
tetrahedral connectivity passed on to other components [[Forces.md|Forces]]
Mass
DiagonalMass UniformMass: less accurate, but allows faster computation FixedConstraint: P (cancels displacements)</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Computational-properties-of-Gaussian-distributions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Computational-properties-of-Gaussian-distributions/</guid><description>Computational properties of Gaussian distributions Parent: [[Gaussian distribution.md|Gaussian distribution]] Backlinks: [[Showing correlation using error ellipses.md|Showing correlation using error ellipses]]
Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
g1 + g2 = g3; all are Gaussians g1 * g2 = g3; g3 is not Gaussian, but proportional to a Gaussian Sum of two Gaussians ![[./_resources/Computational_properties_of_Gaussian_distributions.resources/unknown_filename.1.png]]
Product of two Gaussians: ![[./_resources/Computational_properties_of_Gaussian_distributions.resources/unknown_filename.png]]
Product of multidimensional Gaussians: ![[./_resources/Computational_properties_of_Gaussian_distributions.resources/unknown_filename.2.png]]
Created at 2020-08-31.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Conditional-independence/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Conditional-independence/</guid><description>Conditional independence Source: https://en.wikipedia.org/wiki/Conditional_independence A and B are conditionally independent given C If and only if, given the knowledge that C occurs, the knowledge of whether A occurs provides no information whatsoever on the likelihood of B occurring, and vice versa
Example Weather and delay
Let the two events be the probabilities of persons A and B getting home in time for dinner
The third event C is the fact that a snow storm hit the city.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Constraint-solvers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Constraint-solvers/</guid><description>Constraint solvers Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Simulation algorithms in SOFA.md|Simulation algorithms in SOFA]] Backlinks: [[Scene graph in SOFA.md|Scene graph in SOFA]]
Lagrange multipliers to handle complex constraints (which aren&amp;rsquo;t handle-able using [[projection matrices.md|projection matrices]]) May be combined with explicit or implicit integration ![[./_resources/Constraint_solvers.resources/unknown_filename.png]] phi: bilateral interaction laws (attachments, sliding joints, &amp;hellip;) psi: unilateral interaction laws (contact, friction, &amp;hellip;)
The Lagrange multipliers add force terms to the equation A*dv = b !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Converting-IMU-data-to-inertial-frame/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Converting-IMU-data-to-inertial-frame/</guid><description>Converting IMU data to inertial frame Parent: [[IMU index.md|IMU index]] Source: https://redshiftlabs.com.au/wp-content/uploads/2018/02/an-1005_-_understanding_euler_angles.pdf IMU outputs are in the body frame of the sensor.
Convention used in the article: yaw/psi (z) - pitch/theta (y) - roll/phi (x) around momentary axes Momentary coordinate systems: W -&amp;gt; W' -&amp;gt; W'' -&amp;gt; B Body acceleration to inertial acceleration W_a = R_WB @ B_a
Body angular rate to inertial angular rate
Each angular rate must be converted to the corresponding frame p: gyro_z -&amp;gt; rotated into W: R_w_w' @ R_w'_w'' @ R_w''_B @ q q: gyro_y -&amp;gt; rotated into W': R_w'_w'' @ R_w''_B @ q r: gyro_x -&amp;gt; rotated into W'': R_w''_B @ r !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Converting-velocity-from-CS1-to-CS0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Converting-velocity-from-CS1-to-CS0/</guid><description>Converting velocity from CS1 to CS0 Source: [[Woernle Mehrkörpersysteme.md|Woernle Mehrkörpersysteme]] Backlinks: [[Kinematics primer.md|Kinematics primer]], [[Poisson equation for skew symmetric matrix of angular velocity.md|Poisson equation for skew symmetric matrix of angular velocity]]
Linear velocity ![[./_resources/Converting_velocity_from_CS1_to_CS0.resources/unknown_filename.png]] This is the derivative of r relative to CS0, as depicted in CS0 coordinates
![[./_resources/Converting_velocity_from_CS1_to_CS0.resources/unknown_filename.1.png]] Where the expression in square brackets means: the derivative of r relative to CS0, as depicted in CS1 coordinates
Angular velocity !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Correlation-and-independence/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Correlation-and-independence/</guid><description>Correlation and independence Parent: [[Gaussian distribution.md|Gaussian distribution]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Independent variables are uncorrelated. But the reverse is not always true: uncorrelated variables may be dependent on one another e.g. y=x^2 has no [linear] correlation, but y depends on x nonetheless
Created at 2020-08-31. Last updated at 2020-09-01.
Tagged: #-sa/processed #math/statistics</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Covariance-matrix-P/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Covariance-matrix-P/</guid><description>Covariance matrix P Source: [[SLAM for Dummies.md|SLAM for Dummies]]
s. also EKF matrices Covariance matrix P
Covariance: measure of correlation of two variables Correlation: measure of degree of linear dependence ![[./_resources/Covariance_matrix_P.resources/unknown_filename.png]]
A covariance of the robote POSEupdated in [[step 1 (odometry update).md|step 1 (odometry update)]] 3x3 B .. C covariance on the first .. nth landmark[[Step 3_ New landmarks.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Covisible-keyframes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Covisible-keyframes/</guid><description>Covisible keyframes Source: Palafox 2019 ( thesis ) Backlinks: [[Lamarca 2019 DefSLAM.md|Lamarca 2019 DefSLAM]]
Two keyframes are covisible if they share several common landmarks.
![[./_resources/Covisible_keyframes.resources/unknown_filename.png]]
Created at 2020-09-15. Last updated at 2020-09-15.
Tagged: #localisation/keyframes #-definitions #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Cryosection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Cryosection/</guid><description>Cryosection Source: https://en.wikipedia.org/wiki/Frozen_section_procedure Backlinks: [[GRK 2543_ Intraoperative Multi-sensor Tissue Differentiation in Oncology.md|GRK 2543: Intraoperative Multi-sensor Tissue Differentiation in Oncology]]
aka frozen section procedure allows rapid analysis of a dissected/resected specimen during the course of [[surgery.md|surgery]] the specimen is frozen rapidly and brought to a lab for analysis the results are relayed to the surgeon by intercom benign or malignant when operating on a previously confirmed malignant tissue, information on whether residual cancer was found on the [[resection margin.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Cyril-Stachniss-EKF-SLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Cyril-Stachniss-EKF-SLAM/</guid><description>Cyril Stachniss EKF-SLAM Links:
Course material: http://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/ Lectures: http://www.youtube.com/playlist?list=PLgnQpQtFTOGQrZ4O5QzbIHgl3b1JHimN_&amp;amp;feature=g-list Created at 2021-05-13. Last updated at 2021-05-14.
Tagged: #-resources #filters/EKF #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Cystocopy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Cystocopy/</guid><description>Cystocopy Source: https://en.wikipedia.org/wiki/Cystoscopy Backlinks: [[Some questions.md|Some questions]], [[Cancer biopsy.md|Cancer biopsy]]
Endoscopy of the bladder via the urethra Tool involved: cystoscope https://www.youtube.com/watch?v=ybhzlW7ivro Video of cystocopy and bladder [[biopsy.md|biopsy]] ![[./_resources/Cystocopy.resources/unknown_filename.png]]
Modern Latin for bladder: cystis
Created at 2020-08-08. Last updated at 2020-12-13.
Tagged: #medical/surgery #-definitions #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Data-association-in-DefSLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Data-association-in-DefSLAM/</guid><description>Data association in DefSLAM Source: [[Lamarca 2019 DefSLAM.md|Lamarca 2019 DefSLAM]] See also: [[Data association.md|Data association]]
Goal: match keypoints in current frame (newly extracted) with map points (already in map/system) Use the active matching strategy proposed in [Agudo 2015]: “Simultaneous pose and non-rigid shape with particle dynamics,” Steps  ORB points (keypoints) are detected in current frame
Camera pose Tcw is predicted
using camera motion model camera motion model: function of past camera poses Predict where map points (existing in map) would be imaged, based on last estimated template i.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Data-association/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Data-association/</guid><description>Data association Parents: [[SLAM Index.md|SLAM Index]], [[Basic EKF for SLAM.md|Basic EKF for SLAM]]
Source: [[SLAM for Dummies.md|SLAM for Dummies]] Backlinks: [[What is SLAM_.md|What is SLAM?]], [[Data association in DefSLAM.md|Data association in DefSLAM]], [[Visual SLAM Implementation Framework.md|Visual SLAM Implementation Framework]]
Matching observed landmarks from different scans (different time steps) with each other. Also called &amp;rsquo;re-observing' landmarks.
Problems that can arise:
The landmark(s) might not be observed every time step (bad landmark) Something might be observed as a landmark, but it never appears again (bad landmark) Wrong association of a landmark to a previously seen landmark Goal: define a suitable data-association policy to minimise the first two problems Given: database that stores previously seen landmarks (initially empty) As a rule: a landmark is only considered worthwhile to be used in SLAM once it is seen N times</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Data-structure-in-SOFA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Data-structure-in-SOFA/</guid><description>Data structure in SOFA Source: [[SOFA extended documentation.md|SOFA extended documentation]]
Three different solutions for three relevant levels [of organisation of simulation data].
Scenegraph ( directed acyclic graphs) s. also: [[Visitors.md|Visitors]]
Component attributes
Component params stored using Data&amp;lt;&amp;hellip;&amp;gt; containers e.g. list of particle indices Data&amp;lt;vector&amp;gt; Engines Create connections between Data instances, for synchronisation of values Compute a value from several others (input/output processor) Are only used to apply straightforward relations between model parameters State update algorithms are implemented using visitors [[Mesh geometry.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/DBoW2-weighing-and-scoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/DBoW2-weighing-and-scoring/</guid><description>DBoW2 weighing and scoring Source: https://github.com/dorian3d/DBow
![[./_resources/DBoW2_weighing_and_scoring.resources/unknown_filename.png]]
![[./_resources/DBoW2_weighing_and_scoring.resources/unknown_filename.1.png]]
Created at 2020-12-15. Last updated at 2020-12-15.
Tagged: #-resources/-bibliography #-resources/-bibliography/bib-to-read #vision #discussion/2020/2020-12 #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Dead-reckoning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Dead-reckoning/</guid><description>Dead-reckoning Source: https://en.wikipedia.org/wiki/Dead_reckoning Backlinks: [[What is SLAM_.md|What is SLAM?]]
In navigation, dead reckoning is the process of calculating one&amp;rsquo;s current position by using a previously determined position, by using estimations of speed and course over elapsed time
s. Brian Douglas video on sensor fusion Created at 2020-07-30. Last updated at 2020-10-24.
Tagged: #to-do #localisation #-definitions #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/DefOptimizer__DefPoseOptimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/DefOptimizer__DefPoseOptimization/</guid><description>DefOptimizer::DefPoseOptimization Parent: [[DefTracking__Track.md|DefTracking::Track]]
As far as I understand it:
Uses g2o library for the optimisation (graph-based SLAM) cost function terms are converted to edges and nodes each cost function term seems to correspond to an edge in the graph in g2o paper/tutorial: an edge is fully characterised by its error function and its information matrix int DefPoseOptimization(Frame *pFrame, Map *mMap, double RegLap, double RegInex, double RegTemp, uint NeighboursLayers) // define optimiser, set solver optimizer = new &amp;hellip; optimizer.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/DefOptimizer__poseOptimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/DefOptimizer__poseOptimization/</guid><description>DefOptimizer::poseOptimization Parent: [[DefTracking_TrackWithMotionModel().md|DefTracking:TrackWithMotionModel()]]
int DefOptimizer::poseOptimization(Frame *pFrame)
// Set estimate of solution to current camera pose g2o::VertexSE3Expmap *vSE3 = new g2o::VertexSE3Expmap(); vSE3-&amp;gt;setEstimate(Converter::toSE3Quat(pFrame-&amp;gt;mTcw)); vSE3-&amp;gt;setId(0); vSE3-&amp;gt;setFixed(false); optimizer.addVertex(vSE3);
// Set MapPoint vertices (num. nodes in opt. graph?) const int N = pFrame-&amp;gt;N;
Created at 2020-10-21. Last updated at 2020-10-22.
Tagged: #-sa/processed #SLAM/SLAM-algos/DefSLAM</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/DefSLAM-and-discontinuous-areas-classical-datasets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/DefSLAM-and-discontinuous-areas-classical-datasets/</guid><description>DefSLAM and discontinuous areas (classical datasets) Parent: [[Lamarca 2020 DefSLAM.md|Lamarca 2020 DefSLAM]] Source: https://github.com/UZ-SLAMLab/DefSLAM/issues/1
JoseLamarca: DefSLAM is suitable for rigid areas, proof of that is the abdominal sequence that is kind of rigid. The problem for these sequences is the discontinuous areas. For the monocular case, we are assuming that the surface is smooth that is not usually valid for the classical datasets. Apart from complexity issues that algorithms with RGB-D and stereo cameras could have in those scenes [1] and [2].</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/DefSLAM-branch-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/DefSLAM-branch-overview/</guid><description>DefSLAM branch overview Parent: [[SA TODO.md|SA TODO]]
Repo https://github.com/feudalism/DefSLAM
Dormant
master sa Deprecated
windows - deprecated, changes made for building on Windows imu - deprecated, has Imu tracking functions but dependencies not resolved obs_tuple - initial attempt to incorporate Atlas, attempt to use OS3&amp;rsquo;s structure for MapPoint observations : &amp;lt;KeyFrame, tuple&amp;lt;int, int&amp;raquo; as opposed to &amp;lt;Keyframe, int&amp;gt; in DefSLAM+OS2 Temporary/Experimental
s. to do list</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/DefSLAM-dependency_inheritance-diagram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/DefSLAM-dependency_inheritance-diagram/</guid><description>DefSLAM dependency/inheritance diagram ![[./_resources/DefSLAM_dependency_inheritance_diagram.resources/defslam-dependencies.png]]
Created at 2020-11-17. Last updated at 2020-11-17.
Tagged: #-sa/processed #SLAM/SLAM-algos/DefSLAM</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/DefSLAM-errors-encountered/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/DefSLAM-errors-encountered/</guid><description>DefSLAM errors encountered Rebuilding DefSLAM in Debug mode Error: &amp;ldquo;Virtual memory exhausted: Cannot allocate memory&amp;rdquo; ![[./_resources/DefSLAM_errors_encountered.resources/unknown_filename.png]] Solution: reduce degree of make -j
Segmentation fault in Defslam debug mode ![[./_resources/DefSLAM_errors_encountered.resources/unknown_filename.1.png]]
Based on https://stackoverflow.com/questions/19615371/segmentation-fault-due-to-vectors Changed: surfacePoints_[ind] to surfacePoints_.at(ind)
New error ![[./_resources/DefSLAM_errors_encountered.resources/unknown_filename.2.png]] surfacePoints_ appears to be NULL? Was it instantiated in another thread? https://stackoverflow.com/questions/11645857/debugging-with-gdb-why-this-0x0
Using core dumps with gdb https://jvns.ca/blog/2018/04/28/debugging-a-segfault-on-linux/
Created at 2021-01-13. Last updated at 2021-04-23. Source URL: .</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/DefSLAM-framework/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/DefSLAM-framework/</guid><description>DefSLAM framework Source: [[Lamarca 2019 DefSLAM.md|Lamarca 2019 DefSLAM]]
&amp;ldquo;Fusion of the methods available for processing non-rigid monocular scenes&amp;rdquo;
Deformation tracking [front end]
estimates/recovers/optimises: camera pose scene deformation / deformation of map points (observations) the map points are then embedded into the template Tk (to compute their position on the surface) operates at frame rate SFT-based (shape from template), requires prior geometry (template of scene at rest) for the currently being viewed map Map points are deformed (updated) by solving an optimisation problem min { reprojection error + deformation energy } per frame [[Deformation mapping.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/DefSLAM-simple_camera/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/DefSLAM-simple_camera/</guid><description>DefSLAM simple_camera Without ground truth
App run // Create defSLAM::system, which initializes all threads (local mapping, loop closing, viewer) defSLAM::System SLAM(orbVocab, calibFile, bUseViewer);
// Timestamp uint timestamp := 0;
// Process frames from video capture while (capture.isOpened()) { // Get the capture as a matrix; // SLAM SLAM. TrackMonocular (img_matrix, timestamp); timestamp++; }
Created at 2020-10-19. Last updated at 2020-10-22.
Tagged: #-sa/processed #SLAM/SLAM-algos/DefSLAM</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/defSLAM__System-constructor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/defSLAM__System-constructor/</guid><description>defSLAM::System constructor Parent: [[DefSLAM simple__camera.md|DefSLAM simple_camera]]
defSLAM::System::System(const string &amp;amp;strVocFile, const string &amp;amp;strSettingsFile, const bool bUseViewer)  mSensor(MONOCULAR), mpLoopCloser(NULL), mpViewer(static_cast&amp;lt;Viewer *&amp;gt;(nullptr)), mbReset(false), mbActivateLocalizationMode(false), mbDeactivateLocalizationMode(false) Constructor // initialise mpVocabulary from file // create mpKeyFrameDatabase from mpVocabulary // create map DefMap() // create drawers for viewer DefFrameDrawer DefMapDrawer // initialise tracking, mapping, viewer threads; loop closing not implemented in DefSLAM mpTracker = new DefTracking(&amp;hellip;); mpLocalMapper = new DefLocalMapping(&amp;hellip;); mpViewer = new DefViewer(&amp;hellip;);</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/defSLAM__System__TrackMonocular/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/defSLAM__System__TrackMonocular/</guid><description>defSLAM::System::TrackMonocular Parent: [[DefSLAM simple__camera.md|DefSLAM simple_camera]]
cv::Mat defSLAM::System::TrackMonocular cv::Mat Tcw = mpTracker-&amp;gt; GrabImageMonocular (im, timestamp);
// get information from mpTracker: // get states mTrackingState // get map points mTrackedMapPoints // get key points mTrackedKeyPointsUn
// return camera pose return Tcw;
Created at 2020-10-21. Last updated at 2020-10-22.
Tagged: #-sa/processed #SLAM/SLAM-algos/DefSLAM</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/DefTracking__MonocularInitialization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/DefTracking__MonocularInitialization/</guid><description>DefTracking::MonocularInitialization Parent: [[DefTracking__Track.md|DefTracking::Track]]
Initialises
surface points in the surface If num. features in current frame &amp;gt; 100
set frame pose to origin make new KF (GroundTruthKeyFrame) pKFini add the KF to the map mpMap iterate over the N features get feature kp convert kp to 3d point make new DefMapPoint(3dp, pKFini, mpMap) set pointers between DefMapPoint, GroundTruthKeyFrame, DefMap Save surface using bbs Set mLastFrame := mCurrentFrame Local window: Add KF to local KFs vector, add MapPoints to local MP vector, mpLocalMapper Calculate Tcr from Tcw Initialise SLAM: Set reference KF, reference MapPoints set mState to OK Created at 2020-10-28.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/DefTracking__Track/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/DefTracking__Track/</guid><description>DefTracking::Track Parent: [[Tracking__GrabImageMonocular.md|Tracking::GrabImageMonocular]]
void DefTracking::Track // if: not initialised, do: monocular initialisation // elseif: already initialised, do: track frame { // if: tracking and mapping, do: bOK = TrackWithMotionModel ();
// if bOK (there exists camera pose estimate and matching), track local map // if template is updated (keyframe-rate update) set reference KF from new template do DefPoseOptimization (&amp;hellip;); bOK = TrackLocalMap();
// if: bOK, update motion model (update mVelocity); clean VO matches // check if we should insert a new KF, delete outliers for BA</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/DefTracking_TrackWithMotionModel/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/DefTracking_TrackWithMotionModel/</guid><description>DefTracking:TrackWithMotionModel() Parent: [[DefTracking__Track.md|DefTracking::Track]]
// Initial tracking to locate rigidly the camera and discard outliers. bool DefTracking::TrackWithMotionModel()
// Update last frame relative pose according to its reference keyframe UpdateLastFrame();
// Project points seen in prev frames int th = 15; int nmatches = Defmatcher.SearchByProjection(*mCurrentFrame, mLastFrame, th, mSensor == System::MONOCULAR);
// Optimise frame pose with all matches to initialise camera pose Optimizer:: poseOptimization (mCurrentFrame, myfile);
// Discard outliers
// return: sufficient number of matches?</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Dense_direct-VSLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Dense_direct-VSLAM/</guid><description>Dense/direct VSLAM Parent: [[Visual SLAM Implementation Framework.md|Visual SLAM Implementation Framework]], [[SLAM Index.md|SLAM Index]] See also: [[Feature-based vs direct SLAM workflow.md|Feature-based vs direct SLAM workflow]]
Source: [[Cometlabs What You Need to Know About SLAM.md|Cometlabs What You Need to Know About SLAM]]
Front-end part of the [[Visual SLAM Implementation Framework.md|Visual SLAM Implementation Framework]] Use most or all of the pixels in each received frame Provide more information about the environment Many more pixels, require GPUs [[Feature-based vs direct SLAM workflow.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Deriving-Kalman-filter-from-Discrete-Bayes-using-Gaussians/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Deriving-Kalman-filter-from-Discrete-Bayes-using-Gaussians/</guid><description>Deriving Kalman filter from Discrete Bayes using Gaussians Parent: [[1D Kalman filters.md|1D Kalman filters]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Replacing discrete Bayes with Gaussian distributions ![[./_resources/Deriving_Kalman_filter_from_Discrete_Bayes_using_Gaussians.resources/unknown_filename.png]] where the operators in the circles are as of yet undetermined
Created at 2020-08-31. Last updated at 2020-08-31.
Tagged: #-sa/processed #filters/kalman-filter #filters/bayesian-filter</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Descriptors-in-feature-detection_extraction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Descriptors-in-feature-detection_extraction/</guid><description>Descriptors in feature detection/extraction Source: https://medium.com/data-breach/introduction-to-feature-detection-and-matching-65e27179885d Backlinks: [[Bag of words.md|Bag of words]]
A description of the local appearance around each feature point (keypoint) The descriptor encodes &amp;lsquo;interesting&amp;rsquo; information from the image into numbers and act as an identifier (&amp;lsquo;fingerprint&amp;rsquo;) to differentiate between features The description should ideally be invariant to changes (such as illumination, translation, scale, in-plane rotation) so that the feature can be found again, even if the image is transformed Typically: for each feature point, there is a descriptor vector Classes of descriptors:</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Diagnosis-bladder-cancer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Diagnosis-bladder-cancer/</guid><description>Diagnosis bladder cancer https://www.cancer.org/cancer/bladder-cancer/detection-diagnosis-staging/how-diagnosed.html
Created at 2021-07-30. Last updated at 2021-07-30.
Tagged: #-resources #-master/thesis #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Diagram_-IMU-on-cystoscope/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Diagram_-IMU-on-cystoscope/</guid><description>Diagram: IMU on cystoscope Backlinks: [[Discussion 2021-05-21.md|Discussion 2021-05-21]]
![[./_resources/Diagram__IMU_on_cystoscope.resources/unknown_filename.jpeg]]
Created at 2021-05-21. Last updated at 2021-05-21.
Tagged: #-sa/processed #math/kinematics #discussion/2021/2021-05 #medical/surgery/endoscope</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Difference-between-haptic-feedback-and-vibration-alerting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Difference-between-haptic-feedback-and-vibration-alerting/</guid><description>Difference between haptic feedback and vibration alerting Source: https://www.precisionmicrodrives.com/haptic-feedback/introduction-to-haptic-feedback/ Parent: [[Scope of Studienarbeit.md|Scope of Studienarbeit]]
![[./_resources/Difference_between_haptic_feedback_and_vibration_alerting.resources/unknown_filename.jpeg]]
Created at 2020-06-19. Last updated at 2020-08-09. Source URL: .
Tagged: #misc/haptic-feedback #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Differentiation-in-different-coordinate-systems/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Differentiation-in-different-coordinate-systems/</guid><description>Differentiation in different coordinate systems Source: [[Woernle Mehrkörpersysteme.md|Woernle Mehrkörpersysteme]] Backlinks: [[Kinematics primer.md|Kinematics primer]]
![[./_resources/Differentiation_in_different_coordinate_systems.resources/unknown_filename.2.png]]
![[./_resources/Differentiation_in_different_coordinate_systems.resources/unknown_filename.png]]
![[./_resources/Differentiation_in_different_coordinate_systems.resources/unknown_filename.1.png]]
Created at 2021-06-27. Last updated at 2021-06-27.
Tagged: #-sa/processed #math/kinematics</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Discretising-a-state-space-equation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Discretising-a-state-space-equation/</guid><description>Discretising a state space equation Source: https://en.wikibooks.org/wiki/Control_Systems/State-Space_Equations#Discretization Discretising a state space equation ![[./_resources/Discretising_a_state_space_equation.resources/unknown_filename.png]]
Created at 2021-03-31. Last updated at 2021-03-31.
Tagged: #math #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Discussion-2020-11-03/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Discussion-2020-11-03/</guid><description>Discussion 2020-11-03 Current progress
Find out how g2o works, how the DefSLAM implementation of the tracking optimisation works Incorporate IMU data (s. ORBSLAM3 implementation) IMU initialisation IMU preintegration IMU terms in cost function (which function?) Topics
How g2o works IMU preintegration DefSLAM + IMU cost function Implementation in DefSLAM using g2o [tbd] ORBSLAM3&amp;rsquo;s implementation the IMU cost function terms initialisation preintegration Kalman idea for IMU integration Compile + run Created at 2020-11-02.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Discussion-2021-05-10/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Discussion-2021-05-10/</guid><description>Discussion 2021-05-10 Agenda
Change/Reduction of scope of SA (from fusing IMU with camera) to using sensor fusion to determine transformation parameters between IMU and camera Camera and IMU setup involves kinematic modelling (not fixed transformation as previously assumed!) Offline implementation in Python/MATLAB (scripting language) HiWi tasks can include DefSLAM bindings / interface C++ bindings of skrogh EKF implementation? HiWi prioritises Versuchsstand for now Tasks</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Discussion-2021-05-21/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Discussion-2021-05-21/</guid><description>Discussion 2021-05-21 Notes — Before
Got the ESKF implementation (Solà) to work with my fake IMU data [non-noisy IMU] results look ok for low process noise (trust the prediction more) with relatively high measurement noise [noisy IMU] ok? current assumptions/simplifications: fake data assumes IMU is sitting right on top of camera fake data, as of yet, does not take into account: biases, gravity simplified state vector (no scale estimate, no gravity estimate, no bias estimate etc) TBD: modify equations/states to fit the problem, i.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Discussion-2021-05-25/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Discussion-2021-05-25/</guid><description>Discussion 2021-05-25 Backlinks: [[Discussion 2021-05-10.md|Discussion 2021-05-10]], [[Discussion 2021-05-21.md|Discussion 2021-05-21]]
Notes
IMU-rod transformation: rotation part (spherical joint), translation part predict and update equations? maybe change variables in states vector to local coordinates add gravity later Ausblick: Einfluss der IMU auf verbesserte Lokalisierung &amp;ndash;&amp;gt; evtl eine IMU Koordinate weglassen Next:
generate fake IMU data (delegated, s. [[Obtaining IMU measurements from camera by forward kinematics.md|Obtaining IMU measurements from camera by forward kinematics]]) look for existing literature on IMU fusion/EKF which uses kinematic relations Massenmatrix, Koriolisterme etc.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Discussion-2021-06-01/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Discussion-2021-06-01/</guid><description>Discussion 2021-06-01 Notes
IMU data to be generated using kinematic relations, not via numerical differentiation Reduce loss of data, model for prediction, noise propagation Forward kinematics B &amp;ndash;&amp;gt; C (everything in terms of SLAM coordinates), s. [[Probe forward kinematics.md|Probe forward kinematics]] For visualisation: IMU data in W coordinates Monday: real probe Python robotics toolboxes for generating of forward kinematics matrices, velocity expressions (symbolic differentiation) Predict step</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Distal-and-proximal-ends/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Distal-and-proximal-ends/</guid><description>Distal and proximal ends Source: [[Leiner.md|Leiner]]
distal: far from the surgeon proximal: near the surgeon
Created at 2021-07-21. Last updated at 2021-07-21.
Tagged: #-definitions #-sa/processed #medical/surgery/endoscope</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Distance-between-landmarks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Distance-between-landmarks/</guid><description>Distance between landmarks Source: [[SLAM for Dummies.md|SLAM for Dummies]] Backlinks: [[Nearest Neighbour.md|Nearest Neighbour]]
Methods:
Euclidean distance (suitable for far distances) Mahalanobis distance (better, but more complex) Created at 2020-08-06. Last updated at 2020-08-06.
Tagged: #localisation/landmarks #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Dynamic-Bayesian-Network-formulation-of-SLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Dynamic-Bayesian-Network-formulation-of-SLAM/</guid><description>Dynamic Bayesian Network formulation of SLAM Source: [[Grisetti 2011 - Tutorial graph-based SLAM.md|Grisetti 2011 - Tutorial graph-based SLAM]]
Dynamic Bayesian Network
![[./_resources/Dynamic_Bayesian_Network_formulation_of_SLAM.resources/unknown_filename.png]] Solution of full SLAM problem: ![[./_resources/Dynamic_Bayesian_Network_formulation_of_SLAM.resources/unknown_filename.1.png]] Transition model: ![[./_resources/Dynamic_Bayesian_Network_formulation_of_SLAM.resources/unknown_filename.3.png]] Observation model: ![[./_resources/Dynamic_Bayesian_Network_formulation_of_SLAM.resources/unknown_filename.2.png]]
The observation model is usually multimodal: a single observation may result in multiple edges (in the spatial graph) Therefore, the Gaussian assumption does not hold Created at 2020-10-24. Last updated at 2020-10-25.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Edges-in-g2o/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Edges-in-g2o/</guid><description>Edges in g2o ![[./_resources/Edges_in_g2o.resources/unknown_filename.png]]
Created at 2020-10-22. Last updated at 2020-10-22.
Tagged: #-sa/processed #discussion/2020/2020-10 #software/g2o</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Effects-of-varying-g/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Effects-of-varying-g/</guid><description>Effects of varying g Parent: [[Calculating the estimated state in the GH-filter.md|Calculating the estimated state in the GH-filter]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
![[./_resources/Effects_of_varying_g.resources/unknown_filename.png]]
The greater the g value, the more we follow the measurements rather than rely on our [model-based] predictions.
Created at 2020-08-27. Last updated at 2020-08-27.
Tagged: #-sa/processed #filters/gh-filter</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Effects-of-varying-h/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Effects-of-varying-h/</guid><description>Effects of varying h Parent [[Improving the gh-filter by using h.md|Improving the gh-filter by using h]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
![[./_resources/Effects_of_varying_h.resources/unknown_filename.png]]
The greater the h value, the more we trust the rate of change that we can derive from the measurement data.
a larger h enables us to react to transient (initial condition dependent) changes more rapidly. Because if we have a large difference between our chosen IC and the measurement, this results in a huge residual velocity</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Egomotion-vs-odometry/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Egomotion-vs-odometry/</guid><description>Egomotion (vs odometry) See also: [[Odometry.md|Odometry]]
Source: https://en.wiktionary.org/wiki/egomotion The three-dimensional movement of a camera relative to its environment
Source: https://answers.ros.org/question/296686/what-is-the-differences-between-ego-motion-and-odometry/
Generally used interchangeably with odometry Possible difference: Egomotion is more about the estimation of twist (lin, rotational velocities) Odometry is more about the estimation of path Examples
Wheel odometry: path estimation via time-integration of an estimated twist Visual odometry/Scan matching: direct estimation of pose without time-integration Created at 2020-08-25.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/EKF-matrices_vectors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/EKF-matrices_vectors/</guid><description>EKF matrices/vectors Source: [[SLAM for Dummies.md|SLAM for Dummies]] Backlinks: [[Extended Kalman Filter.md|Extended Kalman Filter]]
System state X [[Estimate of POSE.md|Estimate of POSE]] Jacobian of prediction model Landmark range and bearing Jacobian of measurement model [[Covariance matrix P.md|Covariance matrix P]]
Kalman gain K [[SLAM-specific jacobians.md|SLAM-specific jacobians]]
Created at 2020-07-27. Last updated at 2020-08-06.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/EKF-System-State/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/EKF-System-State/</guid><description>EKF System State Source: [[SLAM for Dummies.md|SLAM for Dummies]] Backlinks: [[EKF matrices.md|EKF matrices]], [[Step 2_ Re-observation.md|Step 2: Re-observation]]
Contains robot POSE and landmark position POSE: (x y theta)_r LM: (x, y)_l1 &amp;hellip; (x,y)_ln; n = num. of landmarks Size: 3+2n rows Created at 2020-08-06. Last updated at 2020-08-06.
Tagged: #filters/EKF #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Empirical-rule-68_95_99.7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Empirical-rule-68_95_99.7/</guid><description>Empirical rule 68/95/99.7 Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Emprical rule, a.k.a. 68–95–99.7 rule About 68% of all values lie within one standard deviation of the mean. ![[./_resources/Empirical_rule_68_95_99.7.resources/unknown_filename.png]]
Created at 2020-08-31. Last updated at 2020-08-31.
Tagged: #-definitions #-sa/processed #math/statistics</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Endoscope-tip/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Endoscope-tip/</guid><description>Endoscope tip Source: https://www.osapublishing.org/ao/fulltext.cfm?uri=ao-43-1-113&amp;amp;id=78236#F2 Note: this is a mini endoscope, probably not the standard construction
![[./_resources/Endoscope_tip.resources/unknown_filename.png]]
![[./_resources/Endoscope_tip.resources/unknown_filename.1.png]]
Created at 2021-06-14. Last updated at 2021-06-14.
Tagged: #medical/surgery/endoscope #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Endoscope_cystoscopy-pics_videos/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Endoscope_cystoscopy-pics_videos/</guid><description>Endoscope/cystoscopy pics/videos Backlinks: [[Discussion 2021-05-21.md|Discussion 2021-05-21]]
Rigid endoscope for cystoscopy
Source: https://www.ebay.com/itm/113780645426 ![[./_resources/Endoscope_cystoscopy_pics_videos.resources/unknown_filename.png]]
Source: https://www.researchgate.net/figure/Intraoperative-image-of-the-rigid-cystoscope-entering-the-bladder-through-the-screw-tip_fig2_322897289 ![[./_resources/Endoscope_cystoscopy_pics_videos.resources/unknown_filename.1.png]]
Source: https://www.youtube.com/watch?v=1gEpz9wijoY ![[./_resources/Endoscope_cystoscopy_pics_videos.resources/unknown_filename.2.png]]![[./_resources/Endoscope_cystoscopy_pics_videos.resources/unknown_filename.3.png]]
https://www.maestro-portal.eu/procedure/detail/4 Videos: Semi-Rigid Ureteroscopy and Laser Lithotripsy for Ureter Stones
Source: https://www.medicinenet.com/how_painful_is_a_cystoscopy/article.htm Created at 2021-05-21. Last updated at 2021-07-21. Source URL: .
Tagged: #-resources #-sa/processed #discussion/2021/2021-05 #medical/surgery/endoscope</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Equations-for-obtaining-omega-angular-velocity-and-acceleration-of-IMU-from-camera/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Equations-for-obtaining-omega-angular-velocity-and-acceleration-of-IMU-from-camera/</guid><description>Equations for obtaining omega (angular velocity) and acceleration of IMU from camera Parent: [[Update 2021-06-11.md|Update 2021-06-11]]
![[./resources/Equations_for_obtaining_omega(angular_velocity)_and_acceleration_of_IMU_from_camera.resources/unknown_filename.png]]
Created at 2021-06-11. Last updated at 2021-06-11.
Tagged: #-sa/processed #math/kinematics #discussion/2021/2021-06</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Error-ellipse_Confidence-ellipse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Error-ellipse_Confidence-ellipse/</guid><description>Error ellipse/Confidence ellipse Parent: [[Multivariate Gaussian distributions.md|Multivariate Gaussian distributions]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Any slice through a multivariate Gaussian is an ellipse ![[./_resources/Error_ellipse_Confidence_ellipse.resources/unknown_filename.png]] Plots show the slice for 3 standard deviations
[[Showing correlation using error ellipses.md|Showing correlation using error ellipses]]
Created at 2020-09-01. Last updated at 2020-09-01.
Tagged: #-definitions #-sa/processed #math/statistics</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/ESKF-repos/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/ESKF-repos/</guid><description>ESKF repos C++ https://github.com/skrogh/msf_ekf https://github.com/je310/ESKF https://github.com/hobbeshunter/IMU_EKF (only IMU)
Python https://github.com/enginBozkurt/Error-State-Extended-Kalman-Filter https://github.com/uoip/stereo_vio_eskf (unsuccessful) &amp;ndash; uses average IMU readings https://github.com/aipiano/ESEKF_IMU Created at 2021-08-12. Last updated at 2021-08-13.
Tagged: #-resources #-sa/processed #discussion/2021/2021-08</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Euler-angles/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Euler-angles/</guid><description>Euler angles Parents: [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]], [[Orientation parametrisations.md|Orientation parametrisations]] Backlinks: [[Orientation parametrisations.md|Orientation parametrisations]]
Source: https://en.wikipedia.org/wiki/Euler_angles Possible representations
Proper Euler angles (e.g. zxz) vs Tait-Bryan (e.g. xyz, zyx) Extrinsic rotations (around fixed CS xyz) vs intrinsic rotations (around body CS XYZ = x''' y''' z''') As a rotation matrix This means either: (s. [[Intrinsic vs extrinsic rotations.md|Intrinsic vs extrinsic rotations]])
extrinsic rotations about z -&amp;gt; y -&amp;gt; x / yaw pitch roll intrinsic rotations about x -&amp;gt; y' -&amp;gt; z'' = Z = z''' Note: Any extrinsic rotation is equivalent to an intrinsic rotation by the same angles but with inverted order of elemental rotations, and vice versa.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Euler-axis_angle-representation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Euler-axis_angle-representation/</guid><description>Euler axis/angle representation Parents: [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]], [[Orientation parametrisations.md|Orientation parametrisations]] See also: [[Rotation vector representation.md|Rotation vector representation]] Backlinks: [[Gibbs _ Rodrigues parameter representation for rotations.md|Gibbs / Rodrigues parameter representation for rotations]]
Source: [[Markley Fundamentals of Spacecraft Attitude Determination.md|Markley Fundamentals of Spacecraft Attitude Determination]]
3 parameters:
there appears to be 4: 1 angle, 3-component unit vector (Euler axis, Euler angle of the rotation) however, the vector e is a unit vector (constrained by !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Expected-value/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Expected-value/</guid><description>Expected value Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Example: if we take a thousand sensor readings, the readings won&amp;rsquo;t always be the same (due to the inherent noise).
The expected value &amp;lsquo;averages&amp;rsquo; all of the readings into a single value. This can be a mean (probabilities of all values assumed equal) Or if incorporating individual and different probabilities, the expectation isn&amp;rsquo;t the mean of the range of values !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Exponential-map/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Exponential-map/</guid><description>Exponential map Parent: [[Quaternion index.md|Quaternion index]], [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]] Backlinks: [[Linearisation of an orientation in SO(3).md|Linearisation of an orientation in SO(3)]]
Source: [[Forster 2017 IMU Preintegration.md|Forster 2017 IMU Preintegration]]
![[./_resources/Exponential_map.resources/unknown_filename.png]] [At the identity] Maps an element of the [[Lie algebra.md|Lie algebra]] (a skew symmetric matrix) to a rotation ![[./_resources/Exponential_map.resources/unknown_filename.1.png]]
First order approximation ![[./_resources/Exponential_map.resources/unknown_filename.2.png]]
Some properties of the exponential map
Perturbations, first order approximation !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Extended-Kalman-Filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Extended-Kalman-Filter/</guid><description>Extended Kalman Filter Parent: [[SLAM Index.md|SLAM Index]] Backlinks: [[RANSAC.md|RANSAC]], [[Nearest Neighbour.md|Nearest Neighbour]], [[Filter localisation methods.md|Filter localisation methods]]
Source: [[SLAM for Dummies.md|SLAM for Dummies]]
keeps track of an estimate of the position uncertainty keeps track of the uncertainty in the features/landmarks seen [[General EKF implementation (non-SLAM).md|General EKF implementation (non-SLAM)]] [[Basic EKF for SLAM.md|Basic EKF for SLAM]]
Diagram: ![[./_resources/Extended_Kalman_Filter.resources/unknown_filename.png]]
Triangle Robot Stars Landmarks Dashed triangle Robot&amp;rsquo;s position based on odometry alone (where it thinks it is) Dotted triangle Robot&amp;rsquo;s position estimate based on EKF Solid line triangle Robot&amp;rsquo;s actual position in real life!</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/extern-c++/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/extern-c++/</guid><description>extern c++ https://en.cppreference.com/w/cpp/language/storage_duration is a storage class specifier that controls storage duration and its linkage.
extern - static or thread storage duration and external linkage
Storage duration
static: storage for the object is allocated when the program begins and deallocated when the program ends. only one instance of the object exists thread storage for the object is allocated when the thread begins and deallocated when the thread ends each thread has its own instance of the object Linkage</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/External-data-in-SOFA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/External-data-in-SOFA/</guid><description>External data in SOFA Parent: [[SofaPython Index.md|SofaPython Index]]
https://www.sofa-framework.org/community/forum/topic/how-to-use-external-data-in-sofa/ https://www.sofa-framework.org/community/forum/topic/how-to-send-data-to-sofa-through-socket/ https://www.sofa-framework.org/community/forum/topic/connecting-sofa-to-an-external-data-com-port/
Created at 2020-07-24. Last updated at 2020-08-22.
Tagged: #software/SOFA/communication-with-SOFA #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Fact-checks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Fact-checks/</guid><description>Fact checks Parent: [[Thesis.md|Thesis]]
Duration of cryosection GRK stuff SLAM rigidity assumption [[Camera-IMU complementarity.md|Camera-IMU complementarity]] Created at 2020-12-28. Last updated at 2021-03-24.
Tagged: #to-do #-sa/processed #-master/thesis</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Factors-affecting-Kalman-filter-performance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Factors-affecting-Kalman-filter-performance/</guid><description>Factors affecting Kalman filter performance Parent: [[1D Kalman filters.md|1D Kalman filters]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Difficulties of creating a well-performing Kalman filter: Includes modeling the sensor performance (what variance most accurately represents the reality? Which probability distribution?)
Factors affecting the performance of the Kalman filter
[[On modelling the process noise_variance.md|On modelling the process noise/variance]] Bad initial estimate Filter can recover from this, because we have a certain belief in the sensor measurements Typically the initial value is set to the first sensor measurement [[Nonlinearity of the system.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/FAST-keypoint-detector/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/FAST-keypoint-detector/</guid><description>FAST keypoint detector Source: https://medium.com/data-breach/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf Parent: [[ORB descriptor.md|ORB descriptor]]
FAST (Features from Accelerated and Segments Test)
How it works
Given: pixel p, surrounded by other pixels in the image
Take the surrounding pixels that are in a small circle around p ![[./_resources/FAST_keypoint_detector.resources/unknown_filename.jpeg]]
If more than half of the surrounding pixels are darker/brighter than p, p is selected as a keypoint
Good for edge detection</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Feature-maps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Feature-maps/</guid><description>Feature maps Parents: [[Mapping representations in robotics.md|Mapping representations in robotics]], [[Sparse_Feature-based VSLAM.md|Sparse/Feature-based VSLAM]]
Source: [[Cometlabs What You Need to Know About SLAM.md|Cometlabs What You Need to Know About SLAM]]
Uses a limited number of sparse objects to represent a map e.g. points, lines
Low computation cost because of the sparsity Map management solutions are good solutions for current applications What&amp;rsquo;s map management (probably storing maps in databases and recognising an existing map) [-] Sensitivity to false data association Created at 2020-07-30.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Feature-matching/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Feature-matching/</guid><description>Feature matching Source: https://medium.com/data-breach/introduction-to-feature-detection-and-matching-65e27179885d Backlinks: [[Bag of words.md|Bag of words]], [[Sparse_Feature-based VSLAM.md|Sparse/Feature-based VSLAM]]
For matching between images, i.e. to establish a relationship (&amp;lsquo;correspondence&amp;rsquo;) between two images of the same scene or object.
Basic algorithm
Find/detect a set of identifying (&amp;lsquo;distinctive&amp;rsquo;) keypoints from all images to be matched Define a search region around each keypoint Extract and normalise the region content Compute a local descriptor from the normalised region Match local descriptors between the images Performance of matching methods depend on</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Feature-based-vs-direct-SLAM-workflow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Feature-based-vs-direct-SLAM-workflow/</guid><description>Feature-based vs direct SLAM workflow Parent: [[SLAM Index.md|SLAM Index]]
Source: [[Cometlabs What You Need to Know About SLAM.md|Cometlabs What You Need to Know About SLAM]]
Feature-based (aka [[sparse.md|sparse]]) Direct (aka [[dense.md|dense]]) Extraction of features required No abstraction necessary Aims to minimise error between point location estimate (from odometry) and location based on camera Tracks objects by minimising photometric error (intensity differences) !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Filter-localisation-methods/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Filter-localisation-methods/</guid><description>Filter localisation methods Parent: [[SLAM Index.md|SLAM Index]] Backlinks: [[Back-end optimisation.md|Back-end optimisation]], [[What is SLAM_.md|What is SLAM?]], [[Filter-based vs optimisation-based SLAM.md|Filter-based vs optimisation-based SLAM]]
Source: [[Wu 2018 Image-based camera localization_ an overview.md|Wu 2018 Image-based camera localization: an overview]] EKF to propagate and update motion states of visual-inertial sensors
Source: [[Scaradozzi 2018 SLAM application in surgery.md|Scaradozzi 2018 SLAM application in surgery]] Filtering techniques in SLAM
Augment/refine the position estimates and map estimates by incorporating new measurements when they become available Generally online, due to their incremental nature Types Kalman filters Particle filters Created at 2020-08-22.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Filter-based-vs-optimisation-based-SLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Filter-based-vs-optimisation-based-SLAM/</guid><description>Filter-based vs optimisation-based SLAM Parent: [[SLAM Index.md|SLAM Index]]
Source: [[Scaradozzi 2018 SLAM application in surgery.md|Scaradozzi 2018 SLAM application in surgery]]
Main paradigms of SLAM
Filters — Kalman filters , Particle filters Graph-based SLAM Estimate the entire trajectory and the map from the full set of measurements (full SLAM) Which SLAM algorithm to use? Depends on application
map resolution update time (real time or not) type of environment type of sensors available Created at 2020-08-23.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Force-classes-in-SOFA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Force-classes-in-SOFA/</guid><description>Force classes in SOFA Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Components of the internal model.md|Components of the internal model]]
More than 30 classes available in SOFA
FEM
for deformable volumes/surfaces
volume: tetrahedron/hexahedron surface: shell/membrane TetrahedralCorotationalFEMForceField: forces based on FEM
corotational/hyperelastic formulations
wire/tubular objects
Springs
SpringForceField: forces generated by the surface (alternative: TriangleFEMFroceField) ConstantForceField: external forces</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Forward-kinematics-IMU-to-camera/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Forward-kinematics-IMU-to-camera/</guid><description>Forward kinematics IMU to camera Backlinks: [[Discussion 2021-05-21.md|Discussion 2021-05-21]], [[Discussion 2021-06-01.md|Discussion 2021-06-01]], [[Update 2021-06-11.md|Update 2021-06-11]]
![[./_resources/Forward_kinematics_IMU_to_camera.resources/unknown_filename.jpeg]] Simplified model (rigid) ![[./_resources/Forward_kinematics_IMU_to_camera.resources/unknown_filename.1.png]]![[./_resources/Forward_kinematics_IMU_to_camera.resources/unknown_filename.3.png]]![[./_resources/Forward_kinematics_IMU_to_camera.resources/unknown_filename.2.png]]
Created at 2021-06-07. Last updated at 2021-06-16.
Tagged: #-sa/processed #math/kinematics #discussion/2021/2021-06</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Frequentist-vs-Bayesian-statistics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Frequentist-vs-Bayesian-statistics/</guid><description>Frequentist vs Bayesian statistics Parent: [[Discrete Bayesian filter.md|Discrete Bayesian filter]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Frequentist vs Bayesian statistics
Probability of flipping a fair coin infinitely many times is 50% - frequentist Probability of flipping a fair coin one more time (which way do I believe it landed?), single event - Bayesian Bayesian statistics takes past information (prior) into account If finding the prior is tricky, frequentist techniques are sometimes used e.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/g-h-filter-or-%CE%B1-%CE%B2-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/g-h-filter-or-%CE%B1-%CE%B2-filter/</guid><description>g-h filter or α-β filter Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
A filter that uses two scaling factors:
g or \alpha for the measurement h or \beta for the rate of change of measurement [[GH filter algorithm.md|GH filter algorithm]]
[[Calculating the estimated state in the GH-filter.md|Calculating the estimated state in the GH-filter]] [[Improving the gh-filter by using h.md|Improving the gh-filter by using h]]</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Gain-g-of-the-gh-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Gain-g-of-the-gh-filter/</guid><description>Gain g of the gh-filter Parent: [[Calculating the estimated state based on measurements and predictions.md|Calculating the estimated state based on measurements and predictions]] Backlinks: [[GH filter algorithm.md|GH filter algorithm]]
Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Which one do we trust more, the meaasurement z or the prediction x? Applying corresponding weights to both, we obtain the estimate x_est
![[./_resources/Gain_g_of_the_gh-filter.resources/unknown_filename.png]]
The prediction is nothing other than a propagated state estimate.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Gauss-Newton-Method-on-Manifold/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Gauss-Newton-Method-on-Manifold/</guid><description>Gauss-Newton Method on Manifold Source: [[Forster 2017 IMU Preintegration.md|Forster 2017 IMU Preintegration]]
Standard approach for optimization on manifold
define a retraction to reparametrise the problem (lifting) retraction ![[./_resources/Gauss-Newton_Method_on_Manifold.resources/unknown_filename.4.png]] bijective map map between an element of the tangent space at x and a neighbourhood of x on the manifold i.e. we work in the tangent space (locally like a Euclidian space) and apply standard optimisation techniques for Gauss-Newton specifically: [ts] squared cost around current estimate [ts] solve the quadratic approximation &amp;ndash;&amp;gt; we get vector !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Gaussian-distribution.1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Gaussian-distribution.1/</guid><description>Gaussian distribution Backlinks: [[Limitations of the discrete Bayes filter.md|Limitations of the discrete Bayes filter]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
a.k.a. Normal distribution Unimodal, continuous probability distribution function (pdf)
The probability of a range of measurements is the area under the graph of the probability distribution between the end values of the range &amp;ndash; cumulative distribution function (cdf)
Background statistics [[Variance, standard deviation, covariances.md|Variance, standard deviation, covariances]] [[Central Limit Theorem.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/General-EKF-implementation-non-SLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/General-EKF-implementation-non-SLAM/</guid><description>General EKF implementation (non-SLAM) Parent: [[Extended Kalman Filter.md|Extended Kalman Filter]]
Source: [[SLAM for Dummies.md|SLAM for Dummies]] General (non-SLAM) implementation of EKF:
only state estimation robot is given a perfect map no map update necessary SLAM implementations of EKF requires map update and therefore the matrices are changed.
Source: [[Scaradozzi 2018 SLAM application in surgery.md|Scaradozzi 2018 SLAM application in surgery]] EKF vs KF circumvents linearity assumption uses nonlinear functions to describe the next state probability measurement probability approximates the state distribution with a Gaussian Random Variable Created at 2020-08-22.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/General-Kalman-Filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/General-Kalman-Filter/</guid><description>General Kalman Filter Backlinks: [[Main paradigms of SLAM.md|Main paradigms of SLAM]] Source: [[Scaradozzi 2018 SLAM application in surgery.md|Scaradozzi 2018 SLAM application in surgery]]
KF original algorithm assumes linearity (rarely ever the case)
Variations of the Kalman filter: Extended Kalman Filter (EKF) Unscented Kalman Filter (UKF) Information filtering (IF) — dual to KF Combination of EKF and IF: CF-SLAM, with the goal to be more efficient w.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Geometric-metric-SLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Geometric-metric-SLAM/</guid><description>Geometric metric SLAM Source: [[Wu 2018 Image-based camera localization_ an overview.md|Wu 2018 Image-based camera localization: an overview]] Parent: [[Classification of image-based camera localization approaches.md|Classification of image-based camera localization approaches]]
Computes 3D maps with accurate mathematical equations
Classification according to sensors
monocular multiocular (most studies focus on binocular vision) [[multisensor fusion.md|multisensor fusion]] (e.g. vision and IMU &amp;ndash; vision and IMU fusion gaining in popularity) Classification according to techniques used</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/GH-filter-algorithm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/GH-filter-algorithm/</guid><description>GH filter algorithm Parent: [[g-h filter or α-β filter.md|g-h filter or α-β filter]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Initialisation
Initialise the state of the filter Initialise our belief in the state Prediction
Use system model to propagate the state to the next time step Adjust our belief to account for uncertainty in the prediction Update
Get a measurement and an associated belief about its accuracy Calculate residual = measurement - estimated state Using a certain gain , our updated state estimate is somewhere on the residual line Created at 2020-08-27.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Gibbs-_-Rodrigues-parameter-representation-for-rotations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Gibbs-_-Rodrigues-parameter-representation-for-rotations/</guid><description>Gibbs / Rodrigues parameter representation for rotations Parent: [[Orientation parametrisations.md|Orientation parametrisations]] Backlinks: [[50.5.1 Observation of the error state (filter correction).md|50.5.1 Observation of the error state (filter correction)]], [[50.5.3 ESKF reset.md|50.5.3 ESKF reset]] See also: [[Rotation error representation.md|Rotation error representation]]
Source: [[Markley Fundamentals of Spacecraft Attitude Determination.md|Markley Fundamentals of Spacecraft Attitude Determination]]
From [[unit quaternions.md|unit quaternions]]: ![[./_resources/Gibbs___Rodrigues_parameter_representation_for_rotations.resources/Image.png]]
From [[Euler axis_angle.md|Euler axis/angle]]: ![[./_resources/Gibbs___Rodrigues_parameter_representation_for_rotations.resources/unknown_filename.2.png]]
To [[unit quaternions.md|unit quaternions]]:  ![[./_resources/Gibbs___Rodrigues_parameter_representation_for_rotations.resources/unknown_filename.1.png]]
![[./_resources/Gibbs___Rodrigues_parameter_representation_for_rotations.resources/unknown_filename.png]]
Plane of the figure contains identity quaternion, origin The circle is a cross section of the quaternion sphere S^3 The upper horizontal axis is the 3D Gibbs vector hyperplane (tangent at the identity quaternion) [+] q and -q map to the same Gibbs vector, therefore there is a 1:1 mapping of rotations between quaternions and the Gibbs parameter [-] the Gibbs vector is infinite for 180 degree rotations (q.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Grisetti-2011-Tutorial-graph-based-SLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Grisetti-2011-Tutorial-graph-based-SLAM/</guid><description>Grisetti 2011 - Tutorial graph-based SLAM temp
Backlinks: [[What is SLAM_.md|What is SLAM?]]
Abstract
formulate SLAM using a graph nodes: poses of the robot (as well as landmark postiions) at different points in time edges: constraints between poses come from sensor measurements/observations robot movement/control input constraints can contradict each other, due to effect of noise in sensor readings solve the graph, i.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/GRK-2543_-Intraoperative-Multi-sensor-Tissue-Differentiation-in-Oncology/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/GRK-2543_-Intraoperative-Multi-sensor-Tissue-Differentiation-in-Oncology/</guid><description>GRK 2543: Intraoperative Multi-sensor Tissue Differentiation in Oncology Sources:
Deutsche Forschungsgemeinschaft project page ISYS project page Background:  Cooperation between Uni Tübingen and Uni Stuttgart Gynelogical and urological application scenarios Aims:
Minimise invasiveness and duration of surgical [[cancer treatment.md|cancer treatment]], while at the same time maximising effectiveness of the treatment Minimise damage to surrounding tissue during tumour [[resection.md|resection]] Aid decision-making during surgery (intraoperative) Reliable differentiation between cancerous tissue and the surrounding healthy tissue Decide whether to preserve tissue or continue with [[surgery.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Handling-the-computational-complexity-of-optimisation-based-SLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Handling-the-computational-complexity-of-optimisation-based-SLAM/</guid><description>Handling the computational complexity of optimisation-based SLAM Parent: [[SLAM Index.md|SLAM Index]] Source: [[Forster 2017 IMU Preintegration.md|Forster 2017 IMU Preintegration]]
Complexity of nonlinear batch optimisation
The trajectory and the map, which comprise the states, grow with time The larger the SLAM problem, the less feasible it is to perform the optimisation in real-time Solutions to improve computational efficiency
Keyframe-based methods: discard frames except for a few selected keyframes Run the optimisation parallelly (e.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Haptic-rendering/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Haptic-rendering/</guid><description>Haptic rendering Source: [[SOFA extended documentation.md|SOFA extended documentation]]
The main interest of interactive simulation is that
the user can modify the course of the computations in real-time when a virtual medical instrument comes into contact with some models of a soft-tissue, instantaneous deformations must be computed This visual feedback of the contact can be enhanced by haptic rendering so that the surgeon can really feel the contact.&amp;quot; Two main issues in SOFA for providing haptics</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Hidden-variables-in-a-multivariate-Kalman-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Hidden-variables-in-a-multivariate-Kalman-filter/</guid><description>Hidden variables in a multivariate Kalman filter Parent: [[Multivariate Kalman filters.md|Multivariate Kalman filters]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Example: ![[./_resources/Hidden_variables_in_a_multivariate_Kalman_filter.resources/unknown_filename.1.png]] Blue error ellipse:
Certainty in position x=0 No idea about the velocity (long in y-axis) We know that position and velocity are correlated, i.e. the next position depends on the current velocity value (red error ellipse — likelihood/prediction for the next step) e.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/https___www.sofa-framework.org_applications_gallery_percutaneous-liver-surgery_/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/https___www.sofa-framework.org_applications_gallery_percutaneous-liver-surgery_/</guid><description>https://www.sofa-framework.org/applications/gallery/percutaneous-liver-surgery/ https://www.sofa-framework.org/applications/gallery/percutaneous-liver-surgery/ Constraint-based haptic rendering ![[./resources/https___www.sofa-framework.org_applications_gallery_percutaneous-liver-surgery.resources/unknown_filename.png]]
Created at 2020-07-23. Last updated at 2020-08-22.
Tagged: #misc/haptic-feedback #-resources/videos #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Identity-quaternion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Identity-quaternion/</guid><description>Identity quaternion Parent: [[Quaternion index.md|Quaternion index]] Source: [[Solà 2017 Quaternion kinematics for ESKF.md|Solà 2017 Quaternion kinematics for ESKF]]
![[./_resources/Identity_quaternion.resources/unknown_filename.png]]
![[./_resources/Identity_quaternion.resources/unknown_filename.1.png]]
Created at 2021-05-14. Last updated at 2021-05-14.
Tagged: #-sa/processed #math/quaternions</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Importing-a-fork-in-Python-instead-of-installed-package/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Importing-a-fork-in-Python-instead-of-installed-package/</guid><description>Importing a fork in Python instead of installed package https://stackoverflow.com/questions/23075397/python-how-to-edit-an-installed-package
Run this in repo that uses the fork (this installs the package as a submodule): python3 -m pip install -e git+[[ssh://git@github-feudalism/feudalism/spatialmath-python.git#egg=f-spatialmath | ssh://git@github-feudalism/feudalism/spatialmath-python.git#egg=f-spatialmath]] &amp;ndash;upgrade
![[./_resources/Importing_a_fork_in_Python_instead_of_installed_package.resources/unknown_filename.png]]
Instructions:
Fork the package repo cd to own repo where you want to use the package Install the fork using the above pip install command. This creates ./src/submodule When making changes to fork: make changes in either the submodule folder (for immediate effect), or in the fork subdirectory + push + reinstall Created at 2021-07-23.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Improving-the-gh-filter-by-using-h/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Improving-the-gh-filter-by-using-h/</guid><description>Improving the gh-filter by using h Parent: [[g-h filter or α-β filter.md|g-h filter or α-β filter]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
[[Implementing the g value without h.md|Implementing the g value without h]]
We improve the estimation, previously by only predicting the state, by now predicting the rate of change of state. i.e. Also predict the weight gain per day instead of setting it at a constant value.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/IMU-data-generation-from-camera_visual-data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/IMU-data-generation-from-camera_visual-data/</guid><description>IMU data generation from camera/visual data Parent: [[IMU index.md|IMU index]] Source: [[MKok 2017 Using inertial sensors for position and orientation estimation.md|MKok 2017 Using inertial sensors for position and orientation estimation]]
![[./_resources/IMU_data_generation_from_camera_visual_data.resources/IMG_4418.HEIC]]
Created at 2021-05-13. Last updated at 2021-05-13.
Tagged: #-sa/processed #sensors-for-SLAM/IMU #math/kinematics</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/IMU-index/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/IMU-index/</guid><description>IMU index Parent: [[SLAM Index.md|SLAM Index]] Backlinks: [[SA TODO.md|SA TODO]]
General [[IMU.md|IMU]] [[Odometry.md|Odometry]] [[Why use the visual-inertial sensor combination_.md|Why use the visual-inertial sensor combination?]] [[IMU to camera coordinate transformations.md|IMU to camera coordinate transformations]]
Practical [[Converting IMU data to inertial frame.md|Converting IMU data to inertial frame]]
Modelling [[Probabilistic models for IMU.md|Probabilistic models for IMU]] [[Choice of model for the IMU motion model.md|Choice of model for the IMU motion model]] [[Choice of states for the IMU motion_kinematics model.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/IMU-kinematic-model-using-Euler-integration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/IMU-kinematic-model-using-Euler-integration/</guid><description>IMU kinematic model using Euler integration Parent: [[IMU index.md|IMU index]] Source: [[Forster 2017 IMU Preintegration.md|Forster 2017 IMU Preintegration]] Backlinks: [[IMU preintegration on manifold.md|IMU preintegration on manifold]], [[IMU measurement model.md|IMU measurement model]]
Kinematic model ![[./_resources/IMU_kinematic_model_using_Euler_integration.resources/unknown_filename.png]]
Using Euler integration assuming acc and angVel are constant in the time interval: ![[./_resources/IMU_kinematic_model_using_Euler_integration.resources/unknown_filename.1.png]]
Using the measurement equations: ![[./_resources/IMU_kinematic_model_using_Euler_integration.resources/unknown_filename.2.png]]
Created at 2020-11-27. Last updated at 2021-05-18.
Tagged: #-sa/processed #sensors-for-SLAM/IMU #sensors-for-SLAM/IMU/preintegration</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/IMU-motion-model-discrete/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/IMU-motion-model-discrete/</guid><description>IMU motion model (discrete) Parent: [[IMU index.md|IMU index]], [[Probabilistic models for IMU.md|Probabilistic models for IMU]] Source: [[MKok 2017 Using inertial sensors for position and orientation estimation.md|MKok 2017 Using inertial sensors for position and orientation estimation]]
Position dynamics ![[./resources/IMU_motion_model(discrete).resources/unknown_filename.png]]
Orientation dynamics (either quaternion or rotation matrix representation) ![[./resources/IMU_motion_model(discrete).resources/unknown_filename.1.png]] with ![[./resources/IMU_motion_model(discrete).resources/unknown_filename.2.png]]
Created at 2021-03-23. Last updated at 2021-05-18.
Tagged: #sensors-for-SLAM/IMU #discussion/2021/2021-04 #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/IMU-preintegration-on-manifold/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/IMU-preintegration-on-manifold/</guid><description>IMU preintegration on manifold Parent: [[IMU index.md|IMU index]] Source: [[Forster 2017 IMU Preintegration.md|Forster 2017 IMU Preintegration]] Backlinks: [[IMU model.md|IMU model]]
Preintegration on manifold
Summarising all measurements between the keyframes i and j into a single measurement This preintegrated IMU measurement constrains the motion between two consecutive keyframes Assume IMU is synchronised with the camera ![[./_resources/IMU_preintegration_on_manifold.resources/unknown_filename.11.png]]
The above equations already provide the summarised IMU measurements, however, the integration has to be repeated whenever the linearisation point at t=t_i changes i.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/IMU-states-dynamics-equations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/IMU-states-dynamics-equations/</guid><description>IMU states, dynamics equations Parent: [[IMU index.md|IMU index]] Source: [[Mur-Artal 2017 VI-ORB.md|Mur-Artal 2017 VI-ORB]]
Evolution of IMU states (world frame to IMU: orientation R, position p, velocity v) between consecutive keyframes
![[./_resources/IMU_states,_dynamics_equations.resources/unknown_filename.1.png]]
Evolution of IMU states (world frame to IMU: orientation R, position p, velocity v) between consecutive frames
Using the [[preintegration.md|preintegration]] terms ![[./_resources/IMU_states,_dynamics_equations.resources/unknown_filename.png]] ![[./_resources/IMU_states,_dynamics_equations.resources/unknown_filename.2.png]]
Preintegration (delta) terms and the Jacobians can be computed iteratively as IMU measurements arrive (s.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/IMU-to-camera-coordinate-transformations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/IMU-to-camera-coordinate-transformations/</guid><description>IMU to camera coordinate transformations Parent: [[IMU index.md|IMU index]] Source: Weiss 2011
![[./_resources/IMU_to_camera_coordinate_transformations.resources/IMG_4417.HEIC]]
Created at 2021-05-13. Last updated at 2021-05-13.
Tagged: #sensors-for-SLAM/IMU #math/kinematics #discussion/2021/2021-05 #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/IMU.11/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/IMU.11/</guid><description>IMU Parent: [[IMU index.md|IMU index]]
Source: [[Mur-Artal 2017 VI-ORB.md|Mur-Artal 2017 VI-ORB]]
measures acceleration (from accelerometer) and angular velocity (from gyrometer) of sensor at regular intervals measurements are affected by sensor noise accelerometer bias gyrometer bias accelerometer is further affected by gravity &amp;ndash;&amp;gt; need to subtract effect of gravity Created at 2021-04-23. Last updated at 2021-04-23.
Tagged: #-definitions #-sa/processed #sensors-for-SLAM/IMU</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/In-vivo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/In-vivo/</guid><description>In vivo Latin for &amp;ldquo;within the living&amp;rdquo; studies in which the effects of various biological entities are tested on whole, living organisms or cells, as opposed to a tissue extract/dead organism Created at 2020-08-24. Last updated at 2020-08-24. Source URL: .
Tagged: #-definitions #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Information-Filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Information-Filter/</guid><description>Information Filter Parent: [[General Kalman Filter.md|General Kalman Filter]]
Source: [[Scaradozzi 2018 SLAM application in surgery.md|Scaradozzi 2018 SLAM application in surgery]]
also same assumptions as the EKF main difference: how the Gaussian belief is represented est. cov. — replaced by information matrix (IM) est. state — replaced by information vector (IV) superior to KF in the following ways data is filtered by summing up the IMs and IVs often numerically more stable Dual character of KF and IF</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Initialisation-of-monocular-SLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Initialisation-of-monocular-SLAM/</guid><description>Initialisation of monocular SLAM Source: [[Lamarca 2019 DefSLAM.md|Lamarca 2019 DefSLAM]]
Depth information has to be generated before localisation can be performed — how?
Capture multiple images which have enough [[parallax.md|parallax]] These images with parallax allows depth information to be calculated (this uses [[motion parallax.md|motion parallax]]) From these images, the map can be generated Localisation can then be carried out with respect to the map (as long as camera doesn&amp;rsquo;t move off to an unexplored region) Created at 2020-11-20.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Initialising-graph-in-SP3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Initialising-graph-in-SP3/</guid><description>Initialising graph in SP3 Parent: [[SofaPython Index.md|SofaPython Index]]
https://github.com/SofaDefrost/plugin.SofaPython3.deprecated/pull/110
&amp;ldquo;Can you share an example of a scene and a component you have in mind ? Because currently to summary the discussion during sofa-meeting the problem with init/bdwInit/reinit is that it that this is severely ill defined and we are considering to totally remove that from Sofa and use alternatives pattern among which:
have an onSimulationStart / onSimulationStop event to detect when the simulation is on or not</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Install-ROSConnector-in-SOFA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Install-ROSConnector-in-SOFA/</guid><description>Install ROSConnector in SOFA Parent: [[SofaPython Index.md|SofaPython Index]]
https://github.com/sofa-framework/SofaROSConnector Documentation (outdated for current SOFA version 20.06.00)
https://www.sofa-framework.org/community/forum/topic/error-configuring-cmake-sofarosconnector/ Pending answer. Last reply 10th July 2020.
Alternative using SoftRobots: https://www.sofa-framework.org/community/forum/topic/error-with-plugins-with-sofarosconnector/#post-15665 https://project.inria.fr/softrobot/
Created at 2020-07-17. Last updated at 2020-08-22.
Tagged: #software/SOFA/sofa-plugins #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Internal-model-as-a-scene-graph-in-SOFA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Internal-model-as-a-scene-graph-in-SOFA/</guid><description>Internal model as a scene graph in SOFA Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Internal model.md|Internal model]]
Scene graph of the internal model
Consists of components which are connected to a common scenegraph node (root of the internal model)
Each component is responsible for a set of tasks Examples: solver, mass, constraints, &amp;hellip;
Each component can query its parent node to get access to the its sibling components such as MechanicalState , topology</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Internal-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Internal-model/</guid><description>Internal model Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Models in SOFA.md|Models in SOFA]]
For the internal deformable mechanics
Contains the independent DOFs, mass and physical laws Mechanical behaviour modelled e.g. by FEM Geometry of this model is optimised for the computation of internal forces usually by using a reduced number of well-shaped tetrahedra this increases speed and stability however not accurate enough for collision detection nor is it smooth enough for visuals !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Intrinsic-vs-extrinsic-rotations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Intrinsic-vs-extrinsic-rotations/</guid><description>Intrinsic vs extrinsic rotations Parent: [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]] See also: [[Active_passive or Alibi_alias rotation transformations.md|Active/passive or Alibi/alias rotation transformations]]
Source: https://rock-learning.github.io/pytransform3d/transformation_ambiguities.html We want to rotate first by R1, then by R2.
In global coordnates, extrinsic rotation: ![[./_resources/Intrinsic_vs_extrinsic_rotations.resources/unknown_filename.png]] ![[./_resources/Intrinsic_vs_extrinsic_rotations.resources/unknown_filename.2.png]]
In local coordinates, intrinsic rotation: ![[./_resources/Intrinsic_vs_extrinsic_rotations.resources/unknown_filename.1.png]] (R1 defines new coordinates in which R2 is applied) ![[./_resources/Intrinsic_vs_extrinsic_rotations.resources/unknown_filename.3.png]]
Specifying the convention is relevant when dealing with Euler angles!</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Intro-to-bladder-cancer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Intro-to-bladder-cancer/</guid><description>Intro to bladder cancer https://www.cancer.net/cancer-types/bladder-cancer/introduction
Created at 2020-12-13. Last updated at 2020-12-13.
Tagged: #-resources #medical/cancer #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Introduction-to-theKalman-Filter-Resourcium/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Introduction-to-theKalman-Filter-Resourcium/</guid><description>https://resourcium.org/journey/introduction-kalman-filter Tags: #-resources #filters/kalman-filter #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Inverse-of-a-homogeneous-transformation-matrix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Inverse-of-a-homogeneous-transformation-matrix/</guid><description>Inverse of a homogeneous transformation matrix Parent: [[Kinematics primer.md|Kinematics primer]] Source: https://mathematica.stackexchange.com/questions/106257/how-do-i-get-the-inverse-of-a-homogeneous-transformation-matrix
![[./_resources/Inverse_of_a_homogeneous_transformation_matrix.resources/unknown_filename.png]]
Created at 2021-06-01. Last updated at 2021-06-01.
Tagged: #-sa/processed #math/kinematics</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Inverse-quaternion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Inverse-quaternion/</guid><description>Inverse quaternion Parent: [[Quaternion index.md|Quaternion index]] Source: [[Solà 2017 Quaternion kinematics for ESKF.md|Solà 2017 Quaternion kinematics for ESKF]]
![[./_resources/Inverse_quaternion.resources/unknown_filename.1.png]] The inverse is the conjugate in case of [[unit quaternions.md|unit quaternions]]
![[./_resources/Inverse_quaternion.resources/unknown_filename.png]]
Created at 2021-05-14. Last updated at 2021-05-14.
Tagged: #-sa/processed #math/quaternions</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Kalman-gain-for-EKF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Kalman-gain-for-EKF/</guid><description>Kalman gain for EKF Source: [[SLAM for Dummies.md|SLAM for Dummies]] Backlinks: [[EKF matrices.md|EKF matrices]]
How much we will trust the observed landmarks
compromise between odometry and landmark correction uses uncertainty of observed landmarks measure of quality of the range measurement device odometry performance Gains for range and brearing (3+2n x 2)
![[./_resources/Kalman_gain_for_EKF.resources/unknown_filename.png]]
Created at 2020-08-06. Last updated at 2020-08-31.
Tagged: #filters/EKF #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Kalman-gain-using-Gaussians/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Kalman-gain-using-Gaussians/</guid><description>Kalman gain using Gaussians Parent: [[1D Kalman filters.md|1D Kalman filters]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Kalman gain in the update step ![[./_resources/Kalman_gain_using_Gaussians.resources/unknown_filename.1.png]]
Basically a scaling term that chooses a value between the sensor distr. mean and the posterior distr. mean Gives greater weight to the term with lower variance (we trust this data more!) Mean and variance in terms of the Kalman gain !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Kalman-vs.-nonlinear-systems/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Kalman-vs.-nonlinear-systems/</guid><description>Kalman vs. nonlinear systems Parent: [[Factors affecting Kalman filter performance.md|Factors affecting Kalman filter performance]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Kalman filter equations are linear ![[./_resources/Kalman_vs._nonlinear_systems.resources/unknown_filename.png]]
Example: approximating a sine-wave signal ![[./_resources/Kalman_vs._nonlinear_systems.resources/unknown_filename.1.png]]
Explanation:
Back to the basic g-h filter structure: the filter output chooses a value on the residual line The process model in the underlying filter assumes constant velocity (0 acceleration), whereas in the sine example above, the signal is always accelerating Created at 2020-08-31.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Key-frames-in-loop-closure-detection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Key-frames-in-loop-closure-detection/</guid><description>Key frames in loop closure detection Source: cometlabs Backlinks: [[Loop closure detection.md|Loop closure detection]]
Most common method to get candidate key frames: use a place recognition approach
approach based on vocab tree feature descriptors of candidate key frames are quantised one colour in image below corresponds to one feature descriptor/&amp;lsquo;vocabulary&amp;rsquo; each point is a &amp;lsquo;word&amp;rsquo; that belongs to a vocabulary the words can then be counted and put into a frequency histogram the histogram is used to compare similarity of images I think similar images then get filtered out, so we get key frames !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/KF-kinematics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/KF-kinematics/</guid><description>KF kinematics Overview of KF states (true, nominal, error) ![[./_resources/KF_kinematics.resources/unknown_filename.1.png]]
Nominal state kinematics ![[./_resources/KF_kinematics.resources/unknown_filename.2.png]]
Error state kinematics ![[./_resources/KF_kinematics.resources/unknown_filename.3.png]]
Old stuff: ![[./_resources/KF_kinematics.resources/unknown_filename.jpeg]]
Created at 2021-07-09. Last updated at 2021-07-15.
Tagged: #to-do/orphan #project-management/sounding-board #-sa/processing #filters/ESKF #discussion/2021/2021-07</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Kidnapped-robot-problem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Kidnapped-robot-problem/</guid><description>Kidnapped robot problem Source: [[Wikipedia Lokalisierung.md|Wikipedia Lokalisierung]] Backlinks: [[Lamarca 2019 DefSLAM.md|Lamarca 2019 DefSLAM]]
Position initially known Then robot is repositioned without knowing it Robot has to be able to realise that the initial successful localisation isn&amp;rsquo;t valid any more &amp;ndash; a new global localisation must be carried out Realise this via unplausible sensor measurements (huge contradiction to prev. measurements) Has to do with the measure of robustness of the localisation method carried out Created at 2020-07-30.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Kinematics-primer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Kinematics-primer/</guid><description>Kinematics primer Source: [[Hibbeler Dynamics.md|Hibbeler Dynamics]], [[Woernle Mehrkörpersysteme.md|Woernle Mehrkörpersysteme]] See also: [[Reversed kinematics relations.md|Reversed kinematics relations]], [[Denavit-Hartenberg convention.md|Denavit-Hartenberg convention]] Backlinks: [[Obtaining IMU measurements from camera by forward kinematics.md|Obtaining IMU measurements from camera by forward kinematics]], [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]]
Prereqs:
[[Chaining rotation matrices and angular velocities.md|Chaining rotation matrices and angular velocities]] [[Converting velocity from CS1 to CS0.md|Converting velocity from CS1 to CS0]] [[Poisson equation for skew symmetric matrix of angular velocity.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Kinematics_-equations-of-motion-IMU-to-camera/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Kinematics_-equations-of-motion-IMU-to-camera/</guid><description>Kinematics: equations of motion IMU to camera Backlinks: [[Discussion 2021-06-01.md|Discussion 2021-06-01]]
![[./_resources/Kinematics__equations_of_motion_IMU_to_camera.resources/unknown_filename.png]]
R_WB = R_WC * R_CB
Notes: ref https://docs.sympy.org/latest/modules/physics/vector/vectors.html for vector calculus (symbolic)
Created at 2021-06-07. Last updated at 2021-06-07.
Tagged: #-sa/processed #math/kinematics #discussion/2021/2021-06</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Landmark-extraction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Landmark-extraction/</guid><description>Landmark extraction Source: [[SLAM for Dummies.md|SLAM for Dummies]] Backlinks: [[Basic EKF for SLAM.md|Basic EKF for SLAM]], [[Nearest Neighbour.md|Nearest Neighbour]]
Basic landmark extraction using a laser scanner
[[Spike algorithm.md|Spike algorithm]] [[RANSAC.md|RANSAC]] ([[EKF.md|EKF]] handles points) Expansion of RANSAC so that EKF handles lines Scan-matching: two successive laser scans are matched Spike and RANSAC are good for indoor environments
Created at 2020-08-06. Last updated at 2020-08-22.
Tagged: #localisation/landmarks #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Laparoscopy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Laparoscopy/</guid><description>Laparoscopy Source: https://en.wikipedia.org/wiki/Laparoscopy Backlinks: [[Related types of surgery.md|Related types of surgery]], [[Some questions.md|Some questions]], [[Trocar.md|Trocar]]
minimally invasive surgery (MIS) / keyhole surgery make a small incision in the abdomen area and operate through it Created at 2020-08-08. Last updated at 2021-05-13.
Tagged: #medical/surgery #-definitions #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Laser-scanners/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Laser-scanners/</guid><description>Laser scanners Source: [[SLAM for Dummies.md|SLAM for Dummies]] Backlinks: [[Sensors (absolute measurements) for measuring distance to landmarks.md|Sensors (absolute measurements) for measuring distance to landmarks]]
Commonly used [+] Precise, efficient, not much processing work necessary [-] Expensive, bad readings with certain surfaces, bad for underwater applications Created at 2020-08-22. Last updated at 2020-08-22.
Tagged: #sensors-for-SLAM #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Lie-group-Lie-algebra/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Lie-group-Lie-algebra/</guid><description>Lie group, Lie algebra Parent: [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]] Backlinks: [[SO(3) 3D rotation group.md|SO(3) 3D rotation group]]
Source: https://www.seas.upenn.edu/~meam620/slides/kinematicsI.pdf A group that is a differentiable (smooth) [[manifold.md|manifold]] is called a Lie group
Source: https://en.wikipedia.org/wiki/3D_rotation_group Lie algebra  Every Lie group has an associated Lie algebra Lie algebra: linear space with same dimension as the Lie group Consists of all skew-symmetric 3x3 matrices Elements of the Lie algebra are elements of the tangent space of the manifold SO(3)/Lie group at the identity element Source: [[Forster 2017 IMU Preintegration.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Limitations-of-the-discrete-Bayes-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Limitations-of-the-discrete-Bayes-filter/</guid><description>Limitations of the discrete Bayes filter Parent: [[Discrete Bayesian filter.md|Discrete Bayesian filter]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Limitations of the discrete Bayes filter
Scaling Dog tracking example is one-dimensional, but in real life we often want to track more things (e.g. 2D coordinates, velocities) Multidimensional case: store probabilities in a grid 4 tracked variables: O(n^4) per time step High computational cost with high dimensionality Filter is discrete and therefore gives discrete output But a lot of applications require continuous output Discretising a solution space can lead to lots of data (depending on accuracy required) &amp;ndash;&amp;gt; calculations for lots of different probabilities!</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Linear-solvers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Linear-solvers/</guid><description>Linear solvers Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Simulation algorithms in SOFA.md|Simulation algorithms in SOFA]]
Conjugate gradient ![[./_resources/Linear_solvers.resources/unknown_filename.png]] J: first-order mapping of a node to its parent path(i): list of mappings from the independent DOFs to the node the force applies to
Computation using a visitor: ![[./_resources/Linear_solvers.resources/unknown_filename.1.png]]
Top down visitor: propagates the given displacement, clears force vector Bottom up visitor: accumulates forces, maps them up to the independent DOFs Direct solvers</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Linearisation-of-an-orientation-in-SO3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Linearisation-of-an-orientation-in-SO3/</guid><description>Linearisation of an orientation in SO(3) Parents: [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]], [[Quaternion index.md|Quaternion index]], [[Orientation parametrisations.md|Orientation parametrisations]] Source: [[MKok 2017 Using inertial sensors for position and orientation estimation.md|MKok 2017 Using inertial sensors for position and orientation estimation]]
Rotation of a vector in SO(3)
The [[SO(3).md|SO(3)]] group is a [[Lie group.md|Lie group]], so there exists an [[exponential map.md|exponential map]] from a corresponding Lie algebra to the SO(3) group a reverse [[log map.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Literature-management/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Literature-management/</guid><description>Literature management Only focus on one paper per topic at a time Skim through and take notes on only the important chapters Link and backlink Note which topics were skimmed Come back later for further literature review Created at 2020-08-04. Last updated at 2020-08-04.
Tagged: #-resources/-bibliography/meta #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Localisation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Localisation/</guid><description>Localisation Parent: [[SLAM Index.md|SLAM Index]]
Source: [[Wikipedia Lokalisierung.md|Wikipedia Lokalisierung]] The positioning of an autonomous mobile robot relative to its environment
The position of a mobile robot is seldom known exactly An unknown initial position / measurement uncertainties while moving Becomes a SLAM problem when neither the position nor the map is known Goal/Output: POSE
Due to uncertainties etc, it&amp;rsquo;s good to have a POSE representation that also shows these uncertainties e.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Logarithm-map/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Logarithm-map/</guid><description>Logarithm map Parent: [[Quaternion index.md|Quaternion index]], [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]] Backlinks: [[Linearisation of an orientation in SO(3).md|Linearisation of an orientation in SO(3)]]
Source: [[Forster 2017 IMU Preintegration.md|Forster 2017 IMU Preintegration]]
Maps a rotation matrix R in [[SO(3).md|SO(3)]] to a skew symmetric matrix ([[Lie algebra.md|Lie algebra]]) ![[./_resources/Logarithm_map.resources/unknown_filename.png]]
Perturbations, first order approximation ![[./_resources/Logarithm_map.resources/unknown_filename.1.png]]
S. Forster [2015] suppplementary material for the inverse Jacobian
Source: [[MKok 2017 Using inertial sensors for position and orientation estimation.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Loop-closing-in-VIORB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Loop-closing-in-VIORB/</guid><description>Loop closing in VIORB Parent: [[SLAM Index.md|SLAM Index]] Source: [[Mur-Artal 2017 VI-ORB.md|Mur-Artal 2017 VI-ORB]]
Overview
To reduce drift accumulated during exploration (when returning to an already mapped location) Loop detection: of large loops using place recognition Loop correction: first do lightweight pose-graph optimisation (PGO), then do full BA in a separate thread (in order not to interfere with real-time operations) Implementation
After loop detection: do match validation (alignment of points between keyframes) Then pose-graph optimisation to reduce the accumulated error in trajectory (PGO: pose-only, ignores IMU info) IMU info ignored, but velocities are corrected by rotating them according to keyframe orientation &amp;ndash;&amp;gt; suboptimal, but should be accurate enough to allow IMU data to be used right after the PGO in ORBSLAM: PGO is 7-DoF optimisation (due to scale + 3 rot + 3 xyz) in VIORB, 6 DoF (scale is known from initialisation bzw.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Loop-closure-detection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Loop-closure-detection/</guid><description>Loop closure detection Parent: [[VSLAM Framework.md|VSLAM Framework]], [[SLAM Index.md|SLAM Index]]
Source: cometlabs Backlinks: [[Step 2_ Re-observation.md|Step 2: Re-observation]]
Process of observing the same scene by non-adjacent frames and adding a constraint (relationship? association?) between them A long-term data association in the [[VSLAM Framework.md|VSLAM Framework]] (part of front end) Sort of incorporates [[topological SLAM.md|topological SLAM]] into metric SLAM ![[./_resources/Loop_closure_detection.resources/unknown_filename.png]] Importance
Final refinement step (in data association) Important for obtaining a globally consistent SLAM solution, especially when optimising over a long period of time Basic loop closure detection Match the current frame to all previous frames using feature matching</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Loose-vs-Tight-coupling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Loose-vs-Tight-coupling/</guid><description>Loose vs Tight coupling Parent: [[SLAM Index.md|SLAM Index]] Backlinks: [[Multisensor fusion.md|Multisensor fusion]] Source: [[Wu 2018 Image-based camera localization_ an overview.md|Wu 2018 Image-based camera localization: an overview]]
In loosely-coupled systems: all sensor states are independently estimated and optimized
easier to process frame and IMU data less accurate/robust compared to tight coupling e.g. Integrated IMU data can be incorporated as independent measurements in stereo vision optimization e.g. Vision-only pose estimates are used to update an EKF so that IMU propagation can be performed In tightly-coupled systems: all sensor states are jointly estimated and optimized</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Manifolds/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Manifolds/</guid><description>Manifolds Source: https://www.euclideanspace.com/maths/geometry/space/surfaces/manifold/index.htm Backlinks: [[SO(3) 3D rotation group.md|SO(3) 3D rotation group]]
Like a surface in n-dimensions (hypersurface) An n-dim manifold looks like R^n locally (locally Euclidian) Circle: 1-dim manifold. If we zoom around a point on the circle, it looks like a line (R^1) Sphere: 2-dim manifold. Zooming onto a point, it looks like a plane (R^2) Source: https://www.seas.upenn.edu/~meam620/slides/kinematicsI.pdf An n-dim manifold is a a set M which is locally homeomorphic to R^n</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/MAP-estimation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/MAP-estimation/</guid><description>MAP estimation Source: [[Forster 2017 IMU Preintegration.md|Forster 2017 IMU Preintegration]]
Factor graph: way of representing the posterior probability of the states ![[./_resources/MAP_estimation.resources/unknown_filename.2.png]] given the available measurements ![[./_resources/MAP_estimation.resources/unknown_filename.png]] and priors ![[./_resources/MAP_estimation.resources/unknown_filename.1.png]] ![[./_resources/MAP_estimation.resources/Image.png]]
![[./_resources/MAP_estimation.resources/unknown_filename.3.png]]
The terms in the equation above are called &amp;lsquo;factors&amp;rsquo;
MAP: maximum a posteriori We want to maximise the probability derived above &amp;ndash;&amp;gt; MAP estimate (aka minimum of negative log posterior) The negative log posterior can be written as a sum of squared residuals, assuming zero-mean Gaussian noise !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Mapping-in-VIORB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Mapping-in-VIORB/</guid><description>Mapping in VIORB Source: [[Mur-Artal 2017 VI-ORB.md|Mur-Artal 2017 VI-ORB]]
Mapping in VIORB Previously in ORBSLAM (only poses are optimised): ![[./_resources/Mapping_in_VIORB.resources/unknown_filename.png]]
Now in VIORB, more states to optimise: ![[./_resources/Mapping_in_VIORB.resources/unknown_filename.1.png]]
Increase in complexity
more states to optimise (v, b) IMU measurements creates constraints between keyframes Original ORBSLAM discards redundants KFs (poses a problem with IMU constraints!) Workaround: in local BA, only allow discarding of KF if, after discarding, the time between two consecutive KFs is short enough (&amp;lt;= 0.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Mapping-representations-in-robotics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Mapping-representations-in-robotics/</guid><description>Mapping representations in robotics Source: [[Cometlabs What You Need to Know About SLAM.md|Cometlabs What You Need to Know About SLAM]]
Feature maps Occupancy grids Grids containing occupancy probability information Useful for path planning, exploration Drawback: computational complexity Created at 2020-07-30. Last updated at 2020-08-23.
Tagged: #localisation/mapping-in-robotics #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Mapping-step-by-step-in-DefSLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Mapping-step-by-step-in-DefSLAM/</guid><description>Mapping step-by-step in DefSLAM Source: [[Lamarca 2019 DefSLAM.md|Lamarca 2019 DefSLAM]] Parent: [[DefSLAM framework.md|DefSLAM framework]]
![[./_resources/Mapping_step-by-step_in_DefSLAM.resources/unknown_filename.1.png]]
Steps
Recover warps between k and k* (s. [[Non-Rigid Guided Matching (b_w KFs) in DefSLAM.md|Non-Rigid Guided Matching (b/w KFs) in DefSLAM]]) with k: anchor keyframes, i.e. KFs where one of the observed map points was initialised with k*: set of best [[covisible keyframes.md|covisible keyframes]] warps: transformation between the images Ik to Ik* In DefSLAM, Schwarps (a family of warps using 2D Schwarzian equation regularisers) is used Schwarps has something to do with the infinitesimal planarity assumption of NRSfM [[[NRSfM.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Mappings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Mappings/</guid><description>Mappings Source: [[SOFA extended documentation.md|SOFA extended documentation]] Backlinks: [[Models in SOFA.md|Models in SOFA]], [[Visual model.md|Visual model]]
Enforces consistency between the many model representations of an object, by propagating information (such as positions, velocities, forces) in a top-down and bottom-up approach. ![[./_resources/Mappings.resources/unknown_filename.png]] Figure: Mappings between liver and grasper models
Master model imposes its displacements to the slave models ([[top-down mapping.md|top-down mapping]]) Slaves, depending on model type, can also pass information (e.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Markov-assumption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Markov-assumption/</guid><description>Markov assumption Source: [[MKok 2017 Using inertial sensors for position and orientation estimation.md|MKok 2017 Using inertial sensors for position and orientation estimation]] Backlinks: [[Grisetti 2011 - Tutorial graph-based SLAM.md|Grisetti 2011 - Tutorial graph-based SLAM]], [[Probabilistic models for IMU.md|Probabilistic models for IMU]]
Models with state x which have the Markov property:
all information up till time t is contained in xt enables marginalisation of state xt at time t+1 Created at 2021-03-26.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Mathematical-model-of-the-internal-model-in-SOFA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Mathematical-model-of-the-internal-model-in-SOFA/</guid><description>Mathematical model of the internal model in SOFA Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Internal model.md|Internal model]] Backlinks: [[ODE solvers.md|ODE solvers]], [[Constraint solvers.md|Constraint solvers]]
Dynamic/quasi-static system of particles (nodes) Independent DOFs: node coordinates, governed by ![[./_resources/Mathematical_model_of_the_internal_model_in_SOFA.resources/unknown_filename.png]]
f: different force functions, e.g. volume, surface and external forces) M: mass matrix P: constraints (projection matrix) each operator corresponds to a simulation component Created at 2020-08-09. Last updated at 2020-08-22.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Measurement-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Measurement-model/</guid><description>Measurement model Source: [[SLAM for Dummies.md|SLAM for Dummies]] Backlinks: [[EKF matrices_vectors.md|EKF matrices/vectors]]
Estimate of the range and bearing (from landmark) in [[Step 2_ Re-observation.md|Step 2: Re-observation]]
![[./_resources/Measurement_model.resources/unknown_filename.png]]
x, y, theta - current position estimate lambdax, y - landmark position
Jacobian H w.r.t. x, y, theta (here for regular EKF, not for extended) ![[./_resources/Measurement_model.resources/unknown_filename.1.png]]
In SLAM we need additional values for the landmarks here for landmark number two in extended EKF !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/MechanicalState/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/MechanicalState/</guid><description>MechanicalState Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parents: [[Components of the internal model.md|Components of the internal model]], [[Internal model as a scene graph in SOFA.md|Internal model as a scene graph in SOFA]] Backlinks: [[VecId.md|VecId]], [[Scene graph in SOFA.md|Scene graph in SOFA]], [[Mesh topology.md|Mesh topology]]
Contains state vectors of each mesh node Coordinates x Velocities v Net force f n nodes: n entries of the state vector Each entry has the same size of the node type (3 for 3D particles) Nodes of different types belong to different MechanicalStates the other MechanicalStates are attached to other scene graph nodes they might be connected with one another using interaction forces Created at 2020-08-09.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Mesh-geometry/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Mesh-geometry/</guid><description>Mesh geometry Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Data structure in SOFA.md|Data structure in SOFA]] See also: [[Mesh topology.md|Mesh topology]]
Mesh geometry: location of vertices in space
Meshes
k-simplices (triangles) k-cubes (quads) --&amp;gt; decomposition into k-cells
1-cell: edges 2-cells: triangles, quads 3-cells: tetrahedron, hexahedron Mesh data:
containers, similar to STL std::vector classes there are as many data structures for mesh data as [[topological elements.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Mesh-topology/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Mesh-topology/</guid><description>Mesh topology Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Data structure in SOFA.md|Data structure in SOFA]] See also: [[Mesh geometry.md|Mesh geometry]]
Mesh topology: how the vertices are connected to each other (using what element?)
Hierarrchy of mesh topology: ![[./_resources/Mesh_topology.resources/unknown_filename.png]]
Topology objects consist of four functional members which creates/modifies/gets topology arrays/geometrical information:
Container Modifier Algorithms Geometry ![[./_resources/Mesh_topology.resources/unknown_filename.1.png]]
Topological mapping:
Define a new mesh topology from an existing one, using the same DOFs e.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Modelling-noise-and-bias-for-IMU/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Modelling-noise-and-bias-for-IMU/</guid><description>Modelling noise and bias for IMU Parent: [[IMU index.md|IMU index]], [[IMU measurement model.md|IMU measurement model]] Source: [[MKok 2017 Using inertial sensors for position and orientation estimation.md|MKok 2017 Using inertial sensors for position and orientation estimation]]
Modelling the noise The noise not only represents measurement noise, but also model uncertainty.
With proper calibration, the three gyroscope axes are independent: ![[./_resources/Modelling_noise_and_bias_for_IMU.resources/unknown_filename.1.png]] Same for accelerometer — assume ![[./_resources/Modelling_noise_and_bias_for_IMU.resources/unknown_filename.3.png]] diagonal for a properly calibrated sensor</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Models-in-SOFA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Models-in-SOFA/</guid><description>Models in SOFA Source: [[SOFA extended documentation.md|SOFA extended documentation]] Backlinks: [[SOFA Introduction.md|SOFA Introduction]]
A simulation object can have several models
Each model is &amp;lsquo;predestined&amp;rsquo; for a certain task Each model is independent of the other Synchronisation of models: via a [[mapping.md|mapping]] mechanism Three typical models for a physical object
Internal mechanical model Collision model [[Visual model.md|Visual model]] ![[./_resources/Models_in_SOFA.resources/unknown_filename.png]]
One of the models acts as the master</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Modes-of-operation-of-the-scope/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Modes-of-operation-of-the-scope/</guid><description>Modes of operation of the scope ![[./_resources/Modes_of_operation_of_the_scope.resources/unknown_filename.jpeg]]
Created at 2021-06-30. Last updated at 2021-07-30.
Tagged: #to-do/orphan #-sa/processing #-master/thesis #medical/surgery/endoscope</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Modified-Denavit-Hartenberg-convention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Modified-Denavit-Hartenberg-convention/</guid><description>Modified Denavit-Hartenberg convention Source: Craig - Introduction to Robotics Backlinks: [[Kinematics primer.md|Kinematics primer]] Note: s. https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1083.6428&amp;amp;rep=rep1&amp;amp;type=pdf for comparison (Lipkin)
Link-frame attachment
Identify joint axes
For joint axes i and i+1, identify the  common perpendicular + where it meets axis i, or point of intersection and let this be the link-frame origin Let Z_i point along the i-th joint axis
Let X_i</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Modified-vs-original-DH/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Modified-vs-original-DH/</guid><description>Modified vs original DH Wikipedia
Modified DH (proximal) Original DH (distal?) a: offset in x (from old origin)alpha: twist of z around old x axisd: offset in z (to next origin)theta: rotation around current z d: offset in z (from prev origin)theta: rotation around prev zr / a: offset in x from prev originalp: twist of z around current x Created at 2021-05-25.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Monocular-cameras/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Monocular-cameras/</guid><description>Monocular cameras Source: [[Cometlabs What You Need to Know About SLAM.md|Cometlabs What You Need to Know About SLAM]] Backlinks: [[Visual sensors for localisation.md|Visual sensors for localisation]]
+ Simpler hardware implementation + Smaller and cheapter systems - need complexer algos and software because of lack of direct depth information from a 2D image How is the shape of the map generated?
Integrating measurements in the chain of frames over time Use triangulation method As well as camera motion, if camera isn&amp;rsquo;t stationary Depths of points are not observed directly (s.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Multisensor-fusion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Multisensor-fusion/</guid><description>Multisensor fusion Parent: [[SLAM Index.md|SLAM Index]], [[Geometric metric SLAM.md|Geometric metric SLAM]]
Source: [[Cometlabs What You Need to Know About SLAM.md|Cometlabs What You Need to Know About SLAM]]
Avoid limitations of using only one sensor Relative measurements: provide precise positioning information constantly At certain times absolute measurements are made to correct potential errors (correct drift) several approaches (for localisation), e.g. merge sensor feeds at the lowest level before being processed homogeneously hierarchical approaches (fuse state estimates derived independently from multiple sensors) s.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Multivariate-Gaussian-distributions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Multivariate-Gaussian-distributions/</guid><description>Multivariate Gaussian distributions Parent: [[Gaussian distribution.md|Gaussian distribution]]
Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]] See also: [[Probability distribution.md|Probability distribution]]
![[./_resources/Multivariate_Gaussian_distributions.resources/unknown_filename.1.png]]
N means for N dimensions ![[./_resources/Multivariate_Gaussian_distributions.resources/unknown_filename.png]]
Variances are now also combined with covariances (to take into account correlation between different dimensions)
Variance: how does a population vary amongst themselves? Covariance: how much do two variables change relative to each other? The correlation helps prediction!</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Multivariate-Kalman-filters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Multivariate-Kalman-filters/</guid><description>Multivariate Kalman filters Parent: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
[[Hidden variables in a multivariate Kalman filter.md|Hidden variables in a multivariate Kalman filter]]
Here:
Focus is on a subset of problems describable using Newton&amp;rsquo;s equations of motion Discretised continuous-time kinematic filters [[Multivariate Kalman filter algorithm.md|Multivariate Kalman filter algorithm]]
Designing the filter
State (x, P) Process (F, Q) Measurement (z, R) Measurement function H Control inputs (B, u) Assumptions of the Kalman filter</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Nearest-Neighbour/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Nearest-Neighbour/</guid><description>Nearest Neighbour Source: [[SLAM for Dummies.md|SLAM for Dummies]] Backlinks: [[Data association.md|Data association]]
Nearest neighbour approach
Get a new laser scan &amp;ndash;&amp;gt; ( landmark extraction ) extract all visible landmarks Associate each extracted LM to the closest LM we have seen more than N times Pass each pairs of association (extracted LM, LM in database) through a validation gate If pair passes &amp;ndash;&amp;gt; n = n + 1 (num.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Non-Rigid-Guided-Matching-b_w-KFs-in-DefSLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Non-Rigid-Guided-Matching-b_w-KFs-in-DefSLAM/</guid><description>Non-Rigid Guided Matching (b/w KFs) in DefSLAM Source: [[Lamarca 2019 DefSLAM.md|Lamarca 2019 DefSLAM]] Backlinks: [[NRSfM in DefSLAM.md|NRSfM in DefSLAM]]
Matching between keyframes (used in deformation mapping in DefSLAM)
Use an estimated warp as a reference
To increase number of matches in the covisible keyframes Process
Matches are given by deformation tracking Estimate an initial warp between k and k* (covisible keyframes) how?</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Non-rigid-monocular-techniques-in-the-literature/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Non-rigid-monocular-techniques-in-the-literature/</guid><description>Non-rigid monocular techniques in the literature Source: [[Lamarca 2019 DefSLAM.md|Lamarca 2019 DefSLAM]]
SfT methods
require: 1 monocular image 1 textured shape at rest (template) &amp;ldquo;geometry&amp;rdquo; as the deformation model different definitions of the deformation model analytic, e.g. isometric deformation-based: assumes preserved geodesic distance between surface points isometry for SfT has proven to be well-posed &amp;ndash;&amp;gt; led to stable, real-time solutions energy-based; jointly minimises {energy shape w.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/NormalSurfacePoint-Segfault/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/NormalSurfacePoint-Segfault/</guid><description>NormalSurfacePoint Segfault ![[./_resources/NormalSurfacePoint_Segfault.resources/unknown_filename.png]]
Crash around frame 65 [New Thread 0x7fff7ffff700 (LWP 3854)] [Thread 0x7fff7ffff700 (LWP 3854) exited] [New Thread 0x7fff7ffff700 (LWP 3855)] [Thread 0x7fff7ffff700 (LWP 3855) exited] [New Thread 0x7fff7ffff700 (LWP 3856)]
Thread 5 &amp;ldquo;DefSLAM&amp;rdquo; received signal SIGSEGV, Segmentation fault. [Switching to Thread 0x7fffc215d700 (LWP 3571)] defSLAM::SurfacePoint::thereisNormal (this=0x6705) at /home/user3/slam/DefSLAM/Modules/Mapping/SurfacePoint.cc:54 54 bool SurfacePoint::thereisNormal() { return NormalOn; } (gdb) bt #0 0x00007ffff78798a0 in defSLAM::SurfacePoint::thereisNormal() (this=0x6705) at /home/user3/slam/DefSLAM/Modules/Mapping/SurfacePoint.cc:54 #1 0x00007ffff7878f95 in defSLAM::Surface::setNormalSurfacePoint(unsigned long, cv::Vec&amp;lt;float, 3&amp;gt;&amp;amp;) (this=0x555565b0e030, ind=ind@entry=939, N=&amp;hellip;) at /home/user3/slam/DefSLAM/Modules/Mapping/Surface.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Notch-positions-due-to-scope-rotation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Notch-positions-due-to-scope-rotation/</guid><description>Notch positions due to scope rotation Backlinks: [[Update 2021-06-11.md|Update 2021-06-11]]
![[./_resources/Notch_positions_due_to_scope_rotation.resources/unknown_filename.jpeg]]
Created at 2021-06-11. Last updated at 2021-06-16.
Tagged: #-sa/processed #discussion/2021/2021-06 #medical/surgery/endoscope</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/note_-KF-with-different-sampling-rate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/note_-KF-with-different-sampling-rate/</guid><description>note: KF with different sampling rate Source: https://stackoverflow.com/questions/59566384/kalman-filter-with-different-sampling-rate
Approach 1: KF with variable dt Approach 2: KF with static dt
&amp;lsquo;Sub&amp;rsquo; updates? e.g.
predict() update() with sensor A skip update() for sensor B since no measurement arrived update() with sensor c repeat Generally discouraged:
If not predicting before each update, there is the risk of the filter lagging behind real world dynamics. The update step at t=k compares a measurement zk to the projected (predicted) state xk.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/note_-KF-with-missing-measurements/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/note_-KF-with-missing-measurements/</guid><description>note: KF with missing measurements Sources https://math.stackexchange.com/questions/982982/kalman-filter-with-missing-measurement-inputs http://opencv-users.1802565.n2.nabble.com/Kalman-filters-and-missing-measurements-td2886593.html
For a missing measurement:
use the last state estimate as a measurement set the covariance matrix of the measurement to essentially infinity. This would cause a Kalman filter to essentially ignore the new measurement since the ratio of the variance of the prediction to the measurement is zero. The result will be a new prediction that maintains velocity/acceleration but whose variance will grow according to the process noise.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/note_-quaternions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/note_-quaternions/</guid><description>note: quaternions Converting from quaternion to angular velocity then back to quaternion https://math.stackexchange.com/questions/2282938/converting-from-quaternion-to-angular-velocity-then-back-to-quaternion
Created at 2021-03-31. Last updated at 2021-05-14. Source URL: .
Tagged: #math/quaternions #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Notes-on-current-thesis-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Notes-on-current-thesis-version/</guid><description>Notes on current thesis version Parent: [[Thesis.md|Thesis]]
Variables table unterhalb der Gleichungen doppelt-gemoppelt? Wenn schon ein Symbolverzeichnis existiert Margin headers — not sure if I like them or not Too many links for repeated abbreviations? Must Prof. Sawodny&amp;rsquo;s name be in my cover page twice? Will write more about
Navigation subtheme of the B focus in the intro chapter ORB for deformable envs, other SLAM algos in the lit review chapter Filter-based SLAM in chapter 2 Optimisation-based SLAM in chapter 2 IMU preintegration om chapter 2 once I understand the math in the preintegration paper&amp;hellip; MAP estimation Data association (in chapter 2) OR DefSLAM-specific data association in chapter 3 ORB-SLAM3 in chapter 3 Will change</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/NRSfM-and-SfT/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/NRSfM-and-SfT/</guid><description>NRSfM and SfT Source: [[Lamarca 2019 DefSLAM.md|Lamarca 2019 DefSLAM]]
In literature, non-rigid monocular scenes are handled by NRSfM and SfT
NRSfM (non-rigid structure from motion)
batch processing of images to recover deformation computationally demanding — slower than SFT SFT (shape from template)
uses only a single image — faster than NRFfM lower computational cost must have a known 3D template (textured model) Created at 2020-11-20.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/NRSfM-in-DefSLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/NRSfM-in-DefSLAM/</guid><description>NRSfM in DefSLAM Parent: [[Mapping step-by-step in DefSLAM.md|Mapping step-by-step in DefSLAM]] Source: [[Lamarca 2019 DefSLAM.md|Lamarca 2019 DefSLAM]]
Assumptions
Isometric deformation Infinitesimal planarity [DEF]: any surface can be approximated as a plane at infinitesimal level, all the while maintaining its curvature at a global level The method used here is a local method &amp;ndash;&amp;gt; implies that it handles missing data and occlusions inherently
surface deformation is modelled locally for each point, under the above assumptions Embedding, phi_k of the scene surface</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Obtaining-IMU-measurements-from-camera-by-forward-kinematics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Obtaining-IMU-measurements-from-camera-by-forward-kinematics/</guid><description>Obtaining IMU measurements from camera by forward kinematics Parent: [[SA TODO.md|SA TODO]] Backlinks: [[Thesis restructure.md|Thesis restructure]]
Done:
[[reverse fwkin.md|reverse fwkin]] (scrapped) omega_B symbolic check links in BC and CB config — new diagrams (split up into &amp;gt;=2 bodies?) check om_B = om_C + om_CB (s. https://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters#Kinematics , [[Woernle.md|Woernle]]) save om_B to container obtain accel. (s. https://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters#Kinematics , [[Kinematics primer.md|Kinematics primer]]) where (ang. vel of body j w.r.t. body i, expressed in CS k)  Chaining velocities and accelerations: !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/ODE-solvers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/ODE-solvers/</guid><description>ODE solvers Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Simulation algorithms in SOFA.md|Simulation algorithms in SOFA]] Backlinks: [[Linear solvers.md|Linear solvers]], [[Constraint solvers.md|Constraint solvers]]
implement animation algorithms at each time step integrate and compute positions and velocities one time step ahead uses state vectors (e.g. for position or force), denoted by symbolic identificators called [[VecId.md|VecId]]s this allows the solver to be implemented completely independently of the physical model !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Odometry.1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Odometry.1/</guid><description>Odometry Parent: [[SLAM Index.md|SLAM Index]], [[IMU index.md|IMU index]] Baclinks: [[Position acquisition.md|Position acquisition]], [[SLAM hardware.md|SLAM hardware]], [[Prediction model.md|Prediction model]] See also: [[Egomotion (vs odometry).md|Egomotion (vs odometry)]]
Source: [[Wikipedia Visual odometry.md|Wikipedia Visual odometry]]
Data can be generated from actuator movements, e.g. rotary encoders that measure motor shaft rotations This data can be used to estimate changes in position over time Usually has precision problems, e.g. due to wheels slipping and sliding, bumpy surfaces The errors are integrated over time and therefore get worse Source: [[cometlabs.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/On-quaternions-and-rotation-matrices/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/On-quaternions-and-rotation-matrices/</guid><description>On quaternions and rotation matrices https://stackoverflow.com/questions/8919086/why-are-quaternions-used-for-rotations
It&amp;rsquo;s worth bearing in mind that all the properties related to rotation are not truly properties of Quaternions: they&amp;rsquo;re properties of Euler-Rodrigues Parameterisations, which is the actual 4-element structure used to describe a 3D rotation. Their relationship to Quaternions is purely due to a paper by Cayley, &amp;ldquo;On certain results related to Quaternions&amp;rdquo;, where the author observes the correlation between Quaternion multiplication and combination of Euler-Rodrigues parameterisations.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Oncology.1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Oncology.1/</guid><description>Oncology Source: https://en.wikipedia.org/wiki/Oncology Backlinks: [[GRK 2543_ Intraoperative Multi-sensor Tissue Differentiation in Oncology.md|GRK 2543: Intraoperative Multi-sensor Tissue Differentiation in Oncology]]
Prevention, diagnosis and treatment of cancer.
Created at 2020-08-09. Last updated at 2020-08-09.
Tagged: #medical/cancer #-definitions #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/OpenCV-Kalman-filter-pre_post-states/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/OpenCV-Kalman-filter-pre_post-states/</guid><description>OpenCV Kalman filter pre/post states ![[./_resources/OpenCV_Kalman_filter_pre_post_states.resources/unknown_filename.png]]
Created at 2021-04-23. Last updated at 2021-04-23.
Tagged: #-sa/processed #software/cpp</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/OptiTrack-in-SOFA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/OptiTrack-in-SOFA/</guid><description>OptiTrack in SOFA Using OptiTrackNatNet C++ implementation? https://www.sofa-framework.org/community/doc/programming-with-sofa/create-your-scene-in-cpp/ XML scenes? OptiTrack + Python otnn_client = root.addObject(&amp;lsquo;OptiTrackNatNetClient&amp;rsquo;, name=&amp;lsquo;otnnClient&amp;rsquo;) &amp;lt;Sofa.Core.Object&amp;gt; dir(otnn_client) [&amp;lsquo;bbox&amp;rsquo;, &amp;lsquo;clientName&amp;rsquo;, &amp;lsquo;componentState&amp;rsquo;, &amp;lsquo;drawOtherMarkersColor&amp;rsquo;, &amp;lsquo;drawOtherMarkersSize&amp;rsquo;, &amp;lsquo;drawTrackedMarkersColor&amp;rsquo;, &amp;lsquo;drawTrackedMarkersSize&amp;rsquo;, &amp;lsquo;listening&amp;rsquo;, &amp;lsquo;name&amp;rsquo;, &amp;lsquo;otherMarkers&amp;rsquo;, &amp;lsquo;printLog&amp;rsquo;, &amp;lsquo;scale&amp;rsquo;, &amp;lsquo;serverName&amp;rsquo;, &amp;lsquo;tags&amp;rsquo;, &amp;lsquo;trackedMarkers&amp;rsquo;] bold: not in API https://www.sofa-framework.org/api/master/plugins/OptiTrackNatNet/html/class_sofa_opti_track_nat_net_1_1_opti_track_nat_net_client.html difference between client name and server name Created at 2020-10-01. Last updated at 2020-10-05.
Tagged: #software/SOFA/SofaPython3 #software/OptiTrack #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/ORB-descriptor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/ORB-descriptor/</guid><description>ORB descriptor Source: https://medium.com/data-breach/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf Backlinks: [[Descriptors in feature detection_extraction.md|Descriptors in feature detection/extraction]]
Oriented FAST and Rotated BRIEF, developed 2011 Was developed as an alternative to SIFT and SURF, and ended up being better/faster than both Build on [[FAST keypoint detector.md|FAST keypoint detector]] BRIEF descriptor ORB using FAST, but with (partial) scale invariance
Use a multiscale image pyramid
Each level of the pyramid is the same image, but scaled at different resolutions (reduced size as you go higher up) !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/ORBSLAM2-mods/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/ORBSLAM2-mods/</guid><description>ORBSLAM2 mods Patch to work with opencv4 https://github.com/Windfisch/ORB_SLAM2 ORBSLAM2 Python bindings https://github.com/jskinn/ORB_SLAM2-PythonBindings Created at 2020-11-20. Last updated at 2020-11-25.
Tagged: #software/python #-sa/processed #SLAM/SLAM-algos/ORBSLAM</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/ORBSLAM2-unofficial-documentation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/ORBSLAM2-unofficial-documentation/</guid><description>ORBSLAM2 unofficial documentation Partially done, abandonned: https://github.com/raulmur/ORB_SLAM2/compare/master&amp;hellip;AlejandroSilvestri:master In Spanish: https://alejandrosilvestri.github.io/os1/doc/html/
Created at 2021-02-17. Last updated at 2021-02-17.
Tagged: #-resources #-sa/processed #SLAM/SLAM-algos/DefSLAM</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/ORBSLAM__Frame-constructor-monocular/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/ORBSLAM__Frame-constructor-monocular/</guid><description>ORBSLAM::Frame constructor (monocular) Source: [[Tracking__GrabImageMonocular.md|Tracking::GrabImageMonocular]]
Set scale level info from ORB extractor Extract ORB features mvKeys (vector of keypoints/features) Set N number of features Make mvpMapPoints (null, but with size N), mvbOutlier (all entries false, size N) If first frame or calibration change: ComputeImageBounds AssignFeaturesToGrid() Created at 2020-10-28. Last updated at 2020-10-28.
Tagged: #-sa/processed #SLAM/SLAM-algos/DefSLAM</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Orientation-parametrisations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Orientation-parametrisations/</guid><description>Orientation parametrisations Parent: [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]], [[Quaternion index.md|Quaternion index]], [[Probabilistic models for IMU.md|Probabilistic models for IMU]] See also: [[Rotation error representation.md|Rotation error representation]]
Source: [[MKok 2017 Using inertial sensors for position and orientation estimation.md|MKok 2017 Using inertial sensors for position and orientation estimation]]
Orientation parametrisations
Note: CCW rotation of a vector x_v to x_u corresponds to a CW rotation of the CS v to CS u.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Parallax/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Parallax/</guid><description>Parallax Source: https://en.wikipedia.org/wiki/Parallax Backlinks: [[Initialisation of monocular SLAM.md|Initialisation of monocular SLAM]]
DEF: The difference in the apparent position of an object viewed from two different positions
This difference is given by the angle between the two lines of sight Binocular vision uses parallax in the overlapping fields of vision in order to gain depth perception Distance measurement (i.e. depth from the viewer) via parallax is based on the principle of triangulation (uses trigonometry) See also [[Monocular depth perception in humans.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Particle-filters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Particle-filters/</guid><description>Particle filters Parent: [[Filter localisation methods.md|Filter localisation methods]]
Source: [[Wikipedia Lokalisierung.md|Wikipedia Lokalisierung]] Particle filter / Monte Carlo localisation / sequential Monte Carlo methods
allow solution of all three localisation problems POSE represented by a particle cloud Each particle : possible POSE The filter checks the plausibility of each particle Increases and decreases the probabilities of each particle accordingly When a lower probability threshold is exceeded, the particle is not considered any longer Source: [[Scaradozzi 2018 SLAM application in surgery.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Perceptual-aliasing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Perceptual-aliasing/</guid><description>Perceptual aliasing Source: https://en.wikipedia.org/wiki/Robotic_mapping Two different places are perceived as the same
Source: https://arxiv.org/abs/1810.11692 Modeling Perceptual Aliasing in SLAM via Discrete-Continuous Graphical Models [Lajoie 2018] (from the abstract)
Phenomenon where different places generate a similar visual footprint Leads to spurious measurements being fed into the SLAM estimator Result: incorrect localisation and map Created at 2020-08-25. Last updated at 2020-08-25. Source URL: .
Tagged: #-definitions #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Pinhole-camera-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Pinhole-camera-model/</guid><description>Pinhole camera model Parent: [[SLAM Index.md|SLAM Index]] Backlinks: [[Camera calibration.md|Camera calibration]], [[Pinhole camera projection function.md|Pinhole camera projection function]], [[Weiss Thesis Vision based navigation for micro helicopters.md|Weiss Thesis Vision based navigation for micro helicopters]] See also: [[World to camera trafo.md|World to camera trafo]]
Source: https://de.mathworks.com/help/vision/ug/camera-calibration.html Does not account for lens distortion (ideal pinhole camera doesn&amp;rsquo;t have a lens) To represent a real camera, the full camera model to be used should include (radial and tangential) lens distortion, (such as the one used in the MATLAB computer vision toolbox)</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Pinhole-camera-projection-function/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Pinhole-camera-projection-function/</guid><description>Pinhole camera projection function Backlinks: [[Pinhole camera model.md|Pinhole camera model]] See also: [[World to camera trafo.md|World to camera trafo]]
Source: [[Mur-Artal 2017 VI-ORB.md|Mur-Artal 2017 VI-ORB]] 3D points ![[./_resources/Pinhole_camera_projection_function.resources/unknown_filename.2.png]]
Projection function ![[./_resources/Pinhole_camera_projection_function.resources/unknown_filename.3.png]]
Transforms 3D points into 2D points on image plane ![[./_resources/Pinhole_camera_projection_function.resources/unknown_filename.4.png]]
Focal length: ![[./_resources/Pinhole_camera_projection_function.resources/unknown_filename.1.png]]
Principal point: ![[./_resources/Pinhole_camera_projection_function.resources/unknown_filename.6.png]]
The projection does not consider the distortion due to the lens
therefore when extracting image features, first undistort their coordinates only then match to projected points (existing features which have undergone projection from 3D to 2D) Source: [[Lamarca 2019 DefSLAM.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Pipenv-venv-in-project-folder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Pipenv-venv-in-project-folder/</guid><description>Pipenv venv in project folder PIPENV_VENV_IN_PROJECT=1
Powershell: $Env:PIPENV_VENV_IN_PROJECT=&amp;ldquo;1&amp;rdquo;
Created at 2020-09-06. Last updated at 2020-09-21. Source URL: .
Tagged: #-sa/processed #software/python/pipenv</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Poisson-equation-for-skew-symmetric-matrix-of-angular-velocity/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Poisson-equation-for-skew-symmetric-matrix-of-angular-velocity/</guid><description>Poisson equation for skew symmetric matrix of angular velocity Source: [[Woernle Mehrkörpersysteme.md|Woernle Mehrkörpersysteme]] Backlinks: [[Kinematics primer.md|Kinematics primer]], [[Converting velocity from CS1 to CS0.md|Converting velocity from CS1 to CS0]]
Skew-symmetric angular velocity: Poisson equation ![[./_resources/Poisson_equation_for_skew_symmetric_matrix_of_angular_velocity.resources/unknown_filename.png]]
Created at 2021-05-24. Last updated at 2021-05-24.
Tagged: #-sa/processed #math/rotations</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Position-acquisition-relative-vs.-absolute/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Position-acquisition-relative-vs.-absolute/</guid><description>Position acquisition (relative vs. absolute) Parent: [[SLAM Index.md|SLAM Index]] See also: [[SLAM hardware.md|SLAM hardware]]
Source: [[Cometlabs What You Need to Know About SLAM.md|Cometlabs What You Need to Know About SLAM]] ![[./resources/Position_acquisition(relative_vs._absolute).resources/unknown_filename.png]]
relative (interoceptive sensors) odometry absolute (exteroceptive sensors) can be used alongside relative measurement sensors in order to correct odometry drift s. [[major sensor types in SLAM (absolute measurements).md|major sensor types in SLAM (absolute measurements)]] incl.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Possible-plugins/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Possible-plugins/</guid><description>Possible plugins Parent: [[Scope of Studienarbeit.md|Scope of Studienarbeit]], [[SofaPython Index.md|SofaPython Index]]
Communication
ZMQCommunication someone&amp;rsquo;s own plugin Optical system
OptiTrackNatNet Mesh geometry/topology
CGALPlugin (computational geometry algorithms) Haptic
Haptics with Geomagic &amp;ndash; requires Geomagic probe, but code/intro may be useful SofaHaptics Haption Flexible - for deformations Sensable Robot arm
SoftRobots [[ROS Connector.md|ROS Connector]] too complicated Scenes
[[STLIB (Sofa Template Library).</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Precision-recall-curve/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Precision-recall-curve/</guid><description>Precision recall curve Parent: [[Loop closure detection.md|Loop closure detection]] Source: cometlabs used to better quantify the performance (balance between false positives and false negatives in [[loop closure detection.md|loop closure detection]]) highlights tradeoff between precision and recall precision (absence of false positives) but may lead to the appearance of false negatives recall (prediction power) e.g. tweaking to improve recall increases sensitivity to similarities in the image thus increases possibility of false positives !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Prediction-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Prediction-model/</guid><description>Prediction model Source: [[SLAM for Dummies.md|SLAM for Dummies]] Backlinks: [[EKF matrices_vectors.md|EKF matrices/vectors]]
Used in the [[prediction step.md|prediction step]]
How to compute an expected position of the robot given the old position and the control input (so basically based on odometry ) Control terms are deltax, deltay, deltatheta) ![[./_resources/Prediction_model.resources/unknown_filename.png]]![[./_resources/Prediction_model.resources/unknown_filename.1.png]] deltat - change in thrust q - error term
Jacobian (assuming linearised version) ![[./_resources/Prediction_model.resources/unknown_filename.2.png]] Not extended for landmarks because only used for robot position prediction</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Preintegration-of-IMU/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Preintegration-of-IMU/</guid><description>Preintegration of IMU Parent: [[IMU.md|IMU]] Backlinks: [[IMU states, dynamics equations.md|IMU states, dynamics equations]]
IMU measurements arrive at a higher frequency (frame rate) compared to camera captures (keyframe rate) IMU measurements constrain consecutive states We want to summarise these &amp;lsquo;in-between&amp;rsquo; IMU measurements into one single relative motion constraint between keyframes Created at 2020-10-20. Last updated at 2021-04-23.
Tagged: #sensors-for-SLAM/IMU #sensors-for-SLAM/IMU/preintegration #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Principle-of-never-throwing-away-information/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Principle-of-never-throwing-away-information/</guid><description>Principle of never throwing away information Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
&amp;ldquo;Two sensors are better than one, even if one is less accurate than the other.&amp;rdquo;
Example 1 Given:
A: a more accurate sensor B: a less accurate sensor Should we therefore choose A as the estimate and discard B? No, because B can improve our knowledge when combined with A.
![[./_resources/Principle_of_never_throwing_away_information.resources/ScreenClip.1.png]]</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Probabilistic-models-for-IMU/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Probabilistic-models-for-IMU/</guid><description>Probabilistic models for IMU Parent: [[IMU index.md|IMU index]] Source: [[MKok 2017 Using inertial sensors for position and orientation estimation.md|MKok 2017 Using inertial sensors for position and orientation estimation]]
Three main components to the probabilistic models
[[IMU measurement model.md|IMU measurement model]] (infer knowledge about pose from measurements) ![[./_resources/Probabilistic_models_for_IMU.resources/unknown_filename.5.png]] [[Prediction model.md|Prediction model]] (how sensor pose changes over time) Models of the initial pose (prior) Knowledge we are interested in: pose of the sensor</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Probability-distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Probability-distribution/</guid><description>Probability distribution Parent: [[Discrete Bayesian filter.md|Discrete Bayesian filter]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Probability distribution:
collection of all possible probabilities for an event the distribution lists all possible events and the probability of each sum up to 1 Prior probability distribution: probability prior to incorporating any measurements or other information
Joint probability P(x,y):
probability of both events happening the multivariate Gaussian distribution is already already a joint probability distribution Marginal probability:</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Program-outline/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Program-outline/</guid><description>Program outline Current assumptions (to take care of later!)
Probe is rigid — DOFs are either 0 or constant — switch to rotating scope later No gravity No bias/offset, no noise in IMU Note: Stuff marked with checkboxes are either to-dos or things I&amp;rsquo;m not sure that I implemented correctly
https://github.com/feudalism/dvi-ekf/tree/eskf; projects generate_data.py Data generation (is called from main.py)
Main objects
Generate camera data (from DefSLAM mono trajectory) Make RigidSimpleProbe (for now, all DOFs are 0 or constant) Make IMU object, generate first (om, acc) values from interpolated camera data ( - should generate it from stereo data instead) Variables</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Programmatic-implementations-of-MonoSLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Programmatic-implementations-of-MonoSLAM/</guid><description>Programmatic implementations of MonoSLAM Parent: [[SLAM resources.md|SLAM resources]]
Python https://github.com/agnivsen/Py-M-SLAM https://github.com/agnivsen/LibMonoSLAM
MATLAB https://perso.ensta-paris.fr/~filliat/Courses/2011_projets_C10-2/BRUNEAU_DUBRAY_MURGUET/monoSLAM_bruneau_dubray_murguet_en.html Created at 2020-08-04. Last updated at 2020-08-23.
Tagged: #-resources #SLAM/VSLAM/monoslam #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Pros-and-cons-of-Gaussian-distributions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Pros-and-cons-of-Gaussian-distributions/</guid><description>Pros and cons of Gaussian distributions Parent: [[Gaussian distribution.md|Gaussian distribution]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Big advantage of using Gaussian distributions (as opposed to discrete ones w/ histogram bins): less data, b/c a Gaussian distribution is represented fully using only two values: the mean and the variance ![[./_resources/Pros_and_cons_of_Gaussian_distributions.resources/unknown_filename.png]]
Limitations of using Gaussian distributions to model the world i.e. deviations from the central limit theorem</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Quaternion-conjugate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Quaternion-conjugate/</guid><description>Quaternion conjugate Parent: [[Quaternion index.md|Quaternion index]] Source: https://en.wikipedia.org/wiki/Quaternion
Flip signs of vector part
Source: [[Solà 2017 Quaternion kinematics for ESKF.md|Solà 2017 Quaternion kinematics for ESKF]]
Multiplying with own conjugate ![[./_resources/Quaternion_conjugate.resources/unknown_filename.png]] (scalar!)
Conjugate operation on quaternion products ![[./_resources/Quaternion_conjugate.resources/unknown_filename.1.png]]
Created at 2021-05-05. Last updated at 2021-05-14.
Tagged: #-sa/processed #math/quaternions</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Quaternion-conventions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Quaternion-conventions/</guid><description>Quaternion conventions Parent: [[Quaternion index.md|Quaternion index]] Source: [[Solà 2017 Quaternion kinematics for ESKF.md|Solà 2017 Quaternion kinematics for ESKF]] Backlinks: [[MEKF.md|MEKF]]
![[./_resources/Quaternion_conventions.resources/unknown_filename.png]]
Source: [ Wikipedia ], [[Markley.md|Markley]]
For [[quaternion multiplication.md|quaternion multiplication]]: change the order to transform between conventions ![[./_resources/Quaternion_conventions.resources/unknown_filename.1.png]]
Hamilton Shuster ![[./_resources/Quaternion_conventions.resources/Image.1.png]] ![[./_resources/Quaternion_conventions.resources/Image.png]] Transpose of the Hamiltonian version Created at 2021-05-14.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Quaternion-differentiation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Quaternion-differentiation/</guid><description>Quaternion differentiation Parent: [[Quaternion index.md|Quaternion index]] Source: J. D. Hol — Sensor fusion and calibration of inertial sensors, vision, ultra-wideband and GPS
![[./_resources/Quaternion_differentiation.resources/unknown_filename.3.png]]
![[./_resources/Quaternion_differentiation.resources/unknown_filename.png]]
![[./_resources/Quaternion_differentiation.resources/unknown_filename.2.png]]
Using the identities: ![[./_resources/Quaternion_differentiation.resources/unknown_filename.1.png]]
https://math.stackexchange.com/questions/189185/quaternion-differentiation Numerical differentiation (Euler)
https://math.stackexchange.com/questions/1896379/how-to-use-the-quaternion-derivative q(t+dt) = q(t)*dq
dq/dt = (1/2)*W*q with W = 0 + wx*i + wy*j + wz*k.
Integrating q(t) = q(t0)*exp((1/2)*W*(t-t0)) --&amp;gt; dq = exp((1/2)*W*dt).
Created at 2021-05-14. Last updated at 2021-05-14.
Tagged: #-sa/processed #math/quaternions</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Quaternion-index/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Quaternion-index/</guid><description>Quaternion index Notation [[Quaternion conventions.md|Quaternion conventions]]
Basic math [[Quaternion multiplication.md|Quaternion multiplication]] [[Identity quaternion.md|Identity quaternion]] [[Quaternion conjugate.md|Quaternion conjugate]] [[Quaternion norm.md|Quaternion norm]] [[Inverse quaternion.md|Inverse quaternion]] [[Unit quaternions.md|Unit quaternions]]
Calculus [[Quaternion differentiation.md|Quaternion differentiation]]
As rotation [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]]
[[Exponential map.md|Exponential map]] [[Logarithm map.md|Logarithm map]] [[Orientation parametrisations.md|Orientation parametrisations]]
[[Which orientation parametrisation to choose_.md|Which orientation parametrisation to choose?]] [[Linearisation of an orientation in SO(3).md|Linearisation of an orientation in SO(3)]] [[Quaternion to rotation matrix.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Quaternion-multiplication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Quaternion-multiplication/</guid><description>Quaternion multiplication Parent: [[Quaternion index.md|Quaternion index]] Source: [[Solà 2017 Quaternion kinematics for ESKF.md|Solà 2017 Quaternion kinematics for ESKF]]
Here: Hamiltonian convention, s. [[Quaternion conventions.md|Quaternion conventions]]
![[./_resources/Quaternion_multiplication.resources/unknown_filename.png]]
Non-commutative ![[./_resources/Quaternion_multiplication.resources/unknown_filename.1.png]] Associative ![[./_resources/Quaternion_multiplication.resources/unknown_filename.2.png]] Distributive![[./_resources/Quaternion_multiplication.resources/unknown_filename.3.png]] Multiplication as a matrix product ![[./_resources/Quaternion_multiplication.resources/unknown_filename.4.png]] With
the matrices ![[./_resources/Quaternion_multiplication.resources/unknown_filename.5.png]]
the skew operator (skew symmetric matrix) ![[./_resources/Quaternion_multiplication.resources/unknown_filename.6.png]] ![[./_resources/Quaternion_multiplication.resources/unknown_filename.7.png]]
s. also cross product ![[./_resources/Quaternion_multiplication.resources/unknown_filename.8.png]]
Source: [[Markley Fundamentals of Spacecraft Attitude Determination.md|Markley Fundamentals of Spacecraft Attitude Determination]]</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Quaternion-norm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Quaternion-norm/</guid><description>Quaternion norm Parent: [[Quaternion index.md|Quaternion index]] Source: [[Solà 2017 Quaternion kinematics for ESKF.md|Solà 2017 Quaternion kinematics for ESKF]]
![[./_resources/Quaternion_norm.resources/unknown_filename.png]]
With the property ![[./_resources/Quaternion_norm.resources/unknown_filename.1.png]]
Created at 2021-05-14. Last updated at 2021-05-14.
Tagged: #-sa/processed #math/quaternions</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Quaternion-to-rotation-matrix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Quaternion-to-rotation-matrix/</guid><description>Quaternion to rotation matrix Parents: [[Quaternion index.md|Quaternion index]], [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]] Source: [[Markley Fundamentals of Spacecraft Attitude Determination.md|Markley Fundamentals of Spacecraft Attitude Determination]]
![[./_resources/Quaternion_to_rotation_matrix.resources/unknown_filename.png]] s. [[Unit quaternions.md|Unit quaternions]] for a non-explanation on double cover (why the angle is halved)
![[./_resources/Quaternion_to_rotation_matrix.resources/unknown_filename.1.png]]
Created at 2021-08-17. Last updated at 2021-08-17.
Tagged: #-sa/processed #math/quaternions</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Random-variable/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Random-variable/</guid><description>Random variable Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Combination of values + associated probabilities. &amp;ldquo;Event&amp;rdquo; e.g. die toss, height of students e.g. in a fair die values = {1, 2, &amp;hellip;, 6} (range of values = sample space) probabilities = {1/6} * 6
Created at 2020-08-31. Last updated at 2020-08-31.
Tagged: #-definitions #-sa/processed #math/statistics</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/RANSAC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/RANSAC/</guid><description>RANSAC Source: [[SLAM for Dummies.md|SLAM for Dummies]] Parent: [[Landmark extraction.md|Landmark extraction]]
Random Sampling Consensus
to extract lines from a laser scan lines then used as landmarks indoors: straight lines from walls line landmarks are found by randomly taking a sample of laser readings (e.g. sample readings from 12deg to 22deg from within a range of 0 to 180deg) least squares approximation for line of best fit RANSAC then checks how many laser readings lie close to the best fit line initially, all readings are assumed to be unassociated to any lines if the num.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Registration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Registration/</guid><description>Registration Parent: [[SofaPython Index.md|SofaPython Index]]
allows a matching between deformable surfaces
finds spatial transformations to align two point sets or two meshes
done based on:
either target surfaces (ClosestPointRegistrationForceField , RegistrationContactForceField)
or target images (IntensityProfileRegistrationForceField), which requires the use of the image plugin Created at 2020-07-23. Last updated at 2020-08-22. Source URL: .
Tagged: #software/SOFA/sofa-plugins #-definitions #-sa/processed #registration</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Related-types-of-surgery/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Related-types-of-surgery/</guid><description>Related types of surgery Source: https://en.wikipedia.org/wiki/Resection_(surgery) Backlinks: [[GRK 2543_ Intraoperative Multi-sensor Tissue Differentiation in Oncology.md|GRK 2543: Intraoperative Multi-sensor Tissue Differentiation in Oncology]], [[Cryosection.md|Cryosection]]
By procedure
Resection: remove all parts or a key part of an internal organ s. also: [[resection margin.md|resection margin]] Excision: cut out only a part of an organ/tissue By degree of invasiveness
minimally-invasive surgery (-scopy) [[laparoscopy.md|laparoscopy]] Created at 2020-08-09.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Resection-margin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Resection-margin/</guid><description>Resection margin Source: https://en.wikipedia.org/wiki/Resection_margin Backlinks: [[Cryosection.md|Cryosection]], [[Related types of surgery.md|Related types of surgery]]
Margin on non-cancerous tissue around a tumour that has been removed.
Negative margin: no tumour Microscopic positive: tumour identified microscopically Macroscopic positive: tumour significantly present Created at 2020-08-09. Last updated at 2020-08-09. Source URL: .
Tagged: #medical/cancer #-definitions #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Reversed-kinematics-relations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Reversed-kinematics-relations/</guid><description>Reversed kinematics relations Source: [[Woernle Mehrkörpersysteme.md|Woernle Mehrkörpersysteme]] See also: [[Kinematics primer.md|Kinematics primer]]
![[./_resources/Reversed_kinematics_relations.resources/unknown_filename.png]]Position![[./_resources/Reversed_kinematics_relations.resources/unknown_filename.1.png]]Velocity![[./_resources/Reversed_kinematics_relations.resources/unknown_filename.2.png]] (given that omega_00 = 0)![[./_resources/Reversed_kinematics_relations.resources/unknown_filename.3.png]] (given that v_00 = 0)Acceleration![[./_resources/Reversed_kinematics_relations.resources/unknown_filename.4.png]]given ![[./_resources/Reversed_kinematics_relations.resources/unknown_filename.5.png]], ![[./_resources/Reversed_kinematics_relations.resources/unknown_filename.2.png]], ![[./_resources/Reversed_kinematics_relations.resources/unknown_filename.6.png]]![[./_resources/Reversed_kinematics_relations.resources/unknown_filename.7.png]] ![[./_resources/Reversed_kinematics_relations.resources/unknown_filename.8.png]] Created at 2021-05-24. Last updated at 2021-05-24.
Tagged: #-sa/processed #math/kinematics</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/RGB-D-cameras/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/RGB-D-cameras/</guid><description>RGB-D cameras Source: [[Cometlabs What You Need to Know About SLAM.md|Cometlabs What You Need to Know About SLAM]] Backlinks: [[Visual sensors for localisation.md|Visual sensors for localisation]], [[Stereo cameras.md|Stereo cameras]]
Provide depth information directly Employed by most of the SLAM systems Generate 3D images through structured light or time of flight technology Structured light camera projects a known pattern onto objects Perceives deformation of pattern by an infrared camera This lets depth and surface information of the objects be calculated Time of flight ToF of a light signal between camera and objects is measured &amp;ndash;&amp;gt; from this, depth is obtained Structured light sensors are sensitive to illumination &amp;ndash; not applicable in direct sunlight Limitations of RGB-D cameras</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Rigid-cystoscope-dimensions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Rigid-cystoscope-dimensions/</guid><description>Rigid cystoscope dimensions ![[./_resources/Rigid_cystoscope_dimensions.resources/unknown_filename.2.png]] ca 20cm x 65 cm = 0.02m x 0.065 m
Source: https://en.wikipedia.org/wiki/Cystoscopy
The sizes of the sheath of the rigid cystoscope are 17 French gauge (5.7 mm diameter), 19 Fr gauge (6.3 mm diameter), and 22 Fr gauge (7.3 mm diameter).
Source: https://www.karlstorz.com/cps/rde/xbcr/karlstorz_assets/ASSETS/3405020.pdf Camera ![[./_resources/Rigid_cystoscope_dimensions.resources/unknown_filename.png]]![[./_resources/Rigid_cystoscope_dimensions.resources/unknown_filename.1.png]]
Created at 2021-08-20. Last updated at 2021-08-20. Source URL: .
Tagged: #to-do/orphan #-sa/processed #medical/surgery/endoscope</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Rigid-cystoscope-mechanism/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Rigid-cystoscope-mechanism/</guid><description>Rigid cystoscope mechanism Parent: [[Update 2021-06-11.md|Update 2021-06-11]]
![[./_resources/Rigid_cystoscope_mechanism.resources/unknown_filename.png]]
Created at 2021-06-10. Last updated at 2021-06-11.
Tagged: #-sa/processed #discussion/2021/2021-06 #medical/surgery/endoscope</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Rotation-error-representation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Rotation-error-representation/</guid><description>Rotation error representation Parents: [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]], [[Quaternion index.md|Quaternion index]] Backlinks: [[50.2.2.1 Variables in ESKF using IMUs.md|50.2.2.1 Variables in ESKF using IMUs]], [[MEKF measurement update.md|MEKF measurement update]] See also: [[Orientation parametrisations.md|Orientation parametrisations]], [[Which orientation parametrisation to choose_.md|Which orientation parametrisation to choose?]]
Source: [[Markley Fundamentals of Spacecraft Attitude Determination.md|Markley Fundamentals of Spacecraft Attitude Determination]]
Note:
Only for small angle approximations! all these representations are equivalent through second order as !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Rotation-vector-representation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Rotation-vector-representation/</guid><description>Rotation vector representation Parents: [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]], [[Orientation parametrisations.md|Orientation parametrisations]] See also: [[Euler axis_angle representation.md|Euler axis/angle representation]]
Source: [[Markley Fundamentals of Spacecraft Attitude Determination.md|Markley Fundamentals of Spacecraft Attitude Determination]]
Combine the [[Euler axis_angle.md|Euler axis/angle]] into a three component rotation vector ![[./_resources/Rotation_vector_representation.resources/unknown_filename.png]]
Convenient for analysis, but not for computation
Created at 2021-08-17. Last updated at 2021-08-17.
Tagged: #-sa/processed #math/rotations</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Rotations-_-SO3-group-index/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Rotations-_-SO3-group-index/</guid><description>Rotations / SO(3) group index Group theory [[SE(3) Special Euclidian Group.md|SE(3) Special Euclidian Group]] [[SO(3) 3D rotation group.md|SO(3) 3D rotation group]] [[Lie group, Lie algebra.md|Lie group, Lie algebra]] [[Exponential map.md|Exponential map]] [[Logarithm map.md|Logarithm map]]
Ambiguities in rotation representations [[Active_passive or Alibi_alias rotation transformations.md|Active/passive or Alibi/alias rotation transformations]] [[Intrinsic vs extrinsic rotations.md|Intrinsic vs extrinsic rotations]]
Rotation representations [[Orientation parametrisations.md|Orientation parametrisations]]
[[Which orientation parametrisation to choose_.md|Which orientation parametrisation to choose?</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Rotations-as-xyz-Bryan-Tait-angles-Kardanwinkel/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Rotations-as-xyz-Bryan-Tait-angles-Kardanwinkel/</guid><description>Rotations as xyz Bryan-Tait angles (Kardanwinkel) Parent: [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]] Source: [[Woernle Mehrkörpersysteme.md|Woernle Mehrkörpersysteme]] Backlinks: [[Euler angles.md|Euler angles]]
Rotation angle nomenclature Euler angles: ZXZ (mitgedrehte Achsen) Kardan-Winkel [de] / Bryan-Tait angles: ZYX (mitgedrehte Achsen)
xyz-Kardan-Winkel
![[./resources/Rotations_as_xyz_Bryan-Tait_angles(Kardanwinkel).resources/unknown_filename.png]] ![[./resources/Rotations_as_xyz_Bryan-Tait_angles(Kardanwinkel).resources/unknown_filename.8.png]]
K3: körperfestes KS K0: Welt-KS
Ausgangslage 1. Drehung um x0 2. Drehung um y1 3. Drehung um z2 !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Running-SOFA-with-Python/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Running-SOFA-with-Python/</guid><description>Running SOFA with Python Parent: [[SofaPython Index.md|SofaPython Index]]
From command line Add to path environment and then execute runSofa via command line
With a python script runSofa -l SofaPython ./script_name.py
How to make SofaPython loaded by default? In bin/plugin_list.conf? Yes sofa-launcher might be useful With pipenv pipenv run runsofa
Created at 2020-07-15. Last updated at 2020-08-22.
Tagged: #software/SOFA #software/python #-sa/processed #software/python/pipenv</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/SA-TODO/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/SA-TODO/</guid><description>SA TODO Parent: [[Scope of Studienarbeit.md|Scope of Studienarbeit]] See also: [[HiWi to-do.md|HiWi to-do]], [[DefSLAM + ORBSLAM3 integration.md|DefSLAM + ORBSLAM3 integration]]
Studienarbeit Camera-based localisation
Find a classification of approaches/techniques Briefly describe each See if it applies to the project Look into the most promising approach &amp;ndash; how to implement (DefSLAM) DefSLAM Install DefSLAM library Skim through an existing VI-SLAM (rigid) implementation to see how sensor fusion is done (as an overview for the coming sensor fusion task) VINS-Mono, VIORB paper Prepare dummy data for testing VI-SLAM (eventually VI-DefSLAM) — interpolate data between frames and add noise/bias Go through code Get the executables working VideoCapture OpenCV problem &amp;ndash; reinstall with all FFMMPEG options Figure out g2o Go through the rest of DefSLAM Go through the rest of ORBSLAM3 IMU term in cost function IMU preintegration IMU initialisation Implement IMU term in optimisation (either using EKF (s.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Scene-graph-general/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Scene-graph-general/</guid><description>Scene graph (general) Source: https://en.wikipedia.org/wiki/Scene_graph See also: [[Scene graph in SOFA.md|Scene graph in SOFA]]
A general data structure Collection of nodes in a graph/tree Created at 2020-08-09. Last updated at 2020-08-09.
Tagged: #software/SOFA/data-types #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Scene-graph-in-SOFA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Scene-graph-in-SOFA/</guid><description>Scene graph in SOFA Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Data structure in SOFA.md|Data structure in SOFA]] Backlinks: [[SOFA Introduction.md|SOFA Introduction]] See also: [[Scene graph (general).md|Scene graph (general)]]
Pool of simulated objects and algorithms in a hierarchical data structure Scenes can be built procedurally or read from XML files Root node represents whole simulation Graph is processed by using [[visitors.md|visitors]] A scene graph node
Gathers components associated with the same DOFs/topology Connections between non-sibling components require explicit references Example: !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Sch%C3%B6nheitsfehler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Sch%C3%B6nheitsfehler/</guid><description>Schönheitsfehler Parent: [[Thesis.md|Thesis]]
recreate diagrams in Inkscape / tikz binding correction and margins (esp on cover page) overfulls consistent colour scheme for diagrams bibliography style: preamble/bibstyle.tex nach ISO &amp;hellip; citeauthor without bibstyle page headers Created at 2020-12-16. Last updated at 2020-12-16.
Tagged: #to-do #-sa/processed #-master/thesis</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/scipy.optimize/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/scipy.optimize/</guid><description>scipy.optimize https://stackoverflow.com/questions/52438263/scipy-optimize-gets-trapped-in-local-minima-what-can-i-do
Local optims sensitive to initial value. Workarounds:
use your optimization in a loop with random starting points inside your boundaries
use an algorithm that can break free of local minima, I can recommend scipy&amp;rsquo;s basinhopping() It repeats your minimize procedure multiple times and get multiple local minimums. The minimal one is the global minimum.
use a global optimization algorithm and use it&amp;rsquo;s result as initial value for a local algorithm.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/SE3-Special-Euclidian-Group/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/SE3-Special-Euclidian-Group/</guid><description>SE(3) Special Euclidian Group Parent: [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]] Source: [[Forster 2017 IMU Preintegration.md|Forster 2017 IMU Preintegration]]
![[./_resources/SE(3)_Special_Euclidian_Group.resources/unknown_filename.png]]
Group of rigid motion in 3D
Created at 2020-11-27. Last updated at 2021-08-16.
Tagged: #-definitions #-sa/processed #math/rotations</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Segfault-in-DefTracking-imu-branch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Segfault-in-DefTracking-imu-branch/</guid><description>Segfault in DefTracking (imu branch) ![[./resources/Segfault_in_DefTracking(imu_branch).resources/unknown_filename.png]]
/home/user3/slam/datasets/mandala0/images/stereo_im_l_1560936003993.png i: 30 POINTS matched:10 Track lost soon after initialisation, reseting&amp;hellip; /home/user3/slam/datasets/mandala0/images/stereo_im_l_1560936004022.png i: 31 System Reseting NORMAL ESTIMATOR IN - NORMALS REESTIMATED : 0 - 0 NORMAL ESTIMATOR OUTPoints potential : 939 70 New template requested Number Of normals 0 0x5555636b1fb0 Not enough normals Reseting Local Mapper&amp;hellip; done Reseting Loop Closing&amp;hellip; done Reseting Database&amp;hellip; done
Thread 1 &amp;ldquo;DefSLAM&amp;rdquo; received signal SIGSEGV, Segmentation fault. 0x00007ffff78d9fae in cv::Mat::Mat (m=&amp;hellip;, this=0x7ffffffeaea0) at /usr/local/include/opencv4/opencv2/core/mat.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Sending-data-using-sockets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Sending-data-using-sockets/</guid><description>Sending data using sockets Parent: [[SofaPython Index.md|SofaPython Index]]
https://github.com/psomers3/PyDataSocket
https://docs.python.org/3/howto/sockets.html Sockets: form of IPC (inter-process communication), for cross-platform communication (Alternatives, for fast IPC: pipes, shared memory)
“client” socket - an endpoint of a conversation e.g. browser, other client applications “server” socket, which is more like a switchboard operator. The client application (your browser, for example) uses e.g. web server (uses both server and client sockets) Roughly, how a socket works (ex: clicking a link on the browser) Client socket (browser) / Receive</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Sensors-absolute-measurements-for-measuring-absolute-POSE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Sensors-absolute-measurements-for-measuring-absolute-POSE/</guid><description>Sensors (absolute measurements) for measuring absolute POSE Parent: [[SLAM hardware.md|SLAM hardware]]
Source: [[Wikipedia Lokalisierung.md|Wikipedia Lokalisierung]]
GPS (only for outdoors) Innenraumsensorik Lidar, Ultra Wide Band (UWB), Wireless Fidelity, etc [[[Wu.md|Wu]]] Compared to these, cameras are flexible and low-cost [[[Wu.md|Wu]]] (are also passive sensors) [ comet ] Radiobaken Created at 2020-08-23. Last updated at 2020-08-23.
Tagged: #sensors-for-SLAM #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Sensors-absolute-measurements-for-measuring-distance-to-landmarks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Sensors-absolute-measurements-for-measuring-distance-to-landmarks/</guid><description>Sensors (absolute measurements) for measuring distance to landmarks Parents: [[SLAM Index.md|SLAM Index]], [[SLAM hardware.md|SLAM hardware]] Backlinks: [[Position acquisition.md|Position acquisition]]
Source:: [[Cometlabs What You Need to Know About SLAM.md|Cometlabs What You Need to Know About SLAM]]
Acoustic (Time of Flight) ToF technique Surfaces need to have good acoustic reflection Lack the ability to use surface properties for localisation examples Sonar Ultrasonic, ultrasound Laser rangerfinders ToF and phase-shift techniques Lack the ability to use surface properties for localisation e.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Several-unwanted-effects-using-gh-filters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Several-unwanted-effects-using-gh-filters/</guid><description>Several unwanted effects using gh filters Parent: [[g-h filter or α-β filter.md|g-h filter or α-β filter]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
Effect of bad initial conditions: ringing (sinusoidal over- and undershooting before finally settling onto a trajectory) ![[./_resources/Several_unwanted_effects_using_gh_filters.resources/unknown_filename.png]]
Effect of very noisy data ![[./_resources/Several_unwanted_effects_using_gh_filters.resources/unknown_filename.1.png]]
Effect of acceleration (in data) ![[./_resources/Several_unwanted_effects_using_gh_filters.resources/unknown_filename.2.png]] Filter lags behind because it uses a model that assumes constant velocity in each propagation step.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Showing-correlation-using-error-ellipses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Showing-correlation-using-error-ellipses/</guid><description>Showing correlation using error ellipses Parent: [[Error ellipse_Confidence ellipse.md|Error ellipse/Confidence ellipse]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
A slanted ellipse implies correlation
The &amp;lsquo;thinner&amp;rsquo; side isn&amp;rsquo;t necessarily more accurate, it just means that the spread of data is reduced along this dimension (when viewing sensor data, for example) Example First epoch ![[./_resources/Showing_correlation_using_error_ellipses.resources/unknown_filename.1.png]] Yellow: prior (very uncertain about position) Green: evidence (more accurate in one of the dimensions than the other; more certainty compared to prior) Blue: posterior via [[multiplication.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Simplex/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Simplex/</guid><description>Simplex Source: https://en.wikipedia.org/wiki/Simplex Parent: [[Barycentric coordinates.md|Barycentric coordinates]]
A triangle in arbitrary dimensions ![[./_resources/Simplex.resources/unknown_filename.jpeg]]
0-simplex point 1-simplex line 2-simplex triangle 3-simplex tetrahedron Created at 2020-08-09. Last updated at 2020-08-09.
Tagged: #-definitions #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Simulation-algorithms-in-SOFA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Simulation-algorithms-in-SOFA/</guid><description>Simulation algorithms in SOFA Source: [[SOFA extended documentation.md|SOFA extended documentation]]
ODE integration ([[ODE solvers.md|ODE solvers]]) Linear equation solution ([[linear solvers.md|linear solvers]]) Complex constraints ( constraint solvers ) Collision detection and response GPU support Created at 2020-08-22. Last updated at 2020-08-22.
Tagged: #-sa/processed #software/SOFA/simulation-algorithms-in-SOFA</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/SLAM-hardware/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/SLAM-hardware/</guid><description>SLAM hardware Parent: [[SLAM Index.md|SLAM Index]] See also: [[Position acquisition (relative vs. absolute).md|Position acquisition (relative vs. absolute)]]
Source: [[SLAM for Dummies.md|SLAM for Dummies]]
Robot parameters to consider Ease of use Odometry performance: how well the robot can estimate its own position, just from the rotation of the wheels Max errors: 2cm per meter moved, 2deg per 45deg turned Bad odometry &amp;ndash;&amp;gt; bad estimation of current position &amp;ndash;&amp;gt; hard to implement SLAM Range measurement device options Source: [[Wikipedia Lokalisierung.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/SLAM-Index/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/SLAM-Index/</guid><description>SLAM Index Definition [[Localisation.md|Localisation]] [[What is SLAM_.md|What is SLAM?]] [[Main paradigms of SLAM.md|Main paradigms of SLAM]]
Sensors for SLAM [[Position acquisition (relative vs. absolute).md|Position acquisition (relative vs. absolute)]] [[SLAM hardware.md|SLAM hardware]]
Relative [[Odometry.md|Odometry]] [[IMU.md|IMU]]
Absolute [[Sensors (absolute measurements) for measuring distance to landmarks.md|Sensors (absolute measurements) for measuring distance to landmarks]] [[Visual sensors for localisation.md|Visual sensors for localisation]] [[Monocular depth perception.md|Monocular depth perception]]
[[Pinhole camera model.md|Pinhole camera model]]
[[Camera calibration.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/SLAM-resources/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/SLAM-resources/</guid><description>SLAM resources Parent: [[SLAM Index.md|SLAM Index]]
Theory
[[Wikipedia SLAM.md|Wikipedia SLAM]] https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation Thrun - Probabilistic Robotics SLAM for dummies Andrew Davison research page at the Department of Computing , Imperial College London about SLAM using vision. Paper 2002 on monocular SLAM SLAM lectures on YouTube https://openslam-org.github.io / Tutorials SLAM summer school SS06: http://www.robots.ox.ac.uk/~SSS06/Website/
Programming [[Programmatic implementations of MonoSLAM.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/SLAM-specific-jacobians/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/SLAM-specific-jacobians/</guid><description>SLAM-specific jacobians Source: [[SLAM for Dummies.md|SLAM for Dummies]] Backlinks: [[EKF matrices_vectors.md|EKF matrices/vectors]]
Jxr
Jacobian of the prediction of landmarks, which does not include prediction of theta, w.r.t. robot POSE same as J_prediction model, except without rotation term ![[./_resources/SLAM-specific_jacobians.resources/unknown_filename.png]] Jz Jacobian of prediction of landmarks, but w.r.t. [range, bearing] ![[./_resources/SLAM-specific_jacobians.resources/unknown_filename.1.png]]
Created at 2020-07-29. Last updated at 2020-08-06.
Tagged: #filters/EKF #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Smooth-polymial-trajectory-generation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Smooth-polymial-trajectory-generation/</guid><description>Smooth polymial trajectory generation Source: FLS handouts
![[./_resources/Smooth_polymial_trajectory_generation.resources/unknown_filename.png]]
Created at 2021-07-19. Last updated at 2021-07-21.
Tagged: #-sa/processed #math</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Smug-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Smug-filter/</guid><description>Smug filter Parent: [[1D Kalman filter algorithm.md|1D Kalman filter algorithm]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
A filter that, once enough measurements are made, becomes very confident in its prediction (P gets smaller with time while the filter becomes more inaccurate!). From then on it will ignore measurements
To avoid this: add a bit of error to the prediction step, e.g. using the process variance Created at 2020-08-31.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/SO3-3D-rotation-group/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/SO3-3D-rotation-group/</guid><description>SO(3) 3D rotation group Parent: [[Rotations _ SO(3) group index.md|Rotations / SO(3) group index]] See also: [[Orientation parametrisations.md|Orientation parametrisations]], [[Linearisation of an orientation in SO(3).md|Linearisation of an orientation in SO(3)]], [[Solà 2017 Quaternion kinematics for ESKF.md|Solà 2017 Quaternion kinematics for ESKF]]
Source: [[MKok 2017 Using inertial sensors for position and orientation estimation.md|MKok 2017 Using inertial sensors for position and orientation estimation]]
All orthogonal matrices with dim 3x3 have the property !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Sockets-Errno-10054/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Sockets-Errno-10054/</guid><description>Sockets Errno 10054 Parent: [[SofaPython Index.md|SofaPython Index]]
WSAECONNRESET 10054 Connection reset by peer. An existing connection was forcibly closed by the remote host. This normally results if the peer application on the remote host is suddenly stopped, the host is rebooted, the host or remote network interface is disabled, or the remote host uses a hard close (see setsockopt for more information on the SO_LINGER option on the remote socket).</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/SOFA-Cataract-surgery/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/SOFA-Cataract-surgery/</guid><description>SOFA Cataract surgery https://www.sofa-framework.org/applications/gallery/eye-surgery-simulator-insimo/
![[./_resources/SOFA_Cataract_surgery.resources/unknown_filename.png]] SOFA – Cataract Surgery – InSimo www.sofa-framework.orgThe SOFA technology is at the core of a advanced eye surgery simulator developed in the context of the HelpMeSee project. HelpMeSee is an American foundation with a singular mission:… read more → Created at 2020-07-16. Last updated at 2020-08-22.
Tagged: #-resources/videos #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/SOFA-extended-documentation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/SOFA-extended-documentation/</guid><description>SOFA extended documentation Source: https://hal.inria.fr/hal-00681539 Authors: Faure et al Backlinks: [[Scope of Studienarbeit.md|Scope of Studienarbeit]]
Abstract
SOFA: open source C++ library mainly for interactive physical/medical simulation modular approach by decomposing simulators into its constituent components (DOF, differential equations, solvers etc), and organising them in a scenegraph data structure multimodel representation of objects (collision model, visual model etc) Chapters
Read
1: [[introduction.md|introduction]] 2: multimodel framework 3: data structures 3.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/SOFA-Introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/SOFA-Introduction/</guid><description>SOFA Introduction Source: [[SOFA extended documentation.md|SOFA extended documentation]]
Goal of SOFA: To provide a highly modular framework for interactive medical simulation, enabling collaboration across different disciplines Concept: [[scene-graph.md|scene-graph]]-based multimodel representatios
How it works:
Simulators are broken down into independent components Component: an aspect of the simulation e.g. DOF, forces, constraints, ODEs/PDEs, solvers, algorithms Components are organised in a [[scene graph.md|scene graph]] data structure Simulated objects represented via several [[models.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/SofaPython-API_Documentation-links/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/SofaPython-API_Documentation-links/</guid><description>SofaPython API/Documentation links Parent: [[SofaPython Index.md|SofaPython Index]]
SP2
SofaPython pdf https://www.sofa-framework.org/api/master/plugins/SofaPython/html/index.html https://sofacomponents.readthedocs.io/en/latest/index.html SP3
https://sofapython3.readthedocs.io/en/latest/menu/SofaPlugin.html Created at 2020-07-17. Last updated at 2020-09-28.
Tagged: #software/SOFA #software/python #-resources #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/SofaPython-Index/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/SofaPython-Index/</guid><description>SofaPython Index Building/setup [[Building SOFA on Windows.md|Building SOFA on Windows]] [[Someone&amp;rsquo;s SP3 setup.md|Someone&amp;rsquo;s SP3 setup]]
Running [[Running SOFA with Python.md|Running SOFA with Python]] [[Using python with existing scene.md|Using python with existing scene]] [[Basic python script in Sofa.md|Basic python script in Sofa]] [[Initialising graph in SP3.md|Initialising graph in SP3]]
Plugins [[Possible plugins.md|Possible plugins]] [[Install ROSConnector in SOFA.md|Install ROSConnector in SOFA]] [[STLIB (Sofa Template Library).md|STLIB (Sofa Template Library)]] [[Registration.md|Registration]]
Communication [[Sending data using sockets.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Some-optimisation-based-tightly-coupled-multisensor-SLAM-algorithms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Some-optimisation-based-tightly-coupled-multisensor-SLAM-algorithms/</guid><description>Some optimisation-based tightly-coupled multisensor SLAM algorithms Parent: [[SLAM Index.md|SLAM Index]]
Source: [[Wu 2018 Image-based camera localization_ an overview.md|Wu 2018 Image-based camera localization: an overview]]
Uses nonlinear optimization may potentially achieve higher accuracy due to the capability to limit linearization errors through repeated linearization of the inherently nonlinear problem
[117] Forster: preintegration theory [118] OKVIS: a novel approach to tightly integrate visual measurements with IMU optimise a joint nonlinear cost function that integrates an IMU error term with the landmark reprojection error in a fully probabilistic manner real-time operation: old states are marginalized to maintain a bounded-sized optimization window Li et al.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Someones-SP3-setup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Someones-SP3-setup/</guid><description>Someone&amp;rsquo;s SP3 setup Parent: [[SofaPython Index.md|SofaPython Index]]
https://gist.github.com/pedroperrusi/9fdd4257db72465c8fb481381f396c51
Created at 2020-09-30. Last updated at 2020-10-01.
Tagged: #software/SOFA/SofaPython3 #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Spaces-in-mathematics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Spaces-in-mathematics/</guid><description>Spaces in mathematics Source: https://upload.wikimedia.org/wikiversity/en/c/cd/Spaces_in_mathematics.pdf Types of spaces in mathematics
Euclidian spaces (3D space, 2D space/Euclidian plane) Linear spaces Topological spaces Hilbert spaces etc. What is a space?
No real definition Made of selected mathematical objects which are treated as points selected relationships between these points Points can be elements of a set functions subspaces Isomorphic spaces are considered identical Isomorphism between two spaces: one-to-one mapping between the points, that preserves the relationships between the points Created at 2020-11-27.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Sparse_Feature-based-VSLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Sparse_Feature-based-VSLAM/</guid><description>Sparse/Feature-based VSLAM Parent: [[Visual SLAM Implementation Framework.md|Visual SLAM Implementation Framework]], [[SLAM Index.md|SLAM Index]] See also: [[Feature-based vs direct SLAM workflow.md|Feature-based vs direct SLAM workflow]]
Source: [[Cometlabs What You Need to Know About SLAM.md|Cometlabs What You Need to Know About SLAM]]
Front-end part of the [[Visual SLAM Implementation Framework.md|Visual SLAM Implementation Framework]] Use only a small selected subset of the pixels in an image frame [[Feature maps.md|Feature maps]] generated are point clouds &amp;ndash;&amp;gt; used to track the camera pose Requires feature extraction and [[matching.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Spelling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Spelling/</guid><description>Spelling Parent: [[Thesis.md|Thesis]]
Hyphenation
real time (real-time as adj) back end minimally invasive Created at 2020-12-16. Last updated at 2020-12-17.
Tagged: #-sa/processed #-master/thesis</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Spherical-wrist/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Spherical-wrist/</guid><description>Spherical wrist http://www1.cs.columbia.edu/~allen/F15/NOTES/forwardspong.pdf
S. also https://www.youtube.com/watch?v=S6TFakW5YcI
![[./_resources/Spherical_wrist.resources/unknown_filename.1.png]]
![[./_resources/Spherical_wrist.resources/unknown_filename.png]]
Created at 2021-05-25. Last updated at 2021-05-25.
Tagged: #to-do/orphan #math/kinematics #-sa/to-be-processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Spike-landmarks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Spike-landmarks/</guid><description>Spike landmarks Source: [[SLAM for Dummies.md|SLAM for Dummies]] Backlinks: [[Landmark extraction.md|Landmark extraction]]
Uses extrema to find [[landmarks.md|landmarks]] Find values in the range of a laser scan, where two values differ by more than a certain amount (e.g. 0.5 m) This finds big changes in the laser scan Alternatively, using three values next to each other: A, B, C (A - B) + (C - B) yields a value Better for finding spikes as it finds actual spikes Rely on the landscape changing a lot between two laser beams Algo will fail in smooth environments Suitable for indoor environments, however is not robust against envs w/ people people are picked up as spikes as theoretically they are good landmarks (just not stationary!</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/State-of-the-art-SLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/State-of-the-art-SLAM/</guid><description>State-of-the-art SLAM Parent: [[SLAM Index.md|SLAM Index]] Backlinks: [[Qin 2019 General Optimization-based Framework (Multisensor).md|Qin 2019 General Optimization-based Framework (Multisensor)]]
Things that I&amp;rsquo;ve seen mentioned several times so far
ORBSLAM: monocular MonoSLAM: monocular (old?) — Andrew Davison OKVIS: visual inertial, stereovision PTAM: parallel tracking and mapping MSCKF: real-time EKF VINS-mono: visual inertial, monocular https://en.wikipedia.org/wiki/List_of_SLAM_Methods Created at 2020-08-07. Last updated at 2020-11-17.
Tagged: #SLAM #discussion/2020/2020-08 #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Step-1_-Odometry-update-Prediction-step/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Step-1_-Odometry-update-Prediction-step/</guid><description>Step 1: Odometry update (Prediction step) Source: [[SLAM for Dummies.md|SLAM for Dummies]] Backlinks: [[Basic EKF for SLAM.md|Basic EKF for SLAM]]
First step in the three-step EKF
Update current state using odometry data Based on the controls given to the robot Calculate estimate of new POSE Update equation: prediction model (x = x + deltax) Or in a simple model, neglect the error term q
State vector gets updated via the prediction model</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Step-2_-Re-observation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Step-2_-Re-observation/</guid><description>Step 2: Re-observation Source: [[SLAM for Dummies.md|SLAM for Dummies]] Backlinks: [[Basic EKF for SLAM.md|Basic EKF for SLAM]]
Second step in the three-step EKF — overview
In this step we update the robot position that we got in step 1 Compensate for errors due to odometry pos_est (odometry-based) - pos_actual (LM-based) = Innovation, (based on the LM that the robot can see)
Use this to update robot position</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Step-3_-New-landmarks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Step-3_-New-landmarks/</guid><description>Step 3: New landmarks Source: [[SLAM for Dummies.md|SLAM for Dummies]] Backlinks: [[Basic EKF for SLAM.md|Basic EKF for SLAM]]
Overview
Landmarks that are new are not dealt with until step 3. Delaying the incorporation of new landmarks until the will decrease the computation cost needed for this step the covariance matrix, P, and the system state, X, are smaller by then. Update state vector x and covariance matrix P with new landmarks Add new landmark to state vector X Add new row and column to covariance matrix Covariance for new landmark Robot-landmark covariance Created at 2020-07-29.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Stereo-cameras/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Stereo-cameras/</guid><description>Stereo cameras Source: [[Cometlabs What You Need to Know About SLAM.md|Cometlabs What You Need to Know About SLAM]] Backlinks: [[Visual sensors for localisation.md|Visual sensors for localisation]]
two cameras separated by a fixed distance (baseline) observations of the position of the same 3D point in both cameras allows depth to be calculated through triangulation (like humans do) depth measurement limited by baseline and resolution generally, wider baseline &amp;ndash;&amp;gt; better depth estimate (but occupies more physical space) s.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/STLIB-Sofa-Template-Library/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/STLIB-Sofa-Template-Library/</guid><description>STLIB (Sofa Template Library) Parent: [[SofaPython Index.md|SofaPython Index]]
https://github.com/SofaDefrost/STLIB
API doc: https://stlib.readthedocs.io/en/latest/index.html
contains sofa scene template common scene template used regularly templates should be compatible with .pyscn and PSL scenes Created at 2020-07-17. Last updated at 2020-08-22.
Tagged: #software/SOFA/sofa-plugins #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Surface-alignment-in-DefSLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Surface-alignment-in-DefSLAM/</guid><description>Surface alignment in DefSLAM Parent: [[Mapping step-by-step in DefSLAM.md|Mapping step-by-step in DefSLAM]] Source: [[Lamarca 2019 DefSLAM.md|Lamarca 2019 DefSLAM]]
Goal:
to scale the up-to-scale surface (output of NRSfM) to the proper dimensions get an idea of the proper dimensions from the already estimated map i.e. resulting surface must match the scale of the template T_(k-1) T_(k-1): deformed map generated by the tracker at the instance of KF=k insertion, with shape-at-rest of S_(k-1) generated from KF:(k-1) result: scale-corrected shape-at-rest Sk Method:</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Symbolic-container-for-Probe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Symbolic-container-for-Probe/</guid><description>Symbolic container for Probe ![[./_resources/Symbolic_container_for_Probe.resources/unknown_filename.1.jpeg]]![[./_resources/Symbolic_container_for_Probe.resources/unknown_filename.jpeg]]
Created at 2021-07-17. Last updated at 2021-07-18.
Tagged: #project-management/sounding-board #-sa/processing</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/System-in-a-VIN-problem-with-IMU-preintegration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/System-in-a-VIN-problem-with-IMU-preintegration/</guid><description>System in a VIN problem with IMU preintegration Source: [[Forster 2017 IMU Preintegration.md|Forster 2017 IMU Preintegration]]
![[./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/Image.png]]
State x_i of the system at time i ![[./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.png]]
with ![[./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.1.png]] ![[./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.2.png]] ![[./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.3.png]]
![[./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.4.png]] All keyframes up till time k ![[./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.5.png]] State of all keyframes ![[./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.6.png]] camera measurements ![[./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.7.png]] IMU measurements between KFs i and j (consecutive) !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/System__forceTrajectory/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/System__forceTrajectory/</guid><description>System::forceTrajectory Parent: [[DefSLAM branch overview.md|DefSLAM branch overview]]
Reference: DefSLAMGT (stereo as ground truth) For testing: DefSLAMVI
Description Force update of DefSLAMVI&amp;rsquo;s current frame pose to that of DefSLAMGT&amp;rsquo;s for the frames 230 to 239
Without System::Reset ![[./_resources/System__forceTrajectory.resources/unknown_filename.png]] Frame pose is &amp;lsquo;updated&amp;rsquo; during the interval, but after the interval, the optimisation (which uses frame pose as an estimate and also uses map node positions) makes the system resume it&amp;rsquo;s trajectory before the update</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Template-for-a-bibliography-entry/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Template-for-a-bibliography-entry/</guid><description>Template for a bibliography entry Source Backlinks
Authors Abstract Contents/Chapters Takeaway
Created at 2020-08-04. Last updated at 2020-08-09.
Tagged: #-resources/-bibliography/meta #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Template-in-DefSLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Template-in-DefSLAM/</guid><description>Template in DefSLAM Source: [[Lamarca 2019 DefSLAM.md|Lamarca 2019 DefSLAM]]
![[./_resources/Template_in_DefSLAM.resources/unknown_filename.1.png]]
Template
2D triangular mesh floating in the 3D space consists of a set of 2D triangular facets F a facet has 3 nodes (set V) and 3 edges (set E) map points observed in keyframe k are embedded in the facets Map point coordinates in [[barycentric coordinates.md|barycentric coordinates]] ![[./_resources/Template_in_DefSLAM.resources/unknown_filename.2.png]] ![[./_resources/Template_in_DefSLAM.resources/unknown_filename.png]]
Created at 2020-11-20. Last updated at 2020-11-20.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Template-substitution-in-DefSLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Template-substitution-in-DefSLAM/</guid><description>Template substitution in DefSLAM Parent: [[Mapping step-by-step in DefSLAM.md|Mapping step-by-step in DefSLAM]] Source: [[Lamarca 2019 DefSLAM.md|Lamarca 2019 DefSLAM]]
![[./_resources/Template_substitution_in_DefSLAM.resources/Image.png]]
Tracking runs at frame-rate, and mapping at keyframe-rate Tracking processes Nm frames during a whole mapping run Process
New keyframe (k) is made. Now at time t=k At this point, the template in the tracking is still based on the old shape-at-rest, S_(k-1) Mapping thread starts creates surface S_k which is aligned to prev.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Tex-stuff/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Tex-stuff/</guid><description>Tex stuff Parent: [[Thesis.md|Thesis]]
tikzexternalize
vimtex custom compile; include makeglossary
vim UltiSnips doesn&amp;rsquo;t work in math contexts! (Solved: https://github.com/SirVer/ultisnips/issues/1193#issuecomment-620455011)
get symbols to work with glossaries or glossaries-extra
set up spellcheck
automated list of symbols
Created at 2020-12-16. Last updated at 2021-02-14.
Tagged: #to-do #-sa/processed #-master/thesis</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/The-making-of-EndoSLAM-dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/The-making-of-EndoSLAM-dataset/</guid><description>The making of EndoSLAM dataset https://www.youtube.com/watch?v=G_LCe0aWWdQ ![[./_resources/The_making_of_EndoSLAM_dataset.resources/unknown_filename.png]] ![[./_resources/The_making_of_EndoSLAM_dataset.resources/unknown_filename.1.png]]
Github: https://github.com/CapsuleEndoscope/EndoSLAM
Created at 2020-11-25. Last updated at 2020-11-25.
Tagged: #-resources #-sa/processed #discussion/2020/2020-12</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Thesis-restructure/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Thesis-restructure/</guid><description>Thesis restructure Parent: [[Thesis.md|Thesis]] Backlinks: [[Next update 2021-06.md|Next update 2021-06]]
Introduction
General Bladder cancer surgery — cryosection The navigation/localisation problem GRK Problem statement Questions arising from the localisation task:
Which localisation algorithm do we use? We have settled on the camera-IMU combination. How do we take into account the non-constant calibration parameters between the camera and IMU (due to the surgeon&amp;rsquo;s manipulation of the cystoscope)?</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Thesis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Thesis/</guid><description>Thesis Parent: [[Scope of Studienarbeit.md|Scope of Studienarbeit]] See also: [[Schönheitsfehler.md|Schönheitsfehler]], [[Tex stuff.md|Tex stuff]], [[Spelling.md|Spelling]], [[Notes on current thesis version.md|Notes on current thesis version]], [[Fact checks.md|Fact checks]]
**TO BE RESTRUCTURED** s. [[Thesis restructure.md|Thesis restructure]]
Introduction
Bereits am Anfang (S.1) auf die Navigation eingehen GRK 2543 more on the navigation subtheme (loc. of other tools within camera view of the main probe &amp;ndash; relate to generalisability of optim.-based methods later on) Problem statement Literature review/Stand der Technik/Other works Visual tracking review: types of algos available VI-Review filt-based, opt-based  DefSLAM use of ORB features in def.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Top-down-mapping-master-to-slave/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Top-down-mapping-master-to-slave/</guid><description>Top-down mapping (master to slave) Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Mappings.md|Mappings]]
Mapping of a master states to the slave states ![[./resources/Top-down_mapping(master_to_slave).resources/unknown_filename.png]] ![[./resources/Top-down_mapping(master_to_slave).resources/unknown_filename.1.png]] with the Jacobian ![[./resources/Top-down_mapping(master_to_slave).resources/unknown_filename.2.png]] (kinematic relation) ![[./resources/Top-down_mapping(master_to_slave).resources/unknown_filename.3.png]]
Linear/nonlinear mappings
In linear mappings, J and J are the same In nonlinear mappings, J is nonlinear w.r.t. x_m, i.e. not a matrix Surfaces
Surfaces embedded in deformable cells: J contains barycentric coordinates Surfaces attached to rigid bodies: each row of J encodes !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Topological-changes-during-elastic-registration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Topological-changes-during-elastic-registration/</guid><description>Topological changes during elastic registration https://www.sofa-framework.org/applications/gallery/augmented-reality-in-nephrology/
https://www.youtube.com/watch?v=3rfdL3-wWE0 ![[./_resources/Topological_changes_during_elastic_registration.resources/unknown_filename.png]]
Created at 2020-07-23. Last updated at 2020-08-22.
Tagged: #-resources/videos #-sa/processed #registration</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Topological-SLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Topological-SLAM/</guid><description>Topological SLAM Parent: [[Classification of image-based camera localization approaches.md|Classification of image-based camera localization approaches]] Source: [[Wu 2018 Image-based camera localization_ an overview.md|Wu 2018 Image-based camera localization: an overview]]
does not need accurate computation of 3D maps represents the environment by connectivity or topology e.g. Kuipers [130] used a hierarchical description of the spatial environment
a topological network description mediates between a control and metrical level distinctive places and paths are defined by their properties at the control level serve as nodes and arcs of the topological model Decreasing in popularity</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Tracking-in-VIORB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Tracking-in-VIORB/</guid><description>Tracking in VIORB Source: [[Mur-Artal 2017 VI-ORB.md|Mur-Artal 2017 VI-ORB]]
Tracking in VIORB
Visual-inertial tracking at frame rate, instead of using an ad-hoc motion model as in the original ORB-SLAM Tracked states: [sensor pose (R, p), velocities v, biases b] Once the camera pose is predicted, map points are projected, then matches with existing features on the frame Then optimise the current frame j, depending on whether the map has just been updated the map is unchanged Here, the optimisation function for tracking (when map unchanged) is: !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Tracking-optimisation-in-DefSLAM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Tracking-optimisation-in-DefSLAM/</guid><description>Tracking optimisation in DefSLAM Source: [[Lamarca 2019 DefSLAM.md|Lamarca 2019 DefSLAM]] Backlinks: [[Template substitution in DefSLAM.md|Template substitution in DefSLAM]]
Optimisation function
Minimises reprojection error (in the image) deformation energy (of the template) boundary nodes of the local zone are fixed (i.e. not set as arguments to the optimisation function) this makes the absolute camera pose observable how? in order to constrain the gauge freedoms Initial guess: values from previous optimisation (i.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Tracking__GrabImageMonocular/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Tracking__GrabImageMonocular/</guid><description>Tracking::GrabImageMonocular Parent: [[defSLAM__System__TrackMonocular.md|defSLAM::System::TrackMonocular]]
cv::Mat ORB_SLAM2::Tracking::GrabImageMonocular
colour conversion make frame using image, timestamp, ORB stuff, calibration data, etc. mCurrentFrame = new Frame (mImGray, timestamp, mpORBextractorLeft, mpORBVocabulary, mK, mDistCoef, mbf, mThDepth, im)
perform tracking: Track (); return camera pose return mCurrentFrame-&amp;gt;mTcw.clone(); Created at 2020-10-21. Last updated at 2020-10-28.
Tagged: #-sa/processed #SLAM/SLAM-algos/DefSLAM</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Transforming-velocities-to-another-frame/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Transforming-velocities-to-another-frame/</guid><description>Transforming velocities to another frame https://physics.stackexchange.com/questions/197009/transform-velocities-from-one-frame-to-an-other-within-a-rigid-body
Transforming velocities to another frame ![[./_resources/Transforming_velocities_to_another_frame.resources/unknown_filename.png]]
Further reading: https://core.ac.uk/download/pdf/154240607.pdf
Created at 2021-05-03. Last updated at 2021-05-03.
Tagged: #-sa/processed #math/kinematics</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Trocar/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Trocar/</guid><description>Trocar Source: https://en.wikipedia.org/wiki/Trocar
Here: surgical trocar
Used in [[laparoscopic surgery.md|laparoscopic surgery]] A medical device which is used to make small incisions and allows insertion of other surgical instruments into the body cavity ![[./_resources/Trocar.resources/unknown_filename.png]]![[./_resources/Trocar.resources/unknown_filename.1.png]]
Source: [[Leiner Digital Endoscope Design.md|Leiner Digital Endoscope Design]] In an arthroscopic procedure (knee?):
trocar is inserted into the cannula both are pushed through the skin trocar is replaced by obturator (blunt rod) to open the area up.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Unit-quaternions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Unit-quaternions/</guid><description>Unit quaternions Parent: [[Quaternion index.md|Quaternion index]], [[Orientation parametrisations.md|Orientation parametrisations]] Backlinks: [[Variables in ESKF.md|Variables in ESKF]], [[Quaternion to rotation matrix.md|Quaternion to rotation matrix]], [[Gibbs _ Rodrigues parameter.md|Gibbs / Rodrigues parameter]]
Source: [[Solà 2017 Quaternion kinematics for ESKF.md|Solà 2017 Quaternion kinematics for ESKF]]
Properties
![[./_resources/Unit_quaternions.resources/unknown_filename.png]] ![[./_resources/Unit_quaternions.resources/unknown_filename.1.png]] Can be written in the form ![[./_resources/Unit_quaternions.resources/unknown_filename.2.png]]![[./_resources/Unit_quaternions.resources/unknown_filename.5.png]]
with u as a unit vector theta is the angle between q and the identity quaternion q_I = [1, 0, 0, 0]</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Univariate-Gaussian-distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Univariate-Gaussian-distribution/</guid><description>Univariate Gaussian distribution Parent: [[Gaussian distribution.md|Gaussian distribution]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
![[./_resources/Univariate_Gaussian_distribution.resources/unknown_filename.png]]
If normalised (area under the graph is 1): Gaussian distribution If not normalised: Gaussian function ![[./_resources/Univariate_Gaussian_distribution.resources/unknown_filename.2.png]]
Notation: ![[./_resources/Univariate_Gaussian_distribution.resources/unknown_filename.1.png]] The random variable X has a Gaussian distribution with mean &amp;hellip; and variance &amp;hellip; .
Created at 2020-08-31. Last updated at 2020-08-31.
Tagged: #-sa/processed #math/statistics</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Unscented-Kalman-Filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Unscented-Kalman-Filter/</guid><description>Unscented Kalman Filter Parent: [[General Kalman Filter.md|General Kalman Filter]]
Source: [[Scaradozzi 2018 SLAM application in surgery.md|Scaradozzi 2018 SLAM application in surgery]]
developed to overcome main problems of the EKF like EKF, approximates the state distribution with a Gaussian Random Variable only the representation is different—using alpha points (a minimal set of sample points) capture posterior mean and covariance accurately for any nonlinearity, up to3rd order Taylor Created at 2020-08-23.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Update-2021-06-11/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Update-2021-06-11/</guid><description>Update 2021-06-11 General model for probe (from [[Forward kinematics IMU to camera.md|Forward kinematics IMU to camera]])
Using the standard DH convention  ![[./_resources/Update_2021-06-11.resources/unknown_filename.png]]![[./_resources/Update_2021-06-11.resources/unknown_filename.3.png]] Note: I have since changed the axis configuration at the camera part—above diagram is no longer up to date; to be updated!
This has the IMU (B) as the base and the camera (C) as the end effector Using robotics-toolbox-python: https://github.com/petercorke/robotics-toolbox-python Simplified model (from [[Forward kinematics IMU to camera.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Update-2021-07-19/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Update-2021-07-19/</guid><description>Update 2021-07-19 Agenda
Comparison prop only, prop + update Comparison P+U plots (Rp 1000, Rp .01, Rp 1e-6) Changes to equations (pc, qc, err_pc, err_theta_c) s. [[KF kinematics.md|KF kinematics]] Currently: getting probe output as function of DOFs Open tasks s. also dvi-eskf project board debug update stage??? get probe outputs as symbols/functions of DOFs switch from rigid probe to rotating scope - at which point do I compensate for notch rotation?</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Using-python-with-existing-scene/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Using-python-with-existing-scene/</guid><description>Using python with existing scene Parent: [[SofaPython Index.md|SofaPython Index]]
In scene graph
Add plugin in the scene using RequiredPlugin Define a PythonScriptController in the scene graph Created at 2020-07-15. Last updated at 2020-08-22.
Tagged: #software/SOFA #software/python #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Validation-gate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Validation-gate/</guid><description>Validation gate Source: [[SLAM for Dummies.md|SLAM for Dummies]] Backlinks: [[Nearest Neighbour.md|Nearest Neighbour]]
An observed landmark is associated to a landmark if the following holds ![[./_resources/Validation_gate.resources/unknown_filename.png]]
v innovation S innovation covariance The Validation gate makes use of the fact that the [[EKF.md|EKF]] implementation gives a bound on the uncertainty of an observation of a LM
Is an observed LM a LM in the database?</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Variance-of-the-1D-Kalman-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Variance-of-the-1D-Kalman-filter/</guid><description>Variance of the 1D Kalman filter Parent: [[1D Kalman filters.md|1D Kalman filters]] Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]]
i.e., what variance is show by the estimated output/posterior?)
Always converges to a fixed value if the sensor and process variances are constant We can run simulations to determine the value to which the filter variance converges Then hard code this value into the filter (+ with first sensor measurement as initial value, the filter should have good performance) Alternative: instead of using the variance value, use the calculated Kalman gain Example implementation using the Kalman gain However, using the Kalman gain obscures the Bayesian approach !</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Variance-standard-deviation-covariances/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Variance-standard-deviation-covariances/</guid><description>Variance, standard deviation, covariances Backlinks: [[Multivariate Gaussian distributions.md|Multivariate Gaussian distributions]]
Source: [[rlabbe Kalman_Bayesian filters in Python.md|rlabbe Kalman/Bayesian filters in Python]] See also: [[Empirical rule 68_95_99.7.md|Empirical rule 68/95/99.7]]
![[./_resources/Variance,_standard_deviation,_covariances.resources/unknown_filename.1.png]] ![[./_resources/Variance,_standard_deviation,_covariances.resources/unknown_filename.png]]
How much do the values vary from the mean?
![[./_resources/Variance,_standard_deviation,_covariances.resources/unknown_filename.2.png]]
There are other ways of calculating variance (e.g. by using absolute values of error instead of error squared). The other methods may be better w.r.t. outliers (outliers get magnified in the square term) Process variance: error in the process model Sensor variance: error in each sensor measurement</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/VecId/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/VecId/</guid><description>VecId Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[ODE solvers.md|ODE solvers]]
Uniquely identifies state vectors (which are scattered over all [[MechanicalStates.md|MechanicalStates]]) Mechanical operations (e.g. allocating a state vector, accumulating forces) are implemented using a specialised [[visitor.md|visitor]] parametrised on VecIds Created at 2020-08-22. Last updated at 2020-08-22.
Tagged: #software/SOFA #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/vi.cc-using-kalman-for-xyz-states-what-goes-on-with-the-map_/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/vi.cc-using-kalman-for-xyz-states-what-goes-on-with-the-map_/</guid><description>vi.cc using kalman for xyz states (what goes on with the map?) Offline Kalman ![[./resources/vi.cc_using_kalman_for_xyz_states(what_goes_on_with_the_map_).resources/unknown_filename.1.png]]
Kalman and live DefSLAM ![[./resources/vi.cc_using_kalman_for_xyz_states(what_goes_on_with_the_map_).resources/unknown_filename.png]]
Created at 2021-03-15. Last updated at 2021-03-15.
Tagged: #-sa/processed #discussion/2021/2021-03</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Viewer-segfault/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Viewer-segfault/</guid><description>Viewer segfault ![[./_resources/Viewer_segfault.resources/unknown_filename.png]]
Error [New Thread 0x7fff86ffd700 (LWP 1117)] NORMALS REESTIMATED : 277 - 277 [Thread 0x7fff86ffd700 (LWP 1117) exited] NORMAL ESTIMATOR OUTPoints potential : 293 70 New template requested Number Of normals 277 0x555566923da0 -0.79956 0.655022 -0.594482POINTS matched:167 Points Scale Error Keyframe : 1 stan dev 0.310974 chi 0.013115 0.01 201 SurfaceRegistration not sucessful (Not enough points to align or chi2 too big
Thread 6 &amp;ldquo;DefSLAM&amp;rdquo; received signal SIGSEGV, Segmentation fault.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Visitors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Visitors/</guid><description>Visitors Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Data structure in SOFA.md|Data structure in SOFA]] Backlinks: [[Scene graph in SOFA.md|Scene graph in SOFA]], [[Simulation algorithms in SOFA.md|Simulation algorithms in SOFA]]
For processing of data structure: parent to child
Allows decoupling of physical model from simulation algo e.g. Easy to replace a time integrator, which wouldn&amp;rsquo;t be the case in a dataflow graph (coupling of data and algo)</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Visual-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Visual-model/</guid><description>Visual model Source: [[SOFA extended documentation.md|SOFA extended documentation]] Parent: [[Models in SOFA.md|Models in SOFA]]
More detailed geometry than that of the [[internal model.md|internal model]], hence uses different meshes [[Mappings.md|Mappings]] are used to update the visual model with the deformations taking place Contains rendering parameters Libraries for rendering graphics
OGRE (external) Open Scene Graph (external) SOFA&amp;rsquo;s own library based on openGL Created at 2020-08-09. Last updated at 2020-08-09.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Visual-sensors-for-localisation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Visual-sensors-for-localisation/</guid><description>Visual sensors for localisation Parents: [[SLAM Index.md|SLAM Index]], [[Sensors (absolute measurements) for measuring distance to landmarks.md|Sensors (absolute measurements) for measuring distance to landmarks]]
Source: [[Wikipedia Visual odometry.md|Wikipedia Visual odometry]]
Process of determining robot POSE by analysing the associated camera images Use sequential camera image to estimate the distance travelled Applications: robotics, computer vision
Source: [[Cometlabs What You Need to Know About SLAM.md|Cometlabs What You Need to Know About SLAM]] Types</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Visual-SLAM-Implementation-Framework/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Visual-SLAM-Implementation-Framework/</guid><description>Visual SLAM Implementation Framework Parent: [[SLAM Index.md|SLAM Index]]
Source: [[Cometlabs What You Need to Know About SLAM.md|Cometlabs What You Need to Know About SLAM]]
Basic principle:
tracking a set of points through successive frames these tracks are used to triangulate the 3D positions of the points to create the map at the same time, using the the est point locations to calculate the pose of the camera, which could have observed them (i.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Visual-inertial-datasets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Visual-inertial-datasets/</guid><description>Visual-inertial datasets https://sites.google.com/view/awesome-slam-datasets/home
https://fpv.ifi.uzh.ch/ Aggressive drone racing http://www.lirmm.fr/aqualoc/ Underwater Monochromatic https://vision.in.tum.de/data/datasets/visual-inertial-dataset TUM indoor/urban, slides fisheye cameraUsed in ORBSLAM3 https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets EUROC MAV stereo, monochrUsed in ORBSLAM3 Created at 2020-11-06. Last updated at 2020-11-17.
Tagged: #-resources #-sa/processed</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/What-is-SLAM_/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/What-is-SLAM_/</guid><description>What is SLAM? Parent: [[SLAM Index.md|SLAM Index]]
Source: [[Scaradozzi 2018.md|Scaradozzi 2018]] Process which allows a mobile robot to
construct a map of its environment (assumed to be unknown) compute its location using the map simultaneously Source: [[Lamarca 2019 DefSLAM.md|Lamarca 2019 DefSLAM]]
Goal is to locate a sensor in an unknown map/environment, which is simultaneously being reconstructed. Typically used in exploratory trajectories (new or changing environments) Source: [[Wikipedia SLAM.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Why-use-the-visual-inertial-sensor-combination_/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Why-use-the-visual-inertial-sensor-combination_/</guid><description>Why use the visual-inertial sensor combination? Parent: [[SLAM Index.md|SLAM Index]] See also: [[Multisensor fusion.md|Multisensor fusion]]
Source: [[Mur-Artal 2017 VI-ORB.md|Mur-Artal 2017 VI-ORB]]
Cheap but also with good potential Cameras provide rich information but are relatively cheap [[IMU.md|IMU]] provides self-motion info, helps recover scale in monocular applications enables estimation of the direction of gravity &amp;ndash;&amp;gt; renders pitch and roll observable Source: [[Forster 2017 IMU Preintegration.md|Forster 2017 IMU Preintegration]]</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/Works-of-possible-interest/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/Works-of-possible-interest/</guid><description>Works of possible interest General SLAM
[[Cadena 2016 Past, Present, and Future of SLAM.md|Cadena 2016 Past, Present, and Future of SLAM]] [[.md|]][[Durrant-Whyte 2006 SLAM Tutorial Part I.md|Durrant-Whyte 2006 SLAM Tutorial Part I]] Prerequisites
g2o paper - graph-based SLAM Existing SLAM algorithms
MonoSLAM, works by Andrew Davison focusing on fusion instead of vision-only SLAM Maplab (filtering-based) not looking at filtering-based algos mentioned in the [[Chen 2018 Review of VI SLAM.</description></item><item><title/><link>https://salehahr.github.io/studienarbeit/World-to-camera-trafo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/World-to-camera-trafo/</guid><description>World to camera trafo Parent: [[SLAM Index.md|SLAM Index]] See also: [[Pinhole camera model.md|Pinhole camera model]], [[Pinhole camera projection function.md|Pinhole camera projection function]] Source: http://www.cse.psu.edu/~rtc12/CSE486/lecture12.pdf
Camera coordinates (X, Y, Z) World coordinates (U, V, W) Image plane (x, y) / Pixel coordinates (u, v) ![[./_resources/World_to_camera_trafo.resources/unknown_filename.1.png]] ![[./_resources/World_to_camera_trafo.resources/unknown_filename.png]] ![[./_resources/World_to_camera_trafo.resources/unknown_filename.2.png]]![[./_resources/World_to_camera_trafo.resources/unknown_filename.3.png]] Forward projection
![[./_resources/World_to_camera_trafo.resources/unknown_filename.4.png]]
Representing 2D point as a fictitious 3D point (x', y', z') [for matrix calculations] Convention: Given (x', y', z'), we can recover the 2D point (x, y) as !</description></item></channel></rss>