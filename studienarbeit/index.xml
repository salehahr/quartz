<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Studienarbeits on Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/</link><description>Recent content in Studienarbeits on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><lastBuildDate>Tue, 05 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://salehahr.github.io/zettelkasten/studienarbeit/index.xml" rel="self" type="application/rss+xml"/><item><title>Bladder cancer surgery procedure</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/bladder-cancer-surgery/</link><pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/bladder-cancer-surgery/</guid><description>See also: related-types-of-surgery BCS procedure (according to my understanding)
Cystocopy to inspect the bladder If tumour is found, do resection &amp;ndash;&amp;gt; cryosection Cancer detected &amp;ndash;&amp;gt; tumour removal</description></item><item><title>scipy.optimize</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/scipy.optimize/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/scipy.optimize/</guid><description>http://stackoverflow.com/questions/52438263/scipy-optimize-gets-trapped-in-local-minima-what-can-i-do
Local optims sensitive to initial value. Workarounds:
use your optimization in a loop with random starting points inside your boundaries
use an algorithm that can break free of local minima, I can recommend scipy&amp;rsquo;s basinhopping() It repeats your minimize procedure multiple times and get multiple local minimums. The minimal one is the global minimum.
use a global optimization algorithm and use it&amp;rsquo;s result as initial value for a local algorithm.</description></item><item><title>Rigid cystoscope dimensions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/rigid-cystoscope-dimensions/</link><pubDate>Fri, 20 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/rigid-cystoscope-dimensions/</guid><description> ca 20cm x 65 cm = 0.02m x 0.065 m
Source: http://en.wikipedia.org/wiki/Cystoscopy
The sizes of the sheath of the rigid cystoscope are 17 French gauge (5.7 mm diameter), 19 Fr gauge (6.3 mm diameter), and 22 Fr gauge (7.3 mm diameter).
Source: http://www.karlstorz.com/cps/rde/xbcr/karlstorz_assets/ASSETS/3405020.pdf Camera</description></item><item><title>50.2.3 Kalman filter initial estimates</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.3-kalman-filter-initial-estimates/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.3-kalman-filter-initial-estimates/</guid><description>Source: Schneider 2013 How to not make the EKF fail Initial state estimate $\mathbf{x}_0$, $\mathbf{P}_0$
Filter generally not badly affected by wrong initial state $\mathbf{x}_0$, but convergence will be slow if we are way off
If $\mathbf{P}_0$ too small whereas $\mathbf{x}_0$ is way off
the gain K becomes small filter relies on the model more than on the measurements Thus: important to have a consistent pair $\mathbf{x}_0$, $\mathbf{P}_0$</description></item><item><title>50.2.40 Kalman filter performance metric</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.40-kalman-filter-performance-metric/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.40-kalman-filter-performance-metric/</guid><description>Source: Schneider 2013 $$ \text{MSE} = \frac{1}{MKN_{n_x}} \sum_{m=1}^M \sum_{j=1}^K \sum_{k=0}^{N-1} \left( \mathbf{\hat{x}}^j(t_k^+) - \mathbf{x}^j(t_k) \right)^\text{T} \left( \mathbf{\hat{x}}^j(t_k^+) - \mathbf{x}^j(t_k) \right) $$
symbols description $k$ time / step $j$ how many EKF runs? in tutorial: EKF was ran 1000 times (non-deterministic system due to noise)</description></item><item><title>Baumgarte stabilisation over the SO(3) rotation group for control</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/baumgarte-stabilisation-over-the-so-3-rotation-group-for-control/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/baumgarte-stabilisation-over-the-so-3-rotation-group-for-control/</guid><description>Author: Sebastien Gros</description></item><item><title>Schneider 2013 How to not make the EKF fail</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/schneider-2013-how-to-not-make-the-ekf-fail/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/schneider-2013-how-to-not-make-the-ekf-fail/</guid><description>Authors: Schneider, Georgakis URL: http://www.researchgate.net/publication/263942618_How_To_NOT_Make_the_Extended_Kalman_Filter_Fail/citations DOI 10.1021/ie300415d Measurement noise R, V (landmark) Kalman filter initial estimates Process noise Q and W (odometry) Kalman filter performance metric</description></item><item><title>50.4.1 Additive quaternion filtering</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.4.1-additive-quaternion-filtering/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.4.1-additive-quaternion-filtering/</guid><description>Parents: Quaternion index , which orientation parametrisation to-choose? Source: Markley 2014 Additive quaternion filtering Additive quaternion error Methods of enforcing the normalisation Renormalise the estimate by brute force Modify KF update equations to enforce a norm constraint using a Lagrange multiplier
[1] and [2] yield biased estimates of the quaternion
Methods that don&amp;rsquo;t enforce normalisation Define the rotation matrix to be guarantees orthogonality introduces unobservable DOF: the quaternion norm Use the above equation without the ||q||-2 factor &amp;ndash;&amp;gt; no orthogonality</description></item><item><title>50.4.2 Multiplicative quaternion filtering (MEKF)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.4.2-multiplicative-quaternion-filtering-mekf/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.4.2-multiplicative-quaternion-filtering-mekf/</guid><description>See also: Which orientation parametrisation to choose? Source: Markley 2014 Main idea is to use
the quaternion as a global rotation representation
a three component state vector as the local representation of rotation errors $$ \begin{aligned} \mathbf{q}\text{tr} &amp;amp;= \delta\mathbf{q} (\delta\mathbf{\theta}) \otimes \mathbf{\hat{q}}\
\mathbf{R}(\mathbf{q}\text{tr}) &amp;amp;= \mathbf{R} (\delta\mathbf{\theta}) \mathbf{R} (\mathbf{\hat{q}}) \end{aligned}$$
each term $(\mathbf{q}_\text{tr},~\delta\mathbf{q},~ \mathbf{\hat{q}})$ is a normalised unit quaternion
Any of the rotation error representations can be used to calculate delta_theta, which is part of the error state of the MEKF.</description></item><item><title>50.7.2 Calculation of K and P in ESKF update</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.2-calculation-of-k-and-p-in-eskf-update/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.2-calculation-of-k-and-p-in-eskf-update/</guid><description>Parent: 50.3 Error-State Kalman Filter , eskf-update See also: Evaluation of the H Jacobian Source: Solà 2017 Quaternion kinematics for ESKF The filter correction equations are (yields a posteriori estimates)
Notes:
Here, the simplest form of the covariance update is used. This has poor numerical stability, however (no guarantee of symmetricity or positive definiteness) More stable forms are e.g. Joseph form (symmetric and positive) Error correction?</description></item><item><title>Rotation vector representation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/rotation-vector-representation/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/rotation-vector-representation/</guid><description>Parents: rotations-so3-group-index , orientation-parametrisations Source: Markley 2014 Combine the Euler axis/angle into a three component rotation vector $$ \mathbf{\theta} \equiv \theta \mathbf{e}$$
Convenient for analysis, but not for computation</description></item><item><title>Maley 2013 MEKF for Nonspinning Guided Projectiles</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/maley-2013-mekf-for-nonspinning-guided-projectiles/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/maley-2013-mekf-for-nonspinning-guided-projectiles/</guid><description>Source: http://apps.dtic.mil/sti/citations/ADA588831</description></item><item><title>Markley 2003 Attitude Error Representations for Kalman Filtering</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/markley-2003-attitude-error-representations-for-kalman-filtering/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/markley-2003-attitude-error-representations-for-kalman-filtering/</guid><description>Source: http://scholar.google.com/scholar?cluster=9266330323139560128&amp;amp;hl=en&amp;amp;as_sdt=0,5 Author: FL Markley
Motivation
Quaternion as an attitude representation
Good: lowest dimensionality while being a globally nonsingular representation Not so good: must obey a unit norm constraint In research, various methods for either getting around the norm constraint, or to enforce it
Most successful method employs the global attitude as a unit quaternion with a 3-comp attitude error representation
MEKF doesn&amp;rsquo;t estimate the quaternion state.</description></item><item><title>Euler angles</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/euler-angles/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/euler-angles/</guid><description>Parents: rotations-so3-group-index , orientation-parametrisations Source: Phil&amp;rsquo;s Lab Three angles that describe the orientation of an object w.r.t. a fixed coordinate system Roll $\phi$, Pitch $\theta$, Yaw $\psi$ Source: http://en.wikipedia.org/wiki/Euler_angles Possible representations Proper Euler angles (e.g. $zxz$) vs Tait-Bryan (e.g. $xyz$, $zyx$) Intrinsic vs. extrinsic rotations Extrinsic rotations (around fixed CS $xyz$) Intrinsic rotations (around body CS $XYZ = x''' y''' z'''$) As a rotation matrix $$R = X(\alpha) Y(\beta) Z(\gamma)$$</description></item><item><title>Whampsey MEKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/whampsey-mekf/</link><pubDate>Sat, 14 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/whampsey-mekf/</guid><description>Source: http://matthewhampsey.github.io/blog/2020/07/18/mekf
Motivation Working with noisy IMU measurements IMUs usually provide redundant information that can be used to improve dead-reckoning Uses: Hamilton quaternion convention .
Which orientation parametrisation to choose? 50.5-error-state-kalman-filter</description></item><item><title>ESKF repos</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/eskf-repos/</link><pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/eskf-repos/</guid><description>C++ http://github.com/skrogh/msf_ekf http://github.com/je310/ESKF http://github.com/hobbeshunter/IMU_EKF (only IMU)
Python http://github.com/enginBozkurt/Error-State-Extended-Kalman-Filter http://github.com/uoip/stereo_vio_eskf (unsuccessful) &amp;ndash; uses average IMU readings http://github.com/aipiano/ESEKF_IMU</description></item><item><title>Diagnosis bladder cancer</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/diagnosis-bladder-cancer/</link><pubDate>Fri, 30 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/diagnosis-bladder-cancer/</guid><description>http://www.cancer.org/cancer/bladder-cancer/detection-diagnosis-staging/how-diagnosed.html</description></item><item><title>Converting IMU data to inertial frame</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/converting-imu-data-to-inertial-frame/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/converting-imu-data-to-inertial-frame/</guid><description>Parent: IMU index Source: http://redshiftlabs.com.au/wp-content/uploads/2018/02/an-1005_-_understanding_euler_angles.pdf IMU outputs are in the body frame of the sensor.
Convention used in the article: yaw/psi (z) - pitch/theta (y) - roll/phi (x) around momentary axes Momentary coordinate systems: W -&amp;gt; W' -&amp;gt; W'' -&amp;gt; B Body acceleration to inertial acceleration W_a = R_WB @ B_a
Body angular rate to inertial angular rate
Each angular rate must be converted to the corresponding frame p: gyro_z -&amp;gt; rotated into W: R_w_w' @ R_w'_w'' @ R_w''_B @ q q: gyro_y -&amp;gt; rotated into W': R_w'_w'' @ R_w''_B @ q r: gyro_x -&amp;gt; rotated into W'': R_w''_B @ r with Gimbal lock: pitch approaches +-90, terms divided by cos90</description></item><item><title>Importing a fork in Python instead of installed package</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/importing-a-fork-in-python-instead-of-installed-package/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/importing-a-fork-in-python-instead-of-installed-package/</guid><description>http://stackoverflow.com/questions/23075397/python-how-to-edit-an-installed-package
Run this in repo that uses the fork (this installs the package as a submodule): python3 -m pip install -e git+[ssh://git@github-feudalism/feudalism/spatialmath-python.git
egg=f-spatialmath](ssh://git@github-feudalism/feudalism/spatialmath-python.git egg=f-spatialmath) &amp;ndash;upgrade Instructions:
Fork the package repo cd to own repo where you want to use the package Install the fork using the above pip install command. This creates ./src/submodule When making changes to fork: make changes in either the submodule folder (for immediate effect), or in the fork subdirectory + push + reinstall</description></item><item><title>Distal and proximal ends</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/distal-and-proximal-ends/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/distal-and-proximal-ends/</guid><description>Source: Leiner distal: far from the surgeon
proximal: near the surgeon</description></item><item><title>Smooth polymial trajectory generation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/smooth-polymial-trajectory-generation/</link><pubDate>Mon, 19 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/smooth-polymial-trajectory-generation/</guid><description>Source: FLS handouts</description></item><item><title>Symbolic container for Probe</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/symbolic-container-for-probe/</link><pubDate>Sat, 17 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/symbolic-container-for-probe/</guid><description/></item><item><title>KF kinematics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kf-kinematics/</link><pubDate>Fri, 09 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kf-kinematics/</guid><description>Overview of KF states (true, nominal, error) Nominal state kinematics Error state kinematics Old stuff:</description></item><item><title>Program outline</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/program-outline/</link><pubDate>Wed, 07 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/program-outline/</guid><description>Current assumptions (to take care of later!)
Probe is rigid — DOFs are either 0 or constant — switch to rotating scope later No gravity No bias/offset, no noise in IMU Note: Stuff marked with checkboxes are either to-dos or things I&amp;rsquo;m not sure that I implemented correctly
http://github.com/feudalism/dvi-ekf/tree/eskf; projects generate_data.py Data generation (is called from main.py)
Main objects
Generate camera data (from DefSLAM mono trajectory) Make RigidSimpleProbe (for now, all DOFs are 0 or constant) Make IMU object, generate first (om, acc) values from interpolated camera data ( - should generate it from stereo data instead) Variables</description></item><item><title>Modes of operation of the scope</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/modes-of-operation-of-the-scope/</link><pubDate>Wed, 30 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/modes-of-operation-of-the-scope/</guid><description/></item><item><title>Differentiation in different coordinate systems</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/differentiation-in-different-coordinate-systems/</link><pubDate>Sun, 27 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/differentiation-in-different-coordinate-systems/</guid><description>Source: Woernle Mehrkörpersysteme Backlinks: Kinematics primer</description></item><item><title>Update 2021-07-19</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/update-2021-07-19/</link><pubDate>Sun, 20 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/update-2021-07-19/</guid><description>Agenda
Comparison prop only, prop + update Comparison P+U plots (Rp 1000, Rp .01, Rp 1e-6) Changes to equations (pc, qc, err_pc, err_theta_c) s. KF kinematics Currently: getting probe output as function of DOFs Open tasks s. also dvi-eskf project board debug update stage??? get probe outputs as symbols/functions of DOFs switch from rigid probe to rotating scope - at which point do I compensate for notch rotation?</description></item><item><title>Camera views as seen by SLAM at distal end of probe/scope</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/camera-views-as-seen-by-slam-at-distal-end-of-probe-scope/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/camera-views-as-seen-by-slam-at-distal-end-of-probe-scope/</guid><description/></item><item><title>Endoscope tip</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/endoscope-tip/</link><pubDate>Mon, 14 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/endoscope-tip/</guid><description>Source: &amp;lt;http://www.osapublishing.org/ao/fulltext.cfm?uri=ao-43-1-113&amp;amp;id=78236
F2&amp;gt; Note: this is a mini endoscope, probably not the standard construction</description></item><item><title>Equations for obtaining omega (angular velocity) and acceleration of IMU from camera</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/equations-for-obtaining-omega-angular-velocity-and-acceleration-of-imu-from-camera/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/equations-for-obtaining-omega-angular-velocity-and-acceleration-of-imu-from-camera/</guid><description>Parent: Update 2021-06-11</description></item><item><title>Notch positions due to scope rotation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/notch-positions-due-to-scope-rotation/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/notch-positions-due-to-scope-rotation/</guid><description>Backlinks: Update 2021-06-11</description></item><item><title>Obtaining IMU measurements from camera by forward kinematics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/obtaining-imu-measurements-from-camera-by-forward-kinematics/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/obtaining-imu-measurements-from-camera-by-forward-kinematics/</guid><description>Parent: SA TODO Backlinks: Thesis restructure Done:
reverse-fwkin (scrapped) omega_B symbolic check links in BC and CB config — new diagrams (split up into &amp;gt;=2 bodies?) check om_B = om_C + om_CB (s. [http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters Kinematics](http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters Kinematics), Woernle ) save om_B to container obtain accel. (s. [http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters Kinematics](http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters Kinematics), Kinematics primer ) where (ang. vel of body j w.r.t. body i, expressed in CS k)  Chaining velocities and accelerations: [validation] reconstruct rot_B from om_B, compare with camera debug first reconstruction of IMU traj, if that doesn&amp;rsquo;t work debug the fake data generation - update readme update robot model with simplification around pivot point validate updated model Anhang</description></item><item><title>Update 2021-06-11</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/update-2021-06-11/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/update-2021-06-11/</guid><description>General model for probe (from Forward kinematics IMU to camera )
Using the standard DH convention  Note: I have since changed the axis configuration at the camera part—above diagram is no longer up to date; to be updated!
This has the IMU (B) as the base and the camera (C) as the end effector Using robotics-toolbox-python: http://github.com/petercorke/robotics-toolbox-python Simplified model (from Forward kinematics IMU to camera ) Currently using a simplified model with all degrees of freedom set to 0 or constant Modification to (above) existing robot model Probably need to modify the rotational joints around the pivot s.</description></item><item><title>Rigid cystoscope mechanism</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/rigid-cystoscope-mechanism/</link><pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/rigid-cystoscope-mechanism/</guid><description>Parent: Update 2021-06-11</description></item><item><title>Discussion 2021-06-01</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-06-01/</link><pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-06-01/</guid><description>Notes
IMU data to be generated using kinematic relations, not via numerical differentiation Reduce loss of data, model for prediction, noise propagation Forward kinematics B &amp;ndash;&amp;gt; C (everything in terms of SLAM coordinates), s. Probe forward kinematics For visualisation: IMU data in W coordinates Monday: real probe Python robotics toolboxes for generating of forward kinematics matrices, velocity expressions (symbolic differentiation) Predict step
Kinematics ( equations of motion IMU to camera ) = f(DOF) p_BC = f(l1, l2) v_BC = f(ang_vel) a_BC = f(acc)</description></item><item><title>Equations of motion IMU to camera</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kinematics-equations-of-motion-imu-to-camera/</link><pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kinematics-equations-of-motion-imu-to-camera/</guid><description>Backlinks: Discussion 2021-06-01 R_WB = R_WC * R_CB
Notes: ref http://docs.sympy.org/latest/modules/physics/vector/vectors.html for vector calculus (symbolic)</description></item><item><title>Forward kinematics IMU to camera</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/forward-kinematics-imu-to-camera/</link><pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/forward-kinematics-imu-to-camera/</guid><description>Backlinks: [Discussion 2021-05-21](discussion 2021-05-21.md), discussion-2021-06-01 , update-2021-06-11 Simplified model (rigid)</description></item><item><title>Inverse of a homogeneous transformation matrix</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/inverse-of-a-homogeneous-transformation-matrix/</link><pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/inverse-of-a-homogeneous-transformation-matrix/</guid><description>Parent: Kinematics primer Source: http://mathematica.stackexchange.com/questions/106257/how-do-i-get-the-inverse-of-a-homogeneous-transformation-matrix</description></item><item><title>Jeon 2009 Kinematic Kalman Filter for Robot End-Effector Sensing</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/jeon-2009-kinematic-kalman-filter-for-robot-end-effector-sensing/</link><pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/jeon-2009-kinematic-kalman-filter-for-robot-end-effector-sensing/</guid><description>Backlinks: Discussion 2021-05-25 Authors: Jeon and Tomizuka
Abstract
inaccuracies in estimation of EE motion can come from kinematic error (error in parameters in kinematic equations)
to overcome this: take direct measurements e.g. using vision, but vision has high latency IMUs are used to provide interframe data fuse camera and IMU in a kinematic Kalman filter (KKF) framework. Note: uses ESKF
effect of camera measurement delay, augmenting the KF states to also estimate the time delay</description></item><item><title>Discussion 2021-05-25</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-05-25/</link><pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-05-25/</guid><description>Backlinks: Discussion 2021-05-10 , discussion-2021-05-21 Notes
IMU-rod transformation: rotation part (spherical joint), translation part predict and update equations? maybe change variables in states vector to local coordinates add gravity later Ausblick: Einfluss der IMU auf verbesserte Lokalisierung &amp;ndash;&amp;gt; evtl eine IMU Koordinate weglassen Next:
generate fake imu data (delegated, s. [obtaining imu measurements from camera by forward kinematics](obtaining imu measurements-from-camera-by-forward-kinematics.md)) look for existing literature on IMU fusion/EKF which uses kinematic relations Massenmatrix, Koriolisterme etc.</description></item><item><title>Modified vs original DH</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/modified-vs-original-dh/</link><pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/modified-vs-original-dh/</guid><description>Wikipedia
Modified DH (proximal) Original DH (distal?) a: offset in x (from old origin)alpha: twist of z around old x axisd: offset in z (to next origin)theta: rotation around current z d: offset in z (from prev origin)theta: rotation around prev zr / a: offset in x from prev originalp: twist of z around current x</description></item><item><title>Spherical wrist</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/spherical-wrist/</link><pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/spherical-wrist/</guid><description>http://www1.cs.columbia.edu/~allen/F15/NOTES/forwardspong.pdf
S. also http://www.youtube.com/watch?v=S6TFakW5YcI</description></item><item><title>(Hibbeler) Dynamics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/hibbeler-dynamics/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/hibbeler-dynamics/</guid><description>Author: Russell Hibbeler Contents
Kinematics, kinetics of particle [planar] rigid body [3D] rigid body Vibrations Kinematics primer</description></item><item><title>(Woernle) Mehrkoerpersysteme</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/woernle-mehrkoerpersysteme/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/woernle-mehrkoerpersysteme/</guid><description>Author: Christoph Woernle Contents:
Kinematics, kinetics (Dynamik) Some basics Converting velocity from CS1 to CS0 Chaining rotation matrices and angular velocities [Poisson equation for skew symmetric matrix of angular velocity](poisson equation for skew-symmetric-matrix-of-angular-velocity.md) Differentiation in different coordinate systems Kinematics primer Reversed kinematics relations Rotations as xyz Bryan-Tait angles (Kardanwinkel) Holonomic systems, non-holonomic systems Holonomic constraints</description></item><item><title>Chaining rotation matrices and angular velocities</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/chaining-rotation-matrices-and-angular-velocities/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/chaining-rotation-matrices-and-angular-velocities/</guid><description>Source: Woernle Mehrkörpersysteme Backlinks: [Kinematics primer](kinematics primer.md), [obtaining imu measurements from camera by forward kinematics](obtaining imu measurements-from-camera-by-forward-kinematics.md) See also: Rotations / SO(3) group index Chaining rotation matrices T_02 transforms a point in CS2 to CS0
compare: Chaining homogeneous transformation matrices Chaining angular velocities (in same CS) ang.vel. of 2 rel to 0 = ang.vel. of 1 rel. to 0 + ang.vel. of 2 rel. to 1</description></item><item><title>Converting velocity from CS1 to CS0</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/converting-velocity-from-cs1-to-cs0/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/converting-velocity-from-cs1-to-cs0/</guid><description>Source: Woernle Mehrkörpersysteme Backlinks: [Kinematics primer](kinematics primer.md), [poisson equation for skew symmetric matrix of angular velocity](poisson equation for skew-symmetric-matrix-of-angular-velocity.md)
Linear velocity This is the derivative of r relative to CS0, as depicted in CS0 coordinates
Where the expression in square brackets means: the derivative of r relative to CS0, as depicted in CS1 coordinates
Angular velocity with : ang.vel. of D relative to B, given in E coordinates
Note : </description></item><item><title>Kinematics primer</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kinematics-primer/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kinematics-primer/</guid><description>Source: Hibbeler Dynamics , woernle-mehrkörpersysteme See also: Reversed kinematics relations , denavit-hartenberg-convention Backlinks: [Obtaining IMU measurements from camera by forward kinematics](obtaining imu measurements from camera by forward kinematics.md), rotations / so(3) group-index Prereqs:
Chaining rotation matrices and angular velocities Converting velocity from CS1 to CS0 [Poisson equation for skew symmetric matrix of angular velocity](poisson equation for skew-symmetric-matrix-of-angular-velocity.md) Inverse of a homogeneous transformation matrix Differentiation in different coordinate systems Position (in world coordinates)velocity (in world coordinates)where the skew symmetric matrix is the angular velocity of cs1 relative to cs0,(~~given in cs1 coordinates?</description></item><item><title>Modified Denavit-Hartenberg convention</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/modified-denavit-hartenberg-convention/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/modified-denavit-hartenberg-convention/</guid><description>Source: Craig - Introduction to Robotics Backlinks: Kinematics primer Note: s. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1083.6428&amp;amp;rep=rep1&amp;amp;type=pdf for comparison (Lipkin)
Link-frame attachment
Identify joint axes
For joint axes i and i+1, identify the  common perpendicular + where it meets axis i, or point of intersection and let this be the link-frame origin Let Z_i point along the i-th joint axis
Let X_i
point along common perpendicular, or be normal to the plane containing the two axes Assign Y_i (right hand coordinate system)</description></item><item><title>Poisson equation for skew symmetric matrix of angular velocity</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/poisson-equation-for-skew-symmetric-matrix-of-angular-velocity/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/poisson-equation-for-skew-symmetric-matrix-of-angular-velocity/</guid><description>Source: Woernle Mehrkörpersysteme Backlinks: [Kinematics primer](kinematics primer.md), converting velocity from cs1 to cs0 Skew-symmetric angular velocity: Poisson equation</description></item><item><title>Reversed kinematics relations</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/reversed-kinematics-relations/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/reversed-kinematics-relations/</guid><description>Source: Woernle Mehrkörpersysteme See also: Kinematics primer PositionVelocity(given that omega_00 = 0)(given that v_00 = 0)Accelerationgiven , , </description></item><item><title>(Leiner) Digital Endoscope Design</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/leiner/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/leiner/</guid><description>Backlinks: Endoscopes URL: http://www.spiedigitallibrary.org/ebooks/SL/Digital-Endoscope-Design/1/Digital-Endoscope-Design/10.1117/3.2235283.ch1?SSO=1
Notes Insertion of an endoscope Types of endoscopes Endoscope system components Endoscope specification</description></item><item><title>Discussion 2021-05-21</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-05-21/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-05-21/</guid><description>Notes — Before
Got the ESKF implementation (Solà) to work with my fake IMU data [non-noisy IMU] results look ok for low process noise (trust the prediction more) with relatively high measurement noise [noisy IMU] ok? current assumptions/simplifications: fake data assumes IMU is sitting right on top of camera fake data, as of yet, does not take into account: biases, gravity simplified state vector (no scale estimate, no gravity estimate, no bias estimate etc) TBD: modify equations/states to fit the problem, i.</description></item><item><title>Endoscope/cystoscopy pics/videos</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/endoscope-cystoscopy-pics-videos/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/endoscope-cystoscopy-pics-videos/</guid><description>Backlinks: Discussion 2021-05-21 Rigid endoscope for cystoscopy
Source: http://www.ebay.com/itm/113780645426 Source: http://www.researchgate.net/figure/Intraoperative-image-of-the-rigid-cystoscope-entering-the-bladder-through-the-screw-tip_fig2_322897289 Source: http://www.youtube.com/watch?v=1gEpz9wijoY http://www.maestro-portal.eu/procedure/detail/4 Videos: Semi-Rigid Ureteroscopy and Laser Lithotripsy for Ureter Stones
Source: http://www.medicinenet.com/how_painful_is_a_cystoscopy/article.htm</description></item><item><title>IMU on cystoscope</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/diagram-imu-on-cystoscope/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/diagram-imu-on-cystoscope/</guid><description>Backlinks: Discussion 2021-05-21</description></item><item><title>40.1 IMU measurement model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/40.1-imu-measurement-model/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/40.1-imu-measurement-model/</guid><description>Parent: [IMU index](imu index.md), probabilistic models-for-imu Backlinks: [IMU motion model](imu motion model.md), imu kinematic model using euler integration An IMU measures, relative to an inertial frame, acceleration and rotation rate.
The measurements are corrupted by bias and noise (often assumed to be white Gaussian noise ). mkok-2017 Additionally, the acceleration measured is affected by gravity. Note the [assumptions in modelling the true angular velocity in IMUs](assumptions in modelling the-true-angular-velocity-in-imus.</description></item><item><title>40.1.1 Assumptions in modelling the true angular velocity in IMUs</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/40.1.1-assumptions-in-modelling-the-true-angular-velocity-in-imus/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/40.1.1-assumptions-in-modelling-the-true-angular-velocity-in-imus/</guid><description>Parent: [IMU index](imu index.md), imu-measurement-model Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
For angular velocity, the termshould really be with
negligible Earth rotation = 0 stationary navigation frame, = 0</description></item><item><title>50.3 IMU motion model in a Kalman filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.3-modelling-imu-in-kf/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.3-modelling-imu-in-kf/</guid><description>Parent: IMU index Source: Solà 2017 Quaternion kinematics for ESKF Which states do we use for the motion model?
Choice of states for the IMU motion/kinematics model How do we model the IMU motion?
Choice of model for the IMU motion model The kinematics (true state) can be partitioned into a nominal part and an error part, s. variables in ESKF . The corresponding [nominal state dynamics and error state dynamics](nominal state-dynamics-and-error-state-dynamics.</description></item><item><title>50.3.1 Choice of states for the IMU motion/kinematics model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.3.1-states-for-imu-motion-model/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.3.1-states-for-imu-motion-model/</guid><description>Parent: IMU index See also: Choice of model for the IMU motion model According to MKok 2017 , we can either
Use the full state vector [+] knowledge about sensor motion is included in model [-] large state vector Or the partial state vector, where the inputs are the inertial measurements from the IMU [+] process noise intuitively represents IMU noise. This is useful when we have no knowledge about the motion model.</description></item><item><title>50.3.2 Choice of model for the KF using IMU readings</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.3.2-imu-model-for-kf/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.3.2-imu-model-for-kf/</guid><description>Parent: IMU index , 50.3-modelling-imu-in-kf According to MKok 2017 , here are some models that assume either a constant acceleration or a constant angular velocity:
Constant acceleration model Constant angular velocity model (Notation: angular velocity of the body with respect to world (n), expressed in body CS)
If motion is unknown, there is also the option of modelling the states using random walk equations.</description></item><item><title>50.5.1 IMU nominal-state and error-state kinematics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.5.1-imu-nominal-state-and-error-state-kinematics/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.5.1-imu-nominal-state-and-error-state-kinematics/</guid><description>Parents: [IMU index](imu index.md), 50.3 error-state-kalman-filter Note on discretisation Solà 2017 :
Convert the differential equations to difference equations (use integration) Integration methods may vary Closed form solutions Numerical integration Integration is done for: The nominal state The error state (deterministic part): error state dynamics and control The error state (stochastic part): noise and perturbations Nominal state Error state Model without noise and perturbations Continuous Discrete summary:with the jacobians defined in imu eskf-prediction-equations</description></item><item><title>50.5.1.1 States of the ESKF for estimating IMU pose</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.5.1.1-states-of-the-eskf-for-estimating-imu-pose/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.5.1.1-states-of-the-eskf-for-estimating-imu-pose/</guid><description>Parent: IMU index Source: Solà 2017 Quaternion kinematics for ESKF Full state Vector with 19 elements The corresponding kinematics equations/motion model is given in IMU kinematic equations/motion model .
Notes The angular error in 3D space is given by the notation $\delta\mathbf{\theta}$.
(s. rotation-error-representation )
The angular error $\delta\mathbf{\theta}$ is defined locally w.r.t. the nominal orientation (classical approach used in most IMU-integration works).
A more optimal approach may be to use a globally-defined angular error.</description></item><item><title>50.5.1.2 The initial gravity vector/orientation for the IMU ESKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.5.1.2-the-initial-gravity-vector-orientation-for-the-imu-eskf/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.5.1.2-the-initial-gravity-vector-orientation-for-the-imu-eskf/</guid><description>Parent: [IMU index](imu index.md), [choice of model for the imu motion model](choice of model-for-the-imu-motion-model.md)
Notes on the initial gravity vector/orientation for the IMU ESKF Solà 2017 For simplicity, it is assumed that  The gravity vector g is estimated in terms of frame q0
This puts the initial uncertainty on the gravity direction, rather than on the initial orientation.
Doing this improves linearity, because now the equation is linear in g and the initiial rotation R0 has no uncertainty</description></item><item><title>50.6 ESKF prediction equations</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.6-eskf-prediction-equations/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.6-eskf-prediction-equations/</guid><description>Parents: [IMU index](imu index.md), 50.3 error-state-kalman-filter Source: Solà 2017 Quaternion kinematics for ESKF Error state system equation becomes: where (s. IMU nominal-state and error-state kinematics for an overview of the nonlinear kinematics equations)
State propagation (without considering noise) — produces a state estimate (a priori) Note: this always returns zero as the mean of the error initialises to zero!
Covariance propagation (considers noise); a priori estimate with the Jacobians (transition matrix approximated using first order Euler, more precise methods are available)</description></item><item><title>50.7 ESKF update / Fusing IMU with complementary sensory data</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.7-eskf-update-fusing-imu-with-complementary-sensory-data/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.7-eskf-update-fusing-imu-with-complementary-sensory-data/</guid><description>Parent: [IMU index](imu index.md), 50.5-error-state-kalman-filter Source: Solà 2017 Quaternion kinematics for ESKF In the ESKF, the arrival of non-IMU sensor data triggers a correction stage. This correction makes the IMU biases observable , allows correct estimation of the biases The correction stage is three-fold:
observe the error state by way of filter correction &amp;lsquo;add&amp;rsquo; the observed errors to the nominal state to get the supposed &amp;lsquo;true&amp;rsquo; state according to the composition rules in variables in ESKF using IMUs reset the error state Source: Markley 2014 What if several measurements come in without IMU / propagation in between (i.</description></item><item><title>50.7.1 Observation of the error state (filter correction)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.1-observation-of-the-error-state-filter-correction/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.1-observation-of-the-error-state-filter-correction/</guid><description>Parents: 50.3 Error-State Kalman Filter , 50.5 eskf update / fusing imu with complementary sensory data Source: Solà 2017 Quaternion kinematics for ESKF Given is a non-IMU sensor with the measurement function [ Solà , Markley ] $$ \mathbf{y} = h(\mathbf{x}_t) + v $$
where $\mathbf{x}_t$ is the true state and $v$ is a white Gaussian noise $$ v \sim \mathcal{N}\left\lbrace 0, \mathbf{V}\right\rbrace $$
Source: Markley 2014 If the measurements are given in quaternion form:</description></item><item><title>50.7.1.1 H Jacobian matrix in the ESKF filter correction</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.1.1-h-jacobian-matrix-in-the-eskf-filter-correction/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.1.1-h-jacobian-matrix-in-the-eskf-filter-correction/</guid><description>Parent: Filter correction , eskf-update Source: Solà 2017 Quaternion kinematics for ESKF Evaluation of the H Jacobian
In the prediction stage, the filter estimates the error state. Therefore, the Jacobian H needs to be defined w.r.t. the error state , and evaluated at the true state estimate  However, as the error state mean is zero (not yet observed), the true state is approximated to the nominal state  Thus we can use the nominal state as the evaluation point The first Jacobian Depends on the sensor&amp;rsquo;s particular measurement function The second Jacobian with Source: Markley 2014 Measurement sensitivity matrix (Jacobian w.</description></item><item><title>50.7.3 ESKF reset</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.3-eskf-reset/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.3-eskf-reset/</guid><description>Parent: Fusing IMU with complementary sensory data Backlinks: 50.3 Error-State Kalman Filter Source: Markley moves the rotation error to the global rotation this keeps the rotation error small and far from any singularities To update the global state, the reset has to obey The reset has to preserve the quaternion norm, therefore an exact unit norm expression must be used, instead of an approximation. Using the Rodrigues parameter , the reset becomes which leads to a two step update (1.</description></item><item><title>50.5 Error-State Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.5-error-state-kalman-filter/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.5-error-state-kalman-filter/</guid><description>Source: Markley An EKF propagates the expectation and covariance of the state The MEKF propagates the expectation and the covariance of the error state Source: Whampsey MEKF Previously: orientation is represented by one state Now: orientation is split up into  a large signal q_nom (nominal orientation) and a small signal (perturbation angle alpha) &amp;ndash; parametrises an error quaternion  This reformulates the error in terms of the group operation and so maintains the rotation invariance (rotation preserves the origin, length, angle between two vectors, orientation, etc.</description></item><item><title>Inverse quaternion</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/inverse-quaternion/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/inverse-quaternion/</guid><description>Parent: Quaternion index Source: Solà 2017 Quaternion kinematics for ESKF The inverse is the conjugate in case of unit quaternions</description></item><item><title>Quaternion conventions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-conventions/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-conventions/</guid><description>Parent: Quaternion index Source: Solà 2017 Quaternion kinematics for ESKF Source: [ Wikipedia ], markley-2014 For quaternion multiplication : change the order to transform between conventions Hamilton Shuster Transpose of the Hamiltonian version</description></item><item><title>Quaternion differentiation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-differentiation/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-differentiation/</guid><description>Parent: Quaternion index Source: J. D. Hol — Sensor fusion and calibration of inertial sensors, vision, ultra-wideband and GPS
Using the identities: http://math.stackexchange.com/questions/189185/quaternion-differentiation Numerical differentiation (Euler)
http://math.stackexchange.com/questions/1896379/how-to-use-the-quaternion-derivative
$$ \begin{aligned} q(t+dt) &amp;amp;= q(t) \otimes dq\
\dfrac{dq}{dt} &amp;amp;= \frac{1}{2} \omega \otimes q \quad \text{with } \omega = \left[ \begin{array}{cccc} 0 &amp;amp; \omega_x &amp;amp; \omega_y &amp;amp; \omega_z \end{array} \right]^\text{T} \end{aligned}$$
Integrating this, assuming $\omega=\text{const.}$ from $t_0$ to $t_0 + dt$: $$ \begin{aligned} q(t) &amp;amp;= q(t_0) \exp \left( \frac{1}{2} \omega \cdot \left( t - t_0\right) \right)\</description></item><item><title>Quaternion norm</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-norm/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-norm/</guid><description>Parent: Quaternion index Source: Solà 2017 Quaternion kinematics for ESKF With the property</description></item><item><title>Solà 2017 Quaternion kinematics for ESKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sol%C3%A0-2017-quaternion-kinematics-for-eskf/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sol%C3%A0-2017-quaternion-kinematics-for-eskf/</guid><description>Link: http://www.iri.upc.edu/people/jsola/JoanSola/objectes/notes/kinematics.pdf
Author: Joan Solà
Abstract Primer on quaternion/rotation group math Math for error state Kalman filters using IMUs Contents/Chapters Unit quaternions , double cover Rotations, s. also SO(3) 3D rotation group Quaternion conventions Perturbations, derivatives, integrals Error-State Kalman Filter for IMU-driven systems Variables in ESKF IMU measurement model IMU motion model The initial gravity vector/orientation for the IMU ESKF IMU nominal-state and error-state kinematics IMU ESKF prediction equations Fusing IMU + other sensors ESKF using global angular errors</description></item><item><title>Cyril Stachniss EKF-SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cyril-stachniss-ekf-slam/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cyril-stachniss-ekf-slam/</guid><description>Links:
Course material: http://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/ Lectures: http://www.youtube.com/playlist?list=PLgnQpQtFTOGQrZ4O5QzbIHgl3b1JHimN_&amp;amp;feature=g-list</description></item><item><title>IMU data generation from camera/visual data</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/imu-data-generation-from-camera-visual-data/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/imu-data-generation-from-camera-visual-data/</guid><description>Parent: IMU index Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)</description></item><item><title>IMU to camera coordinate transformations</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/imu-to-camera-coordinate-transformations/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/imu-to-camera-coordinate-transformations/</guid><description>Parent: IMU index Source: Weiss 2011</description></item><item><title>Solà 2014 SLAM with EKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sol%C3%A0-2014-slam-with-ekf/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sol%C3%A0-2014-slam-with-ekf/</guid><description> Notes on EKF-SLAM that uses landmarks MATLAB code Notes on partial landmark initialisation (convariance matrix) Notes on the linearity of the observation function in scale</description></item><item><title>Trocar</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/trocar/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/trocar/</guid><description>Source: https://en.wikipedia.org/wiki/Trocar
Here: surgical trocar
Used in [laparoscopic surgery](laparoscopic surgery.md) A medical device which is used to make small incisions and allows insertion of other surgical instruments into the body cavity Source: [Leiner Digital Endoscope Design](Leiner Digital Endoscope Design.md) In an arthroscopic procedure (knee?):
trocar is inserted into the cannula both are pushed through the skin trocar is replaced by obturator (blunt rod) to open the area up.</description></item><item><title>Discussion 2021-05-10</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-05-10/</link><pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-05-10/</guid><description>Agenda
Change/Reduction of scope of SA (from fusing IMU with camera) to using sensor fusion to determine transformation parameters between IMU and camera Camera and IMU setup involves kinematic modelling (not fixed transformation as previously assumed!) Offline implementation in Python/MATLAB (scripting language) HiWi tasks can include DefSLAM bindings / interface C++ bindings of skrogh EKF implementation? HiWi prioritises Versuchsstand for now Tasks
Find an EKF implementation that works well and can be used with DefSLAM + IMU data implement kinematic model equations in the prediction-step, s.</description></item><item><title>Quaternion conjugate</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-conjugate/</link><pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-conjugate/</guid><description>Parent: Quaternion index Source: http://en.wikipedia.org/wiki/Quaternion
Flip signs of vector part
Source: Solà 2017 Quaternion kinematics for ESKF Multiplying with own conjugate (scalar!)
Conjugate operation on quaternion products</description></item><item><title>Transforming velocities to another frame</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/transforming-velocities-to-another-frame/</link><pubDate>Mon, 03 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/transforming-velocities-to-another-frame/</guid><description>http://physics.stackexchange.com/questions/197009/transform-velocities-from-one-frame-to-an-other-within-a-rigid-body
Transforming velocities to another frame Further reading: http://core.ac.uk/download/pdf/154240607.pdf</description></item><item><title>(Weiss Thesis) Vision based navigation for micro helicopters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/weiss-thesis-vision-based-navigation-for-micro-helicopters/</link><pubDate>Sun, 25 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/weiss-thesis-vision-based-navigation-for-micro-helicopters/</guid><description>Source Backlinks
Authors Stephan Weiss Abstract
Issues that arise during state estimation and sensor self-calibration Application area: large and unknown areas, micro helicopter Vision based method used uses SfM, is compares mapless and map-based methods Statistical and modular sensor fusion strategy recovery of pose and drifts modular: camera as a black box sensor, allows other sensors additionally Observability analysis Literature review
Fusing IMU with monocular vision: given extrinsic parameters , an IMU is able to recover metric scale, as well as help transition across short vision failure period Armesto et al.</description></item><item><title>IMU states, dynamics equations</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/imu-states-dynamics-equations/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/imu-states-dynamics-equations/</guid><description>Parent: IMU index Source: Mur-Artal 2017 VI-ORB Evolution of IMU states (world frame to IMU: orientation R, position p, velocity v) between consecutive keyframes
Evolution of IMU states (world frame to IMU: orientation R, position p, velocity v) between consecutive frames
Using the preintegration terms Preintegration (delta) terms and the Jacobians can be computed iteratively as IMU measurements arrive (s. Forster&amp;rsquo;s paper on preintegration)</description></item><item><title>Modelling noise and bias for IMU</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/modelling-noise-and-bias-for-imu/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/modelling-noise-and-bias-for-imu/</guid><description>Parent: [IMU index](imu index.md), imu-measurement-model Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
Modelling the noise The noise not only represents measurement noise, but also model uncertainty.
With proper calibration, the three gyroscope axes are independent: Same for accelerometer — assume diagonal for a properly calibrated sensor
Modelling the biases — two approaches
treat bias as constant (due to short experiment times) pre-calibrate in a separate experiment, or make part of the parameters vector treat as slowly time-varying (due to long experiment times or shorter bias stability) make the bias part of the state vector model the bias as a random walk</description></item><item><title>OpenCV Kalman filter pre/post states</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/opencv-kalman-filter-pre-post-states/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/opencv-kalman-filter-pre-post-states/</guid><description/></item><item><title>Besprechung 2021-04-01</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/besprechung-2021-04-01/</link><pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/besprechung-2021-04-01/</guid><description>Agenda
Recap: last meeting (2021-03-15) Offline Kalman — after this works, do a &amp;lsquo;live&amp;rsquo; implementation on DefSLAM run
IMU measurements as [acc, gyro] readings
With noise, but without considering bias, IMU-cam transformation, gravity Two sets of measurements for filter: IMU measurements, DefSLAM (camera) measurements
KF prediction using random walk
Recap: SLAM +filtering terminology (loose coupling, tight coupling) Literature Original suggestion using random walk model Interface (DefSLAM, Python) read: OK write: to do Offline Kalman as a separate repo Good papers?</description></item><item><title>Discretising a state space equation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/discretising-a-state-space-equation/</link><pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/discretising-a-state-space-equation/</guid><description>Source: [http://en.wikibooks.org/wiki/Control_Systems/State-Space_Equations
Discretization](http://en.wikibooks.org/wiki/Control_Systems/State-Space_Equations Discretization) Discretising a state space equation</description></item><item><title>note quaternions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/note-quaternions/</link><pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/note-quaternions/</guid><description>Converting from quaternion to angular velocity then back to quaternion http://math.stackexchange.com/questions/2282938/converting-from-quaternion-to-angular-velocity-then-back-to-quaternion</description></item><item><title>resource IMU common specifications, error models etc</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/resource-imu-common-specifications-error-models-etc/</link><pubDate>Sat, 27 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/resource-imu-common-specifications-error-models-etc/</guid><description>Parent: IMU Source: http://www.vectornav.com/resources/imu-specifications
IMU common specifications, bias, scale factor, orthogonality errors, and acceleration sensitivity for gyroscopes.
Source: Woodman - An introduction to inertial navigation Source: Quinchia - A Comparison between Different Error Modeling of MEMS Applied to GPS/INS Integrated Systems
3.2. State-Space Representation for Different Bias Models
First order Gauss-Markov (GM) Random walk Autoregressive process</description></item><item><title>Markov assumption</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/markov-assumption/</link><pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/markov-assumption/</guid><description>Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md) Backlinks: [Grisetti 2011 - Tutorial graph-based SLAM](grisetti 2011 - tutorial graph-based slam.md), probabilistic models-for-imu Models with state x which have the Markov property:
all information up till time t is contained in xt enables marginalisation of state xt at time t+1</description></item><item><title>note KF with missing measurements</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/note-kf-with-missing-measurements/</link><pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/note-kf-with-missing-measurements/</guid><description>Sources http://math.stackexchange.com/questions/982982/kalman-filter-with-missing-measurement-inputs http://opencv-users.1802565.n2.nabble.com/Kalman-filters-and-missing-measurements-td2886593.html
For a missing measurement:
use the last state estimate as a measurement set the covariance matrix of the measurement to essentially infinity. This would cause a Kalman filter to essentially ignore the new measurement since the ratio of the variance of the prediction to the measurement is zero. The result will be a new prediction that maintains velocity/acceleration but whose variance will grow according to the process noise.</description></item><item><title>note KF with different sampling rate</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/note-kf-with-different-sampling-rate/</link><pubDate>Wed, 24 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/note-kf-with-different-sampling-rate/</guid><description>Source: http://stackoverflow.com/questions/59566384/kalman-filter-with-different-sampling-rate
Approach 1: KF with variable dt Approach 2: KF with static dt
&amp;lsquo;Sub&amp;rsquo; updates? e.g.
predict() update() with sensor A skip update() for sensor B since no measurement arrived update() with sensor c repeat Generally discouraged:
If not predicting before each update, there is the risk of the filter lagging behind real world dynamics. The update step at t=k compares a measurement zk to the projected (predicted) state xk.</description></item><item><title>IMU motion model (discrete)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/imu-motion-model-discrete/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/imu-motion-model-discrete/</guid><description>Parent: [IMU index](imu index.md), probabilistic models-for-imu Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
Position dynamics Orientation dynamics (either quaternion or rotation matrix representation) with</description></item><item><title>Linearisation of an orientation in SO(3)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/linearisation-of-an-orientation-in-so-3/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/linearisation-of-an-orientation-in-so-3/</guid><description>Parents: Rotations / SO(3) group index , Quaternion index , orientation-parametrisations Source: MKok 2017 Rotation of a vector in SO(3)
The SO(3) group is a Lie group , so there exists an exponential map from a corresponding Lie algebra to the SO(3) group a reverse logarithm map Possible to represent orientations using unit quaternions or rotation matrices in SO(3) — linearisation point orientation deviations $\eta_t$ I think this is a global representation</description></item><item><title>MKok 2017 Using inertial sensors for position and orientation estimation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mkok-2017/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mkok-2017/</guid><description>Source: http://arxiv.org/abs/1704.06053 Authors: M Kok, JD Hol, TB Schön
Abstract
Contents/Chapters Quaternions Probabilistic models for IMU Orientation parametrisations Which orientation parametrisation to choose? Linearisation of an orientation in SO(3) IMU measurement model Modelling noise and bias for IMU IMU motion models IMU prior models</description></item><item><title>Orientation parametrisations</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/orientation-parametrisations/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/orientation-parametrisations/</guid><description>Parents: rotations-so3-group-index , quaternion index , probabilistic models-for-imu See also: Rotation error representation Source: MKok 2017 , markley-2014 Orientation parametrisations
Note: CCW rotation of a vector $x_v$ to $x_u$ corresponds to a CW rotation of the CS $v$ to CS $u$ (see also active/passive transformations ). Rotations are a member of SO(3) rotation matrix unique description of orientation Euler axis/angle Rotation vector not unique, due to wrapping Euler angles not unique, due to wrapping and gimbal lock Unit quaternions not unique, -q and q depict the same orientationProof: http://math.</description></item><item><title>Probabilistic models for IMU</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/probabilistic-models-for-imu/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/probabilistic-models-for-imu/</guid><description>Parent: IMU index Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
Three main components to the probabilistic models
IMU measurement model (infer knowledge about pose from measurements)  Prediction model (how sensor pose changes over time) Models of the initial pose (prior) Knowledge we are interested in: pose of the sensor
time-varying variables: states  constants: parameters  Knowledge available to us: sensor dynamics, available sensor measurements Conditional probability distribution</description></item><item><title>Besprechung 2021-03-15</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/besprechung-2021-03-15/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/besprechung-2021-03-15/</guid><description>Status
Last week: generated noisy IMU data (pose) from stereo trajectory, &amp;lsquo;offline&amp;rsquo; Kalman This week: Kalman + &amp;lsquo;live&amp;rsquo; DefSLAM Still to do: design KF (EKF, or other methods&amp;hellip;) &amp;lsquo;Offline&amp;rsquo; Kalman
To learn how to use openCV&amp;rsquo;s Kalman filter without having to rebuild DefSLAM every time Aim was to figure out the update/correction workflow and implement it in live DefSLAM run
Uses pre-extracted trajectories (mono and stereo)</description></item><item><title>On quaternions and rotation matrices</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/on-quaternions-and-rotation-matrices/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/on-quaternions-and-rotation-matrices/</guid><description>http://stackoverflow.com/questions/8919086/why-are-quaternions-used-for-rotations
It&amp;rsquo;s worth bearing in mind that all the properties related to rotation are not truly properties of Quaternions: they&amp;rsquo;re properties of Euler-Rodrigues Parameterisations, which is the actual 4-element structure used to describe a 3D rotation. Their relationship to Quaternions is purely due to a paper by Cayley, &amp;ldquo;On certain results related to Quaternions&amp;rdquo;, where the author observes the correlation between Quaternion multiplication and combination of Euler-Rodrigues parameterisations. This enabled aspects of Quaternion theory to be applied to the representation of rotations and especially to interpolating between them.</description></item><item><title>vi.cc using kalman for xyz states (what goes on with the map?)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/vi.cc-using-kalman-for-xyz-states/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/vi.cc-using-kalman-for-xyz-states/</guid><description>Offline Kalman Kalman and live DefSLAM</description></item><item><title>Besprechung 2021-03-08</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/besprechung-2021-03-08/</link><pubDate>Mon, 08 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/besprechung-2021-03-08/</guid><description>Agenda
DefSLAM + OS3 Up till DefTracking::MonocularInitialization() on hold, working on Kalman stuff for the time being DefSLAM + Kalman new plot (monocular trajectory without any pose updating) to do: use noisy stereo data, plug into update step while discarding images functions in System.cc: read data, update pose DefSLAM + sockets Meeting notes:
next step: implement the Kalman filter. When that is done, discuss next steps e.</description></item><item><title>System::forceTrajectory</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/system-forcetrajectory/</link><pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/system-forcetrajectory/</guid><description>Parent: DefSLAM branch overview Reference: DefSLAMGT (stereo as ground truth) For testing: DefSLAMVI
Description Force update of DefSLAMVI&amp;rsquo;s current frame pose to that of DefSLAMGT&amp;rsquo;s for the frames 230 to 239
Without System::Reset Frame pose is &amp;lsquo;updated&amp;rsquo; during the interval, but after the interval, the optimisation (which uses frame pose as an estimate and also uses map node positions) makes the system resume it&amp;rsquo;s trajectory before the update
(below: with pure monocular trajectory, without any forced updates) With System::Reset The system is reset after every forced pose update (i.</description></item><item><title>World to camera trafo</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/world-to-camera-trafo/</link><pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/world-to-camera-trafo/</guid><description>Parent: SLAM Index See also: [Pinhole camera model](pinhole camera model.md), pinhole camera-projection-function Source: http://www.cse.psu.edu/~rtc12/CSE486/lecture12.pdf
Camera coordinates (X, Y, Z) World coordinates (U, V, W) Image plane (x, y) / Pixel coordinates (u, v) Forward projection
Representing 2D point as a fictitious 3D point (x', y', z') [for matrix calculations] Convention: Given (x', y', z'), we can recover the 2D point (x, y) as World to camera trafo</description></item><item><title>DefSLAM branch overview</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-branch-overview/</link><pubDate>Fri, 19 Feb 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-branch-overview/</guid><description>Parent: SA TODO Repo http://github.com/feudalism/DefSLAM
Dormant
master sa Deprecated
windows - deprecated, changes made for building on Windows imu - deprecated, has Imu tracking functions but dependencies not resolved obs_tuple - initial attempt to incorporate Atlas, attempt to use OS3&amp;rsquo;s structure for MapPoint observations : &amp;lt;KeyFrame, tuple&amp;lt;int, int&amp;raquo; as opposed to &amp;lt;Keyframe, int&amp;gt; in DefSLAM+OS2 Temporary/Experimental
s. to do list
debugging the segfault that seemingly appears in Surface::getNormalSurfacePoint seems to happen after System reset</description></item><item><title>ORBSLAM2 unofficial documentation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam2-unofficial-documentation/</link><pubDate>Wed, 17 Feb 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam2-unofficial-documentation/</guid><description>Partially done, abandonned: http://github.com/raulmur/ORB_SLAM2/compare/master&amp;hellip;AlejandroSilvestri:master In Spanish: http://alejandrosilvestri.github.io/os1/doc/html/</description></item><item><title>Viewer segfault</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/viewer-segfault/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/viewer-segfault/</guid><description>Error [New Thread 0x7fff86ffd700 (LWP 1117)] NORMALS REESTIMATED : 277 - 277 [Thread 0x7fff86ffd700 (LWP 1117) exited] NORMAL ESTIMATOR OUTPoints potential : 293 70 New template requested Number Of normals 277 0x555566923da0 -0.79956 0.655022 -0.594482POINTS matched:167 Points Scale Error Keyframe : 1 stan dev 0.310974 chi 0.013115 0.01 201 SurfaceRegistration not sucessful (Not enough points to align or chi2 too big
Thread 6 &amp;ldquo;DefSLAM&amp;rdquo; received signal SIGSEGV, Segmentation fault. [Switching to Thread 0x7fffc1996700 (LWP 275)] __memmove_avx_unaligned_erms () at .</description></item><item><title>Segfault in DefTracking (imu branch)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/segfault-in-deftracking-imu-branch/</link><pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/segfault-in-deftracking-imu-branch/</guid><description>/home/user3/slam/datasets/mandala0/images/stereo_im_l_1560936003993.png i: 30 POINTS matched:10 Track lost soon after initialisation, reseting&amp;hellip; /home/user3/slam/datasets/mandala0/images/stereo_im_l_1560936004022.png i: 31 System Reseting NORMAL ESTIMATOR IN - NORMALS REESTIMATED : 0 - 0 NORMAL ESTIMATOR OUTPoints potential : 939 70 New template requested Number Of normals 0 0x5555636b1fb0 Not enough normals Reseting Local Mapper&amp;hellip; done Reseting Loop Closing&amp;hellip; done Reseting Database&amp;hellip; done
Thread 1 &amp;ldquo;DefSLAM&amp;rdquo; received signal SIGSEGV, Segmentation fault. 0x00007ffff78d9fae in cv::Mat::Mat (m=&amp;hellip;, this=0x7ffffffeaea0) at /usr/local/include/opencv4/opencv2/core/mat.inl.hpp:545 545 step[0] = m.</description></item><item><title>DefSLAM and discontinuous areas (classical datasets)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-and-discontinuous-areas-classical-datasets/</link><pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-and-discontinuous-areas-classical-datasets/</guid><description>Parent: Lamarca 2020 DefSLAM Source: http://github.com/UZ-SLAMLab/DefSLAM/issues/1
JoseLamarca: DefSLAM is suitable for rigid areas, proof of that is the abdominal sequence that is kind of rigid. The problem for these sequences is the discontinuous areas. For the monocular case, we are assuming that the surface is smooth that is not usually valid for the classical datasets. Apart from complexity issues that algorithms with RGB-D and stereo cameras could have in those scenes [1] and [2].</description></item><item><title>DefSLAM errors encountered</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-errors-encountered/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-errors-encountered/</guid><description>Rebuilding DefSLAM in Debug mode Error: &amp;ldquo;Virtual memory exhausted: Cannot allocate memory&amp;rdquo; Solution: reduce degree of make -j
Segmentation fault in Defslam debug mode Based on http://stackoverflow.com/questions/19615371/segmentation-fault-due-to-vectors Changed: surfacePoints_[ind] to surfacePoints_.at(ind)
New error surfacePoints_ appears to be NULL? Was it instantiated in another thread? http://stackoverflow.com/questions/11645857/debugging-with-gdb-why-this-0x0
Using core dumps with gdb http://jvns.ca/blog/2018/04/28/debugging-a-segfault-on-linux/</description></item><item><title>extern c++</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/extern-c++/</link><pubDate>Mon, 11 Jan 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/extern-c++/</guid><description>http://en.cppreference.com/w/cpp/language/storage_duration is a storage class specifier that controls storage duration and its linkage.
extern - static or thread storage duration and external linkage
Storage duration
static: storage for the object is allocated when the program begins and deallocated when the program ends. only one instance of the object exists thread storage for the object is allocated when the thread begins and deallocated when the thread ends each thread has its own instance of the object Linkage</description></item><item><title>Pizarro 2016 Schwarps</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/pizarro-2016-schwarps/</link><pubDate>Sun, 20 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/pizarro-2016-schwarps/</guid><description>Author: Daniel Pizarro et al.
Abstract
Warp between two images of a deforming surface: a transformation that depict the geometric deformation between the two &amp;lsquo;maps points between images of a deforming surface&amp;rsquo; Current approach to enforce a warp&amp;rsquo;s smoothness: penalise its second order partial derivatives However this favours locally affine warps Does not capture the local projective component of the image deformation Propose: novel penalty to smooth the warp while capturing the deformation&amp;rsquo;s local projective structure Proposed penalty is based on equivalents to the Schwarzian derivatives Schwarzian derivatives: projective differential invariants exactly preserved by homographies Methodology to derive a set of PDEs with only homographies as the solutions Validation: Schwarps outperform existing warps in modeling and extrapolation power: perform better in deformable reconstruction methods Introduction/Related work</description></item><item><title>DBoW2 weighing and scoring</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/dbow2-weighing-and-scoring/</link><pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/dbow2-weighing-and-scoring/</guid><description>Source: http://github.com/dorian3d/DBow</description></item><item><title>Intro to bladder cancer</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/intro-to-bladder-cancer/</link><pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/intro-to-bladder-cancer/</guid><description>http://www.cancer.net/cancer-types/bladder-cancer/introduction</description></item><item><title>Descriptors in feature detection/extraction</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/descriptors-in-feature-detection-extraction/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/descriptors-in-feature-detection-extraction/</guid><description>Source: http://medium.com/data-breach/introduction-to-feature-detection-and-matching-65e27179885d
Backlinks: Bag of words Descriptors A description of the local appearance around each feature point (keypoint) The descriptor encodes &amp;lsquo;interesting&amp;rsquo; information from the image into numbers and act as an identifier (&amp;lsquo;fingerprint&amp;rsquo;) to differentiate between features The description should ideally be invariant to changes (such as illumination, translation, scale, in-plane rotation) so that the feature can be found again, even if the image is transformed Typically: for each feature point, there is a descriptor vector Classes of descriptors: Local descriptor represents the point&amp;rsquo;s local neighbourhood Global descriptor describes the whole image generally not very robust—changes in parts of the image may cause the descriptor to fail Some algorithms for feature detection/descriptor generation SIFT (scale-invariant feature transform) SURF (speeded up robust feature) BRISK (binary robust invariant scalable keypoints) BRIEF (binary robust independent elementary features) ORB (oriented FAST and rotated BRIEF) Source: http://en.</description></item><item><title>FAST keypoint detector</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/fast-keypoint-detector/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/fast-keypoint-detector/</guid><description>Source: http://medium.com/data-breach/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf Parent: ORB descriptor FAST (Features from Accelerated and Segments Test)
How it works
Given: pixel p, surrounded by other pixels in the image
Take the surrounding pixels that are in a small circle around p If more than half of the surrounding pixels are darker/brighter than p, p is selected as a keypoint
Good for edge detection
Drawbacks</description></item><item><title>Feature matching</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/feature-matching/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/feature-matching/</guid><description>Source: http://medium.com/data-breach/introduction-to-feature-detection-and-matching-65e27179885d Backlinks: Bag of words , sparse/feature-based-vslam For matching between images, i.e. to establish a relationship (&amp;lsquo;correspondence&amp;rsquo;) between two images of the same scene or object.
Basic algorithm
Find/detect a set of identifying (&amp;lsquo;distinctive&amp;rsquo;) keypoints from all images to be matched Define a search region around each keypoint Extract and normalise the region content Compute a local descriptor from the normalised region Match local descriptors between the images Performance of matching methods depend on</description></item><item><title>ORB descriptor</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/orb-descriptor/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/orb-descriptor/</guid><description>Source: http://medium.com/data-breach/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf Backlinks: Descriptors in feature detection/extraction Oriented FAST and Rotated BRIEF, developed 2011 Was developed as an alternative to SIFT and SURF, and ended up being better/faster than both Build on FAST keypoint detector BRIEF descriptor ORB using FAST, but with (partial) scale invariance
Use a multiscale image pyramid
Each level of the pyramid is the same image, but scaled at different resolutions (reduced size as you go higher up) Once the pyramid is computed, FAST is used to detect keypoints  ORB detection</description></item><item><title>Gauss-Newton Method on Manifold</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/gauss-newton-method-on-manifold/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/gauss-newton-method-on-manifold/</guid><description>Source: Forster 2017 IMU Preintegration Standard approach for optimization on manifold
define a retraction to reparametrise the problem (lifting) retraction bijective map map between an element of the tangent space at x and a neighbourhood of x on the manifold i.e. we work in the tangent space (locally like a Euclidian space) and apply standard optimisation techniques for Gauss-Newton specifically: [ts] squared cost around current estimate [ts] solve the quadratic approximation &amp;ndash;&amp;gt; we get vector in tangent space [m] update the current guess on the manifold  Consider: Reparametrised: Retraction for SE(3) The exponential map of SE(3) as a retraction is possible, but may not be convenient (computationally)</description></item><item><title>IMU kinematic model using Euler integration</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/imu-kinematic-model-using-euler-integration/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/imu-kinematic-model-using-euler-integration/</guid><description>Parent: IMU index Source: Forster 2017 IMU Preintegration Backlinks: [IMU preintegration on manifold](imu preintegration on-manifold.md), imu-measurement-model Kinematic model Using Euler integration assuming acc and angVel are constant in the time interval: Using the measurement equations:</description></item><item><title>IMU preintegration on manifold</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/imu-preintegration-on-manifold/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/imu-preintegration-on-manifold/</guid><description>Parent: IMU index Source: Forster 2017 IMU Preintegration Backlinks: IMU model Preintegration on manifold
Summarising all measurements between the keyframes i and j into a single measurement This preintegrated IMU measurement constrains the motion between two consecutive keyframes Assume IMU is synchronised with the camera The above equations already provide the summarised IMU measurements, however, the integration has to be repeated whenever the linearisation point at t=t_i changes i.</description></item><item><title>Manifolds</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/manifolds/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/manifolds/</guid><description>Source: https://www.euclideanspace.com/maths/geometry/space/surfaces/manifold/index.htm
Like a surface in $n$-dimensions (hypersurface) An $n$-dim manifold looks like $\mathbb{R}^n$ locally (locally Euclidian) Circle: 1-dim manifold. If we zoom around a point on the circle, it looks like a line ($\mathbb{R}^1$) Sphere: 2-dim manifold. Zooming onto a point, it looks like a plane ($\mathbb{R}^2$) Source: https://www.seas.upenn.edu/~meam620/slides/kinematicsI.pdf
An $n$-dim manifold is a a set $M$ which is locally homeomorphic to $\mathbb{R}^n$</description></item><item><title>MAP estimation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/map-estimation/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/map-estimation/</guid><description>Source: Forster 2017 IMU Preintegration Factor graph: way of representing the posterior probability of the states given the available measurements and priors The terms in the equation above are called &amp;lsquo;factors&amp;rsquo;
MAP: maximum a posteriori We want to maximise the probability derived above &amp;ndash;&amp;gt; MAP estimate (aka minimum of negative log posterior) The negative log posterior can be written as a sum of squared residuals, assuming zero-mean Gaussian noise residual errors (prior, IMU, camera) covariance matrices How do we define these residuals?</description></item><item><title>Spaces in mathematics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/spaces-in-mathematics/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/spaces-in-mathematics/</guid><description>Source: http://upload.wikimedia.org/wikiversity/en/c/cd/Spaces_in_mathematics.pdf Types of spaces in mathematics
Euclidian spaces (3D space, 2D space/Euclidian plane) Linear spaces Topological spaces Hilbert spaces etc. What is a space?
No real definition Made of selected mathematical objects which are treated as points selected relationships between these points Points can be elements of a set functions subspaces Isomorphic spaces are considered identical Isomorphism between two spaces: one-to-one mapping between the points, that preserves the relationships between the points</description></item><item><title>System in a VIN problem with IMU preintegration</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/system-in-a-vin-problem-with-imu-preintegration/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/system-in-a-vin-problem-with-imu-preintegration/</guid><description>Source: Forster 2017 IMU Preintegration State x_i of the system at time i with All keyframes up till time k State of all keyframes camera measurements IMU measurements between KFs i and j (consecutive) Set of measurements up till time k IMU pose: , maps a point in B to W</description></item><item><title>Non-Rigid Guided Matching (b/w KFs) in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/non-rigid-guided-matching-b-w-kfs-in-defslam/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/non-rigid-guided-matching-b-w-kfs-in-defslam/</guid><description>Source: lamarca-2020 Matching between keyframes (used in deformation mapping in DefSLAM) Use an estimated warp as a reference To increase number of matches in the covisible keyframes Process Matches are given by deformation tracking Estimate an initial warp between k and k* (covisible keyframes) how? Using this initial warp, estimate where a point would be seen in k* Define a search region around thesse estimated positions.</description></item><item><title>Surface alignment in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/surface-alignment-in-defslam/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/surface-alignment-in-defslam/</guid><description>Parent: Mapping step-by-step in DefSLAM Source: lamarca-2020 Goal: to scale the up-to-scale surface (output of NRSfM ) to the proper dimensions get an idea of the proper dimensions from the already estimated map i.e. resulting surface must match the scale of the template $\mathcal{T}_{k-1}$ $\mathcal{T}{k-1}$: deformed map generated by the tracker at the instance of KF $=k$ insertion, with shape-at-rest of $\mathcal{S}{k-1}$ generated from KF: $(k-1)$ result: scale-corrected shape-at-rest $\mathcal{S}_k$ Method: alignment of the map points using Sim(3)</description></item><item><title>Template substitution in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/template-substitution-in-defslam/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/template-substitution-in-defslam/</guid><description>Parent: Mapping step-by-step in DefSLAM Source: lamarca-2020 Tracking runs at frame-rate, and mapping at keyframe-rate Tracking processes Nm frames during a whole mapping run Process New keyframe $k$ is made. Now at time $t=k$ At this point, the template in the tracking is still based on the old shape-at-rest, S_(k-1) Mapping thread starts creates surface S_k which is aligned to prev. template T_(k-1) k is set as the reference keyframe from S_k, create template T_k and from now on use this template instead of the old one T_(k-1) At time t=k+Nm, use data from the tracking thread image points at t=k+Nm deform the recently computed template T_k based on these images use SfT but neglecting the temporal term (to allow large deformation, &amp;ldquo;as a lot might have happened in the time span of Nm&amp;rdquo;) so now we get a T_k that is deformed (updated) to the most recent image points we do this extra step instead of passing T_k (from step 1) to the tracker immediately because, due to the new points occurring at t=k+Nm, using the original T_k might lead to data association errors mapper passes the new template T_k (t=k+Nm) to the tracker</description></item><item><title>The making of EndoSLAM dataset</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/the-making-of-endoslam-dataset/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/the-making-of-endoslam-dataset/</guid><description>http://www.youtube.com/watch?v=G_LCe0aWWdQ Github: http://github.com/CapsuleEndoscope/EndoSLAM</description></item><item><title>Camera calibration</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/camera-calibration/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/camera-calibration/</guid><description>Parent: SLAM Index Source: http://de.mathworks.com/help/vision/ug/camera-calibration.html
estimates lens/sensor parameters e.g. to correct lens distortion, determine position, measurement etc there are several camera models, e.g. fisheye, pinhole Camera parameters
intrinsic extrinsic distortion coefficients How to solve for camera parameters?
Need to have 3D world points and the corresponding 2D image points Take multiple images of a calibration pattern to obtain these correspondences With the mapping 3Dp -&amp;gt; 2Dp, solve for camera parameters Evaluate accuracy of estimated camera parameters:</description></item><item><title>Data association in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/data-association-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/data-association-in-defslam/</guid><description>Source: Lamarca 2020 DefSLAM See also: Data association Goal: match keypoints in current frame (newly extracted) with map points (already in map/system) Use the active matching strategy proposed in [Agudo 2015]: “Simultaneous pose and non-rigid shape with particle dynamics,” Steps  ORB points (keypoints) are detected in current frame
Camera pose Tcw is predicted
using camera motion model camera motion model: function of past camera poses Predict where map points (existing in map) would be imaged, based on last estimated template i.</description></item><item><title>DefSLAM framework</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-framework/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-framework/</guid><description>Source: Lamarca 2020 DefSLAM See also: template-substitution-in-defslam &amp;ldquo;Fusion of the methods available for processing non-rigid monocular scenes&amp;rdquo;
Deformation tracking [front end]
estimates/recovers/optimises: camera pose scene deformation / deformation of map points (observations) the map points are then embedded into the template Tk (to compute their position on the surface) operates at frame rate SFT-based ( shape from template ), requires prior geometry (template of scene at rest) for the currently being viewed map Map points are deformed (updated) by solving an optimisation problem min { reprojection error + deformation energy } per frame Deformation mapping [back end]</description></item><item><title>Mapping step-by-step in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-step-by-step-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-step-by-step-in-defslam/</guid><description>Source: lamarca-2020 Parent: DefSLAM framework Steps Recover warps between k and k* (s. [Non-Rigid Guided Matching (b/w KFs) in DefSLAM](non-rigid guided-matching-(b_w-kfs)-in-defslam.md)) with k: anchor keyframes, i.e. KFs where one of the observed map points was initialised with k*: set of best covisible keyframes warps: transformation between the images Ik to Ik* In DefSLAM, Schwarps (a family of warps using 2D Schwarzian equation regularisers) is used Schwarps has something to do with the infinitesimal planarity assumption of NRSfM [ NRSfM ] Process k* to get estimate of an up-to-scale surface  Input of NRSfM: warps [ Surface alignment ] Up-to-scale surface (\hat{S}_k) is aligned with the whole map in order to obtained the scaled surface Sk w.</description></item><item><title>Non-rigid Surface from Motion</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm/</guid><description>Notes:
Original NRSFM paper?
https://www.cs.dartmouth.edu/~lorenzo/Papers/TorrHertzBreg-pami08.pdf A Phd thesis [Kumar] https://openresearch-repository.anu.edu.au/handle/1885/164278?mode=full Source: Kumar
The problem with dynamic or non-rigid scenes:
if we project a scene point into a camera image plane, there will be several possible 3D configurations! Allowing arbitrary deformations makes the 3D reconstruction an ill posed problem (underconstrained) &amp;ndash;&amp;gt; need to make additional assumptions about the object or scene (make more constraints)! Source: lamarca-2020 See also: nrsfm-in-defslam , sfm NRSfM (non-rigid structure from motion) batch processing of images to recover deformation computationally demanding — slower than SfT Orthographic NRSfM usually fails with very large deformations uses an orthographic camera projection/model (weak approximation to the perspective camera) — a limitation, as many vision-related applications have a significant perspective effect exploits spatial constraints temporal constraints spatiotemporal constraints usually ok for small deformations, but not for very large deformations Perspective NRSfM the perspective camera model is more accurate than the orthographic one also uses the isometry assumption (as in SfT methods), which has produced good results in NRSfM Parashar 2018 &amp;ldquo;Isometric NRSfM&amp;rdquo; [6] local method that handles occlusions and missing data well</description></item><item><title>NRSfM in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm-in-defslam/</guid><description>Parent: Mapping step-by-step in DefSLAM Source: lamarca-2020 See also: NRSfM Assumptions Isometric deformation Infinitesimal planarity [DEF]: any surface can be approximated as a plane at infinitesimal level, all the while maintaining its curvature at a global level Locality The method used here is a local method &amp;ndash;&amp;gt; implies that it handles missing data and occlusions inherently
surface deformation is modelled locally for each point, under the above assumptions Embedding, $\phi_k$ of the scene surface is a parametrisation — transforms an image point to a point on a 3D surface uses the normalised coordinates of the image Ik (xhat, yhat) Procedure A point is matched in more than two keyframes (warps are used in the matching process )</description></item><item><title>ORBSLAM2 mods</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam2-mods/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam2-mods/</guid><description>Patch to work with opencv4 http://github.com/Windfisch/ORB_SLAM2 ORBSLAM2 Python bindings http://github.com/jskinn/ORB_SLAM2-PythonBindings</description></item><item><title>Pinhole camera projection function</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/pinhole-camera-projection-function/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/pinhole-camera-projection-function/</guid><description>Backlinks: Pinhole camera model See also: World to camera trafo Source: Mur-Artal 2017 VI-ORB 3D points Projection function  Transforms 3D points into 2D points on image plane Focal length:  Principal point:  The projection does not consider the distortion due to the lens
therefore when extracting image features, first undistort their coordinates only then match to projected points (existing features which have undergone projection from 3D to 2D) Source: Lamarca 2019 DefSLAM 3D point: Projection function maps</description></item><item><title>Shape from Motion</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sft/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sft/</guid><description>Initial paper?: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010934https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010934
Goal reconstruct the surface of an object
reference 3D shape (template) of the object is available under a specific deformation constraint Source: lamarca-2020 SFT (shape from template) uses only a single image — faster than nrsfm lower computational cost must have a known 3D template (textured model) SfT methods require: 1 monocular image 1 textured shape at rest (template) &amp;ldquo;geometry&amp;rdquo; as the deformation model different definitions of the deformation model analytic, e.</description></item><item><title>Template in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/template-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/template-in-defslam/</guid><description>Source: Lamarca 2019 DefSLAM Template
2D triangular mesh floating in the 3D space consists of a set of 2D triangular facets F a facet has 3 nodes (set V) and 3 edges (set E) map points observed in keyframe k are embedded in the facets Map point coordinates in barycentric coordinates</description></item><item><title>Tracking optimisation in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-optimisation-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-optimisation-in-defslam/</guid><description>Source: lamarca-2020 Optimisation function Minimises reprojection error (in the image) deformation energy (of the template) boundary nodes of the local zone are fixed (i.e. not set as arguments to the optimisation function) this makes the absolute camera pose observable how? in order to constrain the gauge freedoms Initial guess: values from previous optimisation (i.e. previous frame: t-1) Reprojection error robust against outliers due to Huber robust kernel</description></item><item><title>Bag of words</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/bag-of-words/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/bag-of-words/</guid><description>Parent: SLAM Index Source: http://towardsdatascience.com/bag-of-visual-words-in-a-nutshell-9ceea97ce0fb
Has its origins in natural language processing (NLP), information retrieval
A text can be seen as a bag of words, with each word having different frequencies from one another This can be used to compare and classify texts (similar histograms) In vision Instead of words we have features (identifying pattern in an image) An image is represented as a set of features Features consist of Keypoints: points that are invariant to transformation Descriptors : description of the keypoint, for feature representation Construct a frequency histogram of features in the image Workflow Feature detection/extraction &amp;ndash;&amp;gt; build vocabulary/codewords &amp;ndash;&amp;gt; make histogram = BoW</description></item><item><title>DefSLAM dependency/inheritance diagram</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-dependency-inheritance-diagram/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-dependency-inheritance-diagram/</guid><description/></item><item><title>Forster 2017 IMU Preintegration</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/forster-2017-imu-preintegration/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/forster-2017-imu-preintegration/</guid><description>Authors: Forster et al
Abstract:
First contribution: preintegration theory (building up on Lupton&amp;rsquo;s work) what&amp;rsquo;s different from Lupton&amp;rsquo;s: addresses manifold structure of the rotation group, analytic derivation of all Jacobians Lupton&amp;rsquo;s work uses Euler angles Using Euler angles and techniques of Euclidian spaces for state propagation/covariance estimation is not properly invariant under rigid transformations uncertainty propagation, a-posteriori bias correction same as Lupton: integration performed in local frame, eliminating need for reintegrating when linearisation point changes Second contribution: integration of the preintegrated IMU model into a visual-inertial pipeline The system presented uses incremental smoothing for fast computation of the optimal MAP estimate Uses structureless model (3D landmarks are not part of the variables to be estimated) for visual measurements &amp;ndash;&amp;gt; allows eliminating large numbers of variables Motivation:</description></item><item><title>Handling the computational complexity of optimisation-based SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/handling-the-computational-complexity-of-optimisation-based-slam/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/handling-the-computational-complexity-of-optimisation-based-slam/</guid><description>Parent: SLAM Index Source: Forster 2017 IMU Preintegration Complexity of nonlinear batch optimisation
The trajectory and the map, which comprise the states, grow with time The larger the SLAM problem, the less feasible it is to perform the optimisation in real-time Solutions to improve computational efficiency
Keyframe-based methods: discard frames except for a few selected keyframes Run the optimisation parallelly (e.g. tracking and mapping threads) Fixed-lag smoothing: Use of a local map of fixed size, with marginalisation of the old states (summarise the old states into a prior term) Filtering is a special case of this: window of size 1, i.</description></item><item><title>Visual-inertial datasets</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/visual-inertial-datasets/</link><pubDate>Fri, 06 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/visual-inertial-datasets/</guid><description>http://sites.google.com/view/awesome-slam-datasets/home
http://fpv.ifi.uzh.ch/ Aggressive drone racing http://www.lirmm.fr/aqualoc/ Underwater Monochromatic http://vision.in.tum.de/data/datasets/visual-inertial-dataset TUM indoor/urban, slides fisheye cameraUsed in ORBSLAM3 http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets EUROC MAV stereo, monochrUsed in ORBSLAM3</description></item><item><title>Discussion 2020-11-03</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2020-11-03/</link><pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2020-11-03/</guid><description>Current progress
Find out how g2o works, how the DefSLAM implementation of the tracking optimisation works Incorporate IMU data (s. ORBSLAM3 implementation) IMU initialisation IMU preintegration IMU terms in cost function (which function?) Topics
How g2o works IMU preintegration DefSLAM + IMU cost function Implementation in DefSLAM using g2o [tbd] ORBSLAM3&amp;rsquo;s implementation the IMU cost function terms initialisation preintegration Kalman idea for IMU integration Compile + run</description></item><item><title>DefTracking::MonocularInitialization</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-monocularinitialization/</link><pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-monocularinitialization/</guid><description>Parent: DefTracking::Track Initialises
surface points in the surface If num. features in current frame &amp;gt; 100
set frame pose to origin make new KF (GroundTruthKeyFrame) pKFini add the KF to the map mpMap iterate over the N features get feature kp convert kp to 3d point make new DefMapPoint(3dp, pKFini, mpMap) set pointers between DefMapPoint, GroundTruthKeyFrame, DefMap Save surface using bbs Set mLastFrame := mCurrentFrame Local window: Add KF to local KFs vector, add MapPoints to local MP vector, mpLocalMapper Calculate Tcr from Tcw Initialise SLAM: Set reference KF, reference MapPoints set mState to OK</description></item><item><title>ORBSLAM::Frame constructor (monocular)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam-frame-constructor-monocular/</link><pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam-frame-constructor-monocular/</guid><description>Source: Tracking::GrabImageMonocular Set scale level info from ORB extractor Extract ORB features mvKeys (vector of keypoints/features) Set N number of features Make mvpMapPoints (null, but with size N), mvbOutlier (all entries false, size N) If first frame or calibration change: ComputeImageBounds AssignFeaturesToGrid()</description></item><item><title>Dynamic Bayesian Network formulation of SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/dynamic-bayesian-network-formulation-of-slam/</link><pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/dynamic-bayesian-network-formulation-of-slam/</guid><description>Source: Grisetti 2011 - Tutorial graph-based SLAM Dynamic Bayesian Network
Solution of full SLAM problem: Transition model: Observation model:  The observation model is usually multimodal: a single observation may result in multiple edges (in the spatial graph) Therefore, the Gaussian assumption does not hold</description></item><item><title>Grisetti 2011 - Tutorial graph-based SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/grisetti-2011/</link><pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/grisetti-2011/</guid><description>temp
Backlinks: What is SLAM? Abstract
formulate SLAM using a graph nodes: poses of the robot (as well as landmark postiions) at different points in time edges: constraints between poses come from sensor measurements/observations robot movement/control input constraints can contradict each other, due to effect of noise in sensor readings solve the graph, i.e. compute the map: find the spatial configuration of the nodes that best satisfy the constraints/edges tutorial for back-end (optimisation) part of graph-based SLAM :: Navigation task: requires a map and knowledge of current position relative to locations in the map</description></item><item><title>DefOptimizer::DefPoseOptimization</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defoptimizer-defposeoptimization/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defoptimizer-defposeoptimization/</guid><description>Parent: DefTracking::Track As far as I understand it:
Uses g2o library for the optimisation (graph-based SLAM) cost function terms are converted to edges and nodes each cost function term seems to correspond to an edge in the graph in g2o paper/tutorial: an edge is fully characterised by its error function and its information matrix int DefPoseOptimization(Frame *pFrame, Map *mMap, double RegLap, double RegInex, double RegTemp, uint NeighboursLayers) // define optimiser, set solver optimizer = new &amp;hellip; optimizer.</description></item><item><title>Edges in g2o</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/edges-in-g2o/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/edges-in-g2o/</guid><description/></item><item><title>DefOptimizer::poseOptimization</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defoptimizer-poseoptimization/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defoptimizer-poseoptimization/</guid><description>Parent: DefTracking:TrackWithMotionModel() int DefOptimizer::poseOptimization(Frame *pFrame)
// Set estimate of solution to current camera pose g2o::VertexSE3Expmap *vSE3 = new g2o::VertexSE3Expmap(); vSE3-&amp;gt;setEstimate(Converter::toSE3Quat(pFrame-&amp;gt;mTcw)); vSE3-&amp;gt;setId(0); vSE3-&amp;gt;setFixed(false); optimizer.addVertex(vSE3);
// Set MapPoint vertices (num. nodes in opt. graph?) const int N = pFrame-&amp;gt;N;</description></item><item><title>defSLAM::System constructor</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-system-constructor/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-system-constructor/</guid><description>Parent: DefSLAM simple_camera defSLAM::System::System(const string &amp;amp;strVocFile, const string &amp;amp;strSettingsFile, const bool bUseViewer)  mSensor(MONOCULAR), mpLoopCloser(NULL), mpViewer(static_cast&amp;lt;Viewer *&amp;gt;(nullptr)), mbReset(false), mbActivateLocalizationMode(false), mbDeactivateLocalizationMode(false) Constructor // initialise mpVocabulary from file // create mpKeyFrameDatabase from mpVocabulary // create map DefMap() // create drawers for viewer DefFrameDrawer DefMapDrawer // initialise tracking, mapping, viewer threads; loop closing not implemented in DefSLAM mpTracker = new DefTracking(&amp;hellip;); mpLocalMapper = new DefLocalMapping(&amp;hellip;); mpViewer = new DefViewer(&amp;hellip;);
Attributes eSensor mSensor ORBVocabulary *mpVocabulary KeyFrameDatabase *mpKeyFrameDatabase Map *mpMap // stores pointers to all KFs, all MapPoints</description></item><item><title>defSLAM::System::TrackMonocular</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-system-trackmonocular/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-system-trackmonocular/</guid><description>Parent: DefSLAM simple_camera cv::Mat defSLAM::System::TrackMonocular cv::Mat Tcw = mpTracker-&amp;gt; GrabImageMonocular (im, timestamp);
// get information from mpTracker: // get states mTrackingState // get map points mTrackedMapPoints // get key points mTrackedKeyPointsUn
// return camera pose return Tcw;</description></item><item><title>DefTracking::Track</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-track/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-track/</guid><description>Parent: Tracking::GrabImageMonocular void DefTracking::Track // if: not initialised, do: monocular initialisation // elseif: already initialised, do: track frame { // if: tracking and mapping, do: bOK = TrackWithMotionModel ();
// if bOK (there exists camera pose estimate and matching), track local map // if template is updated (keyframe-rate update) set reference KF from new template do DefPoseOptimization (&amp;hellip;); bOK = TrackLocalMap();
// if: bOK, update motion model (update mVelocity); clean VO matches // check if we should insert a new KF, delete outliers for BA</description></item><item><title>DefTracking:TrackWithMotionModel()</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-trackwithmotionmodel-/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-trackwithmotionmodel-/</guid><description>Parent: DefTracking::Track // Initial tracking to locate rigidly the camera and discard outliers. bool DefTracking::TrackWithMotionModel()
// Update last frame relative pose according to its reference keyframe UpdateLastFrame();
// Project points seen in prev frames int th = 15; int nmatches = Defmatcher.SearchByProjection(*mCurrentFrame, mLastFrame, th, mSensor == System::MONOCULAR);
// Optimise frame pose with all matches to initialise camera pose Optimizer:: poseOptimization (mCurrentFrame, myfile);
// Discard outliers
// return: sufficient number of matches?</description></item><item><title>Tracking_GrabImageMonocular</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-grabimagemonocular/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-grabimagemonocular/</guid><description>Parent: defSLAM::System::TrackMonocular cv::Mat ORB_SLAM2::Tracking::GrabImageMonocular
colour conversion make frame using image, timestamp, ORB stuff, calibration data, etc. mCurrentFrame = new Frame (mImGray, timestamp, mpORBextractorLeft, mpORBVocabulary, mK, mDistCoef, mbf, mThDepth, im)
perform tracking: Track (); return camera pose return mCurrentFrame-&amp;gt;mTcw.clone();</description></item><item><title>Badias 2020 MORPH-DSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/badias-2020-morph-dslam/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/badias-2020-morph-dslam/</guid><description>URL: http://arxiv.org/pdf/2009.00576.pdf Video: http://www.youtube.com/watch?v=P_QN8Nv&amp;ndash;_g To read!</description></item><item><title>IMU index</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/imu-index/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/imu-index/</guid><description>Parent: SLAM Index General
IMU Gyroscope Odometry Why use the visual-inertial sensor combination? IMU to camera coordinate transformations phils-lab-sensor-fusion sensor-fusion Practical
Converting IMU data to inertial frame Modelling
Probabilistic models for IMU [Choice of model for the IMU motion model](choice of model-for-the-imu-motion-model.md) * [Choice of states for the IMU motion/kinematics model](choice of states-for-the-imu-motion_kinematics-model.</description></item><item><title>Mapping in VIORB</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-in-viorb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-in-viorb/</guid><description>Source: Mur-Artal 2017 VI-ORB Mapping in VIORB Previously in ORBSLAM (only poses are optimised): Now in VIORB, more states to optimise: Increase in complexity
more states to optimise (v, b) IMU measurements creates constraints between keyframes Original ORBSLAM discards redundants KFs (poses a problem with IMU constraints!) Workaround: in local BA, only allow discarding of KF if, after discarding, the time between two consecutive KFs is short enough (&amp;lt;= 0.</description></item><item><title>Pinhole camera model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/pinhole-camera-model/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/pinhole-camera-model/</guid><description>Parent: SLAM Index Backlinks: [Camera calibration](camera calibration.md), [pinhole camera projection function](pinhole camera projection function.md), [weiss thesis vision based navigation for micro helicopters](weiss thesis vision-based-navigation-for-micro-helicopters.md) See also: World to camera trafo Source: http://de.mathworks.com/help/vision/ug/camera-calibration.html Does not account for lens distortion (ideal pinhole camera doesn&amp;rsquo;t have a lens) To represent a real camera, the full camera model to be used should include (radial and tangential) lens distortion, (such as the one used in the MATLAB computer vision toolbox)</description></item><item><title>Preintegration of IMU</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/preintegration-of-imu/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/preintegration-of-imu/</guid><description>Parent: IMU Backlinks: IMU states, dynamics equations IMU measurements arrive at a higher frequency (frame rate) compared to camera captures (keyframe rate) IMU measurements constrain consecutive states We want to summarise these &amp;lsquo;in-between&amp;rsquo; IMU measurements into one single relative motion constraint between keyframes</description></item><item><title>Tracking in VIORB</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-in-viorb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-in-viorb/</guid><description>Source: Mur-Artal 2017 VI-ORB Tracking in VIORB
Visual-inertial tracking at frame rate, instead of using an ad-hoc motion model as in the original ORB-SLAM Tracked states: [sensor pose (R, p), velocities v, biases b] Once the camera pose is predicted, map points are projected, then matches with existing features on the frame Then optimise the current frame j, depending on whether the map has just been updated the map is unchanged Here, the optimisation function for tracking (when map unchanged) is:</description></item><item><title>DefSLAM simple_camera</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-simple-camera/</link><pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-simple-camera/</guid><description>Without ground truth
App run // Create defSLAM::system, which initializes all threads (local mapping, loop closing, viewer) defSLAM::System SLAM(orbVocab, calibFile, bUseViewer);
// Timestamp uint timestamp := 0;
// Process frames from video capture while (capture.isOpened()) { // Get the capture as a matrix; // SLAM SLAM. TrackMonocular (img_matrix, timestamp); timestamp++; }</description></item><item><title>Structure from Motion</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sfm/</link><pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sfm/</guid><description>Source: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Schonberger_Structure-From-Motion_Revisited_CVPR_2016_paper.pdf
Note:
This paper uses incremental SfM Corresponding paper for COLMAP Structure from motion Reconstruction of 3D structure from a sequence of 2D images of that structure, taken from different viewpoints.
Search for correspondence between images &amp;ndash;&amp;gt; output: scene graph (nodes: images, edges: verified pairs) Feature extraction Feature matching Output: set of image pairs and their associated feature correspondences Verification: do features map to the same scene point?</description></item><item><title>OptiTrack in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/optitrack-in-sofa/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/optitrack-in-sofa/</guid><description> Using OptiTrackNatNet C++ implementation? http://www.sofa-framework.org/community/doc/programming-with-sofa/create-your-scene-in-cpp/ XML scenes? OptiTrack + Python otnn_client = root.addObject(&amp;lsquo;OptiTrackNatNetClient&amp;rsquo;, name=&amp;lsquo;otnnClient&amp;rsquo;) &amp;lt;Sofa.Core.Object&amp;gt; dir(otnn_client) [&amp;lsquo;bbox&amp;rsquo;, &amp;lsquo;clientName&amp;rsquo;, &amp;lsquo;componentState&amp;rsquo;, &amp;lsquo;drawOtherMarkersColor&amp;rsquo;, &amp;lsquo;drawOtherMarkersSize&amp;rsquo;, &amp;lsquo;drawTrackedMarkersColor&amp;rsquo;, &amp;lsquo;drawTrackedMarkersSize&amp;rsquo;, &amp;lsquo;listening&amp;rsquo;, &amp;lsquo;name&amp;rsquo;, &amp;lsquo;otherMarkers&amp;rsquo;, &amp;lsquo;printLog&amp;rsquo;, &amp;lsquo;scale&amp;rsquo;, &amp;lsquo;serverName&amp;rsquo;, &amp;lsquo;tags&amp;rsquo;, &amp;lsquo;trackedMarkers&amp;rsquo;] bold: not in API http://www.sofa-framework.org/api/master/plugins/OptiTrackNatNet/html/class_sofa_opti_track_nat_net_1_1_opti_track_nat_net_client.html difference between client name and server name</description></item><item><title>Cheatsheet</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cheatsheet/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cheatsheet/</guid><description>Parent: [SofaPython Index](SofaPython Index.md)
Imports/Plugins import SofaRuntime from SofaRuntime import PluginRepository PluginRepository.addFirstPath(SOFA_INSTALL_DIR) SofaRuntime.importPlugin(&amp;ldquo;SofaComponentAll&amp;rdquo;) SofaRuntime.importPlugin(&amp;ldquo;SofaPython3&amp;rdquo;) SofaRuntime.importPlugin(&amp;ldquo;SofaOpenglVisual&amp;rdquo;)
From root root = Sofa.Core.Node(&amp;ldquo;root&amp;rdquo;) c = root.createObject(&amp;ldquo;MechanicalObject&amp;rdquo;, name=&amp;ldquo;t&amp;rdquo;, position=[ [0, 0, 0], [1, 1, 1], [2, 2, 2]]) c1 = root.addObject(&amp;ldquo;MechanicalObject&amp;rdquo;, name=&amp;ldquo;c1&amp;rdquo;)
From nonroot nonroot_node = Sofa.Core.Node(&amp;ldquo;a_node&amp;rdquo;) nonroot_node.addObject(&amp;ldquo;MechanicalObject&amp;rdquo;, name=&amp;ldquo;c1&amp;rdquo;) .addData(&amp;ldquo;d&amp;rdquo;, value=&amp;ldquo;coucou&amp;rdquo;, type=&amp;ldquo;string&amp;rdquo;) .addData(&amp;ldquo;data1&amp;rdquo;, value=&amp;quot;@/c1.d&amp;quot;) # @ is root
Add data from relative/absolute paths c4.addData(&amp;lsquo;data1&amp;rsquo;, value=&amp;quot;@/n1/c3.data1&amp;quot;) # absolute path (chained) c4.addData(&amp;lsquo;data2&amp;rsquo;, value=&amp;quot;@../n1/c3.data1&amp;quot;) # relative path (down, chained) c1.</description></item><item><title>Someone's SP3 setup</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/someones-sp3-setup/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/someones-sp3-setup/</guid><description>Parent: SofaPython Index http://gist.github.com/pedroperrusi/9fdd4257db72465c8fb481381f396c51</description></item><item><title>Initialising graph in SP3</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/initialising-graph-in-sp3/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/initialising-graph-in-sp3/</guid><description>Parent: SofaPython Index http://github.com/SofaDefrost/plugin.SofaPython3.deprecated/pull/110
&amp;ldquo;Can you share an example of a scene and a component you have in mind ? Because currently to summary the discussion during sofa-meeting the problem with init/bdwInit/reinit is that it that this is severely ill defined and we are considering to totally remove that from Sofa and use alternatives pattern among which:
have an onSimulationStart / onSimulationStop event to detect when the simulation is on or not avoid using getContext() to fetch other components unless you store them in SingleLink.</description></item><item><title>Covisible keyframes</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/covisible-keyframes/</link><pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/covisible-keyframes/</guid><description>Source: Palafox 2019 ( thesis ) Backlinks: Lamarca 2019 DefSLAM Two keyframes are covisible if they share several common landmarks.</description></item><item><title>Pipenv venv in project folder</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/pipenv-venv-in-project-folder/</link><pubDate>Sun, 06 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/pipenv-venv-in-project-folder/</guid><description>PIPENV_VENV_IN_PROJECT=1
Powershell: $Env:PIPENV_VENV_IN_PROJECT=&amp;ldquo;1&amp;rdquo;</description></item><item><title>50.2.30 Multivariate Kalman filter algorithm</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.30-multivariate-kalman-filter-algorithm/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.30-multivariate-kalman-filter-algorithm/</guid><description>Parent: Multivariate Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Initialisation
Initialise filter state Initialise belief in the state Predict
Propagate state to the next time step using the system model [prediction] Adjust belief to take into account the prediction uncertainty [prior] Update
Obtain measurement and associated belief about its accuracy Calculate residual (prior - measurement) Calculate scaling factor/Kalman gain Set estimated state to be on the residual line based on the scaling factor Update the belief in the state based on measurement certainty Designing the measurement function</description></item><item><title>Error ellipse/Confidence ellipse</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/error-ellipse-confidence-ellipse/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/error-ellipse-confidence-ellipse/</guid><description>Parent: Multivariate Gaussian distributions Source: rlabbe Kalman/Bayesian filters in Python Any slice through a multivariate Gaussian is an ellipse Plots show the slice for 3 standard deviations
Showing correlation using error ellipses</description></item><item><title>Hidden variables in a multivariate Kalman filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/hidden-variables-in-a-multivariate-kalman-filter/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/hidden-variables-in-a-multivariate-kalman-filter/</guid><description>Parent: Multivariate Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Example: Blue error ellipse:
Certainty in position x=0 No idea about the velocity (long in y-axis) We know that position and velocity are correlated, i.e. the next position depends on the current velocity value (red error ellipse — likelihood/prediction for the next step) e.g. if v=5m/s, the next position is 5m +- position uncertainties
We get a position update (new blue error ellipse) The new covariance (posterior) is obtained by multiplying the previous two covariances —&amp;gt; intersection The posterior&amp;rsquo;s tilt implies that there is some correlation between position and velocity Not only are we now more certain about the velocity, but our position certainty also increases (compared to not considering the velocity at all)!</description></item><item><title>Multivariate Kalman filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/multivariate-kalman-filters/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/multivariate-kalman-filters/</guid><description>Parent: rlabbe Kalman/Bayesian filters in Python [Hidden variables in a multivariate Kalman filter](hidden variables-in-a-multivariate-kalman-filter.md)
Here:
Focus is on a subset of problems describable using Newton&amp;rsquo;s equations of motion Discretised continuous-time kinematic filters Multivariate Kalman filter algorithm Designing the filter
State (x, P) Process (F, Q) Measurement (z, R) Measurement function H Control inputs (B, u) Assumptions of the Kalman filter
The sensors and motion model have Gaussian noise Everything is linear If the assumptions are true, then the Kalman filter is optimal in a least squares sense</description></item><item><title>Showing correlation using error ellipses</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/showing-correlation-using-error-ellipses/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/showing-correlation-using-error-ellipses/</guid><description>Parent: Error ellipse/Confidence ellipse Source: rlabbe Kalman/Bayesian filters in Python A slanted ellipse implies correlation
The &amp;lsquo;thinner&amp;rsquo; side isn&amp;rsquo;t necessarily more accurate, it just means that the spread of data is reduced along this dimension (when viewing sensor data, for example) Example First epoch Yellow: prior (very uncertain about position) Green: evidence (more accurate in one of the dimensions than the other; more certainty compared to prior) Blue: posterior via multiplication Posterior retains the shape of the evidence (which has more certainty than the prior)</description></item><item><title>50.2. Bayes' Theorem</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.-bayes-theorem/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.-bayes-theorem/</guid><description>Parent: Bayesian Filter Update Step Source: rlabbe Kalman/Bayesian filters in Python How do we compute the probability of an event given previous information? (s. also Frequentist vs Bayesian statistics )
Formula to compute new information into existing information
Used in the update step of a Bayesian filter (valid for both probabilities as well as probability distributions) where || . || expresses normalisation
B Evidence (sensor measurements z) p(A) Prior p(B|A) Likelihood p(A|B) Posterior In filtering systems, computing p(x|z) is nearly impossible, but computing p(z|x) is fairly straightforward, which then facilitates the computation of p(x|z) via the Bayes' theorem formula.</description></item><item><title>50.2.10 Discrete Bayesian filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10-discrete-bayesian-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10-discrete-bayesian-filter/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python The Kalman filter is a subset of Bayesian filters
Predict and update steps like in the g-h filter Here: error percentages are used to implicitly compute the g and h parameters Steps
[Initialise our belief in the state] The predict step always degrades our knowledge (belief/prior) However, in the update step , we add another measurement. This, will always improve our knowledge regardless of noise, enabling convergence Limitations of the discrete Bayes filter</description></item><item><title>50.2.10.1 Discrete Bayesian Filter Predict Step</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10.1-discrete-bayesian-filter-predict-step/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10.1-discrete-bayesian-filter-predict-step/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python The predict step uses the total probability theorem.
Computes total probability of multiple possible events Uses the system model (propagates the states from prev. time step [posterior] to the next one); prediction Accounts for the uncertainty (kernel) in the prediction: produces a prior Generalise the uncertainty using a kernel (distributes the uncertainty over a range around the prediction) Integrate the kernel into the calculations by using convolution * Convolving the &amp;ldquo;current probabilistic estimate&amp;rdquo; with the &amp;ldquo;probabilistic estimate of how much we think the position has changed&amp;rdquo; (from system model) The prior is a &amp;lsquo;degraded&amp;rsquo; version of the belief i.</description></item><item><title>50.2.10.2 Bayesian Filter Update Step</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10.2-bayesian-filter-update-step/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10.2-bayesian-filter-update-step/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python The update step uses Bayes' Theorem Produces the posterior by using the likelihood and the prior Also incorporates sensor data (measurements), as the measurements go into the likelihood calculation Update algorithm
Get a measurement, and associated belief about its accuracy Compute likelihood from the measurement and the measurement accuracy assumption Update the posterior using the likelihood and the prior</description></item><item><title>50.2.20 1D Kalman filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.20-1d-kalman-filters/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.20-1d-kalman-filters/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python [Deriving Kalman filter from Discrete Bayes using Gaussians](deriving kalman filter-from-discrete-bayes-using-gaussians.md) 1D Kalman filter algorithm Kalman gain using Gaussians Variance of the 1D Kalman filter Factors affecting Kalman filter performance</description></item><item><title>50.2.20.1 1D Kalman filter algorithm</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.20.1-1d-kalman-filter-algorithm/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.20.1-1d-kalman-filter-algorithm/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Initialisation:
Initialise the state of the filter Initialise the belief in the state Predict step:
Gaussian addition prior = predict(x, process_model) Incorporate process variance in order to prevent smug filtering Update step:
Gaussian multiplication likelihood = gaussian(z, sensor_var) x = update(prior, likelihood) The output of both steps is a Gaussian probability distribution N(mean, var)</description></item><item><title>Central Limit Theorem</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/central-limit-theorem/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/central-limit-theorem/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python If we make many measurements, the measurements will be normally distributed. (only applies under certain conditions)</description></item><item><title>Computational properties of Gaussian distributions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/computational-properties-of-gaussian-distributions/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/computational-properties-of-gaussian-distributions/</guid><description>Parent: Gaussian distribution Backlinks: Showing correlation using error ellipses Source: rlabbe Kalman/Bayesian filters in Python g1 + g2 = g3; all are Gaussians g1 * g2 = g3; g3 is not Gaussian, but proportional to a Gaussian Sum of two Gaussians Product of two Gaussians: Product of multidimensional Gaussians:</description></item><item><title>Correlation and independence</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/correlation-and-independence/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/correlation-and-independence/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python Independent variables are uncorrelated. But the reverse is not always true: uncorrelated variables may be dependent on one another e.g. y=x^2 has no [linear] correlation, but y depends on x nonetheless</description></item><item><title>Deriving Kalman filter from Discrete Bayes using Gaussians</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/deriving-kalman-filter-from-discrete-bayes-using-gaussians/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/deriving-kalman-filter-from-discrete-bayes-using-gaussians/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Replacing discrete Bayes with Gaussian distributions where the operators in the circles are as of yet undetermined</description></item><item><title>Empirical rule 68/95/99.7</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/empirical-rule-68-95-99.7/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/empirical-rule-68-95-99.7/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python Emprical rule, a.k.a. 68–95–99.7 rule About 68% of all values lie within one standard deviation of the mean.</description></item><item><title>Factors affecting Kalman filter performance</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/factors-affecting-kalman-filter-performance/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/factors-affecting-kalman-filter-performance/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Difficulties of creating a well-performing Kalman filter: Includes modeling the sensor performance (what variance most accurately represents the reality? Which probability distribution?)
Factors affecting the performance of the Kalman filter
On modelling the process noise/variance Bad initial estimate Filter can recover from this, because we have a certain belief in the sensor measurements Typically the initial value is set to the first sensor measurement Nonlinearity of the system</description></item><item><title>Frequentist vs Bayesian statistics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/frequentist-vs-bayesian-statistics/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/frequentist-vs-bayesian-statistics/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python Frequentist vs Bayesian statistics
Probability of flipping a fair coin infinitely many times is 50% - frequentist Probability of flipping a fair coin one more time (which way do I believe it landed?), single event - Bayesian Bayesian statistics takes past information (prior) into account If finding the prior is tricky, frequentist techniques are sometimes used e.</description></item><item><title>Gaussian distribution</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/gaussian-distribution.1/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/gaussian-distribution.1/</guid><description>Backlinks: Limitations of the discrete Bayes filter Source: rlabbe Kalman/Bayesian filters in Python a.k.a. Normal distribution Unimodal, continuous probability distribution function (pdf)
The probability of a range of measurements is the area under the graph of the probability distribution between the end values of the range &amp;ndash; cumulative distribution function (cdf)
Background statistics Variance, standard deviation, covariances Central Limit Theorem Correlation and independence Types Univariate Gaussian distribution Multivariate Gaussian distributions Computational properties of Gaussian distributions Pros and cons of Gaussian distributions</description></item><item><title>Kalman gain using Gaussians</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-gain-using-gaussians/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-gain-using-gaussians/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Kalman gain in the update step Basically a scaling term that chooses a value between the sensor distr. mean and the posterior distr. mean Gives greater weight to the term with lower variance (we trust this data more!) Mean and variance in terms of the Kalman gain Variance of the filter (i.e., what variance is show by the estimated output/posterior?</description></item><item><title>Kalman vs. nonlinear systems</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-vs.-nonlinear-systems/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-vs.-nonlinear-systems/</guid><description>Parent: Factors affecting Kalman filter performance Source: rlabbe Kalman/Bayesian filters in Python Kalman filter equations are linear Example: approximating a sine-wave signal Explanation:
Back to the basic g-h filter structure: the filter output chooses a value on the residual line The process model in the underlying filter assumes constant velocity (0 acceleration), whereas in the sine example above, the signal is always accelerating</description></item><item><title>Limitations of the discrete Bayes filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/limitations-of-the-discrete-bayes-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/limitations-of-the-discrete-bayes-filter/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python Limitations of the discrete Bayes filter
Scaling Dog tracking example is one-dimensional, but in real life we often want to track more things (e.g. 2D coordinates, velocities) Multidimensional case: store probabilities in a grid 4 tracked variables: O(n^4) per time step High computational cost with high dimensionality Filter is discrete and therefore gives discrete output But a lot of applications require continuous output Discretising a solution space can lead to lots of data (depending on accuracy required) &amp;ndash;&amp;gt; calculations for lots of different probabilities!</description></item><item><title>Multivariate Gaussian distributions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/multivariate-gaussian-distributions/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/multivariate-gaussian-distributions/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python See also: Probability distribution N means for N dimensions Variances are now also combined with covariances (to take into account correlation between different dimensions)
Variance: how does a population vary amongst themselves? Covariance: how much do two variables change relative to each other? The correlation helps prediction!
Here: only linear correlation considered; however nonlinear correlations also exist.
Error ellipse/Confidence ellipse</description></item><item><title>Probability distribution</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/probability-distribution/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/probability-distribution/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python Probability distribution:
collection of all possible probabilities for an event the distribution lists all possible events and the probability of each sum up to 1 Prior probability distribution: probability prior to incorporating any measurements or other information
Joint probability P(x,y):
probability of both events happening the multivariate Gaussian distribution is already already a joint probability distribution Marginal probability:</description></item><item><title>Pros and cons of Gaussian distributions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/pros-and-cons-of-gaussian-distributions/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/pros-and-cons-of-gaussian-distributions/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python Big advantage of using Gaussian distributions (as opposed to discrete ones w/ histogram bins): less data, b/c a Gaussian distribution is represented fully using only two values: the mean and the variance Limitations of using Gaussian distributions to model the world i.e. deviations from the central limit theorem
Not all situations are describable by Gaussian distributions e.g. sensors in the real world have fat tails (kurtosis) — don&amp;rsquo;t extend to infinity and skew Can&amp;rsquo;t depict any arbitrary probability distributions like in e.</description></item><item><title>Random variable</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/random-variable/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/random-variable/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python Combination of values + associated probabilities. &amp;ldquo;Event&amp;rdquo; e.g. die toss, height of students e.g. in a fair die values = {1, 2, &amp;hellip;, 6} (range of values = sample space) probabilities = {1/6} * 6</description></item><item><title>Smug filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/smug-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/smug-filter/</guid><description>Parent: 1D Kalman filter algorithm Source: rlabbe Kalman/Bayesian filters in Python A filter that, once enough measurements are made, becomes very confident in its prediction (P gets smaller with time while the filter becomes more inaccurate!). From then on it will ignore measurements
To avoid this: add a bit of error to the prediction step, e.g. using the process variance</description></item><item><title>Univariate Gaussian distribution</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/univariate-gaussian-distribution/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/univariate-gaussian-distribution/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python If normalised (area under the graph is 1): Gaussian distribution If not normalised: Gaussian function Notation: The random variable X has a Gaussian distribution with mean &amp;hellip; and variance &amp;hellip; .</description></item><item><title>Variance of the 1D Kalman filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/variance-of-the-1d-kalman-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/variance-of-the-1d-kalman-filter/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python i.e., what variance is show by the estimated output/posterior?)
Always converges to a fixed value if the sensor and process variances are constant We can run simulations to determine the value to which the filter variance converges Then hard code this value into the filter (+ with first sensor measurement as initial value, the filter should have good performance) Alternative: instead of using the variance value, use the calculated Kalman gain Example implementation using the Kalman gain However, using the Kalman gain obscures the Bayesian approach</description></item><item><title>Variance, standard deviation, covariances</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/variance-standard-deviation-covariances/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/variance-standard-deviation-covariances/</guid><description>Backlinks: Multivariate Gaussian distributions Source: rlabbe Kalman/Bayesian filters in Python See also: Empirical rule 68/95/99.7 How much do the values vary from the mean?
There are other ways of calculating variance (e.g. by using absolute values of error instead of error squared). The other methods may be better w.r.t. outliers (outliers get magnified in the square term) Process variance: error in the process model Sensor variance: error in each sensor measurement</description></item><item><title>(AtsushiSakai) PythonRobotics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/atsushisakai-pythonrobotics/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/atsushisakai-pythonrobotics/</guid><description>http://nbviewer.jupyter.org/github/AtsushiSakai/PythonRobotics/</description></item><item><title>(rlabbe) Kalman/Bayesian filters in Python</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/rlabbe-kalman-bayesian-filters-in-python/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/rlabbe-kalman-bayesian-filters-in-python/</guid><description>URL: http://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python nbviewer link: http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb Abstract:
Introductory text with Python code Caveat: most of the code is written for didactic purposes, may not be the most efficient solution (nor numerically stable) Recommended other works s. Works of possible interest Chapters
Preface Why Kalman filters? [Aim and main principle of Kalman filters](aim and-main-principle-of-kalman-filters.md) Expected value g-h filter or α-β filter Discrete Bayesian filter Gaussian distribution 1D Kalman filters Multivariate Kalman filters</description></item><item><title>50.1.1 Aim and main principle of Kalman filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.1.1-aim-and-main-principle-of-kalman-filters/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.1.1-aim-and-main-principle-of-kalman-filters/</guid><description>Source: rlabbe Aim Aim of the Kalman/Bayesian filters: to accumulate (or to somehow blend)
our noisy and limited knowledge (of system behaviour) noisy and limited sensor readings and with these, make the best possible prediction (estimate) of the system state.
Main principles: use past information to make predictions for the future never throw away information predict/propagation step: calculate prediction based on process model and using previous state data (previous estimate) update step: calculate the estimates based on prediction and measurement Prediction step a.</description></item><item><title>Calculating the estimated state in the GH-filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/calculating-the-estimated-state-in-the-gh-filter/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/calculating-the-estimated-state-in-the-gh-filter/</guid><description>Parent: g-h filter or α-β filter Source: rlabbe Kalman/Bayesian filters in Python Using a gain , the estimate therefore always falls between the measurements (circles) and the predictions (in red). The prediction is dependent on the previous filter output (i.e. last estimate). Here it is modelled to increase by 1 from the previous estimate.
The estimates are not a straight line, but definitely closer in shape to the ground truth than the measurements alone.</description></item><item><title>Effects of varying g</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/effects-of-varying-g/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/effects-of-varying-g/</guid><description>Parent: [Calculating the estimated state in the GH-filter](calculating the-estimated-state-in-the-gh-filter.md) Source: rlabbe Kalman/Bayesian filters in Python The greater the g value, the more we follow the measurements rather than rely on our [model-based] predictions.</description></item><item><title>Effects of varying h</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/effects-of-varying-h/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/effects-of-varying-h/</guid><description>Parent Improving the gh-filter by using h Source: rlabbe Kalman/Bayesian filters in Python The greater the h value, the more we trust the rate of change that we can derive from the measurement data.
a larger h enables us to react to transient (initial condition dependent) changes more rapidly. Because if we have a large difference between our chosen IC and the measurement, this results in a huge residual velocity</description></item><item><title>Expected value</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/expected-value/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/expected-value/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python Example: if we take a thousand sensor readings, the readings won&amp;rsquo;t always be the same (due to the inherent noise).
The expected value &amp;lsquo;averages&amp;rsquo; all of the readings into a single value. This can be a mean (probabilities of all values assumed equal) Or if incorporating individual and different probabilities, the expectation isn&amp;rsquo;t the mean of the range of values Proven: the average of a large number of measurements will be very close to the actual weight</description></item><item><title>g-h filter or α-β filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/g-h-filter-or-%CE%B1-%CE%B2-filter/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/g-h-filter-or-%CE%B1-%CE%B2-filter/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python A filter that uses two scaling factors:
g or \alpha for the measurement h or \beta for the rate of change of measurement GH filter algorithm [Calculating the estimated state in the GH-filter](calculating the-estimated-state-in-the-gh-filter.md) Improving the gh-filter by using h Several unwanted effects using gh filters Basis for many other filters, e.g.
Kalman filter Least squares filter Benedict-Bordner filter etc.</description></item><item><title>Gain g of the gh-filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/gain-g-of-the-gh-filter/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/gain-g-of-the-gh-filter/</guid><description>Parent: [Calculating the estimated state based on measurements and predictions](calculating the estimated state-based-on-measurements-and-predictions.md) Backlinks: GH filter algorithm Source: rlabbe Kalman/Bayesian filters in Python Which one do we trust more, the meaasurement z or the prediction x? Applying corresponding weights to both, we obtain the estimate x_est
The prediction is nothing other than a propagated state estimate.
[Me] The prediction is basesd on the model (a priori knowledge) If the model also depends on previous states (which are themselves an output of the filter, i.</description></item><item><title>GH filter algorithm</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/gh-filter-algorithm/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/gh-filter-algorithm/</guid><description>Parent: g-h filter or α-β filter Source: rlabbe Kalman/Bayesian filters in Python Initialisation
Initialise the state of the filter Initialise our belief in the state Prediction
Use system model to propagate the state to the next time step Adjust our belief to account for uncertainty in the prediction Update
Get a measurement and an associated belief about its accuracy Calculate residual = measurement - estimated state Using a certain gain , our updated state estimate is somewhere on the residual line</description></item><item><title>Improving the gh-filter by using h</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/improving-the-gh-filter-by-using-h/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/improving-the-gh-filter-by-using-h/</guid><description>Parent: g-h filter or α-β filter Source: rlabbe Kalman/Bayesian filters in Python Implementing the g value without h We improve the estimation, previously by only predicting the state, by now predicting the rate of change of state. i.e. Also predict the weight gain per day instead of setting it at a constant value. We use the sensor information for this! Even if it&amp;rsquo;s noisy, there&amp;rsquo;s information in there somewhere, and data is always better than a guess.</description></item><item><title>Principle of never throwing away information</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/principle-of-never-throwing-away-information/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/principle-of-never-throwing-away-information/</guid><description>Source: rlabbe &amp;ldquo;Two sensors are better than one, even if one is less accurate than the other.&amp;rdquo;
Example 1 Given:
A: a more accurate sensor B: a less accurate sensor Should we therefore choose A as the estimate and discard B? No, because B can improve our knowledge when combined with A.
Using the measurements from B further narrows the range of estimates (overlap between the error bars).</description></item><item><title>Several unwanted effects using gh filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/several-unwanted-effects-using-gh-filters/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/several-unwanted-effects-using-gh-filters/</guid><description>Parent: g-h filter or α-β filter Source: rlabbe Kalman/Bayesian filters in Python Effect of bad initial conditions: ringing (sinusoidal over- and undershooting before finally settling onto a trajectory) Effect of very noisy data Effect of acceleration (in data) Filter lags behind because it uses a model that assumes constant velocity in each propagation step. Hence, a filter is only as good as the mathematical model used to describe the phenomenon.</description></item><item><title>Cadena 2016 Past, Present, and Future of SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cadena-2016/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cadena-2016/</guid><description>Authors Cadena et al
Abstract
cited by 1.2k people &amp;ldquo;This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM&amp;rdquo; Recommended other works s. Works of possible interest Contents/Chapters Takeaway</description></item><item><title>Conditional independence</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/conditional-independence/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/conditional-independence/</guid><description>Source: http://en.wikipedia.org/wiki/Conditional_independence A and B are conditionally independent given C If and only if, given the knowledge that C occurs, the knowledge of whether A occurs provides no information whatsoever on the likelihood of B occurring, and vice versa
Example Weather and delay
Let the two events be the probabilities of persons A and B getting home in time for dinner
The third event C is the fact that a snow storm hit the city.</description></item><item><title>Durrant-Whyte 2006 SLAM Tutorial Part I</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/durrant-whyte-2006-slam-tutorial-part-i/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/durrant-whyte-2006-slam-tutorial-part-i/</guid><description>Backlinks: Works of possible interest Authors: Bailey, Durrant-Whyte
Abstract:
Contents/Chapters
Takeaway</description></item><item><title>Perceptual aliasing</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/perceptual-aliasing/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/perceptual-aliasing/</guid><description>Source: http://en.wikipedia.org/wiki/Robotic_mapping Two different places are perceived as the same
Source: http://arxiv.org/abs/1810.11692 Modeling Perceptual Aliasing in SLAM via Discrete-Continuous Graphical Models [Lajoie 2018] (from the abstract)
Phenomenon where different places generate a similar visual footprint Leads to spurious measurements being fed into the SLAM estimator Result: incorrect localisation and map</description></item><item><title>Chen 2018 SLAM-based dense surface reconstruction in MIS with AR</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/chen-2018-mis-slam/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/chen-2018-mis-slam/</guid><description>Authors Chen et al
Abstract Intra-operative dense surface reconstruction framework to provide geometry information from only monocular videos The proposed framework works well with rapid camera movements, however is not suitable for large deformations Only tweaks ORBSLAM to adjust between point density and computational performance Contents/Chapters Problems in medical AR:
tissue surface illumination tissue deformation rapid movements of the medical tool e.g. endoscope (s. also kidnapped robot problem for relocalisation, tracking mus therefore be robust) field of view often very small &amp;ldquo;A typical human uses 14 visual cues to perceive depth, only 3/14 are binocular vision related.</description></item><item><title>In vivo</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/in-vivo/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/in-vivo/</guid><description> Latin for &amp;ldquo;within the living&amp;rdquo; studies in which the effects of various biological entities are tested on whole, living organisms or cells, as opposed to a tissue extract/dead organism</description></item><item><title>Song 2018 MIS-SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/song-2018-mis-slam/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/song-2018-mis-slam/</guid><description>Authors: Song et al
Note: referred to in Lamarca 2020 as a stereovisual deformable SLAM, uses CPU and GPU, nonlinear optimisation Video: http://www.youtube.com/watch?v=2pXokldQBWM
Abstract Uses CPU and GPU CPU for ORBSLAM (initial global position) GPU for deformable tracking and dense mapping Contents/Chapters Poor localisation of scope in MIS, compared with open surgery Related works mentioned don&amp;rsquo;t provide a RT and robust solution for localisation while reconstructing dense deformable surfaces focus on the monocular scope, fail to solve the problem of missing scale Fast movement makes visual odometry unstable causes blurry images worsens registrations ORB-SLAM proven to be suitable for coupling with dense deformable SLAM Initial tracking: ORB-SLAM ORB features and global pose are uploaded to GPU Upload the matched ORB features every time a new observation is made The initial global pose makes the system significantly robuster Deformable tracking and dense mapping Receives initial global pose from the CPU Initialises the model with an estimated depth From model: extract potential visible points Project these points onto 2D depth images Registration: to estimate optimum global pose to estimate non-rigid warping field Deform the model based on this transformation (registration) Fuse the deformed model with the new observation Includes in-vivo validation with deformation (Hamlyn datasets)</description></item><item><title>Filter-based vs optimisation-based SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/filter-based-vs-optimisation-based-slam/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/filter-based-vs-optimisation-based-slam/</guid><description>Parent: SLAM Index Source: Scaradozzi 2018 SLAM application in surgery Main paradigms of SLAM
Filters — Kalman filters , Particle filters Graph-based SLAM Estimate the entire trajectory and the map from the full set of measurements (full SLAM) Which SLAM algorithm to use? Depends on application
map resolution update time (real time or not) type of environment type of sensors available</description></item><item><title>General Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/general-kalman-filter/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/general-kalman-filter/</guid><description>Backlinks: Main paradigms of SLAM Source: Scaradozzi 2018 SLAM application in surgery KF original algorithm assumes linearity (rarely ever the case)
Variations of the Kalman filter: Extended Kalman Filter (EKF) Unscented Kalman Filter (UKF) Information filtering (IF) — dual to KF Combination of EKF and IF: CF-SLAM, with the goal to be more efficient w.r.t. computational complexity</description></item><item><title>Information Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/information-filter/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/information-filter/</guid><description>Parent: General Kalman Filter Source: Scaradozzi 2018 SLAM application in surgery also same assumptions as the EKF main difference: how the Gaussian belief is represented est. cov. — replaced by information matrix (IM) est. state — replaced by information vector (IV) superior to KF in the following ways data is filtered by summing up the IMs and IVs often numerically more stable Dual character of KF and IF</description></item><item><title>Particle filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/particle-filters/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/particle-filters/</guid><description>Parent: Filter localisation methods Source: Wikipedia Lokalisierung Particle filter / Monte Carlo localisation / sequential Monte Carlo methods
allow solution of all three localisation problems POSE represented by a particle cloud Each particle : possible POSE The filter checks the plausibility of each particle Increases and decreases the probabilities of each particle accordingly When a lower probability threshold is exceeded, the particle is not considered any longer Source: Scaradozzi 2018 SLAM application in surgery Particle filters (sequential Monte Carlo)</description></item><item><title>Unscented Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/unscented-kalman-filter/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/unscented-kalman-filter/</guid><description>Parent: General Kalman Filter Source: Scaradozzi 2018 SLAM application in surgery developed to overcome main problems of the EKF like EKF, approximates the state distribution with a Gaussian Random Variable only the representation is different—using alpha points (a minimal set of sample points) capture posterior mean and covariance accurately for any nonlinearity, up to3rd order Taylor</description></item><item><title>Wikipedia Lokalisierung</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-lokalisierung/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-lokalisierung/</guid><description>Source: http://de.wikipedia.org/wiki/Lokalisierung_(Robotik Localisation Particle filters</description></item><item><title>Collision detection and response</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/collision-detection-and-response/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/collision-detection-and-response/</guid><description>Source: SOFA extended documentation Parent: Simulation algorithms in SOFA split into several phases each phase is scheduled by a CollisionPipeline component an object which can potentially collide is associated with a collision geometry returns pairs of colliding bounding volumes (broad phase component) returns pairs of geometric primitives + contact points (narrow phase component) the returned pairs are passed to the contact manager the contact manager creates contact interactions &amp;hellip; (skimmed)</description></item><item><title>Constraint solvers</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/constraint-solvers/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/constraint-solvers/</guid><description>Source: SOFA extended documentation Parent: Simulation algorithms in SOFA Backlinks: Scene graph in SOFA Lagrange multipliers to handle complex constraints (which aren&amp;rsquo;t handle-able using projection matrices ) May be combined with explicit or implicit integration phi: bilateral interaction laws (attachments, sliding joints, &amp;hellip;) psi: unilateral interaction laws (contact, friction, &amp;hellip;)
The Lagrange multipliers add force terms to the equation A*dv = b The H matrices are stored in the MechanicalState of each node.</description></item><item><title>General EKF implementation (non-SLAM)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/general-ekf-implementation-non-slam/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/general-ekf-implementation-non-slam/</guid><description>Parent: Extended Kalman Filter Source: SLAM for Dummies General (non-SLAM) implementation of EKF:
only state estimation robot is given a perfect map no map update necessary SLAM implementations of EKF requires map update and therefore the matrices are changed.
Source: Scaradozzi 2018 SLAM application in surgery EKF vs KF circumvents linearity assumption uses nonlinear functions to describe the next state probability measurement probability approximates the state distribution with a Gaussian Random Variable</description></item><item><title>Linear solvers</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/linear-solvers/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/linear-solvers/</guid><description>Source: SOFA extended documentation Parent: Simulation algorithms in SOFA Conjugate gradient J: first-order mapping of a node to its parent path(i): list of mappings from the independent DOFs to the node the force applies to
Computation using a visitor: Top down visitor: propagates the given displacement, clears force vector Bottom up visitor: accumulates forces, maps them up to the independent DOFs Direct solvers
can be used as preconditioners of the conjugate gradient algorithm can be used to solve the equation system A*dv=b implementations are based external libraries</description></item><item><title>Mesh topology</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mesh-topology/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mesh-topology/</guid><description>Source: SOFA extended documentation Parent: Data structure in SOFA See also: Mesh geometry Mesh topology: how the vertices are connected to each other (using what element?)
Hierarrchy of mesh topology: Topology objects consist of four functional members which creates/modifies/gets topology arrays/geometrical information:
Container Modifier Algorithms Geometry Topological mapping:
Define a new mesh topology from an existing one, using the same DOFs e.g. for subsetting a set of nodes, edges, or to split quads into 2 triangles each these topologies are therefore assigned to the same MechanicalState</description></item><item><title>ODE solvers</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/ode-solvers/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/ode-solvers/</guid><description>Source: SOFA extended documentation Parent: Simulation algorithms in SOFA Backlinks: Linear solvers , constraint-solvers implement animation algorithms at each time step integrate and compute positions and velocities one time step ahead uses state vectors (e.g. for position or force), denoted by symbolic identificators called VecId s this allows the solver to be implemented completely independently of the physical model Each statement in the example above is implemented using a visitor</description></item><item><title>Simulation algorithms in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/simulation-algorithms-in-sofa/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/simulation-algorithms-in-sofa/</guid><description>Source: SOFA extended documentation ODE integration ( ODE solvers ) Linear equation solution ( linear solvers ) Complex constraints ( constraint solvers ) Collision detection and response GPU support</description></item><item><title>SofaPython Index</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sofapython-index/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sofapython-index/</guid><description>Building/setup Building SOFA on Windows Someone&amp;rsquo;s SP3 setup Running Running SOFA with Python Using python with existing scene Basic python script in Sofa Initialising graph in SP3 Plugins Possible plugins Install ROSConnector in SOFA STLIB (Sofa Template Library) Registration Communication Sending data using sockets Sockets Errno 10054 External data in SOFA Documentation SofaPython API/Documentation links Cheatsheet</description></item><item><title>VecId</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/vecid/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/vecid/</guid><description>Source: [SOFA extended documentation](SOFA extended documentation.md) Parent: [ODE solvers](ODE solvers.md)
Uniquely identifies state vectors (which are scattered over all MechanicalStates ) Mechanical operations (e.g. allocating a state vector, accumulating forces) are implemented using a specialised visitor parametrised on VecIds</description></item><item><title>B1 Modelling of tissue and sensor, navigation of multimodal sensors</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/b1-modelling-of-tissue-and-sensor-navigation-of-multimodal-sensors/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/b1-modelling-of-tissue-and-sensor-navigation-of-multimodal-sensors/</guid><description>Source: http://www.isys.uni-stuttgart.de/forschung/medizintechnik/intraoperative-multisensorische-gewebedifferenzierung/ Backlinks: [GRK 2543: Intraoperative Multi-sensor Tissue Differentiation in Oncology](grk 2543_ intraoperative-multi-sensor-tissue-differentiation-in-oncology.md)
cross-domain modelling (tissue and sensor) tissue parameters change depending on status (benign/malignant), obtained or derived from sensor signals sensor signal must be synchronised with the current position of the sensor probe on the tissue</description></item><item><title>Barycentric coordinates</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/barycentric-coordinates/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/barycentric-coordinates/</guid><description>Source: SOFA extended documentation Baclinks: [Top-down mapping (master to slave)](top-down mapping (master to slave).md), lamarca-2019-defslam Barycentre: centre of mass A coordinate system, in which the location of point of a simplex (line, triangle, tetrahedron, etc) is specified as the centre of mass of the masses placed at its vertices x_i vertices of a simplex p a point in space The a_i coefficients are the barycentric coordinates of p w.</description></item><item><title>Bottom-up mapping (slave to master)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/bottom-up-mapping-slave-to-master/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/bottom-up-mapping-slave-to-master/</guid><description>Source: SOFA extended documentation Parent: Mappings Mapping of a slave forces to the master forces Newton&amp;rsquo;s law f=Ma applies
Equivalence of power using the kinematic relation using the principle of virtual work</description></item><item><title>Components of the internal model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/components-of-the-internal-model/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/components-of-the-internal-model/</guid><description>Source: SOFA extended documentation Parent: [Internal model as a scene graph in SOFA](internal model as-a-scene-graph-in-sofa.md)
MeshLoader: topology, geometry
MechanicalState TetrahedronSetTopologyContainer
tetrahedral connectivity passed on to other components Forces Mass
DiagonalMass UniformMass: less accurate, but allows faster computation FixedConstraint: P (cancels displacements)
EulerSolver: integration scheme</description></item><item><title>Cryosection</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cryosection/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cryosection/</guid><description>Source: https://en.wikipedia.org/wiki/Frozen_section_procedure aka frozen section procedure allows rapid analysis of a dissected/resected specimen during the course of surgery the specimen is frozen rapidly and brought to a lab for analysis the results are relayed to the surgeon by intercom benign or malignant when operating on a previously confirmed malignant tissue, information on whether residual cancer was found on the [resection margin](resection margin.md) of the tissue the surgeon makes his decision on how to continue the operation based on the results</description></item><item><title>Force classes in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/force-classes-in-sofa/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/force-classes-in-sofa/</guid><description>Source: SOFA extended documentation Parent: Components of the internal model More than 30 classes available in SOFA
FEM
for deformable volumes/surfaces
volume: tetrahedron/hexahedron surface: shell/membrane TetrahedralCorotationalFEMForceField: forces based on FEM
corotational/hyperelastic formulations
wire/tubular objects
Springs
SpringForceField: forces generated by the surface (alternative: TriangleFEMFroceField) ConstantForceField: external forces</description></item><item><title>Internal model as a scene graph in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/internal-model-as-a-scene-graph-in-sofa/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/internal-model-as-a-scene-graph-in-sofa/</guid><description>Source: SOFA extended documentation Parent: Internal model Scene graph of the internal model
Consists of components which are connected to a common scenegraph node (root of the internal model)
Each component is responsible for a set of tasks Examples: solver, mass, constraints, &amp;hellip;
Each component can query its parent node to get access to the its sibling components such as MechanicalState , topology
Components are independent of one another — modularity</description></item><item><title>Mathematical model of the internal model in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mathematical-model-of-the-internal-model-in-sofa/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mathematical-model-of-the-internal-model-in-sofa/</guid><description>Source: SOFA extended documentation Parent: Internal model Backlinks: ODE solvers , constraint-solvers Dynamic/quasi-static system of particles (nodes) Independent DOFs: node coordinates, governed by  f: different force functions, e.g. volume, surface and external forces) M: mass matrix P: constraints (projection matrix) each operator corresponds to a simulation component</description></item><item><title>MechanicalState</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mechanicalstate/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mechanicalstate/</guid><description>Source: [SOFA extended documentation](SOFA extended documentation.md) Parents: [Components of the internal model](Components of the internal model.md), [Internal model as a scene graph in SOFA](Internal model as a scene graph in SOFA.md) Backlinks: VecId , [Scene graph in SOFA](Scene graph in SOFA.md), [Mesh topology](Mesh topology.md)
Contains state vectors of each mesh node Coordinates x Velocities v Net force f n nodes: n entries of the state vector Each entry has the same size of the node type (3 for 3D particles) Nodes of different types belong to different MechanicalStates the other MechanicalStates are attached to other scene graph nodes they might be connected with one another using interaction forces</description></item><item><title>Oncology</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/oncology/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/oncology/</guid><description>Source: https://en.wikipedia.org/wiki/Oncology Backlinks: [GRK 2543: Intraoperative Multi-sensor Tissue Differentiation in Oncology](GRK 2543_ Intraoperative Multi-sensor Tissue Differentiation in Oncology.md)
Prevention, diagnosis and treatment of cancer.</description></item><item><title>Related types of surgery</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/related-types-of-surgery/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/related-types-of-surgery/</guid><description>Source: http://en.wikipedia.org/wiki/Resection_(surgery) By procedure Resection: remove all parts or a key part of an internal organ s. also: resection margin Excision: cut out only a part of an organ/tissue By degree of invasiveness minimally-invasive surgery (-scopy) laparoscopy</description></item><item><title>Resection margin</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/resection-margin/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/resection-margin/</guid><description>Source: http://en.wikipedia.org/wiki/Resection_margin Backlinks: Cryosection , related types-of-surgery Margin on non-cancerous tissue around a tumour that has been removed.
Negative margin: no tumour Microscopic positive: tumour identified microscopically Macroscopic positive: tumour significantly present</description></item><item><title>Scene graph (general)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/scene-graph-general/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/scene-graph-general/</guid><description>Source: http://en.wikipedia.org/wiki/Scene_graph See also: Scene graph in SOFA A general data structure Collection of nodes in a graph/tree</description></item><item><title>Simplex</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/simplex/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/simplex/</guid><description>Source: https://en.wikipedia.org/wiki/Simplex Parent: [Barycentric coordinates](Barycentric coordinates.md)
A triangle in arbitrary dimensions 0-simplex point 1-simplex line 2-simplex triangle 3-simplex tetrahedron</description></item><item><title>Top-down mapping (master to slave)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/top-down-mapping-master-to-slave/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/top-down-mapping-master-to-slave/</guid><description>Source: SOFA extended documentation Parent: Mappings Mapping of a master states to the slave states with the Jacobian (kinematic relation) Linear/nonlinear mappings
In linear mappings, J and J are the same In nonlinear mappings, J is nonlinear w.r.t. x_m, i.e. not a matrix Surfaces
Surfaces embedded in deformable cells: J contains barycentric coordinates Surfaces attached to rigid bodies: each row of J encodes for each vertex</description></item><item><title>Visual model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/visual-model/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/visual-model/</guid><description>Source: SOFA extended documentation Parent: Models in SOFA More detailed geometry than that of the internal model , hence uses different meshes Mappings are used to update the visual model with the deformations taking place Contains rendering parameters Libraries for rendering graphics
OGRE (external) Open Scene Graph (external) SOFA&amp;rsquo;s own library based on openGL</description></item><item><title>Cancer biopsy</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cancer-biopsy/</link><pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cancer-biopsy/</guid><description>Source: http://en.wikipedia.org/wiki/Biopsy
Biopsy type of medical test in which a cell/tissue sample is extracted in order to detect disease
Types of biopsy: excisional biopsy: removal of entire suspicious area to be diagnosed incisional biopsy: removal of only samples of the abnormal tissue needle aspiration biopsy: removal of cells via needle Diagnosing / Pathological examination to determine whether the abnormality is benign or malignant (classification of the cancer) to determine how far it has spread negative margins: no disease found at the edge of specimen positive margins: disease was found at edge, further excision may be in order In bladders: usually done using a cystocopy</description></item><item><title>Cystocopy</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cystocopy/</link><pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cystocopy/</guid><description>Source: https://en.wikipedia.org/wiki/Cystoscopy
Backlinks: [Some questions](Some questions.md)
Endoscopy of the bladder via the urethra Tool involved: cystoscope https://www.youtube.com/watch?v=ybhzlW7ivro
Video of cystocopy and bladder biopsy Modern Latin for bladder: cystis</description></item><item><title>Laparoscopy</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/laparoscopy/</link><pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/laparoscopy/</guid><description>Source: https://en.wikipedia.org/wiki/Laparoscopy Backlinks: [Related types of surgery](Related types of surgery.md), [Some questions](Some questions.md), Trocar minimally invasive surgery (MIS) / keyhole surgery make a small incision in the abdomen area and operate through it</description></item><item><title>Qin 2019 General Optimization-based Framework (Multisensor)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/qin-2019-general-optimization-based-framework-multisensor/</link><pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/qin-2019-general-optimization-based-framework-multisensor/</guid><description>Authors: Qin et al Code: http://github.com/HKUST-Aerial-Robotics/VINS-Fusion (uses ROS)
Abstract:
odometry estimation with multiple sensors, general framework which is optimisation-based demonstrated combinations: stereo cameras monocular cam + IMU stereo cams + IMU sensor = factor in the framework comparison with other state-of-the-art algos Aim:
to create a general algo which supports different multisensor suites also for redundancy: in case of sensor failure, it can be switched out easily Related work:</description></item><item><title>State-of-the-art SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/state-of-the-art-slam/</link><pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/state-of-the-art-slam/</guid><description>Parent: SLAM Index Backlinks: Qin 2019 General Optimization-based Framework (Multisensor) Things that I&amp;rsquo;ve seen mentioned several times so far
ORBSLAM: monocular MonoSLAM: monocular (old?) — Andrew Davison OKVIS: visual inertial, stereovision PTAM: parallel tracking and mapping MSCKF: real-time EKF VINS-mono: visual inertial, monocular http://en.wikipedia.org/wiki/List_of_SLAM_Methods</description></item><item><title>(Scaradozzi 2018) SLAM application in surgery</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/scaradozzi-2018/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/scaradozzi-2018/</guid><description>Abstract: SLAM&amp;rsquo;s potential in image-guided surgery assuming static environment Review of main techniques in general robotics SLAM Insight into visual SLAM SLAM in surgery Chapters what-is-slam Filter-based vs optimisation-based SLAM General Kalman Filter General EKF Unscented Kalman Filter Information Filter &amp;hellip;.
Takeaway EKF is popular in surgery SLAM techniques Deformable environment encumbers precise registration and data fusion</description></item><item><title>EKF System State</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-system-state/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-system-state/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices , step-2:-re-observation Contains robot POSE and landmark position POSE: (x y theta)_r LM: (x, y)_l1 &amp;hellip; (x,y)_ln; n = num. of landmarks Size: 3+2n rows</description></item><item><title>Kalman gain for EKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-gain-for-ekf/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-gain-for-ekf/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices How much we will trust the observed landmarks
compromise between odometry and landmark correction uses uncertainty of observed landmarks measure of quality of the range measurement device odometry performance Gains for range and brearing (3+2n x 2)</description></item><item><title>Lamarca 2020 DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/lamarca-2020/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/lamarca-2020/</guid><description>URL: http://arxiv.org/abs/1908.08918
Authors: Lamarca et al
Code: http://github.com/UZ-SLAMLab/DefSLAM
Results (video): http://www.youtube.com/watch?v=6mmhD2_t6Gs Summary First monocular SLAM for deformable environments in real-time Most other SLAM implementations assume rigidity Main techniques used (techniques for monocular non-rigid scenes): isometric shape from template ( SfT ) non-rigid structure from motion ( NRSfM ) Main principle: computation in two parallel threads (s. DefSLAM framework) Deformation tracking [front end] Deformation mapping [back end] The map from the mapping thread defines the shape-at-rest template used by deformation tracking Validation: compare with ORBSLAM (rigid) Assumes isometric deformation Future work: relocalisation (s.</description></item><item><title>Literature management</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/literature-management/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/literature-management/</guid><description> Only focus on one paper per topic at a time Skim through and take notes on only the important chapters Link and backlink Note which topics were skimmed Come back later for further literature review</description></item><item><title>Programmatic implementations of MonoSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/programmatic-implementations-of-monoslam/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/programmatic-implementations-of-monoslam/</guid><description>Parent: SLAM resources Python http://github.com/agnivsen/Py-M-SLAM http://github.com/agnivsen/LibMonoSLAM
MATLAB http://perso.ensta-paris.fr/~filliat/Courses/2011_projets_C10-2/BRUNEAU_DUBRAY_MURGUET/monoSLAM_bruneau_dubray_murguet_en.html</description></item><item><title>Template for a bibliography entry</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/template-for-a-bibliography-entry/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/template-for-a-bibliography-entry/</guid><description>Source Backlinks
Authors Abstract Contents/Chapters Takeaway</description></item><item><title>Back-end optimisation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/back-end-optimisation/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/back-end-optimisation/</guid><description>Parent: Visual SLAM Implementation Framework Source: cometlabs (Camera pose optimisation)
To compensate for drift of pose estimation Traditionally using EKF ( filter-based ) simple implementation therefore good for small scale estimations Alternative: bundle adjustment (graph optimisation) joint optimisation of the camera pose and the 3D structure parameters combines numerical methods and graph theory increasingly favoured over filtering, due to the latter&amp;rsquo;s inherent inconsistency more efficient when combined with sub-mapping</description></item><item><title>Feature-based vs direct SLAM workflow</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/feature-based-vs-direct-slam-workflow/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/feature-based-vs-direct-slam-workflow/</guid><description>Parent: SLAM Index Source: cometlabs Feature-based (aka sparse ) direct (aka dense ) Extraction of features required No abstraction necessary Aims to minimise error between point location estimate (from odometry) and location based on camera Tracks objects by minimising photometric error (intensity differences)</description></item><item><title>Precision recall curve</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/precision-recall-curve/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/precision-recall-curve/</guid><description>Parent: Loop closure detection Source: cometlabs used to better quantify the performance (balance between false positives and false negatives in loop closure detection ) highlights tradeoff between precision and recall precision (absence of false positives) but may lead to the appearance of false negatives recall (prediction power) e.g. tweaking to improve recall increases sensitivity to similarities in the image thus increases possibility of false positives</description></item><item><title>Classification of image-based camera localization approaches</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/classification-of-image-based-camera-localization-approaches/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/classification-of-image-based-camera-localization-approaches/</guid><description>Parent: SLAM Index Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)
Unknown environment (must be reconstructed from image data) Online/real-time mapping (SLAM)
geometric metric SLAM (accurate computations, therefore still widely used in practice) monocular multiocular multi-kind sensor (active) loosely-coupled closely-coupled learning SLAM (active) needs a prior dataset to train NN &amp;ndash; dataset determines performanace of the SLAM low generalisation capability, therefore not as flexible as geom.</description></item><item><title>Dense/direct VSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/dense-direct-vslam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/dense-direct-vslam/</guid><description>Parent: Visual SLAM Implementation Framework , slam_index See also: Feature-based vs direct SLAM workflow Source: cometlabs Front-end part of the Visual SLAM Implementation Framework Use most or all of the pixels in each received frame Provide more information about the environment Many more pixels, require GPUs Feature-based vs direct SLAM workflow Disadvantages: Don&amp;rsquo;t handle outliers very well (outliers will be processed and implemented into the final map) Slower than feature-based variants Aims to minimise photometric error (intensity differences) Semi-dense</description></item><item><title>Feature maps</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/feature-maps/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/feature-maps/</guid><description>Parents: [Mapping representations in robotics](mapping representations-in-robotics.md), sparse/feature-based-vslam Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Uses a limited number of sparse objects to represent a map e.g. points, lines
Low computation cost because of the sparsity Map management solutions are good solutions for current applications What&amp;rsquo;s map management (probably storing maps in databases and recognising an existing map) [-] Sensitivity to false data association</description></item><item><title>Geometric metric SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/geometric-metric-slam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/geometric-metric-slam/</guid><description>Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md) Parent: Classification of image-based camera localization approaches Computes 3D maps with accurate mathematical equations
Classification according to sensors
monocular multiocular (most studies focus on binocular vision) multisensor fusion (e.g. vision and IMU &amp;ndash; vision and IMU fusion gaining in popularity) Classification according to techniques used
Filter-based SLAM Keyframe -based SLAM (active) Feature-based (keyframe-based feature SLAM) / sparse Direct / dense Grid-based SLAM (mainly deals with laser data, deals only a bit with image data) According to [76] keyframe-based can provide more accurate results compared to filter-based</description></item><item><title>Kidnapped robot problem</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kidnapped-robot-problem/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kidnapped-robot-problem/</guid><description>Source: Wikipedia Lokalisierung Backlinks: Lamarca 2020 DefSLAM Position initially known Then robot is repositioned without knowing it Robot has to be able to realise that the initial successful localisation isn&amp;rsquo;t valid any more &amp;ndash; a new global localisation must be carried out Realise this via unplausible sensor measurements (huge contradiction to prev. measurements) Has to do with the measure of robustness of the localisation method carried out</description></item><item><title>Localisation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/localisation/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/localisation/</guid><description>Parent: [SLAM Index](SLAM Index.md)
Source: [Wikipedia Lokalisierung](Wikipedia Lokalisierung.md) The positioning of an autonomous mobile robot relative to its environment
The position of a mobile robot is seldom known exactly An unknown initial position / measurement uncertainties while moving Becomes a SLAM problem when neither the position nor the map is known Goal/Output: POSE
Due to uncertainties etc, it&amp;rsquo;s good to have a POSE representation that also shows these uncertainties e.</description></item><item><title>Loose vs Tight coupling</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/loose-vs-tight-coupling/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/loose-vs-tight-coupling/</guid><description>Parent: SLAM Index Backlinks: Multisensor fusion Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)
In loosely-coupled systems: all sensor states are independently estimated and optimized
easier to process frame and IMU data less accurate/robust compared to tight coupling e.g. Integrated IMU data can be incorporated as independent measurements in stereo vision optimization e.g. Vision-only pose estimates are used to update an EKF so that IMU propagation can be performed In tightly-coupled systems: all sensor states are jointly estimated and optimized</description></item><item><title>Mapping representations in robotics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-representations-in-robotics/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-representations-in-robotics/</guid><description>Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Feature maps Occupancy grids Grids containing occupancy probability information Useful for path planning, exploration Drawback: computational complexity</description></item><item><title>Multisensor fusion</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/multisensor-fusion/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/multisensor-fusion/</guid><description>Parent: SLAM Index , geometric-metric-slam Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Avoid limitations of using only one sensor Relative measurements: provide precise positioning information constantly At certain times absolute measurements are made to correct potential errors (correct drift) several approaches (for localisation), e.g. merge sensor feeds at the lowest level before being processed homogeneously hierarchical approaches (fuse state estimates derived independently from multiple sensors) s.</description></item><item><title>SA TODO</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sa-todo/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sa-todo/</guid><description>Studienarbeit Camera-based localisation
Find a classification of approaches/techniques Briefly describe each See if it applies to the project Look into the most promising approach &amp;ndash; how to implement (DefSLAM) DefSLAM Install DefSLAM library Skim through an existing VI-SLAM (rigid) implementation to see how sensor fusion is done (as an overview for the coming sensor fusion task) VINS-Mono, VIORB paper Prepare dummy data for testing VI-SLAM (eventually VI-DefSLAM) — interpolate data between frames and add noise/bias Go through code Get the executables working VideoCapture OpenCV problem &amp;ndash; reinstall with all FFMMPEG options Figure out g2o Go through the rest of DefSLAM Go through the rest of ORBSLAM3 IMU term in cost function IMU preintegration IMU initialisation implement imu term in optimisation (either using ekf (s.</description></item><item><title>Sparse/Feature-based VSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sparse-feature-based-vslam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sparse-feature-based-vslam/</guid><description>Parent: Visual SLAM Implementation Framework , slam_index See also: Feature-based vs direct SLAM workflow Source: cometlabs Front-end part of the Visual SLAM Implementation Framework Use only a small selected subset of the pixels in an image frame Feature maps generated are point clouds &amp;ndash;&amp;gt; used to track the camera pose Requires feature extraction and matching To minimise: reprojection error (difference between a point&amp;rsquo;s tracked location and where it is expected to be given camera pose estimate) Pose estimation based on RANSAC A frame with most of its features concentrated in a small area: bad as the features are more likely to overlap Sparse</description></item><item><title>Topological SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/topological-slam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/topological-slam/</guid><description>Parent: Classification of image-based camera localization approaches Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)
does not need accurate computation of 3D maps represents the environment by connectivity or topology e.g. Kuipers [130] used a hierarchical description of the spatial environment
a topological network description mediates between a control and metrical level distinctive places and paths are defined by their properties at the control level serve as nodes and arcs of the topological model Decreasing in popularity</description></item><item><title>50.2.2 Measurement noise R, V (landmark)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.2-measurement-noise-r-v-landmark/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.2-measurement-noise-r-v-landmark/</guid><description>Parent: Multivariate Kalman filter algorithm Source: rlabbe Kalman/Bayesian filters in Python R models the noise in the sensors as a covariance matrix dim(R) = m x m (m: number of sensors) Possible complications in multisensor systems, the correlation between the sensors might not be clear sensor noise might not be pure Gaussian Source: http://www.linkedin.com/pulse/tuning-extended-kalman-filter-process-noise-training-alex-thompson Ways to obtain R
Using the variances given in the sensor specifications Comopare the measurements against a strong ground truth and derive the variance variable by variable Record the steady state measurements over a long period of time and measure the variance (look at the histogram) Source: [Schneider 2013 How to not make the EKF fail](schneider 2013 how to-not-make-the-ekf-fail.</description></item><item><title>Measurement model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/measurement-model/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/measurement-model/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices/vectors Estimate of the range and bearing (from landmark) in Step 2: Re-observation x, y, theta - current position estimate lambdax, y - landmark position
Jacobian H w.r.t. x, y, theta (here for regular EKF, not for extended) In SLAM we need additional values for the landmarks here for landmark number two in extended EKF Upper row is for information, not part of matrix First three columns are regular H Landmarks don&amp;rsquo;t have any rotation</description></item><item><title>SLAM-specific jacobians</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/slam-specific-jacobians/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/slam-specific-jacobians/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices/vectors Jxr
Jacobian of the prediction of landmarks, which does not include prediction of theta, w.r.t. robot POSE same as J_prediction model, except without rotation term Jz Jacobian of prediction of landmarks, but w.r.t. [range, bearing]</description></item><item><title>Step 2 Re-observation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-2-reobservation/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-2-reobservation/</guid><description>Parent: Basic EKF for SLAM Source: SLAM for Dummies Second step in the three-step EKF — overview
In this step we update the robot position that we got in [step 1]](studienarbeit/step-1-odometry-update-prediction-step.md) Compensate for errors due to odometry pos_est (odometry-based) - pos_actual (LM-based) = Innovation, (based on the LM that the robot can see)
Use this to update robot position
Update the uncertainty of each observed LM to reflect recent changes e.</description></item><item><title>Step 3 New landmarks</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-3-new-landmarks/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-3-new-landmarks/</guid><description>Parent: Basic EKF for SLAM Source: SLAM for dummies Overview
Landmarks that are new are not dealt with until step 3. Delaying the incorporation of new landmarks until the will decrease the computation cost needed for this step the covariance matrix, P, and the system state, X, are smaller by then. Update state vector x and covariance matrix P with new landmarks Add new landmark to state vector X Add new row and column to covariance matrix Covariance for new landmark Robot-landmark covariance</description></item><item><title>EKF matrices/vectors</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-matrices-vectors/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-matrices-vectors/</guid><description>Source: SLAM for Dummies System state X Estimate of POSE Jacobian of prediction model Landmark range and bearing Jacobian of measurement model Covariance matrix P Kalman gain K SLAM-specific jacobians</description></item><item><title>SLAM resources</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/slam-resources/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/slam-resources/</guid><description>Parent: SLAM Index Theory
Wikipedia SLAM http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation Thrun - Probabilistic Robotics SLAM for dummies Andrew Davison research page at the Department of Computing , Imperial College London about SLAM using vision. Paper 2002 on monocular SLAM SLAM lectures on YouTube http://openslam-org.github.io / Tutorials SLAM summer school SS06: http://www.robots.ox.ac.uk/~SSS06/Website/
Programming Programmatic implementations of MonoSLAM</description></item><item><title>Wikipedia SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-slam/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-slam/</guid><description>Source: http://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping Parents: SLAM Index , slam-resources Different types of sensors give rise to different SLAM algorithms whose assumptions are most appropriate to the sensors.
At one extreme, visual features provide details of many points within an area &amp;ndash;&amp;gt; rendering SLAM unnecessary shapes in these point clouds can be easily and unambiguously aligned at each step via image registration . At the opposite extreme, tactile sensors are extremely sparse they contain only information about points very close to the agent require strong prior models to compensate in purely tactile SLAM.</description></item><item><title>Wikipedia Visual odometry</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-visual-odometry/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-visual-odometry/</guid><description>Source: http://en.wikipedia.org/wiki/Visual_odometry Odometry Visual sensors for localisation</description></item><item><title>External data in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/external-data-in-sofa/</link><pubDate>Fri, 24 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/external-data-in-sofa/</guid><description>Parent: SofaPython Index http://www.sofa-framework.org/community/forum/topic/how-to-use-external-data-in-sofa/ http://www.sofa-framework.org/community/forum/topic/how-to-send-data-to-sofa-through-socket/ http://www.sofa-framework.org/community/forum/topic/connecting-sofa-to-an-external-data-com-port/</description></item><item><title>http://www.sofa-framework.org/applications/gallery/percutaneous-liver-surgery/</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/https-www.sofa-framework.org-applications-gallery-percutaneous-liver-surgery-/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/https-www.sofa-framework.org-applications-gallery-percutaneous-liver-surgery-/</guid><description>http://www.sofa-framework.org/applications/gallery/percutaneous-liver-surgery/ Constraint-based haptic rendering</description></item><item><title>Registration</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/registration/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/registration/</guid><description> Aligning two points, each in different spaces respectively, together Examples aligning an endoscope coordinate frame to CT data (based on a similarity metric between endoscopic image and CT image) &amp;ndash; from https://pubmed.ncbi.nlm.nih.gov/25991876/ match virtual surface to corresponding endoscopic video &amp;ndash; https://ieeexplore.ieee.org/document/958638 Parent: SofaPython Index allows a matching between deformable surfaces finds spatial transformations to align two point sets or two meshes done based on: either target surfaces (ClosestPointRegistrationForceField , RegistrationContactForceField) or target images (IntensityProfileRegistrationForceField), which requires the use of the image plugin</description></item><item><title>Topological changes during elastic registration</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/topological-changes-during-elastic-registration/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/topological-changes-during-elastic-registration/</guid><description>http://www.sofa-framework.org/applications/gallery/augmented-reality-in-nephrology/
http://www.youtube.com/watch?v=3rfdL3-wWE0</description></item><item><title>Sockets Errno 10054</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sockets-errno-10054/</link><pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sockets-errno-10054/</guid><description>Parent: SofaPython Index WSAECONNRESET 10054 Connection reset by peer. An existing connection was forcibly closed by the remote host. This normally results if the peer application on the remote host is suddenly stopped, the host is rebooted, the host or remote network interface is disabled, or the remote host uses a hard close (see setsockopt for more information on the SO_LINGER option on the remote socket). This error may also result if a connection was broken due to keep-alive activity detecting a failure while one or more operations are in progress.</description></item><item><title>Building SOFA on Windows</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/building-sofa-on-windows/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/building-sofa-on-windows/</guid><description>Parent: SofaPython Index http://www.sofa-framework.org/community/doc/getting-started/build/windows/</description></item><item><title>Install ROSConnector in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/install-rosconnector-in-sofa/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/install-rosconnector-in-sofa/</guid><description>Parent: SofaPython Index http://github.com/sofa-framework/SofaROSConnector Documentation (outdated for current SOFA version 20.06.00)
http://www.sofa-framework.org/community/forum/topic/error-configuring-cmake-sofarosconnector/ Pending answer. Last reply 10th July 2020.
Alternative using SoftRobots: &amp;lt;http://www.sofa-framework.org/community/forum/topic/error-with-plugins-with-sofarosconnector/
post-15665&amp;gt; http://project.inria.fr/softrobot/</description></item><item><title>Sending data using sockets</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sending-data-using-sockets/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sending-data-using-sockets/</guid><description>Parent: SofaPython Index http://github.com/psomers3/PyDataSocket
http://docs.python.org/3/howto/sockets.html Sockets: form of IPC (inter-process communication), for cross-platform communication (Alternatives, for fast IPC: pipes, shared memory)
“client” socket - an endpoint of a conversation e.g. browser, other client applications “server” socket, which is more like a switchboard operator. The client application (your browser, for example) uses e.g. web server (uses both server and client sockets) Roughly, how a socket works (ex: clicking a link on the browser) Client socket (browser) / Receive</description></item><item><title>SofaPython API/Documentation links</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sofapython-api-documentation-links/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sofapython-api-documentation-links/</guid><description>Parent: SofaPython Index SP2
SofaPython pdf http://www.sofa-framework.org/api/master/plugins/SofaPython/html/index.html http://sofacomponents.readthedocs.io/en/latest/index.html SP3
http://sofapython3.readthedocs.io/en/latest/menu/SofaPlugin.html</description></item><item><title>STLIB (Sofa Template Library)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/stlib-sofa-template-library/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/stlib-sofa-template-library/</guid><description>Parent: SofaPython Index http://github.com/SofaDefrost/STLIB
API doc: http://stlib.readthedocs.io/en/latest/index.html
contains sofa scene template common scene template used regularly templates should be compatible with .pyscn and PSL scenes</description></item><item><title>Possible plugins</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/possible-plugins/</link><pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/possible-plugins/</guid><description>Parent: Scope of Studienarbeit , sofapython-index Communication
ZMQCommunication someone&amp;rsquo;s own plugin Optical system
OptiTrackNatNet Mesh geometry/topology
CGALPlugin (computational geometry algorithms) Haptic
Haptics with Geomagic &amp;ndash; requires Geomagic probe, but code/intro may be useful SofaHaptics Haption Flexible - for deformations Sensable Robot arm
SoftRobots ROS Connector  too complicated Scenes
STLIB (Sofa Template Library) Registration</description></item><item><title>SOFA Cataract surgery</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sofa-cataract-surgery/</link><pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sofa-cataract-surgery/</guid><description>http://www.sofa-framework.org/applications/gallery/eye-surgery-simulator-insimo/
SOFA – Cataract Surgery – InSimo www.sofa-framework.orgThe SOFA technology is at the core of a advanced eye surgery simulator developed in the context of the HelpMeSee project. HelpMeSee is an American foundation with a singular mission:… read more →</description></item><item><title>Basic python script in Sofa</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/basic-python-script-in-sofa/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/basic-python-script-in-sofa/</guid><description>Parent: SofaPython Index Imports import Sofa
General functions
createGraph(self, root) reset() onKeyPressed() &amp;hellip; Required in every script: createScene(rootNode)
Create a child from a node node.createChild(&amp;lsquo;Name&amp;rsquo;)
Add an object component to the node: node.createObject(type in string, kwargs**)</description></item><item><title>Collision model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/collision-model/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/collision-model/</guid><description>Source: SOFA extended documentation Parent: Models in SOFA Primitives coming into contact — we need
collision detection collision response Collision detection approaches:
Distances between pairs of geometric primitives Points in distance fields Distances between colliding meshes using ray-tracing Intersection volume using images Collision model
Similar to internal model
Topology/geometry is different (geometry specially for contact model), can be stored in a data structure dedicated to collision detection e.</description></item><item><title>Data structure in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/data-structure-in-sofa/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/data-structure-in-sofa/</guid><description>Source: SOFA extended documentation Three different solutions for three relevant levels [of organisation of simulation data].
Scenegraph ( directed acyclic graphs) s. also: Visitors Component attributes
Component params stored using Data&amp;lt;&amp;hellip;&amp;gt; containers e.g. list of particle indices Data&amp;lt;vector&amp;gt; Engines Create connections between Data instances, for synchronisation of values Compute a value from several others (input/output processor) Are only used to apply straightforward relations between model parameters State update algorithms are implemented using visitors Mesh geometry and mesh-topology</description></item><item><title>Haptic rendering</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/haptic-rendering/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/haptic-rendering/</guid><description>Source: SOFA extended documentation The main interest of interactive simulation is that
the user can modify the course of the computations in real-time when a virtual medical instrument comes into contact with some models of a soft-tissue, instantaneous deformations must be computed This visual feedback of the contact can be enhanced by haptic rendering so that the surgeon can really feel the contact.&amp;quot; Two main issues in SOFA for providing haptics</description></item><item><title>Internal model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/internal-model/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/internal-model/</guid><description>Source: SOFA extended documentation Parent: Models in SOFA For the internal deformable mechanics
Contains the independent DOFs, mass and physical laws Mechanical behaviour modelled e.g. by FEM Geometry of this model is optimised for the computation of internal forces usually by using a reduced number of well-shaped tetrahedra this increases speed and stability however not accurate enough for collision detection nor is it smooth enough for visuals * Boxes: fixed nodes* Arrows: external forces [Mathematical model of the internal model in SOFA](mathematical model of-the-internal-model-in-sofa.</description></item><item><title>Mappings</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mappings/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mappings/</guid><description>Source: [SOFA extended documentation](SOFA extended documentation.md) Backlinks: Models in SOFA , Visual model Enforces consistency between the many model representations of an object, by propagating information (such as positions, velocities, forces) in a top-down and bottom-up approach. Figure: Mappings between liver and grasper models
Master model imposes its displacements to the slave models ([top-down mapping](top-down mapping.md)) Slaves, depending on model type, can also pass information (e.g. forces) back to the master (bottom-up) A mapped model can be master of another model Also used to connect generalised coordinates (e.</description></item><item><title>Mesh geometry</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mesh-geometry/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mesh-geometry/</guid><description>Source: SOFA extended documentation Parent: Data structure in SOFA See also: Mesh topology Mesh geometry: location of vertices in space
Meshes
k-simplices (triangles) k-cubes (quads) --&amp;gt; decomposition into k-cells
1-cell: edges 2-cells: triangles, quads 3-cells: tetrahedron, hexahedron Mesh data:
containers, similar to STL std::vector classes there are as many data structures for mesh data as topological elements , e.g. vertices, edges, triangles, quads, tetras, hexas e.</description></item><item><title>Models in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/models-in-sofa/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/models-in-sofa/</guid><description>Source: SOFA extended documentation Backlinks: SOFA Introduction A simulation object can have several models
Each model is &amp;lsquo;predestined&amp;rsquo; for a certain task Each model is independent of the other Synchronisation of models: via a mapping mechanism Three typical models for a physical object
Internal mechanical model Collision model Visual model One of the models acts as the master
typically the internal model imposes its displacements to slaves using mappings (synchronisation of the models)</description></item><item><title>Running SOFA with Python</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/running-sofa-with-python/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/running-sofa-with-python/</guid><description>Parent: SofaPython Index From command line Add to path environment and then execute runSofa via command line
With a python script runSofa -l SofaPython ./script_name.py
How to make SofaPython loaded by default? In bin/plugin_list.conf? Yes sofa-launcher might be useful With pipenv pipenv run runsofa</description></item><item><title>Scene graph in SOFA</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/scene-graph-in-sofa/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/scene-graph-in-sofa/</guid><description>Source: SOFA extended documentation Parent: Data structure in SOFA Backlinks: SOFA Introduction See also: Scene graph (general) Pool of simulated objects and algorithms in a hierarchical data structure Scenes can be built procedurally or read from XML files Root node represents whole simulation Graph is processed by using visitors A scene graph node
Gathers components associated with the same DOFs/topology Connections between non-sibling components require explicit references Example: The collision spheres of the rigid object are in a child contact node of their own, because they are not independent DOFs (separate from independent DOFs in MechanicalState ) they are of a different data type Interactions between the rigid and deformable objects are handled by a shared component (ContactSpring) defined as a sibling node to both (coupling) Soft coupling using penalty forces Can be modelled by a constant interaction force (assumption) during each time step Compatible with all explicit time intergration schemes Hard coupling using penalty forces / constraint-based interaction via Lagrange multipliers Stiff interaction forces Implicit integration necessary, for large time steps without any instabilities Generally more efficient to process independent interaction groups using separate solvers</description></item><item><title>SOFA extended documentation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sofa-extended-documentation/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sofa-extended-documentation/</guid><description>Source: http://hal.inria.fr/hal-00681539 Authors: Faure et al Backlinks: Scope of Studienarbeit Abstract
SOFA: open source C++ library mainly for interactive physical/medical simulation modular approach by decomposing simulators into its constituent components (DOF, differential equations, solvers etc), and organising them in a scenegraph data structure multimodel representation of objects (collision model, visual model etc) Chapters
Read
1: introduction 2: multimodel framework 3: data structures 3.1 scenegraph and visitors 3.</description></item><item><title>SOFA Introduction</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sofa-introduction/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sofa-introduction/</guid><description>Source: SOFA extended documentation Goal of SOFA: To provide a highly modular framework for interactive medical simulation, enabling collaboration across different disciplines Concept: scene-graph -based multimodel representatios
How it works:
Simulators are broken down into independent components Component: an aspect of the simulation e.g. DOF, forces, constraints, ODEs/PDEs, solvers, algorithms Components are organised in a scene graph data structure Simulated objects represented via several models</description></item><item><title>Using python with existing scene</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/using-python-with-existing-scene/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/using-python-with-existing-scene/</guid><description>Parent: SofaPython Index In scene graph
Add plugin in the scene using RequiredPlugin Define a PythonScriptController in the scene graph</description></item><item><title>Visitors</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/visitors/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/visitors/</guid><description>Source: [SOFA extended documentation](SOFA extended documentation.md) Parent: [Data structure in SOFA](Data structure in SOFA.md) Backlinks: [Scene graph in SOFA](Scene graph in SOFA.md), [Simulation algorithms in SOFA](Simulation algorithms in SOFA.md)
For processing of data structure: parent to child
Allows decoupling of physical model from simulation algo e.g. Easy to replace a time integrator, which wouldn&amp;rsquo;t be the case in a dataflow graph (coupling of data and algo)</description></item><item><title>(GRK 2543) Intraoperative Multi-sensor Tissue Differentiation in Oncology</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/grk-2543/</link><pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/grk-2543/</guid><description>Sources:
Deutsche Forschungsgemeinschaft project page ISYS project page Background:  Cooperation between Uni Tübingen and Uni Stuttgart Gynelogical and urological application scenarios Aims: Minimise invasiveness and duration of surgical cancer treatment , while at the same time maximising effectiveness of the treatment Minimise damage to surrounding tissue during tumour resection Aid decision-making during surgery (intraoperative) Reliable differentiation between cancerous tissue and the surrounding healthy tissue Decide whether to preserve tissue or continue with surgery Complement existing techniques Histological Imaging Optical (IR, Raman spectroscopy) Current standard of intraoperative tissue identification: frozen section diagnostics (takes about 30 minutes)</description></item><item><title>Difference between haptic feedback and vibration alerting</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/difference-between-haptic-feedback-and-vibration-alerting/</link><pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/difference-between-haptic-feedback-and-vibration-alerting/</guid><description>Source: http://www.precisionmicrodrives.com/haptic-feedback/introduction-to-haptic-feedback/ Parent: Scope of Studienarbeit</description></item><item><title/><link>https://salehahr.github.io/zettelkasten/studienarbeit/optical-flow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/optical-flow/</guid><description>https://en.wikipedia.org/wiki/Optical_flow https://nanonets.com/blog/optical-flow/</description></item><item><title>Introduction to the Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/introduction-to-the-kalman-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/introduction-to-the-kalman-filter/</guid><description> http://resourcium.org/journey/introduction-kalman-filter</description></item></channel></rss>