<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Studienarbeit notes"><title>Zettelkasten - Kalman gain for EKF</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=../../icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&family=Source+Sans+Pro:wght@400;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><style>:root{--light:#faf8f8;--dark:#141021;--secondary:#284b63;--tertiary:#84a59d;--visited:#afbfc9;--primary:#f28482;--gray:#4e4e4e;--lightgray:#f0f0f0;--outlinegray:#dadada}[saved-theme=dark]{--light:#1e1e21 !important;--dark:#fbfffe !important;--secondary:#5b778a !important;--visited:#4a575e !important;--tertiary:#84a59d !important;--primary:#f58382 !important;--gray:#d4d4d4 !important;--lightgray:#292633 !important;--outlinegray:#343434 !important}.backlinks-container>ul>li{display:inline}.backlinks-container>ul>li:after{content:", "}.backlinks-container>ul>li:last-child:after{content:""}h3+ul{margin-top:-10px}</style><style>:root{--lt-colours-light:var(--light) !important;--lt-colours-lightgray:var(--lightgray) !important;--lt-colours-dark:var(--secondary) !important;--lt-colours-secondary:var(--tertiary) !important;--lt-colours-gray:var(--outlinegray) !important}h1,h2,h3,h4,ol,ul,thead{font-family:Inter;color:var(--dark)}p,ul,text{font-family:source sans pro,sans-serif;color:var(--gray);fill:var(--gray)}a{font-family:Inter;font-weight:700;font-size:1em;text-decoration:none;transition:all .2s ease;color:var(--secondary)}a:hover{color:var(--tertiary)!important}#TableOfContents>ol{counter-reset:section;margin-left:0;padding-left:1.5em}#TableOfContents>ol>li{counter-increment:section}#TableOfContents>ol>li>ol{counter-reset:subsection}#TableOfContents>ol>li>ol>li{counter-increment:subsection}#TableOfContents>ol>li>ol>li::marker{content:counter(section)"." counter(subsection)"  "}#TableOfContents>ol>li::marker{content:counter(section)"  "}#TableOfContents>ol>li::marker,#TableOfContents>ol>li>ol>li::marker{font-family:Source Sans Pro;font-weight:700}footer{margin-top:4em;text-align:center}table{width:100%}img{width:100%;border-radius:3px;margin:1em 0}p>img+em{display:block;transform:translateY(-1em)}sup{line-height:0}p,tbody,li{font-family:Source Sans Pro;color:var(--gray);line-height:1.5em}h2{opacity:.85}h3{opacity:.75}blockquote{margin-left:0;border-left:3px solid var(--secondary);padding-left:1em;transition:border-color .2s ease}blockquote:hover{border-color:var(--tertiary)}table{padding:1.5em}td,th{padding:.1em .5em}.footnotes p{margin:.5em 0}article a{font-family:Source Sans Pro;font-weight:600;text-decoration:underline;text-decoration-color:var(--tertiary);text-decoration-thickness:.15em}sup>a{text-decoration:none;padding:0 .1em 0 .2em}pre{font-family:fira code;padding:.75em;border-radius:3px;overflow-x:scroll}code{font-family:fira code;font-size:.85em;padding:.15em .3em;border-radius:5px;background:var(--lightgray)}html{scroll-behavior:smooth}body{margin:0;height:100vh;width:100vw;overflow-x:hidden;background-color:var(--light)}@keyframes fadeIn{0%{opacity:0}100%{opacity:1}}footer{margin-top:4em}footer>a{font-size:1em;color:var(--secondary);padding:0 .5em 3em}hr{width:25%;margin:4em auto;height:2px;border-radius:1px;border-width:0;color:var(--dark);background-color:var(--dark)}a[href^="/"]{text-decoration:none;background-color:#afbfc922;padding:0 .2em;border-radius:3px}.singlePage{margin:4em 30vw}@media all and (max-width:1200px){.singlePage{margin:25px 5vw}}.listPage{margin:4em 30vw}@media all and (max-width:1200px){.listPage{margin:25px 5vw}}.page-end{display:flex;flex-direction:row}@media all and (max-width:780px){.page-end{flex-direction:column}}.page-end>*{flex:1 0}.page-end>.backlinks-container>ul{list-style:none;padding-left:0;margin-right:2em}.page-end>.backlinks-container>ul>li{margin:.5em 0;padding:.25em 1em;border:var(--outlinegray)1px solid;border-radius:5px}.page-end #graph-container{border:var(--outlinegray)1px solid;border-radius:5px}.centered{margin-top:30vh}header{display:flex;flex-direction:row;align-items:center}@media all and (max-width:600px){header>nav{display:none}}header>nav>a{margin-left:2em}header>.spacer{flex:auto}header>svg{cursor:pointer;width:18px;min-width:18px;margin:0 1em}header>svg:hover .search-path{stroke:var(--tertiary)}header>svg .search-path{stroke:var(--gray);stroke-width:2px;transition:stroke .5s ease}#search-container{position:fixed;z-index:9999;left:0;top:0;width:100vw;height:100vh;display:none;backdrop-filter:blur(4px);-webkit-backdrop-filter:blur(4px)}#search-container>div{width:50%;margin-top:15vh;margin-left:auto;margin-right:auto}@media all and (max-width:1200px){#search-container>div{width:90%}}#search-container>div>*{width:100%;border-radius:4px;background:var(--light);box-shadow:0 14px 50px rgba(27,33,48,.12),0 10px 30px rgba(27,33,48,.16);margin-bottom:2em}#search-container>div>input{box-sizing:border-box;padding:.5em 1em;font-family:Inter,sans-serif;color:var(--dark);font-size:1.1em;border:1px solid var(--outlinegray)}#search-container>div>input:focus{outline:none}#search-container>div>#results-container>.result-card{padding:1em;cursor:pointer;transition:background .2s ease;border:1px solid var(--outlinegray);border-bottom:none}#search-container>div>#results-container>.result-card:hover{background:rgba(180,180,180,.15)}#search-container>div>#results-container>.result-card:first-of-type{border-top-left-radius:5px;border-top-right-radius:5px}#search-container>div>#results-container>.result-card:last-of-type{border-bottom-left-radius:5px;border-bottom-right-radius:5px;border-bottom:1px solid var(--outlinegray)}#search-container>div>#results-container>.result-card>h3,#search-container>div>#results-container>.result-card>p{margin:0}#search-container>div>#results-container>.result-card .search-highlight{background-color:#afbfc966;padding:.05em .2em;border-radius:3px}</style><style>.darkmode{float:right;padding:1em;min-width:30px;position:relative}@media all and (max-width:450px){.darkmode{padding:1em}}.darkmode>.toggle{display:none;box-sizing:border-box}.darkmode svg{opacity:0;position:absolute;width:20px;height:20px;top:calc(50% - 10px);margin:0 7px;fill:var(--gray);transition:opacity .1s ease}.toggle:checked~label>#dayIcon{opacity:0}.toggle:checked~label>#nightIcon{opacity:1}.toggle:not(:checked)~label>#dayIcon{opacity:1}.toggle:not(:checked)~label>#nightIcon{opacity:0}</style><style>.chroma{color:#f8f8f2;background-color:#282a36}.chroma .lntd{vertical-align:top;padding:0;margin:0;border:0}.chroma .lntable{border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block}.chroma .hl{display:block;width:100%;background-color:#ffc}.chroma .lnt{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .ln{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .k{color:#ff79c6}.chroma .kc{color:#ff79c6}.chroma .kd{color:#8be9fd;font-style:italic}.chroma .kn{color:#ff79c6}.chroma .kp{color:#ff79c6}.chroma .kr{color:#ff79c6}.chroma .kt{color:#8be9fd}.chroma .na{color:#50fa7b}.chroma .nb{color:#8be9fd;font-style:italic}.chroma .nc{color:#50fa7b}.chroma .nf{color:#50fa7b}.chroma .nl{color:#8be9fd;font-style:italic}.chroma .nt{color:#ff79c6}.chroma .nv{color:#8be9fd;font-style:italic}.chroma .vc{color:#8be9fd;font-style:italic}.chroma .vg{color:#8be9fd;font-style:italic}.chroma .vi{color:#8be9fd;font-style:italic}.chroma .s{color:#f1fa8c}.chroma .sa{color:#f1fa8c}.chroma .sb{color:#f1fa8c}.chroma .sc{color:#f1fa8c}.chroma .dl{color:#f1fa8c}.chroma .sd{color:#f1fa8c}.chroma .s2{color:#f1fa8c}.chroma .se{color:#f1fa8c}.chroma .sh{color:#f1fa8c}.chroma .si{color:#f1fa8c}.chroma .sx{color:#f1fa8c}.chroma .sr{color:#f1fa8c}.chroma .s1{color:#f1fa8c}.chroma .ss{color:#f1fa8c}.chroma .m{color:#bd93f9}.chroma .mb{color:#bd93f9}.chroma .mf{color:#bd93f9}.chroma .mh{color:#bd93f9}.chroma .mi{color:#bd93f9}.chroma .il{color:#bd93f9}.chroma .mo{color:#bd93f9}.chroma .o{color:#ff79c6}.chroma .ow{color:#ff79c6}.chroma .c{color:#6272a4}.chroma .ch{color:#6272a4}.chroma .cm{color:#6272a4}.chroma .c1{color:#6272a4}.chroma .cs{color:#6272a4}.chroma .cp{color:#ff79c6}.chroma .cpf{color:#ff79c6}.chroma .gd{color:#8b080b}.chroma .ge{text-decoration:underline}.chroma .gh{font-weight:700}.chroma .gi{font-weight:700}.chroma .go{color:#44475a}.chroma .gu{font-weight:700}.chroma .gl{text-decoration:underline}.lntd:first-of-type>.chroma{padding-right:0}.chroma code{font-family:fira code!important;font-size:.85em;line-height:1em;background:0 0;padding:0}.chroma{border-radius:3px;margin:0}</style><script>const userPref=window.matchMedia('(prefers-color-scheme: light)').matches?'light':'dark',currentTheme=localStorage.getItem('theme')??userPref;currentTheme&&document.documentElement.setAttribute('saved-theme',currentTheme);const switchTheme=a=>{a.target.checked?(document.documentElement.setAttribute('saved-theme','dark'),localStorage.setItem('theme','dark')):(document.documentElement.setAttribute('saved-theme','light'),localStorage.setItem('theme','light'))};window.addEventListener('DOMContentLoaded',()=>{const a=document.querySelector('#darkmode-toggle');a.addEventListener('change',switchTheme,!1),currentTheme==='dark'&&(a.checked=!0)})</script></head><body><main><div id=search-container><div><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/gh/nextapps-de/flexsearch@0.7.2/dist/flexsearch.bundle.js></script><script>const removeMarkdown=(c,b={listUnicodeChar:!1,stripListLeaders:!0,gfm:!0,useImgAltText:!1,preserveLinks:!1})=>{let a=c||"";a=a.replace(/^(-\s*?|\*\s*?|_\s*?){3,}\s*$/gm,"");try{b.stripListLeaders&&(b.listUnicodeChar?a=a.replace(/^([\s\t]*)([\*\-\+]|\d+\.)\s+/gm,b.listUnicodeChar+" $1"):a=a.replace(/^([\s\t]*)([\*\-\+]|\d+\.)\s+/gm,"$1")),b.gfm&&(a=a.replace(/\n={2,}/g,"\n").replace(/~{3}.*\n/g,"").replace(/~~/g,"").replace(/`{3}.*\n/g,"")),b.preserveLinks&&(a=a.replace(/\[(.*?)\][\[\(](.*?)[\]\)]/g,"$1 ($2)")),a=a.replace(/<[^>]*>/g,"").replace(/^[=\-]{2,}\s*$/g,"").replace(/\[\^.+?\](\: .*?$)?/g,"").replace(/\s{0,2}\[.*?\]: .*?$/g,"").replace(/\!\[(.*?)\][\[\(].*?[\]\)]/g,b.useImgAltText?"$1":"").replace(/\[(.*?)\][\[\(].*?[\]\)]/g,"$1").replace(/^\s{0,3}>\s?/g,"").replace(/(^|\n)\s{0,3}>\s?/g,"\n\n").replace(/^\s{1,2}\[(.*?)\]: (\S+)( ".*?")?\s*$/g,"").replace(/^(\n)?\s{0,}#{1,6}\s+| {0,}(\n)?\s{0,}#{0,} {0,}(\n)?\s{0,}$/gm,"$1$2$3").replace(/([\*_]{1,3})(\S.*?\S{0,1})\1/g,"$2").replace(/([\*_]{1,3})(\S.*?\S{0,1})\1/g,"$2").replace(/(`{3,})(.*?)\1/gm,"$2").replace(/`(.+?)`/g,"$1").replace(/\n{2,}/g,"\n\n")}catch(a){return console.error(a),c}return a}</script><script>const contentIndex=new FlexSearch.Worker({tokenize:"strict",charset:"latin:advanced",context:!0,depth:3,cache:10,suggest:!0}),scrapedContent={"/":{content:"---\ntitle: Zettelkasten\naliases:\n- /_index/\n---\n\n# Zettelkasten\nWelcome to my [Zettelkasten](https://en.wikipedia.org/wiki/Zettelkasten).\n\nThis implementation is optimised for view in [Obsidian](https://www.obsidian.md/).  \nThe web version is currently very buggy.\n\n## Master's thesis\nComing soon\n\n\n\u003cpre\u003e\u003c/pre\u003e\n## Studienarbeit\n* [Permanent notes](/permanent/_index.md)\n* [SLAM index](SLAM/slam_index.md)\n* [Rotations index](rotations/rotations-so3-group-index.md)\n* [Quaternion index](rotations/quaternion-index.md)\n\n\u003cpre\u003e\u003c/pre\u003e\n* [Bibliography](bibliography/_index.md)\n* [Works of interest](bibliography/works-of-interest.md)\n\n\u003cpre\u003e\u003c/pre\u003e\n## Other topics",title:"Untitled Page"},"/SLAM/50.2.1-process-noise-q-and-w-odometry":{content:'---\ntitle: 50.2.1 Process noise Q and W (odometry)\ndate: "2020-07-29"\ntags:\n  - filters/EKF\n  - -sa/processed\n  - -permanent\n  - -published\n---\n\n**See also**: [Factors affecting Kalman filter performance](studienarbeit/factors-affecting-kalman-filter-performance.md)\n\n**Source**: [Tereshkov 2015](http://www.researchgate.net/publication/271532640_A_Simple_Observer_for_Gyro_and_Accelerometer_Biases_in_Land_Navigation_Systems)\n\n*   Process noise covariance matrix has no clear physical meaning, cannot be deduced from sensor characteristics\n*   Leads to non-intuitive, iterative procedures to tune KFs\n*   Which means that KF optimality is rarely achieved in practice\n\nAlternative to KF tuning: the use of geometric observers\n\n*   estimates are expresssed only in terms of quantities with clear geometrical meaning\n\n***\n\n**Source**: [Schneider 2013](studienarbeit/schneider-2013-how-to-not-make-the-ekf-fail.md)\n\n*   If perfect model: $Q$ only describes the covariance of the random process noise\n*   Not perfect model, has:\n    *   parametric errors (-\u003e parameter identification)\n    *   structural erors (error in model structure)\n    *   workaround: e.g. choosing from a set of candidate models, etc.\n*   If model parameters are to be estimated: (param\\_est, Cov\\_param\\_est)\n    *   the covariance of the estimated parameter is often ignored in EKF design\n\n![unknown_filename.11.png](studienarbeit/_resources/50.2.1_Process_noise_Q_and_W_(odometry).resources/unknown_filename.11.png)\n![unknown_filename.12.png](studienarbeit/_resources/50.2.1_Process_noise_Q_and_W_(odometry).resources/unknown_filename.12.png) ( if representing the stochastic part as an additive component)\n\n![unknown_filename.7.png](studienarbeit/_resources/50.2.1_Process_noise_Q_and_W_(odometry).resources/unknown_filename.7.png)\n\nTo calculate $Q$ from $C_p$\n\n1.  Set\n	$$ \\mathbf{p} = \\hat{\\mathbf{p}} + \\delta \\mathbf{p}$$ \n2.  Do Taylor series expansion on prediction equation\n	$$ \\dot{\\mathbf{x}}(t)\n		= \\mathbf{f} \\left( \\mathbf{x}(t),\n				\\mathbf{u}(t), \\mathbf{\\hat{p}} \\right)\n				+ \\mathbf{J}_{\\hat{\\mathbf{p}}}(t)\n					\\delta \\mathbf{p}  	$$\n	$$ \\mathbf{J}_{\\hat{\\mathbf{p}}}(t)\n		= \\left( \\dfrac{\\delta \\mathbf{f}}{\\delta \\mathbf{p}} \\right)\n		_{\\mathbf{x}(t), \\mathbf{u}(t), \\mathbf{\\hat{p}} }$$\n	\n    \n3.  Calculate $Q$\n    ![unknown_filename.10.png](studienarbeit/_resources/50.2.1_Process_noise_Q_and_W_(odometry).resources/unknown_filename.10.png)\n    \n\n$k_Q \\geq 1$ (make larger if it plant-model seems very mismatched)\n\n*   $C_p$ is time invariant\n*   $Q$ is time-varying as J is time-varying\n\n* * *\n\nSource: [rlabbe](studienarbeit/rlabbe-kalman-bayesian-filters-in-python.md)\n\n### What does process noise mean?\nDeviation of real system from deterministic system, e.g. due to unforeseen disturbances (human intervention, weather, wind, etc)\n$$ \\dot{\\mathbf{x}} = f(\\mathbf{x}) + w $$\n\nNo noise is added to $\\mathbf{x}$ because the noise is white (0 mean)\n\n### Formula for Q\n$$ Q = \\mathbb{E} \\left[ \\mathbf{w} \\mathbf{w}^\\text{T} \\right] $$\n\n*   Important that there is a nonzero process noise or process variance, to avoid the filter becoming a [smug filter](smug-filter.md)\n*   A small value for process noise tells the filter that the prediction is very trustworthy, so this will result in a filter estimate that is quite straight (i.e. unnoisy), even though the sensor measurements are themselves very noisy\n\n![unknown_filename.3.png](studienarbeit/_resources/50.2.1_Process_noise_Q_and_W_(odometry).resources/unknown_filename.3.png)\n\n*   However, as the process variance indicates how much the system changes with time, if it is very low, the filter will be slow to react / won\'t correctly respond to meaasurements\n*   In short: the filter requires the variance to correctly describe the system behaviour in order for it to perform well!\n\n* * *\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\nUsed in [step 1 (odometry update/prediction step)](SLAM/ekf-1-prediction.md)\n\nProcess is assumed to have a gaussian noise proportional to the controls $\\Delta x$, $\\Delta y$, $\\Delta t$.\n\nProcess noise $Q \\in \\mathbb{R}^{3\\times 3}$\n$$\n\\begin{aligned}\n	Q \u0026= c \\cdot \\left[ \\begin{array}{ccc}\n		\\Delta x^2 \u0026 \\cdots\\\\\n		\u0026 \\Delta y^2\u0026 \\cdots\\\\\n		\u0026\u0026 \\Delta t^2\n		\\end{array} \\right]\\\\\n	\u0026= c \\cdot \\left[ \\begin{array}{rrr}\n		\\Delta x^2 \u0026 \\Delta x \\Delta y \u0026 \\Delta x \\Delta t\\\\\n		\\Delta y \\Delta x \u0026 \\Delta y^2 \u0026 \\Delta y \\Delta t\\\\\n		\\Delta t \\Delta x \u0026 \\Delta t \\Delta y \u0026 \\Delta t^2\n		\\end{array} \\right]\\\\\n\\end{aligned}\n$$\n\n## $C$\n*   Gaussian sample\n*   Representation of how exact the odometry is\n*   Set according to the robot odometry performance\n    *   e.g. by experiments and then tuning the value\n\n## $W$\n$$W = \\left[\\begin{array}{ccc}\n	\\Delta t \\cos \\theta\n	\u0026 \\Delta t \\sin \\theta\n	\u0026 \\Delta \\theta\n	\\end{array}\\right]^\\text{T}$$\n	\n$$Q = WCW^\\text{T}$$\n\n',title:"Untitled Page"},"/SLAM/algos-optimisation-based":{content:'---\ntitle: Some optimisation-based tightly-coupled multisensor SLAM algorithms\ndate: "2020-07-30"\ntags:\n  - SLAM/filter-vs-optim/opt-based\n  - -sa/processed\n  - SLAM/algos\n  - -published\n---\n\n**Source**: [Wu 2018](bibliography/wu-2018.md)\n\n* Uses nonlinear optimization\n* may potentially achieve higher accuracy due to the capability to limit linearization errors through repeated linearization of the inherently nonlinear problem\n\n\u003cpre\u003e\u003c/pre\u003e\n*   \\[117\\] [Forster 2017](studienarbeit/forster-2017-imu-preintegration.md): preintegration theory\n*   \\[118\\] OKVIS: a novel approach\n    *   to tightly integrate visual measurements with IMU\n    *   optimise a joint nonlinear cost function that integrates an IMU error term with the landmark reprojection error in a fully probabilistic manner\n    *   real-time operation: old states are marginalized to maintain a bounded-sized optimization window\n*   Li et al. \\[119\\] proposed tightly coupled, optimization based, monocular visual-inertial state estimation for camera localization in complex environments. This method can run on mobile devices with a lightweight [loop closure](SLAM/loop-closure-detection.md).\n*   [VIORB](bibliography/mur-artal-2017-vi-orb.md) a tightly coupled visual-inertial slam system was proposed, following ORBSLAM monocular SLAM\n\n',title:"Untitled Page"},"/SLAM/basic-ekf-for-slam":{content:'---\ntitle: Basic EKF for SLAM\ndate: "2020-07-27"\ntags:\n  - filters/EKF\n  - -sa/processed\n  - -published\n---\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\nA basic EKF implementation of [SLAM](SLAM/what-is-slam.md)   consists of multiple parts:\n\n*   [Landmark extraction](SLAM/landmark-extraction.md)\n*   [Data association](SLAM/data-association.md)\n\n1.  After odometry change (due to robot moving), [state estimation](SLAM/ekf-1-prediction.md) from odometry\n2.  [Update of the estimated state using re-observed landmark data](studienarbeit/ekf-2-reobservation.md)\n3.  [Update landmark database with new landmarks](studienarbeit/ekf-3-new-landmarks.md)\n\nNote: at any point in the three steps on the left, the EKF will have an estimate of the robots current position\n\n![Image.png](studienarbeit/_resources/Basic_EKF_for_SLAM.resources/Image.png)\n\n',title:"Untitled Page"},"/SLAM/covariance-matrix-p":{content:'---\ntitle: Covariance matrix P\ndate: "2020-07-29"\ntags:\n  - filters/EKF\n  - -sa/processed\n  - -published\n---\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\ns. also [EKF matrices](studienarbeit/ekf-matrices-vectors.md)\n\nCovariance matrix P\n\n*   Covariance: measure of correlation of two variables\n*   Correlation: measure of degree of linear dependence\n\n![unknown_filename.png](studienarbeit/_resources/Covariance_matrix_P.resources/unknown_filename.png)\n\n|     |     |     |\n| --- | --- | --- |\n| A   | covariance of the robote POSE\u003cbr\u003eupdated in [Step 1: Odometry update](SLAM/ekf-1-prediction.md)| 3x3 |\n| B .. C | covariance on the first .. nth landmark\u003cbr\u003e[Step 3: New landmarks](studienarbeit/ekf-3-new-landmarks.md) | 2x2 |\n| D   | covariance between POSE and first LM\u003cbr\u003eupdated in [Step 1: Odometry update](SLAM/ekf-1-prediction.md) | 2x3 |\n| E, etc | E = D^T, etc\u003cbr\u003eupdated in [Step 1: Odometry update](SLAM/ekf-1-prediction.md) | 3x2 |\n| F=G^T | [Step 3: New landmarks](studienarbeit/ekf-3-new-landmarks.md) |     |\n\nInitially $P = A$ (robot has not seen any LMs)\n\nInclude initial uncertainty\n\n*   There will often be a singular error if the initial uncertainty is not included\n*   good idea to include some initial error even though there is reason to believe that the initial robot position is exact\n*   initialise default values for the diagonal\n\nTop three rows: cross correlation between POSE and landmarks\nupdated in [step 1 (odometry update)](SLAM/ekf-1-prediction.md)\n',title:"Untitled Page"},"/SLAM/data-association":{content:"---\ntitle: Data association\ndate: \"2020-07-27\"\ntags:\n  - localisation/data-association\n  - -sa/processed\n  - -published\n---\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)  \n\nMatching observed [landmarks](SLAM/landmarks.md) from different scans (different time steps) with each other.\nAlso called 're-observing' landmarks.\n\n## Problems that can arise\n\n*   The landmark(s) might not be observed every time step (bad landmark)\n*   Something might be observed as a landmark, but it never appears again (bad landmark)\n*   Wrong association of a landmark to a previously seen landmark\n\n## Goal\nDefine a suitable data-association policy to minimise the first two problems\n\n* Given: database that stores previously seen landmarks (initially empty)\n* As a rule: a landmark is only considered worthwhile to be used in SLAM once it is seen N times\n\n## Data association policies\ne.g. [Nearest Neighbour](SLAM/nearest-neighbour.md)\n\n",title:"Untitled Page"},"/SLAM/distance-between-landmarks":{content:'---\ntitle: Distance between landmarks\ndate: "2020-08-06"\ntags:\n  - localisation/landmarks\n  - -sa/processed\n  - -published\n---\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\nMethods for calculating distance between [landmarks](SLAM/landmarks.md):\n\n*   Euclidean distance (suitable for far distances)\n*   Mahalanobis distance (better, but more complex)\n\n',title:"Untitled Page"},"/SLAM/ekf-1-prediction":{content:'---\ntitle: Step 1 Odometry update (Prediction step)\ndate: "2020-07-29"\ntags: \n- filters/EKF \n- -sa/processed\n- -published\n---\n\n**Parent**: [Basic EKF for SLAM](SLAM/basic-ekf-for-slam.md)\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\nFirst step in the three-step [EKF](SLAM/basic-ekf-for-slam.md)\n\n*   Update current state using odometry data\n*   Based on the controls given to the robot\n*   Calculate estimate of new POSE\n\nUpdate equation:  [prediction model](SLAM/prediction-model.md) ($x = x + \\Delta x \\cdot q$)  \nOr in a simple model, neglect the error term $q$\n\n1.  State vector gets updated via the prediction model\n2.  Jacobian of the prediction model also needs to be updated every iteration (with the controls deltax, ...)\n3.  [Process noise Q](SLAM/50.2.1-process-noise-q-and-w-odometry.md) updated to reflect control terms\n    ![unknown_filename.png](studienarbeit/_resources/Step_1__Odometry_update_(Prediction_step).resources/unknown_filename.png)\n    \n4.  Calculate new covariance for robot POSE (onlt the top left of the [P matrix](SLAM/covariance-matrix-p.md) using the Jacobian of pred. model A and process noise Q\n    ![unknown_filename.1.png](studienarbeit/_resources/Step_1__Odometry_update_(Prediction_step).resources/unknown_filename.1.png)\n    \n5.  New cross correlations in [P matrix](SLAM/covariance-matrix-p.md) (top three rows)\n    ![unknown_filename.2.png](studienarbeit/_resources/Step_1__Odometry_update_(Prediction_step).resources/unknown_filename.2.png)\n    \n6.  Transpose the cross correlations to the leftmost three columns\n\nDue to odometry errors, this estimate is not exact.\n\n',title:"Untitled Page"},"/SLAM/extended-kalman-filter":{content:"---\ntitle: Extended Kalman Filter\ndate: \"2020-07-27\"\ntags:\n  - filters/EKF\n  - -sa/processed\n---\n\nParent: [SLAM Index](SLAM/slam_index.md)\nBacklinks: [RANSAC](SLAM/ransac.md),  [nearest neighbour](SLAM/nearest-neighbour.md), [Filtering in localisation](SLAM/filter-localisation-methods.md)\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\n*   keeps track of an estimate of the position uncertainty\n*   keeps track of the uncertainty in the features/landmarks seen\n\n[General EKF implementation (non-SLAM)](studienarbeit/general-ekf-implementation-non-slam.md)  \n[Basic EKF for SLAM](SLAM/basic-ekf-for-slam.md)\n\nDiagram:\n![unknown_filename.png](studienarbeit/_resources/Extended_Kalman_Filter.resources/unknown_filename.png)\n\n| Triangle | Robot |\n| --- | --- |\n| Stars | Landmarks |\n| Dashed triangle | Robot's position based on odometry alone (where it thinks it is) |\n| Dotted triangle | Robot's position estimate based on EKF |\n| Solid line triangle | Robot's actual position in real life! |\n\n[EKF matrices](studienarbeit/ekf-matrices-vectors.md)\n\n",title:"Untitled Page"},"/SLAM/filter-localisation-methods":{content:'---\ntitle: Filter localisation methods\ndate: "2020-08-22"\ntags:\n  - SLAM/filter-vs-optim/filter-based\n  - -sa/processed\n---\n\nBacklinks: [Back-end optimisation](back-end-optimisation.md), [what is slam?](SLAM/what-is-slam.md) [Filtering vs. optimisation](filter-based-vs-optimisation-based-slam.md)\n\n**Source**: [Wu 2018](bibliography/wu-2018.md)\n\n[EKF](SLAM/extended-kalman-filter.md) to propagate and update motion states of visual-inertial sensors\n\n***\n\n**Source**: [Scaradozzi 2018](studienarbeit/scaradozzi-2018.md)\nFiltering techniques in SLAM\n\n*   Augment/refine the position estimates and map estimates by incorporating new measurements when they become available\n*   Generally online, due to their incremental nature\n*   Types\n    *   [Kalman filters](studienarbeit/general-kalman-filter.md)\n    *   [Particle filters](studienarbeit/particle-filters.md)\n\n',title:"Untitled Page"},"/SLAM/initialisation-of-monocular-slam":{content:'---\ntitle: Initialisation of monocular SLAM\ndate: "2020-11-20"\ntags:\n  - SLAM/VSLAM\n  - -sa/processed\n  - SLAM/initialisation\n---\n\nSource:  [Lamarca 2019 DefSLAM](studienarbeit/lamarca-2020.md)\n\nDepth information has to be generated before localisation can be performed — how?\n\n*   Capture multiple images which have enough [parallax](definitions/parallax.md)\n*   These images with parallax allows depth information to be calculated (this uses [motion parallax](definitions/motion-parallax.md))\n    *   From these images, the map can be generated\n    *   Localisation can then be carried out with respect to the map (as long as camera doesn\'t move off to an unexplored region)\n\n',title:"Untitled Page"},"/SLAM/keyframes-in-loop-closure-detection":{content:"---\ntitle: Key frames in loop closure detection\ndate: \"2020-08-03\"\ntags:\n  - localisation/keyframes\n  - -sa/processed\n---\n\n**Source**: [cometlabs](bibliography/cometlabs.md)\n\nMost common method to get candidate key frames: use a place recognition approach\n\n*   approach based on vocab tree\n*   feature descriptors of candidate key frames are quantised\n    *   one colour in image below corresponds to one feature descriptor/'vocabulary'\n    *   each point is a 'word' that belongs to a vocabulary\n    *   the words can then be counted and put into a frequency histogram\n    *   the histogram is used to compare similarity of images\n    *   I think similar images then get filtered out, so we get key frames\n\n![kf-loop-closure](_img/kf-loop-closure.png)\n",title:"Untitled Page"},"/SLAM/landmark-extraction":{content:'---\ntitle: Landmark extraction\ndate: "2020-08-06"\ntags:\n  - localisation/landmarks\n  - -sa/processed\n---\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\nBasic [landmark](SLAM/landmarks.md) extraction using a laser scanner\n\n1.  [Spike algorithm](SLAM/spike-landmarks.md)\n2.  [RANSAC](SLAM/ransac.md) ([EKF](SLAM/basic-ekf-for-slam.md) handles points)\n3.  Expansion of RANSAC so that EKF handles lines\n4.  Scan-matching: two successive laser scans are matched\n\nSpike and RANSAC are good for indoor environments',title:"Untitled Page"},"/SLAM/landmarks":{content:'---\ntitle: Landmarks\ndate: "2020-07-27"\ntags:\n  - localisation/landmarks\n  - -sa/processed\n  - -published\n---\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\nFeatures which can easily be re-observed and distinguished from the environment\n\nCharacteristics:\n\n*   Re-observable from different positions and angles\n*   Unique (i.e. no mix-up with other landmarks)\n*   Plentiful -- should not be so few that robot gets lost (robot spends extended time w/o enough visible landmarks)\n*   Stationary\n\n[Basic landmark extraction](SLAM/landmark-extraction.md)\n\n',title:"Untitled Page"},"/SLAM/loop-closing-in-viorb":{content:'---\ntitle: Loop closing in VIORB\ndate: "2020-10-20"\ntags:\n  - SLAM/loop-detection\n  - SLAM/algos/VIORB\n  - -sa/processed\n---\n\n**Source**: [Mur-Artal 2017 VI-ORB](bibliography/mur-artal-2017-vi-orb.md)  \n**See also**: [Loop closure detection (general)](SLAM/loop-closure-detection.md)\n\n## Overview\n\n*   To reduce drift accumulated during exploration (when returning to an already mapped location)\n*   Loop detection: of large loops using place recognition\n*   Loop correction: first do lightweight pose-graph optimisation (PGO), then do full BA in a separate thread (in order not to interfere with real-time operations)\n\n## Implementation\n\n*   After loop detection: do match validation (alignment of points between keyframes)\n*   Then pose-graph optimisation to reduce the accumulated error in trajectory\n    *   (PGO: pose-only, ignores IMU info)\n    *   IMU info ignored, but velocities are corrected by rotating them according to keyframe orientation --\u003e suboptimal, but should be accurate enough to allow IMU data to be used right after the PGO\n    *   in ORBSLAM: PGO is 7-DoF optimisation (due to scale + 3 rot + 3 xyz)\n    *   in VIORB, 6 DoF (scale is known from initialisation bzw. observable)\n\n',title:"Untitled Page"},"/SLAM/loop-closure-detection":{content:'---\ntitle: Loop closure detection\ndate: "2020-08-03"\ntags:\n  - -sa/processed\n  - SLAM/loop-detection\n  - -published\n---\n\n**Source**: [cometlabs](bibliography/cometlabs.md)   \n**See also**: [loop-closing-in-viorb](SLAM/loop-closing-in-viorb.md)\n\n\n## Loop closure\n*   Process of observing the same scene by non-adjacent frames and adding a constraint (relationship? association?) between them\n*   A long-term data association in the [VSLAM Framework](SLAM/vslam-framework.md) (part of front end)\n*   Sort of incorporates [topological SLAM](topological-slam.md) into metric SLAM\n\n![loop-closure](_img/loop-closure.png)\n\n## Importance\n\n*   Final refinement step (in data association)\n*   Important for obtaining a globally consistent SLAM solution, especially when optimising over a long period of time\n\n## Basic loop closure detection\nMatch the current frame to all previous frames using feature matching\n\n*   Computationally expensive, not always suitable for real-time applications\n*   For RT—a possible solution: Define [key frames](SLAM/keyframes-in-loop-closure-detection.md) and perform the comparison with only the key frames\n\n## Problems with loop closure\n\n*   False positive/perceptual aliasing: Places are different, but are perceived as the same\n*   False negative: Places are the same, but perceived as being different\n*   Abhilfe: [precision recall curve](precision-recall-curve.md)\n\n',title:"Untitled Page"},"/SLAM/nearest-neighbour":{content:'---\ntitle: Nearest Neighbour\ndate: "2020-08-06"\ntags:\n  - localisation/data-association\n  - -sa/processed\n---\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\nNearest neighbour approach\n\n1.  Get a new laser scan --\u003e ([landmark extraction](SLAM/landmark-extraction.md)) extract all visible landmarks\n2.  Associate each extracted LM to the [closest LM](SLAM/distance-between-landmarks.md) we have seen more than $N$ times\n3.  Pass each pairs of association (extracted LM, LM in database) through a [validation gate](SLAM/validation-gate.md)\n    1.  If pair passes --\u003e $n = n + 1$ (num. times seen)\n    2.  If pair fails --\u003e add new LM to database, with $n := 1$\n\n',title:"Untitled Page"},"/SLAM/prediction-model":{content:'---\ntitle: Prediction model\ndate: "2020-07-29"\ntags:\n  - filters/EKF\n  - -sa/processed\n---\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)  \n\nUsed in the [prediction step](SLAM/ekf-1-prediction.md).\n\nHow to compute an expected position of the robot given the old position and the control input (so basically based on [odometry](definitions/odometry.md).\n\nControl terms are $\\Delta x, \\Delta y, \\Delta \\theta$\n\n$$\n\\begin{align}\nf \u0026= \\left[ \n	\\begin{array}{c}\n	x + \\Delta t \\cos \\theta + q \\Delta t \\cos \\theta \\\\\n	y + \\Delta t \\sin \\theta + q \\Delta t \\sin \\theta \\\\\n	\\theta + \\Delta \\theta + q \\Delta\\theta\n	\\end{array}\n	\\right]\\\\\n  \u0026= \\left[ \n	\\begin{array}{c}\n	x + \\Delta x + q \\Delta x \\\\\n	y + \\Delta y + q \\Delta y \\\\\n	\\theta + \\Delta \\theta + q \\Delta\\theta\n	\\end{array}\n	\\right]\n\\end{align}\n$$\n\nVariable	| Description \n--- 		| ---\n$\\Delta t$ 	| change in thrust  \n$q$ 		| error term\n\nJacobian (assuming linearised version)\n\n$$\n\\left[ \n\\begin{array}{ccc}\n1 \u0026 0 \u0026 -\\Delta t \\sin\\theta\\\\\n0 \u0026 1 \u0026 \\Delta\\cos\\theta\\\\\n0 \u0026 0 \u0026 1\n\\end{array}\n\\right]\n$$\n\nNot extended for landmarks because only used for robot position prediction\n\n',title:"Untitled Page"},"/SLAM/ransac":{content:'---\ntitle: RANSAC\ndate: "2020-07-27"\ntags:\n  - localisation/landmarks\n  - -sa/processed\n---\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\n# Random Sampling Consensus\n\n*   to extract lines from a laser scan\n*   lines then used as landmarks\n*   indoors: straight lines from walls\n*   line landmarks are found by\n    *   randomly taking a sample of laser readings (e.g. sample readings from 12deg to 22deg from within a range of 0 to 180deg)\n    *   least squares approximation for line of best fit\n    *   RANSAC then checks how many laser readings lie close to the best fit line\n        *   initially, all readings are assumed to be unassociated to any lines\n        *   if the num. of readings \u003e threshold, then a line has been seen\n        *   this threshold is the consensus\n\nThe [EKF](SLAM/extended-kalman-filter.md) implementation assumes that landmarks come in as single points with range and bearing \\[r, b\\] from the robots position\n\nTranslate a line to a fixed point:\n\n*   Take a fixed point in the world coordinates\n*   Calculate point on the line closest to the fixed point\n*   Using the robot\'s position and the position of the FP on the line, a range and bearing can be calculated\n\nAnother possibility: EKF handles lines instead of just points\n\n',title:"Untitled Page"},"/SLAM/slam":{content:'---\ntitle: SLAM Index\ndate: "2020-08-06"\ntags:\n  - SLAM\n  - -master\n  - -sa/processed\n  - -published\naliases:\n  - /slam/_index/\n  - /slam/index/\nhas_content: True\n---\n\n## Definition\n- [Localisation](studienarbeit/localisation.md)\n- [What is SLAM?](SLAM/what-is-slam.md)\n\n## Sensors for SLAM\n- [Position acquisition (relative vs. absolute)](sensors/position-acquisition.md)\n- [SLAM hardware](sensors/slam-hardware.md)\n\n### Relative\n- [Odometry](definitions/odometry.md)\n- [IMU](sensors/imu.md)\n\n### Absolute\n- [Sensors (absolute measurements) for measuring distance to landmarks](sensors/sensors-absolute.md)\n\n\n- [Visual sensors for localisation](sensors/visual-sensors-for-localisation.md)\n  - [Monocular depth perception](permanent/10-monocular-depth-perception.md)\n  - [Pinhole camera model](studienarbeit/pinhole-camera-model.md)\n  - [Camera calibration](studienarbeit/camera-calibration.md)\n  - [World to camera trafo](studienarbeit/world-to-camera-trafo.md)\n\n### Fusion\n- [Multisensor fusion](studienarbeit/multisensor-fusion.md)\n- [Loose vs Tight coupling](studienarbeit/loose-vs-tight-coupling.md)\n- [Why use the visual-inertial sensor combination?](SLAM/why-use-the-visual-inertial-sensor-combination.md)\n\n## Visual SLAM\n- [Classification of image-based camera localization approaches](classification-of-image-based-camera-localization-approaches.md)\n- [Visual SLAM Implementation Framework](SLAM/vslam-framework.md)\n- [Feature-based vs direct SLAM workflow](feature-based-vs-direct-slam-workflow.md)\n\n### Sparse vs dense\n- [Sparse/Feature-based VSLAM](studienarbeit/sparse-feature-based-vslam.md)\n- [Dense/direct VSLAM](studienarbeit/dense-direct-vslam.md)\n\n### Landmarks/Feature extraction\n- [landmarks](SLAM/landmarks.md)\n- [Bag of words](studienarbeit/bag-of-words.md)\n\n[Data association](SLAM/data-association.md)\n\n### Loop closure\n- [Loop closing in VIORB](SLAM/loop-closing-in-viorb.md)\n- [Loop closure detection](SLAM/loop-closure-detection.md)\n\n### Filtering vs optimisation\n[Filter-based vs optimisation-based SLAM](studienarbeit/filter-based-vs-optimisation-based-slam.md)\n\n#### Filter-based\n- [Filter localisation methods](SLAM/filter-localisation-methods.md)\n- [Extended Kalman Filter](SLAM/extended-kalman-filter.md) - [basic ekf-for-slam](SLAM/basic-ekf-for-slam.md)\n- [rlabbe Kalman/Bayesian filters in Python](studienarbeit/rlabbe-kalman-bayesian-filters-in-python.md)\n\n#### Keyframe-based\n- [Some optimisation-based tightly-coupled multisensor SLAM algorithms](SLAM/algos-optimisation-based.md)\n- [Handling the computational complexity of optimisation-based SLAM](studienarbeit/handling-the-computational-complexity-of-optimisation-based-slam.md)\n\n### Established SLAM algorithms\n- [Some optimisation-based tightly-coupled multisensor SLAM algorithms](SLAM/algos-optimisation-based.md)\n- [State-of-the-art SLAM](studienarbeit/state-of-the-art-slam.md)\n- [Mur-Artal 2017 VI-ORB](bibliography/mur-artal-2017-vi-orb.md)\n- [Lamarca 2020 DefSLAM](studienarbeit/lamarca-2020.md)\n\n### Resources\n[SLAM resources](slam-resources.md)\n\n',title:"Untitled Page"},"/SLAM/spike-landmarks":{content:'---\ntitle: Spike landmarks\ndate: "2020-07-27"\ntags:\n  - localisation/landmarks\n  - -sa/processed\n---\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\n*   Uses extrema to find [landmarks](SLAM/landmarks.md)\n*   Find values in the range of a laser scan, where two values differ by more than a certain amount (e.g. 0.5 m)\n    *   This finds **big changes in the laser scan**\n*   Alternatively, using three values next to each other: $A, B, C$\n    *   $(A - B) + (C - B)$ yields a value\n    *   Better for finding spikes as it finds actual spikes\n*   Rely on the landscape changing a lot between two laser beams\n    *   Algo will fail in smooth environments\n*   Suitable for indoor environments, however is not robust against envs w/ people\n    *   people are picked up as spikes as theoretically they are good landmarks (just not stationary!)\n\n',title:"Untitled Page"},"/SLAM/validation-gate":{content:'---\ntitle: Validation gate\ndate: "2020-08-06"\ntags:\n  - localisation/data-association\n  - -sa/processed\n---\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\nAn observed landmark is associated to a landmark if the following holds\n\n$$\n\\begin{aligned}\nv_i^T S_i^{-1} v_i \\leq \\lambda\n\\end{aligned}\n$$\n\n|     |     |\n| --- | --- |\n| $v$   | innovation |\n| $S$   | innovation covariance |\n\nThe validation gate makes use of the fact that the [EKF](SLAM/extended-kalman-filter.md) implementation gives a bound on the uncertainty of an observation of a [landmark](SLAM/landmarks.md).\n\n*   Is an observed LM a LM in the database?\n*   Check if the observed LM lies within the area of uncertainty\n*   Can usually be shown in a graphic (error ellipse)\n\n',title:"Untitled Page"},"/SLAM/vslam-framework":{content:'---\ntitle: Visual SLAM Implementation Framework\ndate: "2020-07-30"\ntags:\n  - SLAM/VSLAM\n  - discussion/2020/2020-08\n  - -sa/processed\n  - -published\n---\n\n**Source**: [Cometlabs](bibliography/cometlabs.md)\n\n## Basic principle:\n\n*   tracking a set of points through successive frames\n*   these tracks are used to triangulate the 3D positions of the points to create the map\n*   at the same time, using the the est point locations to calculate the pose of the camera, which could have observed them (i.e. calculate real time 3D structure of a scene from the estimated motion of the camera)\n\n![vslam-triangulation](_img/vslam-triangulation.png)\n\n## Architecture\n\n1.  Front-end\n    *   Abstracts sensor data into models (which are good for estimation) / Processing\n    *   [Data association](SLAM/data-association.md)\n        *   Short term (feature tracking); features in consecutive sensor measurements\n            *   Either from [sparse maps](studienarbeit/sparse-feature-based-vslam.md) or [dense-maps](studienarbeit/dense-direct-vslam.md)\n        *   Long term ([loop closure](SLAM/loop-closure-detection.md); associating new measurements to older landmarks\n2.  [Back-end](studienarbeit/back-end-optimisation.md)\n    *   Performs inference on the abstracted data produced by the front end\n\n![vslam-architecture](_img/vslam-architecture.png)\n\n',title:"Untitled Page"},"/SLAM/what-is-slam":{content:'---\ntitle: What is SLAM?\ndate: "2020-07-30"\ntags:\n  - SLAM\n  - -sa/processed\n  - -published\n---\n\n**Parent**: [SLAM index](SLAM/slam_index.md)  \n**See also**: [slam-hardware](sensors/slam-hardware.md)\n\n**Source**: [Scaradozzi 2018](studienarbeit/scaradozzi-2018.md)\n\nProcess which allows a mobile robot to\n\n*   construct a map of its environment (assumed to be unknown)\n*   compute its location using the map simultaneously\n\n* * * \n\n**Source**: [Lamarca 2020](studienarbeit/lamarca-2020.md)\n\n*   Goal is to locate a sensor in an unknown map/environment, which is simultaneously being reconstructed.\n*   Typically used in exploratory trajectories (new or changing environments)\n\n* * *\n\n**Source**: [Wikipedia SLAM](wikipedia-slam.md)\n\nSimultaneous localization and mapping (SLAM)\n\n*   computational problem\n*   construct/update a map of an unknown environment\n*   simultaneously keep track of an agent\'s location within it\n\n* * *\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)  \n\n**Goal**: to use the environment to update robot position\n\n*   robot odometry is often erroneous, therefore we cannot rely directly on odometry\n*   in this specific example, laser scans are used to correct the position of the robot in a [basic EKF-SLAM](SLAM/basic-ekf-for-slam.md) implementation\n    *   extract features from environment\n    *   update position estimate based on the features\n    *   re-observe when the robot moves around\n\n* * *\n\n**Source**: [Cometlabs](cometlabs.md)\n\n*   Getting precise measurements based on known maps --\u003e ok\n*   Building a map of the environment --\u003e ok\n\nBut solving both simultaneously is harder!\n\nWith a map (i.e. prior knowledge of environment, e.g. via storage of known landmarks in a database)\n\n*   mobile robot can perform a set of tasks like path planning\n*   error in estimating the state is limited\n*   robot can \'reset\' localisation error by revisiting known areas\n\nWithout a map\n\n*   [Dead-reckoning / odometry](definitions/odometry.md) would quickly drift over time\n*   [Data association](SLAM/data-association.md) becomes much more difficult\n    *   Unknown mapping b/w existing landmarks and observations\n\n* * *\n\n**Source**: [Grisetti 2011 - Tutorial graph-based SLAM](studienarbeit/grisetti-2011.md)\n\nAnother way of defining SLAM: "learning maps under pose uncertainty"\n\n',title:"Untitled Page"},"/SLAM/why-use-the-visual-inertial-sensor-combination":{content:'---\ntitle: Why use the visual-inertial sensor combination?\ndate: "2020-10-20"\ntags:\n  - SLAM/multisensor-SLAM\n  - -sa/processed\n  - -published\n---\n\n**See also**: [Multisensor fusion](multisensor-fusion.md)\n\n**Source**: [Mur-Artal 2017 VI-ORB](bibliography/mur-artal-2017-vi-orb.md)\n\n*   Cheap but also with good potential\n*   [Cameras](sensors/visual-sensors-for-localisation.md) provide rich information but are relatively cheap\n*   [IMU](sensors/imu.md)\n    *   provides self-motion info, helps recover scale in monocular applications\n    *   enables estimation of the direction of gravity --\u003e renders pitch and roll observable\n\n* * *\n\n**Source**: [Forster 2017 IMU Preintegration](forster-2017-imu-preintegration.md)\n\n*   Visual-inertial fusion for 3D structure and motion estimation\n*   Both cameras and IMUs are cheap, easy to find and complement each other well\n*   Camera\n    *   exteroceptive sensor\n    *   measures, up to a to-be-determined metric scale, appearance and geometrical structure of a 3D scene\n*   IMU\n    *   interoceptive sensor\n    *   makes metric scale of monocular cameras, as well as the direction of gravity, observable\n\n* * *\n\n**Source**: [(Wu 2018) Image-based camera localization](bibliography/wu-2018.md)\n\n*   Cameras provide rich information of a scene\n*   IMU provide odometry\n    *   self-motion information and\n    *   accurate short-term motion estimates at high frequency\n\n* * *\n\n**Source**: Mirzaei 2008 A Kalman Filter-Based Algorithm for IMU-Camera Calibration: Observability Analysis and Performance Evaluation\n\nMotivation\nInertial navigation systems (INSs) combine IMU with GPS. However, GPS is not always utilisable (e.g. indoor locations) --\u003e cameras/visual sensors are used as an alternative.\nPoints in favour of cameras: small size, light-weight, passive sensors that provide rich information at low cost.\n\nCameras and IMUs are complementary in terms of accuracy and frequency response.\n\n*   An IMU is ideal for tracking over\n    *   short periods of time\n    *   motions with high dynamic profile\n*   A camera is best suited for\n    *   state estimation over longer periods of time\n    *   smoother motion profiles\n\n',title:"Untitled Page"},"/bibliography/":{content:"---\ntitle: Bibliography\ntag:\n- -published/meta\naliases:\n- /bibliography/_index/\n---",title:"Untitled Page"},"/bibliography/chen-2018-review":{content:'---\ntitle: Chen 2018 Review of VI SLAM\ndate: "2020-08-24"\nexternal_url: "http://www.researchgate.net/publication/327064224_A_Review_of_Visual-Inertial_Simultaneous_Localization_and_Mapping_from_Filtering-Based_and_Optimization-Based_Perspectives"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-read\n  - SLAM/multisensor\n  - -sa/processed\n  - -published\n---\n\n**Source**: \u003chttp://www.mdpi.com/2218-6581/7/3/45\u003e  \n**Authors**: Chen et. al\n\n## Abstract\n\n*   Survey on visual-inertial SLAM over the last 10 years\n*   Aspects: filtering vs optimisation based, camera type, sensor fusion type\n*   Explains core theory of SLAM, feature extraction, feature tracking, loop closure\n*   Experimental comparison of filtering-based and optimisation-based methods\n*   Research trends for VI-SLAM\n\n## Recommended other works\ns. [Works of possible interest](bibliography/works-of-interest.md)\n\n## Contents/Chapters\n### SLAM\nSLAM: build a real-time map of the unknown environment based on sensor data, while the sensor (robot) itself is traversing the environment\n\nGrowing prevalence of visual SLAM: rich in information compared yet low-cost (camera vs other sensors)\n\n### Filtering-based:\n\n*   Loose vs tight coupling\n*   Feature extraction, feature tracking\n*   Basic method framework (three step): propagation, image registration, update\n*   Algorithms: MSCKF, Maplab\n\n*   Loosely-coupled: usually only fuses the IMU to estimate partial state, not full POSE\n*   Tightly-coupled: camera and IMU states are fused together into a motion and observation equation --\u003e more common\n\n### Optimisation-based:\n\n*   front- vs back-end division (map construction vs pose optimisation)\n*   Loop closure (odometry-based or appearance-based), preintegration\n*   Algorithms: OKVIS (stereo), [VIORB](bibliography/mur-artal-2017-vi-orb.md), VINS-mono\n\n## Takeaway\n| **Filtering-based** | **Optimisation-based** |\n| --- | --- |\n| More advantageous w.r.t. computing resources | Good localisation accuracy with lower memory utilisation |\n\n',title:"Untitled Page"},"/bibliography/cometlabs":{content:'---\ntitle: Cometlabs What You Need to Know About SLAM\ndate: "2020-07-30"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-read\n  - -sa/processed\n  - -published\n---\n\n**Source**: \u003chttp://blog.cometlabs.io/teaching-robots-presence-what-you-need-to-know-about-slam-9bf0ca037553\u003e\n\n1.  [SLAM chicken and egg problem](SLAM/what-is-slam.md)\n2.  [Position acquisition](sensors/position-acquisition.md)\n3.  [Multisensor fusion](multisensor-fusion.md)\n4.  [Sensors (absolute measurements) for measuring distance to landmarks](sensors/sensors-absolute.md)\n5.  [Mapping representations in robotics](mapping-representations-in-robotics.md)\n6.  [Visual SLAM Implementation Framework](SLAM/vslam-framework.md)\n    1.  [Feature-based vs direct SLAM workflow](feature-based-vs-direct-slam-workflow.md)\n    2.  [Sparse/Feature-based VSLAM](studienarbeit/sparse-feature-based-vslam.md)\n    3.  [Dense/direct VSLAM](studienarbeit/dense-direct-vslam.md)\n\n',title:"Untitled Page"},"/bibliography/liu-2020":{content:'---\ntitle: (Liu 2020) Learned Descriptor\ndate: "2020-10-05"\nexternal_url: https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Extremely_Dense_Point_Correspondences_Using_a_Learned_Feature_Descriptor_CVPR_2020_paper.pdf\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-to-read\n  - -sa/to-be-processed\n  - to-do/missing-tag\n---\n\n**Note**: I\'m only reading this paper for the into to SLAM/SfM\n\n## Abstract\n* Problem: 3D reconstuction has subpar performance when dealing with endoscopic videos, partly due to local [descriptors](studienarbeit/descriptors-in-feature-detection-extraction.md) ...\n\n\n## Introduction\n* Correspondence estimation: match between 2D points in image and corresponding 3D location (s. [registration](studienarbeit/registration.md))\n* Correspondence estimation is needed by SfM, SLAM, ...\n* SfM + SLAM combination has been shown to be effective for surgical navigation in endoscopy -- simultaneous estimation of\n	* sparse 3D structure of the observed scene\n	* camera trajectory\n\n\n### Complementarity of SfM + SLAM\nGood camera tracking requires dense 3D reconstruction\n\nSLAM | SfM\n--- | ---\ngood for real time applications | limited to offline estimation (due to the global optimisation used in the bundle adjustment)\nusually limited to local optimisation (due to computational constraints) | prioritises high density and accuracy for the sparse 3D structure\nprone to drift errors when no loop closure |\n\n\n### Example\n* SfM only pipeline: [COLMAP](studienarbeit/sfm.md)\n* SLAM only pipeline: ORB-SLAM\n\n\n## Challenges for correspondence estimation in endoscopic video\n1. Tissue deformation (violates static scene assumption in the pipelines)\n2. Textures in endoscopy\n	*  often smooth and repetitive\n	*  sparse matching with local descriptors are in this case prone to error\n	*  possible workarounds:\n		*  adding textures\n			*  Widya: use of dye to manually texturise the surface (this improves matching performance of the descriptors)\n			*  Qiu: project patterns onto the surface\n		*  methods to work with texture-scarce surfaces',title:"Untitled Page"},"/bibliography/markley-2014":{content:'---\ntitle: (Markley 2014) Fundamentals of Spacecraft Attitude Determination and Control\ndate: "2021-08-17"\nexternal_url: "http://www.springer.com/de/book/9781493908011"\ntags:\n  - -resources/-bibliography\n  - -sa/processed\n  - -resources/-bibliography/bib-skimmed\n  - -published\n---\n\n**Authors**: FL Markley, John Crassidis  \n**DOI**: 10.1007/978-1-4939-0802-8\n\n## Note/Nomenclature:\n\n*   This book interpetes rotations/transformations in the [passive/alias](rotations/active-passive-or-alibi-alias-rotation-transformations.md) sense (I\'m not a fan)\n*   Quaternions in JPL [conventions](studienarbeit/quaternion-conventions.md) instead of Hamiltonian (not a fan of this either...)\n*   Rotation matrix = attitude matrix\n\n## Introduction\n\n*   Attitude determination: memoryless approach without using statistics\n*   Attitude estimation: approaches with memory, uses statistical info from a series of measurements\n    *   filter approaches\n    *   uses a dynamic motion model of the object\n\n## Quaternions\n* [Quaternion conventions](quaternion-conventions.md)\n* [Quaternion multiplication](rotations/quaternion-multiplication.md)\n\n## Rotations\n"Euler\'s theorem:  any rotation is a rotation about a fixed axis"\n\n[orientation-parametrisations](studienarbeit/orientation-parametrisations.md)\n* [Euler axis/angle representation](rotations/euler-axis-angle-representation.md)\n* [Rotation vector representation](rotation-vector-representation.md)\n* [Gibbs / Rodrigues parameter representation](rotations/gibbs-rodrigues-parameter.md)\n* Modified Rodrigues parameters\n\n[Rotation error representation](rotations/rotation-error-representation.md)\n\n## Filtering\n* [Which orientation parametrisation to choose?](rotations/20.4-which-orientation-parametrisation.md)\n* [Additive quaternion filtering](studienarbeit/50.4.1-additive-quaternion-filtering.md)\n* [Error-State Kalman Filter](studienarbeit/50.5-error-state-kalman-filter.md) / [multiplicative-quaternion-filtering](studienarbeit/50.4.2-multiplicative-quaternion-filtering-mekf.md)\n* [Observation-of-the-error-state-filter-correction](studienarbeit/50.7.1-observation-of-the-error-state-filter-correction.md)\n* [H-jacobian-matrix-in-the-eskf-filter-correction](studienarbeit/50.7.1.1-h-jacobian-matrix-in-the-eskf-filter-correction.md)\n* [ESKF reset](studienarbeit/50.7.3-eskf-reset.md)\n\n',title:"Untitled Page"},"/bibliography/mur-artal-2017-vi-orb":{content:'---\ntitle: (Mur-Artal 2017) VI-ORB\ndate: "2020-10-20"\ntags:\n  - -resources/-bibliography\n  - to-do/go-through-literature-later\n  - -resources/-bibliography/bib-read\n  - -sa/processed\n  - discussion/2020/2020-10\n  - SLAM/algos/VIORB\n  - -published\n---\n\n**URL**: \u003chttp://ieeexplore.ieee.org/abstract/document/7817784\u003e  \n**Authors**: Mur-Artal, Tardós  \n**Code**: \u003chttp://paperswithcode.com/paper/visual-inertial-monocular-slam-with-map-reuse\u003e  \n**Results (video)**: \u003chttp://www.youtube.com/watch?v=JXRCSovuxbA\u003e\n\n## Abstract\n\n*   current VI odometry approaches: drift accumulates due to lack of loop closure\n*   therefore there is a need for tightly-coupled VI-SLAM with loop closure and map reuse\n*   here: focus on monocular case, but applicable to other camera configurations\n*   builds on ORB-SLAM (from same author)\n*   IMU initialisation method (initialises: scale, gravity direction, velocities, gyroscope bias, accelerometer bias) depends on visual monocular initialisation (coupled initialisation)\n\nOther works: recent tightly-coupled VIO (both filtering- and optimisation-based) lack loop closure, so drift accumulates\n\n## Introduction\n[Why use the visual-inertial sensor combination?](SLAM/why-use-the-visual-inertial-sensor-combination.md)\n\n*   Camera: [Pinhole camera projection function](pinhole-camera-projection-function.md)\n*   [IMU](sensors/imu.md) (frame notation: $B$): [states, dynamics equations](studienarbeit/imu-states-dynamics-equations.md)\n*   Rigid transformation between camera and IMU ![unknown_filename.png](./_resources/[Mur-Artal_2017]_VI-ORB.resources/unknown_filename.png)\n\n## Framework of VI-ORB\n\n3 parallel threads\n\n1.  \\[**Front end**\\] [Tracking](studienarbeit/tracking-in-viorb)\n    Tracking via optimisation of the current frame, assuming a fixed map (unchanged map)\n    \n2.  \\[**Back end**\\] [Local mapping](studienarbeit/mapping-in-viorb)\n    *   local BA over several keyframes (sliding window)\n    *   this local BA is a compromise between\n        *   full smoothing (over all keyframes) — high computational complexity\n        *   marginalising out past keyframes (loss of information)\n3.  [Loop closure](SLAM/loop-closing-in-viorb.md)\n\n## Initialisation\n\n*   Need for a reliable VI initialisation that provides accurate state estimate\n*   Because both the tracking (front end) and BA (back end) fix the states in their optimisations, this can bias the solution --\u003e need for initialisation [x] fix states?: states which aren\'t the argument of the optimisation function, i.e. fixed (not optimised)\n*   Optimal solution for initialisation of all the required init. variables \\[scale, gravity, biases, velocities, structure of the opt. graph, camera pose\\] would require a full BA,\n    *   however this is split into smaller steps\n*   The proposed initialisation is general and applicable to any keyframe-based monocular SLAM\n*   **Requirement**: any two consecutive keyframes must be close in time (to reduce IMU noise integration)\n\n\n1.  Process the first few seconds of video with visual monocular SLAM (here using ORBSLAM)\n    *   This gets the structure estimate as well as several keyframe poses scaled by an unknown scale\n    *   Use a motion that makes all variables observable\n2.  Compute gravity bias from orientation of keyframes\n3.  Initial guess for scale, accelerometer bias (using known magnitude of gravity 9.81 m/s2)\n4.  Refine the scale and gravity direction\n5.  Get velocities for all keyframes\n\n\nWhen reinitialising after relocalisation (after a long time; using place recognition):\n\n*   Reinitialise bg gyrometer bias\n*   Scale s and gravity g already known from first initialisation, so no need to calculate anew\n*   Estimate ba accelerometer bias from the same equation used during initialisation (simplified now due to knowledge of s, g)\n\n',title:"Untitled Page"},"/bibliography/phils-lab-sensor-fusion":{content:"---\ntitle: (Phil's Lab) Sensor Fusion series\ndate: \"2021-09-29\"\nmathjax: true\ntags:\n  - -sa/processing\n  - -resources/videos\n  - sensors/IMU\n---\n\n**Source**: https://www.youtube.com/watch?v=RZd6XDx5VXo\n\n* [Sensor fusion](sensors/sensor-fusion.md)\n* [Gyroscope](sensors/gyroscope.md)\n\n## Example: UAV attitude estimation\n\n![uav-angles](_img/uav-angles.png)\n\n**Goal**:\n* To estimate roll and pitch angles of aircraft\n* These angles are needed for feedback in autonomous control of an unmanned UAV\n\n**We have**:\n* 3-axis accelerometer [$\\text{m/s}^2$]\n* 3-axis gyroscope [$\\text{rad/s}$]\n\n**Note**:  \nHere, angles are measured in *body* frame and not in fixed/inertial frame!\n\n**Measured quantities**:\n* acceleration  \n	$\\mathbf{a}_B = \\left[\\begin{array}{ccc}\n			a_x \u0026 a_y \u0026 a_z \\end{array}\\right]^\\text{T}$\n* roll rates from gyrometer (body) $\\neq$ derivative of [Euler angles](euler-angles) (fixed)  \n	$\\mathbf{\\omega}_B = \\left[\\begin{array}{ccc}\n			p \u0026 q \u0026 r \\end{array}\\right]^\\text{T}\n			\\neq \\left[\\begin{array}{ccc}\n			\\dot{\\phi} \u0026 \\dot{\\theta} \u0026 \\dot{\\psi} \\end{array}\\right]^\\text{T}$\n\n\n## Accelerometer\n$$\\mathbf{a} = \\left[\\begin{array}{ccc}\n			a_x \u0026 a_y \u0026 a_z \\end{array}\\right]^\\text{T}$$\n			\n### Accelerometer model\n(Preliminary, doesn't account for everything yet -- the extra acceleration terms to be added would introduce a cross-correlation between the yaw $\\psi$ and the other angles.)\n$$\\begin{aligned}\n\\left[\\begin{array}{c}\n			a_x  \\\\ a_y \\\\ a_z \\end{array}\\right]\n	\u0026= \\frac{d\\mathbf{v}}{dt}\n		+ \\mathbf{\\omega}_B \\times \\mathbf{v}\n		- \\mathbf{R} \\left( \\begin{array}{c}\n				0 \\\\ 0 \\\\ g\n				\\end{array} \\right)\n		+ \\mathbf{\\beta}(t)\n		+ \\mathbf{\\eta}(t)\n\\end{aligned}$$\n\nVariables	| Description\n--- | ---\n$\\frac{d\\mathbf{v}}{dt}$ | linear acceleration\n$\\mathbf{\\omega}_B \\times \\mathbf{v}$ | acceleration due to translation + rotation\n$\\mathbf{g}$ | gravity vector\n$\\mathbf{\\beta}$ | bias\n$\\mathbf{\\eta}$ | noise (zero-mean, Gaussian)\n\nIf the accelerometer is at rest,\nand ignoring noise/bias terms, the model is simplified to\n$$\\begin{aligned}\n\\left[\\begin{array}{c}\n			a_x  \\\\ a_y \\\\ a_z \\end{array}\\right]\n	\u0026= \\left[ \\begin{array}{c}\n				g \\sin \\theta \\\\\n				-g \\cos\\theta \\sin\\phi\\\\\n				-g \\cos\\theta \\cos\\phi\n				\\end{array} \\right]\n\\end{aligned}$$\nand we can just rearrange the terms to get the roll $\\phi$ and pitch $\\theta$ angles.\n$$\n\\begin{aligned}\n\\hat{\\phi}_\\text{acc} \u0026= \\tan^{-1}\\left( \\frac{a_y}{a_z} \\right)\\\\\n\\hat{\\theta}_\\text{acc} \u0026= \\sin^{-1}\\left( \\frac{a_x}{g} \\right)\n\\end{aligned}\n$$\n\nDrawbacks of the above angle estimation:\n* Only true for accelerometer at rest\n* Noise is present in real life (typically high frequency noise --\u003e apply low-pass filter to measurements!)\n* Time-varying bias term  \n	--\u003e how to estimate and cancel it out?  \n	--\u003e how to perform initial calibration?",title:"Untitled Page"},"/bibliography/riisgaard-slam-for-dummies":{content:'---\ntitle: Riisgaard SLAM for dummies\ndate: "2020-07-27"\nexternal_url: "http://dspace.mit.edu/bitstream/handle/1721.1/36832/16-412JSpring2004/NR/rdonlyres/Aeronautics-and-Astronautics/16-412JSpring2004/A3C5517F-C092-4554-AA43-232DC74609B3/0/1Aslam_blas_report.pdf"\ntags:\n  - -resources/-bibliography\n  - SLAM\n  - -resources/-bibliography/bib-read\n  - -sa/processed\n  - discussion/2020/2020-09\n  - -published\n---\n\nAuthors:  Søren Riisgaard and Morten Rufus Blas\nParent: [SLAM resources](slam-resources.md)\n\nAbstract:\n\n*   Tutorial introduction to SLAM, with minimal prerequisites for the understanding of SLAM as explained here\n*   Mostly explains a single approach to the steps involved in SLAM\n*   Complete solution for SLAM using EKF (extended Kalman filter)\n*   Only considers 2D motion, not 3D\n\nChapters\n\n3.  [What is SLAM?](SLAM/what-is-slam.md)\n4.  [Overview of SLAM using EKF](overview-of-slam-using-ekf.md) \n5.  [Hardware](sensors/slam-hardware.md)\n    *   Robot\n    *   Range measurement device\n6.  [SLAM process](http://www.evernote.com/shard/s484/nl/217355218/dad6e2b1-b186-40d3-94f5-3064f1043376)\n    1.  [Step 1: Odometry update](SLAM/ekf-1-prediction.md)\n    2.  [Step 2: Reobservation](studienarbeit/ekf-2-reobservation.md)\n    3.  [Step 3: Add new landmarks](studienarbeit/ekf-3-new-landmarks.md)\n7.  Laser data\n8.  [Odometry data](http://www.evernote.com/shard/s484/nl/217355218/d6e4227d-18b0-4633-9967-b72012e0cd6b)\n9.  [Landmarks](SLAM/landmarks.md)\n10.  [Landmark extraction](SLAM/landmark-extraction.md)\n    1.  [Spike algorithm](SLAM/spike-landmarks.md)\n    2.  [RANSAC](SLAM/ransac.md)\n11.  [Data association](SLAM/data-association.md)\n12.  [EKF](SLAM/extended-kalman-filter.md)\n    1.  [EKF matrices](studienarbeit/ekf-matrices-vectors.md)\n    2.  [Prediction model](SLAM/prediction-model.md)\n    3.  [Measurement model](studienarbeit/measurement-model.md)\n    4.  [SLAM-specific Jacobians](studienarbeit/slam-specific-jacobians.md)\n    5.  [Process noise](SLAM/50.2.1-process-noise-q-and-w-odometry.md)\n    6.  [Measurement noise](studienarbeit/50.2.2-measurement-noise-r,-v-landmark.md)\n13.  Final remarks\n\nQuestions\n\n*   [x] Under which classification does the algo. used here fall?\n    *   multikind sensor\n    *   filter-based, feature-based\n\n',title:"Untitled Page"},"/bibliography/science-focus":{content:'---\ntitle: (Science Focus) How can one eye alone provide depth perception\ndate: "2021-05-17"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-read\n  - -sa/processed\n  - vision\n  - -published\n---\n\n**Source**: \u003chttp://www.sciencefocus.com/the-human-body/how-can-one-eye-alone-provide-depth-perception/\u003e  \n**Author**: Hilary Guite\n\n* In humans with normal binocular vision, depth perception is obtained using the [parallax](definitions/parallax.md) in the two overlapping fields of vision ("binocular disparity")\n* Each single field of vision has a slightly different view to the other\n\n* If vision in one eye is impaired, depth perception is still obtainable even with only one eye. Some tricks that the brain uses:\n    1.  We know the real size of things\n    2.  Using perspective, e.g. parallel lines converging to a perspective point\n    3.  Motion parallax (closer objects visually shift much more than distant ones when we move)\n\n',title:"Untitled Page"},"/bibliography/works-of-interest":{content:'---\ntitle: Works of possible interest\ndate: "2020-08-07"\nexternal_url: "http://link.springer.com/article/10.1007/s40903-015-0032-7"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-to-read\n  - -master\n  - discussion/2020/2020-08\n  - -sa/processed\n  - discussion/2020/2020-09\n  - discussion/2020/2020-10\n  - -published\n---\n\n## General SLAM\n- [ ] [Cadena 2016 -- Past, Present, and Future of SLAM](studienarbeit/cadena-2016.md)\n- [ ] [durrant-whyte 2006 slam tutorial part i](studienarbeit/durrant-whyte-2006-slam-tutorial-part-i.md)\n\n## Prerequisites\n- [x] g2o paper - graph-based SLAM\n\n## Existing SLAM algorithms\n- [x] MonoSLAM, works by Andrew Davison focusing on fusion instead of vision-only SLAM\n- [x] Maplab (filtering-based) not looking at filtering-based algos\n\n- mentioned in the [Chen 2018 Review of VI SLAM](bibliography/chen-2018-review.md) paper:\n	- [ ] ORB-SLAM paper — ORB features\n	- [x] [VIORB](bibliography/mur-artal-2017-vi-orb.md) implementation\n	- [ ] ORB-SLAM3 (improves on ORBSLAM, incl. VI, pub. 2020)\n	- [x] Qin 2018 - VINS-Mono: A Robust and Versatile  Monocular Visual-Inertial State Estimator\n	- [x] [qin 2019 general optimization-based framework (multisensor)](studienarbeit/qin-2019-general-optimization-based-framework-(multisensor).md)\n\n- Deformable SLAM\n	- [x] [lamarca-2019-defslam](studienarbeit/lamarca-2020.md)\n	- [ ] [badias-2020-morph-dslam](studienarbeit/badias-2020-morph-dslam.md)\n\n## Camera/IMU models\n- [ ] [mkok 2017 using inertial sensors for position and orientation estimation](studienarbeit/mkok-2017.md)\n- [ ] Forster 2017 preintegration paper\n- [ ] Furgale 2013 - calibration of IMU\n\n## Kalman filter\n- [ ] [rlabbe kalman/bayesian filters in-python](rlabbe-kalman_bayesian-filters-in-python.md)\n- [ ] [Weiss](studienarbeit/weiss-phd-thesis.md) (loose coupling IMU with SLAM using EKF)\n\n## Older/classical SLAM papers or related books as recommended by  [Cadena 2016](studienarbeit/cadena-2016.md)\n### Classical SLAM (1998 - 2004); the first 20 years\n- [ ] T. Bailey and H. F. Durrant-Whyte, “Simultaneous localisation and mapping (SLAM): Part II,”\n\n*       main probabilistic formulations for SLAM\n    *   EKF\n    *   Rao-Blackwellised particle filters\n    *   MLE\n*   challenges associated with efficiency and robust data association\n\n- [ ] Thrun - Probabilistic robotics (2005)\n- [ ] Thrun, Stachniss Ch 46 - SLAM “Simultaneous localization and mapping,” in Springer Handbook of Robotics, B. Siciliano and O. 2016, ch. 46, pp. 1153–1176.\n\n### Algorithmic analysis age (2004 - 2015)\n- [ ] Dissanayake G. Dissanayake, S. Huang, Z. Wang, and R. Ranasinghe, “A review of recent developments in simultaneous localization and mapping,” in Proc. Int. Conf. Ind. Inform. Syst., 2011, pp. 477–482.\n\n### General Kalman filters:\nas recommended by  [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n- [ ] Grewal and Andrew\'s Kalman Filtering\n- [ ] Paul Zarchan\'s book Fundamentals of Kalman Filtering\n\n- [ ] Simo Särkkä\n\n## Misc reading\n\u003chttp://link.springer.com/article/10.1007/s40903-015-0032-7\u003e\nAn Overview to Visual Odometry and Visual SLAM: Applications to Mobile Robotics (difference between VIO and VSLAM)\n*   as far as I understand it, VO doesn\'t involve map creation/updates\n\nas recommended by  [Cadena 2016](cadena-2016.md)\n*   Lowry 2016 - Visual place recognition: A survey (also reviews topological SLAM)\n\n',title:"Untitled Page"},"/bibliography/wu-2018":{content:'---\ntitle: (Wu 2018) Image-based camera localization\ndate: "2020-07-30"\nexternal_url: "http://link.springer.com/article/10.1186/s42492-018-0008-z"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-read\n  - SLAM/VSLAM\n  - -sa/processed\n  - -published\n---\n\n**Authors**: Wu, Tang, Li\n\n## Abstract/Contents\n\n*   overview (classification) of image-based camera localization\n*   classification of image-based camera localization approaches\n*   techniques, trends\n*   only considers 2D cameras\n*   focuses on points as features in images (not lines etc)\n\n## Chapters\n- [Classification of image-based camera localization approaches](classification-of-image-based-camera-localization-approaches.md)\n- [Multisensor fusion](studienarbeit/multisensor-fusion.md) —  [why use the visual-inertial sensor combination?](SLAM/why-use-the-visual-inertial-sensor-combination.md)\n- [Loose vs Tight coupling](loose-vs-tight-coupling.md)\n- [Filter localisation methods](SLAM/filter-localisation-methods.md)\n- [Some optimisation-based tightly-coupled multisensor SLAM algorithms](SLAM/algos-optimisation-based.md)\n\n## Questions\n\n*   [x] What\'s a metric map -- normal map (with landmarks, normal distances) as opposed to a topological one\n\n![cam-localisation-overview](_img/cam-localisation-overview.png)\n\n## Takeaway\n\n*   Learning SLAM is gaining in popularity, but geometric SLAM is often the chosen method for most applications, as it is more generalisable and at the same time reasonably accurate\n*   For reliability and low cost practical applications, multisensor vision-centred fusion is an effective method.\n*   Possibly interesting: embedded SLAM algorithms\n*   Trend for a practical SLAM system: integrating all possible techniques\n    *   e.g. geometric and learning fusion, multi-sensor fusion, multi-feature fusion, feature based and direct approaches fusion\n    *   may solve the current challenges of poorly textured scenes, large illumination changes, repetitive textures, highly dynamic motions\n\n',title:"Untitled Page"},"/definitions/":{content:"---\ntitle: Definitions\ntag:\n- -published/meta\naliases:\n- /definitions/_index/\n---",title:"Untitled Page"},"/definitions/egomotion-vs-odometry":{content:'---\ntitle: Egomotion (vs odometry)\ndate: "2020-08-25"\nexternal_url: "http://en.wiktionary.org/wiki/egomotion"\ntags:\n  - -definitions\n  - -sa/processed\n  - localisation/odometry\n---\n\nSee also: [Odometry](definitions/odometry.md)\n\n**Source**: \u003chttp://en.wiktionary.org/wiki/egomotion\u003e  \nThe three-dimensional movement of a camera relative to its environment\n\n***\n\n**Source**: \u003chttp://answers.ros.org/question/296686/what-is-the-differences-between-ego-motion-and-odometry/\u003e\n\n*   Generally used interchangeably with odometry\n*   Possible difference:\n    *   Egomotion is more about the estimation of twist (lin, rotational velocities)\n    *   Odometry is more about the estimation of path\n\n## Examples\n\n*   Wheel odometry: path estimation via time-integration of an estimated twist\n*   Visual odometry/Scan matching: direct estimation of pose without time-integration\n\n',title:"Untitled Page"},"/definitions/motion-parallax":{content:"---\ntitle: Motion parallax\ntags:\n- vision\n- -definitions\n- -sa/processed\n---\n\n# Motion parallax\ncloser objects visually shift much more than distant ones when we move",title:"Untitled Page"},"/definitions/odometry":{content:"---\ntitle: Odometry\ndate: \"2020-08-22\"\ntags:\n  - sensors\n  - -definitions\n  - -sa/processed\n  - localisation/odometry\n  - -published\n  - to-do\n---\n\n**See also**: [Egomotion (vs odometry)](definitions/egomotion-vs-odometry.md)\n\n\n\n**Source**: [https://en.wikipedia.org/wiki/Dead\\_reckoning](https://en.wikipedia.org/wiki/Dead_reckoning)  \n\nIn navigation, dead reckoning is the process of calculating one's current position by using a previously determined position, by using estimations of speed and course over elapsed time\n\n- [ ] s. Brian Douglas video on sensor fusion\n\n---\n\n**Source**: [Wikipedia Visual odometry](studienarbeit/wikipedia-visual-odometry.md)\n\n*   Data can be generated from actuator movements, e.g. rotary encoders that measure motor shaft rotations\n*   This data can be used to estimate changes in position over time\n*   Usually has precision problems, e.g. due to wheels slipping and sliding, bumpy surfaces\n*   The errors are integrated over time and therefore get worse\n\n***\n\n**Source**: [cometlabs](bibliography/cometlabs.md)\n\n*   Acceleration is obtained: integrate to get velocity, displacement \\[estimates\\]\n*   However, as the estimates drift over time and get integrated, this leads to increased errors\n*   Subject to\n    *   non-systematic errors e.g. human intervention\n    *   systematic errors, e.g. due to imperfections in robots' structure\n*   Examples:\n    *   wheel odometry, via encoders -- error source: wheel slippage\n    *   [IMU](sensors/imu.md): measures lin and rotational acceleration, error sources: extensive drift, sensitive to bumpy terrain\n\n***\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\n*   Provides approximate position of the robot as measured by its relative movement\n*   Acts as an initial guess for the EKF\n\nProblem: getting timing right between odometry data and laser data, so as not to compare outdated data to newer data\nSolution: extrapolate the data, easiest to extrapolate odometry data (since the controls are known)\n\n",title:"Untitled Page"},"/definitions/parallax":{content:'---\ntitle: Parallax\ndate: "2020-11-20"\ntags:\n  - -definitions\n  - -sa/processed\n  - vision\n---\n\n**Source**: \u003chttps://en.wikipedia.org/wiki/Parallax\u003e  \n**See also**: [motion parallax](definitions/motion-parallax.md)\n\n**Definition**: The difference in the apparent position of an object viewed from two different positions\n\n*   This difference is given by the angle between the two lines of sight\n*   Binocular vision uses parallax in the overlapping fields of vision in order to gain depth perception\n*   Distance measurement (i.e. depth from the viewer) via parallax is based on the principle of triangulation (uses trigonometry, s. [Monocular depth perception in humans](permanent/10-monocular-depth-perception.md))\n\n',title:"Untitled Page"},"/permanent/":{content:"---\ntitle: Permanent notes\ntag:\n- -published/meta\naliases:\n- /permanent/_index/\nhas_content: True\n---\n\n1. [Monocular depth perception](permanent/10-monocular-depth-perception.md)\n2. [Which orientation parametrisation to choose](rotations/20.4-which-orientation-parametrisation.md)\n3. [Endoscopes](permanent/30-endoscopes-index.md)",title:"Untitled Page"},"/permanent/10-monocular-depth-perception":{content:'---\ntitle: Monocular depth perception\ndate: "2021-05-17"\ntags:\n  - -sa/processed\n  - vision\n  - -permanent\n---\n\n## Depth perception in real life\n\n*   In nature, prey animals typically have eyes on either side of their head to maximise field of view, while most predators have forward-facing eyes with overlapping fields of vision (binocular vision) for maximum depth perception. Humans also have binocular vision.\n    (Some exceptions: fruit bats, killer whales)\n    \n*   We perceive depth, or distance to the objects that we see, based on several visual cues.\n    *   One of the cues is the [parallax](definitions/parallax.md) in the two overlapping fields of vision, or the \'binocular disparity\'. ([Science Focus](bibliography/science-focus.md))\n    *   However, even if vision is impaired in one eye, a human can still perceive depth by using non-binocular visual cues.\n    *   In fact, according to ([Chen 2018](bibliography/chen-2018-review.md)), "a typical human uses 14 visual cues to perceive depth, only 3/14 are binocular vision related."\n*   Some of the monocular cues that the brain uses: [Science Focus](bibliography/science-focus.md)\n	1.  We know the real size of things\n	2.  Using perspective, e.g. parallel lines converging to a perspective point\n	3.  [Motion parallax](definitions/motion-parallax.md)\n\n## Depth perception in computer vision\n\n*   Depth is not recoverable from a single camera image.\n*   This is called the "scale availability issue." s. [monocular cameras](sensors/monocular-cameras.md), whereby the scale is the factor that relates the estimated camera position to the real camera position\n*   To work around this, there are several options, such as using stereo or RGBD cameras.\n*   However, even if we are restricted to monocular cameras, using similar depth cues such as those used by the human brain in monocular vision can help recover scale.\n    *   Some algorithms compare the obtained visual measurements to an object of known scale, e.g. using a calibration checkerboard\n    *   Other algorithms make use of [motion parallax](definitions/motion-parallax.md), e.g. the [DefSLAM initialisation](SLAM/initialisation-of-monocular-slam.md)\n\n',title:"Untitled Page"},"/permanent/30-endoscopes-index":{content:'---\ntitle: Endoscopes\ndate: "2021-07-21"\ntags:\n  - -sa/processed\n  - -permanent\n  - medical/surgery/endoscope\n  - -published\n  - -master\n---\n\n- [Endoscopy](permanent/30.1.1-endoscopy.md)\n- [Endoscopes (general)](permanent/30.1.2-endoscope.md)\n	- [Types of endoscopes](permanent/30.1.2.1-types-of-endoscopes.md)\n	- [Endoscope system components](permanent/30.1.2.2-endoscope-system-components.md)\n	- [Endoscope specification](permanent/30.1.2.3-endoscope-specification.md)',title:"Untitled Page"},"/permanent/30.1.1-endoscopy":{content:"---\ntitle: Endoscopy\ndate: \"2021-07-21\"\nexternal_url: \"http://en.wikipedia.org/wiki/Endoscopy\"\ntags:\n  - medical/surgery\n  - -definitions\n  - -sa/processed\n  - -permanent\n  - -published\n---\n\n**Source**: [NHS](http://www.nhs.uk/conditions/endoscopy/)  \n**Backlinks**: [Endoscopes](permanent/30-endoscopes-index.md)\n\n*   Endoscopy is a procedure which enables inspection of organs inside the body.\n*   The prefix 'endo' comes from the Greek language and means 'within' or 'inside', *cf*. the prefix 'exo'/'ecto' meaning 'outside'.\n*   Endo- from Greek ἔνδον (within, inside), cf. exo-/ecto- from έκτός (outside)\n\nThe instrument: [endoscope](permanent/30.1.2-endoscope.md)",title:"Untitled Page"},"/permanent/30.1.2-endoscope":{content:'---\ntitle: Endoscope\ndate: "2021-07-21"\ntags:\n  - -definitions\n  - -sa/processed\n  - -permanent\n  - medical/surgery/endoscope\n  - -published\n---\n\n**Source**: [NHS](http://www.nhs.uk/conditions/endoscopy/)  \n\nThe general term for the medical instrument used to perform [endoscopy](permanent/30.1.1-endoscopy.md) is, correspondingly, the endoscope.\nIt is a device that makes use of optical technology to relay images from one end of the scope to another.\n\n### Functions\n*   not only for looking inside, but also\n*   have additional functionalities such as removing small tissue samples (biopsy)\n\n### Insertion\neither through\n*   natural body orifices (e.g. mouth, urethra)\n*   small incision in case a keyhole surgery is being performed\n\n## Subtopics\n* [Types of endoscopes](permanent/30.1.2.1-types-of-endoscopes.md)\n* [Components](permanent/30.1.2.2-endoscope-system-components.md)\n* [Specification](permanent/30.1.2.3-endoscope-specification.md)\n\n',title:"Untitled Page"},"/permanent/30.1.2.1-types-of-endoscopes":{content:"---\ntitle: Types of endoscopes\ndate: \"2021-05-21\"\ntags:\n  - -sa/processed\n  - -permanent\n  - medical/surgery/endoscope\n  - -published\n---\n\n**Source**: [Leiner](studienarbeit/leiner.md)  \n\n* Endoscopes can be classified according to their flexibility, thus resulting in 'rigid', 'flexible' and 'semi-rigid' variants.\n    *   Video sensor at distal (\"away from the surgeon\", opposite of proximal) end allows rigid endoscope to be converted to a flexible one solely by mechanical design\n    *   **Note**: the term endoscope in hospital environments typically refers to the flexible variant\n* More specialised endoscopes may be referred to by specific names, such as the cystoscope (for scoping bladders).\n\n## Rigid endoscopes\n*   In rigid endoscopes, an array of rod lenses (lenses which are much longer than their diameter) within a metal tube — also known as a telescope — are used to transmit the images from the [distal end to the proximal end](studienarbeit/distal-and-proximal-ends.md). the name hopkins can be seen on many karl storz endoscopes, in honour of the inventor-of-the-rod-lens-endoscope. [[Leiner](studienarbeit/leiner.md)]\n    *   Use of rod lenses (compared to shorter lenses): enables reduction of f-number, increases optical throughput, reduces vignetting [Leiner](studienarbeit/leiner.md)\n        ![leiner-rod-lenses.png](_img/leiner-rod-lenses.png)\n        Rod lens array. [[Leiner](studienarbeit/leiner.md)]\n        \n## Flexible endoscopes\nuse fibre optics as the image relay system. [Leiner](studienarbeit/leiner.md)\n    ![leiner-fibre-relay.png](_img/leiner-fibre-relay.png)\n\n",title:"Untitled Page"},"/permanent/30.1.2.2-endoscope-system-components":{content:'---\ntitle: 30.1.2.2 Endoscope system components\ndate: "2021-07-21"\ntags:\n  - -sa/processed\n  - -permanent\n  - medical/surgery/endoscope\n  - -published\n---\n\n**Source**: [Leiner](leiner.md)  \n\n![unknown_filename.png](studienarbeit/_resources/30.1.2.2_Endoscope_system_components.resources/unknown_filename.png)\n\n*   Imaging system (rod lens array for rigid endoscopes) within a tube/shaft\n*   Illumination/Light source (it\'s dark inside the body)\n    *   separate from the imaging system in order to reduce glare \\[low contrast of received image\\]\n    *   surrounds the imaging system like a ring light\n        ![unknown_filename.1.png](studienarbeit/_resources/30.1.2.2_Endoscope_system_components.resources/unknown_filename.1.png)\n        \n*   Video camera\n*   Coupling device (camera to imaging system)\n*   Sheath and bridge which contain the telescope, fibre optics, and provides a channel for other stuff like irrigation, forceps, other surgical instruments\n    *   Examination sheath (thinner) and operating sheath (bulkier, more space for the accessories)\n\n![unknown_filename.2.jpeg](studienarbeit/_resources/30.1.2.2_Endoscope_system_components.resources/unknown_filename.2.jpeg)\n\u003chttp://www.osapublishing.org/boe/fulltext.cfm?uri=boe-1-2-463\u0026id=204840\u003e\n\n![unknown_filename.3.png](studienarbeit/_resources/30.1.2.2_Endoscope_system_components.resources/unknown_filename.3.png)\n[http://www.researchgate.net/publication/236106568\\_Automated\\_Objective\\_Routine\\_Examination\\_of\\_Optical\\_Quality\\_of\\_Rigid\\_Endoscopes\\_in\\_a\\_Clinical\\_Setting/figures?lo=1](http://www.researchgate.net/publication/236106568_Automated_Objective_Routine_Examination_of_Optical_Quality_of_Rigid_Endoscopes_in_a_Clinical_Setting/figures?lo=1)\n\n![unknown_filename.4.png](studienarbeit/_resources/30.1.2.2_Endoscope_system_components.resources/unknown_filename.4.png)\n\n\u003chttp://www.scopecare.com/rigid-and-semi-rigid-endoscopes/\u003e\n\n',title:"Untitled Page"},"/permanent/30.1.2.3-endoscope-specification":{content:"---\ntitle: 30.1.2.3 Endoscope specification\ndate: \"2021-07-21\"\ntags:\n  - -sa/processed\n  - -permanent\n  - medical/surgery/endoscope\n  - -published\n---\n\n**Source**: [Leiner](leiner.md)\n\n*   Field of view: maximum angle that can be viewed [Leiner](leiner.md)\n    *   Generally constructed so that the FOV is wide enough so that the 'head on' view is visible even when the DOV is not zero. [Leiner](leiner.md)\n    *   This is to reduce the chances of bumping the instrument into anatomy right in front of the shaft. [Leiner](leiner.md)\n*   Direction of view: angular offset of the optical axis from the longitudinal axis of the endoscope shaft\n    *   Realised by adding a reflecting prism at the endoscope tip, which inclines the direction of view [Leiner](leiner.md)\n    *   Enables the surgeon to be able to look to the side or even to the back, by only rotating the scope along its longitudinal axis—useful for cases where range of motion is very limited due to anatomical restrictions  [Leiner](leiner.md)\n        ![unknown_filename.png](studienarbeit/_resources/30.1.2.3_Endoscope_specification.resources/unknown_filename.png)\n        \n*   Diameter: the smaller the better, to reduce trauma to patient\n*   Image resolution\n*   f-number: ratio of the optical system's focal length to aperture\n    *   Higher f-number: higher resolution\n    *   Lower f-number: increases brightness, decreases depth of field\n\n",title:"Untitled Page"},"/private/thesis-sa/ausblick":{content:'---\ntitle: Ausblick\ndate: "2021-05-26"\ntags:\n  - -sa/processed\n  - -master/thesis\n---\n\npossible improvements, further work to be done\n\n*   not real time --\u003e realtime\n*   erweitern auf optim.basierten Ansatz\n*   experimental validation using test setup: assume perfect data (ground truth from OptiTrack) --\u003e real data\n*   Einfluss der IMU auf verbesserte Lokalisierung --\u003e evtl eine IMU Koordinate weglassen\n\n',title:"Untitled Page"},"/private/thesis-sa/tex-stuff":{content:'---\ntitle: Tex stuff\ndate: "2020-12-16"\ntags:\n  - to-do\n  - -sa/processed\n  - -master/thesis\n---\n\n- [ ] tikzexternalize\n- [x] vimtex custom compile; include makeglossary\n- [x] vim UltiSnips doesn\'t work in math contexts! (Solved: \u003chttp://github.com/SirVer/ultisnips/issues/1193\n- issuecomment-620455011\u003e)\n\n- [ ] get symbols to work with glossaries or glossaries-extra\n- [ ] set up spellcheck\n- [ ] automated list of symbols\n\n',title:"Untitled Page"},"/private/thesis-sa/thesis":{content:'---\ntitle: Thesis\ndate: "2021-06-21"\ntags:\n  - -sounding-board\n  - -master/thesis\n  - -sa/processed\n  - -master/thesis\n  - discussion/2020/2020-10\n  - discussion/2021/2021-07\n  - discussion/2021/2021-08\n---\n\n## Introduction\n\n*   General\n    *   Bladder cancer surgery — cryosection\n    *   The navigation/localisation problem\n    *   GRK\n    *   [diagnosis-bladder-cancer](studienarbeit/diagnosis-bladder-cancer.md)\n*   Problem statement\n    *   Questions arising from the localisation task:\n        1.  Which localisation algorithm do we use?\n        2.  We have settled on the camera-IMU combination.\n            How do we take into account the non-constant calibration parameters between the camera and IMU (due to the surgeon\'s manipulation of the cystoscope)?\n            \n    *   Also, modularity of setup: the calibration estimation allows the IMU-camera combination to be applied to different devices \n*   Corresponding contributions\n	1.  Literature research deformable VI localisation\n	2.  Modelling the kinematic relations between the IMU and camera on the cystoscope\n		Estimation of IMU calibration parameters for VI localisation on cystoscope\n    \n\n## Literature review (I)\n*   Monocular visual SLAM: problem: depth info, s. [monocular depth perception](permanent/10-monocular-depth-perception.md)  \n*   SLAM (general)\n    *   Subsets of SLAM\n    	*   [ ] feature-based vs direct SLAM\n    *   SLAM rigidity assumption\n    *   SLAM front end\n        *   Begriffe data association, loop closure etc.\n    *   SLAM back end (filtering/optim type)\n        *   (adaptability/generalisability to other problems)\n        *   MAP estimation stuff\n    *   KF vs optimisation-based SLAM\n    	*  [ ] the generalisability of optim.-based methods\n    *   sparse vs. dense approach\n    *   front end\n    *   back end\n        *   filtering\n        *   optim\n*   use of ORB features in def. envs\n*   [ ] ORB-SLAM3 in lit review\n*   Sensors for VI-SLAM\n*   DefSLAM (deformable environment)\n*   other papers on SLAM for deformable envs e.g. MIS-SLAM ([Chen](studienarbeit/chen-2018-mis-slam.md)), Song\n*   [Works of possible interest](bibliography/works-of-interest.md)\n*   Computer vision\n    *   Descriptors: ORB, etc.\n        *   Use of ORB for deformable envs.\n    *   Bag of words\n    *   Camera model (projection equation)\n\n## Background\n\n*   [Endoscopes](permanent/30-endoscopes-index.md)\n    *   Components and specifications\n    *   Effect of the angled tip -- direction of view\n    *   Coupling mechanism, [modes of operation of the scope](studienarbeit/modes-of-operation-of-the-scope.md)\n    *   [ ]  Relate endoscopy to bladder cancery surgery ...\n*   Camera model\n*   IMU model\n*   General rotation + Quaternion stuff? or move to appendix. or skip entirely.\n*   ESKF\n\n## Implementation\n\n*   In this preliminary work, use KF \u0026 sparse approach\n    *   Future work may make use of optimisation-based approaches and dense maps -- more for the localisation side though\n\n*   Model of probe, kinematics\n    *   forward kinematics using rtb\n*   Generation of synthetic IMU data from camera data and probe kinematics, s. [Obtaining IMU measurements from camera by forward kinematics](obtaining imu measurements-from-camera-by-forward-kinematics.md)\n*   EKF (tight/loose coupled approach) --\u003e [ ] move to lit review\n*   ESKF\n    *   Our assumptions, simplified states\n\n## Validation/Results (II)\n\n*   Validating the generation of synthetic IMU data\n*   Results from the calib. param. estimation\n    \n\n?maybe: Compare DefSLAM stereo to mono+IMU\n\n## Conclusion and future work\nThis (estimation of the IMU-camera calibration parameters) provides the groundwork for implementing actual SLAM (VI fusion)\n[Ausblick](private/thesis-sa/ausblick.md)\n\n\n**To check before submitting**:\n- [ ] [Tex stuff](private/thesis-sa/tex-stuff.md)',title:"Untitled Page"},"/rotations/20.4-which-orientation-parametrisation":{content:'---\ntitle: Which orientation parametrisation to choose?\ndate: "2021-08-15"\nmathjax: true\ntags:\n  - -sa/processed\n  - math/rotations\n  - -permanent\n  - -published\n---\n\n**Source**: [MKok 2017](../studienarbeit/mkok-2017.md)\n\nEstimation algorithms (filtering, smoothing) usually assume that the unknown states and parameters are represented in Euclidean space\n\n*   Due to wrapping and gimbal lock, Euclidian addition and subtraction don\'t work\n*   Also generally don\'t work for rotation matrices and unit quaternions\n*   Constraints (unit quaternion norm, rotation matrix orthogonality) are usually hard to implement in estimation algorithms\n\nThese concerns led to the development of the [MEKF](../studienarbeit/50.4.2-multiplicative-quaternion-filtering-mekf.md).\n\nTo deal with this: [Linearisation of an orientation in SO(3)](../studienarbeit/linearisation-of-an-orientation-in-so-3.md)\nAlternative method to estimate orientation:\n\n*   assume that the orientation states lie on a manifold, e.g. using a spherical distribution, e.g. Bingham distribution\n*   this constrains the orientation estimates and their uncertainties to SO(3)\n\nCommon practice:\n\n*   Using [unit quaternions](rotations/unit-quaternions.md) in estimation algorithms and to normalise the resulting quaternions every time they lose their normalisations\n*   Or other methods to handle quaternion normalisation\n\n* * *\n\n**Source**: [Markley 2014](bibliography/markley-2014.md)\n\n*   All three-parameter representations of the rotation group have discontinuities or singularities\n*   The estimator must implement some way of avoiding the singular points\n\n*   R-P-Y Euler angles singular at $P = 90$ or $P=-90^\\circ$\n*   Gibbs / Rodrigues parameters are not suitable for filtering, as they are unable to represent $180^\\circ$ rotations\n\nThe quaternion has become the standard for representing rotations in filtering, as\n\n*   it is the lowest-dimensional parametrisation that is free of singularities.\n*   easier to normalise quaternions rather than enforce orthogonality constraint on a rotation matrix\n\nHowever, the normalisation constraint imposed on unit quaternions are not compatible with vanilla EKF.\n\n$$\\begin{aligned}\n\\hat{\\mathbf{q}}^+\n	= \\hat{\\mathbf{q}}^-\n		+ \\mathbf{K} \\left[ \\mathbf{y} - \\mathbf{h}(\\hat{\\mathbf{x}})^- \\right]\n\\end{aligned}$$\n\nThe addition causes the normalisation constraint to be violated!\ns. [additive quaternion filtering](../studienarbeit/50.4.1-additive-quaternion-filtering.md), [multiplicative quaternion-filtering (mekf)](../studienarbeit/50.4.2-multiplicative-quaternion-filtering-mekf.md)\n\n* * *\n\n**Source**: [Whampsey MEKF](../studienarbeit/whampsey-mekf.md)\n\nProblem for rotation propagation in KFs:\n\n$$\n\\begin{aligned}\n\\mathbf{x}_{k|k} \u0026= \\mathbf{x}_{k|k-1} + \\mathbf{K}_k \\mathbf{y}_k\\\\\n\\text{upd} \u0026= \\text{pred} + \\mathbf{K} \\cdot \\text{residual}\n\\end{aligned}\n$$\n\n$\\mathbf{x}_{k|k-1}$ is a unit quaternion; adding a term to it makes it non-unitary!\n\n*   deviation from the SO(3) group\n*   the vanilla KF update equation clashes with the invariant properties of rotations\n    *   it assumes the states are in Euclidian space, i.e. vector operations are applicable\n    *   but rotations are SO(3) and are therefore not really valid KF states. We need the SO(3) group operations!\n        "Groups are only closed under their respective group operations."\n\nHow to get around this?\n\n*   Renormalise the sum  \n    May work to a certain extent, but this throws off the above eqn (a posteriori correction) from the original KF implementation, and\n\n\u003cdiv class="math math-block"\u003e\n	\\begin{aligned}\n\\mathbf{P}_{k|k} = \\text{cov} (\\mathbf{x} - \\mathbf{\\hat{x}}_{k|k}) = ???\n\\end{aligned}\n\u003c/div\u003e\n\n*   [Multiplicative quaternion filtering (MEKF)](../studienarbeit/50.4.2-multiplicative-quaternion-filtering-mekf.md)\n\n',title:"Untitled Page"},"/rotations/active-passive-or-alibi-alias-rotation-transformations":{content:'---\ntitle: Active/passive or Alibi/alias rotation transformations\ndate: "2021-08-15"\ntags:\n  - -sa/processed\n  - math/rotations\n  - -published\n---\n\n**Parent**: [Rotations / SO(3) group index](rotations/rotations-so3-group-index.md)   \n**See also**: [Intrinsic vs extrinsic rotations](rotations/intrinsic-vs-extrinsic-rotations.md)\n\n**Source**: http://en.wikipedia.org/wiki/Rotation_matrix%20-%20Ambiguities\n\n| Alibi / Active    | Alias / Passive    |\n| --- | --- |\n| ![unknown_filename.4.png](studienarbeit/_resources/Active_passive_or_Alibi_alias_rotation_transformations.resources/unknown_filename.4.png) | ![unknown_filename.3.png](studienarbeit/_resources/Active_passive_or_Alibi_alias_rotation_transformations.resources/unknown_filename.3.png) |\n| CS is fixed | CS is rotated |\n| Point rotates within fixed CS | Point remains stationary but is represented within a new CS |\n| ![unknown_filename.1.png](studienarbeit/_resources/Active_passive_or_Alibi_alias_rotation_transformations.resources/unknown_filename.1.png)\u003cbr\u003eCounterclockwise rotation by theta | ![unknown_filename.2.png](studienarbeit/_resources/Active_passive_or_Alibi_alias_rotation_transformations.resources/unknown_filename.2.png) |\n| ![5c2e55a0c26e29eecd9dd6e93ce2be77cd6611e0](http://wikimedia.org/api/rest_v1/media/math/render/svg/5c2e55a0c26e29eecd9dd6e93ce2be77cd6611e0) |     |\n| mathematics | physics, robotics |\n\n---\n\n**Source**: [http://rock-learning.github.io/pytransform3d/transformation\\_ambiguities.html](http://rock-learning.github.io/pytransform3d/transformation_ambiguities.html)\nTo transform between one another: use the inverse \n\n![unknown_filename.png](studienarbeit/_resources/Active_passive_or_Alibi_alias_rotation_transformations.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/rotations/bryan-tait-kardanwinkel":{content:'---\ntitle: Rotations as xyz Bryan-Tait angles (Kardanwinkel)\ndate: "2021-05-24"\ntags:\n  - -sa/processed\n  - math/rotations\n---\n\n**Parent**: [rotations-so3-group-index](rotations/rotations-so3-group-index.md)  \n**Source**: [Woernle](woernle-mehrkoerpersysteme.md)\n\n## Rotation angle nomenclature\n* Euler angles: ZXZ (mitgedrehte Achsen)\n* Kardan-Winkel \\[de\\] / Bryan-Tait angles: ZYX (mitgedrehte Achsen)\n\n## xyz-Kardan-Winkel\n\n![unknown_filename.png](studienarbeit/_resources/Rotations_as_xyz_Bryan-Tait_angles_(Kardanwinkel).resources/unknown_filename.png)\n![unknown_filename.8.png](studienarbeit/_resources/Rotations_as_xyz_Bryan-Tait_angles_(Kardanwinkel).resources/unknown_filename.8.png)\n\nK3: körperfestes KS\nK0: Welt-KS\n\n| Ausgangslage | 1.  Drehung um x0 | 2.  Drehung um y1 | 3.  Drehung um z2 |\n| --- | --- | --- | --- |\n| ![unknown_filename.10.png](studienarbeit/_resources/Rotations_as_xyz_Bryan-Tait_angles_(Kardanwinkel).resources/unknown_filename.10.png) | ![unknown_filename.9.png](studienarbeit/_resources/Rotations_as_xyz_Bryan-Tait_angles_(Kardanwinkel).resources/unknown_filename.9.png) | ![unknown_filename.11.png](studienarbeit/_resources/Rotations_as_xyz_Bryan-Tait_angles_(Kardanwinkel).resources/unknown_filename.11.png) | ![unknown_filename.1.png](studienarbeit/_resources/Rotations_as_xyz_Bryan-Tait_angles_(Kardanwinkel).resources/unknown_filename.1.png) |\n|     | ![unknown_filename.6.png](studienarbeit/_resources/Rotations_as_xyz_Bryan-Tait_angles_(Kardanwinkel).resources/unknown_filename.6.png) | ![unknown_filename.3.png](studienarbeit/_resources/Rotations_as_xyz_Bryan-Tait_angles_(Kardanwinkel).resources/unknown_filename.3.png) | ![unknown_filename.4.png](studienarbeit/_resources/Rotations_as_xyz_Bryan-Tait_angles_(Kardanwinkel).resources/unknown_filename.4.png) |\n\n![unknown_filename.5.png](studienarbeit/_resources/Rotations_as_xyz_Bryan-Tait_angles_(Kardanwinkel).resources/unknown_filename.5.png)\n\nWinkelgeschwindigkeit\n\n![unknown_filename.7.png](studienarbeit/_resources/Rotations_as_xyz_Bryan-Tait_angles_(Kardanwinkel).resources/unknown_filename.7.png) (either depicted in CS0, or CS3)\n![unknown_filename.2.png](studienarbeit/_resources/Rotations_as_xyz_Bryan-Tait_angles_(Kardanwinkel).resources/unknown_filename.2.png)\n\n',title:"Untitled Page"},"/rotations/euler-axis-angle-representation":{content:'---\ntitle: Euler axis/angle representation\ndate: "2021-08-17"\ntags:\n  - -sa/processed\n  - math/rotations\n---\n\n**Parents**: [rotations-so3-group-index](rotations/rotations-so3-group-index.md), [orientation-parametrisations](orientation-parametrisations.md)    \n**See also**: [Rotation vector representation](rotation-vector-representation.md)  \n\n**Source**: [Markley 2014](bibliography/markley-2014.md)\n\n3 parameters:\n\n*  there appears to be 4 parameters: 1 angle, 3-component unit vector (Euler axis, Euler angle of the rotation)\n*   however, the vector $\\mathbf{e}$ is a unit vector (constrained by $\\left\\lVert \\mathbf{e} \\right\\rVert = 1$\n\n### To rotation matrix\n![unknown_filename.png](studienarbeit/_resources/Euler_axis_angle_representation.resources/unknown_filename.png)\n![unknown_filename.4.png](studienarbeit/_resources/Euler_axis_angle_representation.resources/unknown_filename.4.png)\n\nThe matrix is periodic (period 2\\*pi)\n![unknown_filename.1.png](studienarbeit/_resources/Euler_axis_angle_representation.resources/unknown_filename.1.png)\n\n### From rotation matrix\n![unknown_filename.2.png](studienarbeit/_resources/Euler_axis_angle_representation.resources/unknown_filename.2.png)\n\nIf -1 \u003c cos theta \u003c 1: \n![unknown_filename.5.png](studienarbeit/_resources/Euler_axis_angle_representation.resources/unknown_filename.5.png)\n\nIf cos theta = 1: axis undefined\n\nIf cos theta = -1, e = any column of the following (apply normalisation to the column), whichever sign\n![unknown_filename.3.png](studienarbeit/_resources/Euler_axis_angle_representation.resources/unknown_filename.3.png)\n\n',title:"Untitled Page"},"/rotations/exponential-map":{content:'---\ntitle: Exponential map\ndate: "2021-07-23"\ntags:\n  - -definitions\n  - -sa/processed\n  - math/rotations\n  - math/quaternions\n  - -published\n---\n\n**Parents**: [Quaternion index](rotations/quaternion-index.md), [Rotations / SO(3) group index](rotations-so3-group-index.md)\n\n## Notation\n\n### Variables\n$$\\begin{alignedat}{3}\n\u0026\\phi \u0026\u0026\\in \\mathbb{R}^3\\\\\n\u0026\\phi^\\wedge \u0026\u0026\\in \\mathfrak{so}(3)\\\\\n\u0026\\mathbf{R} \u0026\u0026\\in \\text{SO}(3)\\\\\n\\end{alignedat}$$\n\n### Functions\n$$\\begin{alignedat}{3}\n\\text{(skew) } \\wedge \u0026:\u0026 \\mathbb{R}^3 \u0026\u0026\\rightarrow \\mathfrak{so}(3) \u0026\\\\\n\\exp		\u0026:\u0026 ~\u0026\u0026\\mathfrak{so}(3) \u0026\\rightarrow \\text{SO}(3)\\\\\n\\text{Exp}	\u0026:\u0026 ~\\mathbb{R}^3	 \n		\u0026\u0026\n		\u0026\\rightarrow \\text{SO}(3)\n\\end{alignedat}$$\n\n\u003cpre\u003e\u003c/pre\u003e\nThus,\n$$\\begin{alignedat}{3}\n\u0026\\exp(\\phi^\\wedge) = \\text{Exp}(\\phi) = \\mathbf{R}\n\\end{alignedat}$$\n\n----\n\n**Source**: [Forster 2017 IMU Preintegration](forster-2017-imu-preintegration.md)\n\n## At the identity \nMaps an element of the [Lie algebra](lie-group-lie-algebra.md)\n($\\phi^\\wedge \\in \\mathfrak{so}(3)$, a skew symmetric matrix)\nto a rotation\n![unknown_filename.1.png](studienarbeit/_resources/Exponential_map.resources/unknown_filename.1.png)\n\nFirst order approximation\n$$ \\exp(\\phi^\\wedge) \\approx \\mathbf{I} + \\phi^\\wedge$$\n\n\n## Some properties of the exponential map\n*   Perturbations, first order approximation\n    ![unknown_filename.3.png](studienarbeit/_resources/Exponential_map.resources/unknown_filename.3.png)\n    with the right Jacobian of SO(3)\n    ![unknown_filename.4.png](studienarbeit/_resources/Exponential_map.resources/unknown_filename.4.png)\n    \n*   $J_r(\\phi) = \\mathbf{I}$ for very small angles\n*   Following the Adjoint expression\n    ![unknown_filename.5.png](studienarbeit/_resources/Exponential_map.resources/unknown_filename.5.png)\n    \n\n- [x] difference between Exp and exp? — I think exp acts on the skew-sym matrix; Exp acts on the corresp. rotation vector\n\n* * *\n\n**Source**: [MKok 2017](mkok-2017.md)\n\n## Approximations for small perturbations\n$$\n\\begin{aligned}\n\\exp_q(\\eta) \u0026\\approx \\left( \\begin{array}{c}1 \\\\ \\eta\\end{array} \\right)\\\\\n\\exp_R(\\eta) \u0026\\approx \\mathbf{I}_3 + \\left[ \\eta \\times\\right]\n\\end{aligned}\n$$\n\n**Note**: in this source, $\\exp_q$ and $\\exp_R$ equivalent to the $\\text{Exp}$ notation in other sources.\nSo here, $\\eta$ is implicitly converted either to $\\left[ \\eta \\times\\right]$ or $\\eta/2$ in an intermediate step.\n',title:"Untitled Page"},"/rotations/gibbs-rodrigues-parameter":{content:'---\ntitle: Gibbs / Rodrigues parameter representation for rotations\ndate: "2021-08-17"\ntags:\n  - to-do/to-clarify\n  - -sa/processed\n  - math/rotations\n  - math/quaternions\n---\n\n**Parent**: [Orientation parametrisations](orientation-parametrisations.md)   \n**See also**: [Rotation error representation](rotations/rotation-error-representation.md)\n\n**Source**: [Markley 2014](bibliography/markley-2014.md)\n\nFrom [unit quaternions](rotations/unit-quaternions.md):\n![Image.png](studienarbeit/_resources/Gibbs___Rodrigues_parameter_representation_for_rotations.resources/Image.png)\n\nFrom [euler-axis-angle-representation](rotations/euler-axis-angle-representation.md):\n![unknown_filename.2.png](studienarbeit/_resources/Gibbs___Rodrigues_parameter_representation_for_rotations.resources/unknown_filename.2.png)\n\nTo [unit quaternions](rotations/unit-quaternions.md): \n\n ![unknown_filename.1.png](studienarbeit/_resources/Gibbs___Rodrigues_parameter_representation_for_rotations.resources/unknown_filename.1.png)\n\n![unknown_filename.png](studienarbeit/_resources/Gibbs___Rodrigues_parameter_representation_for_rotations.resources/unknown_filename.png)\n\n*   Plane of the figure contains [identity quaternion](rotations/identity-quaternion.md), origin\n*   The circle is a cross section of the quaternion sphere S^3\n*   The upper horizontal axis is the 3D Gibbs vector hyperplane (tangent at the identity quaternion)\n\n\\[+\\] q and -q map to the same Gibbs vector, therefore there is a 1:1 mapping of rotations between quaternions and the Gibbs parameter\n\\[-\\] the Gibbs vector is infinite for 180 degree rotations (q.w = q4 = 0? - [ ] )\n\n*   therefore not good for global orientation representations,\n*   but good for representation of small rotations\n\n',title:"Untitled Page"},"/rotations/identity-of-a-group":{content:'---\ntitle: Identity of a group\ndate: "2021-09-30"\ntags:\n  - -sa/processed\n  - math/rotations\n  - -definitions\n---\n\n**Source**: https://vladimirbozovic.net/univerzitet/wp-content/uploads/2010/11/group-theory-chapter.pdf\n\nIn every group, there is an element $E$, such that\n$$EA = AE = A$$\nfor every member $A$ of the group.',title:"Untitled Page"},"/rotations/identity-quaternion":{content:'---\ntitle: Identity quaternion\ndate: "2021-05-14"\ntags:\n  - -sa/processed\n  - math/quaternions\n---\n\n**Parent**: [Quaternion index](rotations/quaternion-index.md)  \n**Source**: [Solà 2017](solà-2017-quaternion-kinematics-for-eskf.md)\n\n$$\\mathbf{q}_I = 1 = \\left[ \\begin{array}{c} 1\\\\ \\mathbf{0}_3 \\end{array} \\right]$$\n\n$$\\mathbf{q}_I \\otimes \\mathbf{q}\n= \\mathbf{q} \\otimes \\mathbf{q}_I = \\mathbf{q}$$',title:"Untitled Page"},"/rotations/intrinsic-vs-extrinsic-rotations":{content:'---\ntitle: Intrinsic vs extrinsic rotations\ndate: "2021-08-17"\nexternal_url: "http://rock-learning.github.io/pytransform3d/transformation_ambiguities.html"\ntags:\n  - -sa/processed\n  - math/rotations\n  - -published\n---\n\n**Parent**: [Rotations/SO(3) Group Index](rotations/rotations-so3-group-index.md)  \n**See also**: [Active/passive or Alibi/alias rotation transformations](rotations/active-passive-or-alibi-alias-rotation-transformations.md)\n\n**Source**: [http://rock-learning.github.io/pytransform3d/transformation\\_ambiguities.html](http://rock-learning.github.io/pytransform3d/transformation_ambiguities.html)\n\nWe want to rotate first by $R_1$, then by $R_2$.\n\n## Extrinsic rotation\nIn global coordinates, extrinsic rotation: $R_2 \\cdot R_1$  \n![unknown_filename.2.png](studienarbeit/_resources/Intrinsic_vs_extrinsic_rotations.resources/unknown_filename.2.png)\n\n## Intrinsic rotation\nIn local coordinates, intrinsic rotation: $R_1 \\cdot R_2$  \n($R_1$ defines new coordinates in which $R_2$ is applied)  \n![unknown_filename.3.png](studienarbeit/_resources/Intrinsic_vs_extrinsic_rotations.resources/unknown_filename.3.png)\n\nSpecifying the convention is relevant when dealing with Euler angles!!!\n\n',title:"Untitled Page"},"/rotations/lie-group-lie-algebra":{content:'---\ntitle: Lie group, Lie algebra\ndate: "2021-07-23"\ntags:\n  - -sa/processed\n  - math/rotations\n  - -published\n  - -definitions\n---\n\n# Lie group\n\n**Parent**: [Rotations / SO(3) group index](rotations-so3-group-index.md)  \n**Source**: \u003chttp://www.seas.upenn.edu/~meam620/slides/kinematicsI.pdf\u003e\n\nA group that is a differentiable (smooth) [manifold](manifolds.md) is called a Lie group.\n\n\n# Lie algebra\n\n**Source**: [http://en.wikipedia.org/wiki/3D\\_rotation\\_group](http://en.wikipedia.org/wiki/3D_rotation_group)\n\nLie algebra $\\mathfrak{so}(3)$\n\n*   Every Lie group has an associated Lie algebra\n*   Lie algebra: linear space with same dimension as the Lie group\n*   Consists of all skew-symmetric 3x3 matrices\n*   Elements of the Lie algebra $\\mathfrak{so}(3)$ are elements of the tangent space of the manifold SO(3)/Lie group at the [identity element](rotations/identity-of-a-group.md).\n\n---\n\n**Source**: [Forster 2017 IMU Preintegration](forster-2017-imu-preintegration.md)\n\nAn Euler vector $\\mathbf{\\omega} = \\left(x,y,z\\right) \\in \\mathbb{R}^3$ can be represented by a skew symmetric matrix in the Lie algebra\n\n$$\n\\hat{\\mathbf{\\omega}} = \\left[\\begin{array}{ccc}\n	0 \u0026 -z \u0026 y\\\\\n	z \u0026 0 \u0026 -x\\\\\n	-y \u0026 x \u0026 0\n	\\end{array}\\right] \\in \\mathfrak{so}(3)\n$$\n\n![lie-group-maps](_img/lie-group-maps.png)\n\n**Mappings**:\n* [Exponential map](rotations/exponential-map.md)\n* [Logarithm map](rotations/logarithm-map.md)\n\n',title:"Untitled Page"},"/rotations/logarithm-map":{content:'---\ntitle: Logarithm map\ndate: "2021-07-23"\ntags:\n  - -definitions\n  - -sa/processed\n  - math/rotations\n  - math/quaternions\n---\n\n**Parents**: [Quaternion index](rotations/quaternion-index.md), [rotations / so(3) group-index](rotations-so3-group-index.md)  \nBacklinks: [Linearisation of an orientation in SO(3)](linearisation-of-an-orientation-in-so-3.md)\n\n**Source**: [Forster 2017 IMU Preintegration](forster-2017-imu-preintegration.md)\n\nMaps a rotation matrix $R$ in [SO(3)](rotations/so3-3d-rotation-group.md) to a skew-symmetric matrix ([Lie algebra](rotations/lie-group-lie-algebra.md))\n![unknown_filename.png](studienarbeit/_resources/Logarithm_map.resources/unknown_filename.png)\n\nPerturbations, first order approximation\n![unknown_filename.1.png](studienarbeit/_resources/Logarithm_map.resources/unknown_filename.1.png)\n\nS. Forster \\[2015\\] suppplementary material for the inverse Jacobian\n\n* * *\n\nSource: [MKok 2017](mkok-2017.md)\n\n![Image.png](studienarbeit/_resources/Logarithm_map.resources/Image.png)\n\nApproximations for small perturbations\n![unknown_filename.2.png](studienarbeit/_resources/Logarithm_map.resources/unknown_filename.2.png)\n\n',title:"Untitled Page"},"/rotations/quaternion-double-cover":{content:'---\ntitle: Quaternion double cover\ndate: "2021-09-30"\ntags:\n  - -sa/processed\n  - math/quaternions\n  - -published\n---\n\n**Parent**: [Quaternion index](rotations/quaternion-index.md)\n\n**Source**: [Solà 2017](solà-2017-quaternion-kinematics-for-eskf.md)\n\n$$\n\\mathbf{q}\n	= \\left[ \\begin{array}{c}\n		q_w\\\\ \\mathbf{q}_v\n		\\end{array} \\right]\n	= \\left[ \\begin{array}{c}\n		\\cos\\frac{\\phi}{2} \\\\ \\mathbf{u} \\sin\\frac{\\phi}{2}\n		\\end{array} \\right]\n$$\nwhere $\\phi$ is the angle rotated by $\\mathbf{q}$ on objects in the 3D space $\\mathbb{R}^3$.\n\n![quaternion-double-cover](_img/quaternion-double-cover.png)\n\n### Recap\n* $\\theta$ is the angle in quaternion space (s. [unit quaternions](rotations/unit-quaternions.md))\n* $\\phi$ as the angle in 3D space $\\mathbb{R}^3$\n\nTherefore, the angle is halved in quaternion space.\n$$\\theta = \\phi / 2$$\n',title:"Untitled Page"},"/rotations/quaternion-index":{content:'---\ntitle: Quaternion index\ndate: "2021-05-14"\ntags:\n  - -master\n  - -sa/processed\n  - math/quaternions\n  - -published\n---\n\n## Notation\n[Quaternion conventions](quaternion-conventions.md)\n\n## Basic math/properties\n* [Quaternion multiplication](rotations/quaternion-multiplication.md)\n* [Identity quaternion](rotations/identity-quaternion.md)\n* [Quaternion conjugate](quaternion-conjugate.md)\n* [Quaternion norm](quaternion-norm.md)\n* [Inverse quaternion](inverse-quaternion.md)\n* [Unit quaternions](rotations/unit-quaternions.md)\n* [Double cover](rotations/quaternion-double-cover.md)\n\n### Calculus\n[Quaternion differentiation](quaternion-differentiation.md)\n\n## As rotation\n* [rotations-so3-group-index](rotations/rotations-so3-group-index.md)\n	* [exponential-map](rotations/exponential-map.md)\n	* [logarithm-map](rotations/logarithm-map.md)\n* [orientation-parametrisations](studienarbeit/orientation-parametrisations.md)\n	* [Which-orientation-parametrisation](rotations/20.4-which-orientation-parametrisation.md)\n	* [linearisation-of-an-orientation-in-so-3](studienarbeit/linearisation-of-an-orientation-in-so-3.md)\n* [Quaternion to rotation matrix](rotations/quaternion-to-rotation-matrix.md)\n* [Rotation error representation](rotations/rotation-error-representation.md)\n\n## For filtering\n* [Additive-quaternion-filtering](studienarbeit/50.4.1-additive-quaternion-filtering.md)\n* [Multiplicative-quaternion-filtering-mekf](studienarbeit/50.4.2-multiplicative-quaternion-filtering-mekf.md)\n\n## Literature\n* [Solà 2017 Quaternion kinematics for ESKF](solà-2017-quaternion-kinematics-for-eskf.md)\n* [MKok 2017 Using inertial sensors for position and orientation estimation](mkok-2017.md)\n\n',title:"Untitled Page"},"/rotations/quaternion-multiplication":{content:'---\ntitle: Quaternion multiplication\ndate: "2021-05-14"\ntags:\n  - -sa/processed\n  - math/quaternions\n  - -published\n---\n\n**Parent**: [Quaternion index](rotations/quaternion-index.md)  \n**Source**: [Solà 2017 Quaternion kinematics for ESKF](solà-2017-quaternion-kinematics-for-eskf.md)\n\nHere: Hamiltonian convention, s. [Quaternion conventions](quaternion-conventions.md)\n\n![unknown_filename.png](studienarbeit/_resources/Quaternion_multiplication.resources/unknown_filename.png)\n\n*   Non-commutative ![unknown_filename.1.png](studienarbeit/_resources/Quaternion_multiplication.resources/unknown_filename.1.png)\n*   Associative ![unknown_filename.2.png](studienarbeit/_resources/Quaternion_multiplication.resources/unknown_filename.2.png)\n*   Distributive![unknown_filename.3.png](studienarbeit/_resources/Quaternion_multiplication.resources/unknown_filename.3.png)\n\nMultiplication as a matrix product\n![unknown_filename.4.png](studienarbeit/_resources/Quaternion_multiplication.resources/unknown_filename.4.png)\nWith\n\n*   the matrices\n    ![unknown_filename.5.png](studienarbeit/_resources/Quaternion_multiplication.resources/unknown_filename.5.png)\n    \n*   the skew operator (skew symmetric matrix)\n    ![unknown_filename.6.png](studienarbeit/_resources/Quaternion_multiplication.resources/unknown_filename.6.png)\n    ![unknown_filename.7.png](studienarbeit/_resources/Quaternion_multiplication.resources/unknown_filename.7.png)\n    \n    s. also cross product\n    ![unknown_filename.8.png](studienarbeit/_resources/Quaternion_multiplication.resources/unknown_filename.8.png)\n\n---\n\n**Source**: [Markley 2014](bibliography/markley-2014.md)\n\n![unknown_filename.9.png](studienarbeit/_resources/Quaternion_multiplication.resources/unknown_filename.9.png)\n\nwith the matrices\n![unknown_filename.10.png](studienarbeit/_resources/Quaternion_multiplication.resources/unknown_filename.10.png)\n![unknown_filename.11.png](studienarbeit/_resources/Quaternion_multiplication.resources/unknown_filename.11.png)\n\n',title:"Untitled Page"},"/rotations/quaternion-to-rotation-matrix":{content:'---\ntitle: Quaternion to rotation matrix\ndate: "2021-08-17"\ntags:\n  - -sa/processed\n  - math/quaternions\n  - -published\n---\n\n**Parents**: [quaternion-index](rotations/quaternion-index.md), [rotations-so3-group-index](rotations/rotations-so3-group-index.md)  \n**See also**: [orientation-parametrisations](studienarbeit/orientation-parametrisations.md)\n\n\n**Source**: [Markley 2014](bibliography/markley-2014.md)\n\n# [Unit quaternion](rotations/unit-quaternions.md) to rotation matrix\n\n![unknown_filename.png](studienarbeit/_resources/Quaternion_to_rotation_matrix.resources/unknown_filename.png)\n\n![unknown_filename.1.png](studienarbeit/_resources/Quaternion_to_rotation_matrix.resources/unknown_filename.1.png)\n\n',title:"Untitled Page"},"/rotations/rotation-error-representation":{content:'---\ntitle: Rotation error representation\ndate: "2021-08-17"\ntags:\n  - -sa/processed\n  - math/rotations\n  - math/quaternions\n  - -published\n---\n\n**Parents**: [rotations-so3-group-index](rotations/rotations-so3-group-index.md), [quaternion-index](rotations/quaternion-index.md)  \n**See also**: [Orientation parametrisations](orientation-parametrisations.md), [which orientation parametrisation to-choose?](rotations/20.4-which-orientation-parametrisation.md)\n\n**Source**: [Markley 2014](bibliography/markley-2014.md)\n\nNote:\n\n*   Only for small angle approximations!\n*   all these representations are equivalent through second order as\n    ![unknown_filename.1.png](studienarbeit/_resources/Rotation_error_representation.resources/unknown_filename.1.png)\n    \n\nIn terms of...\n\nRotation vector\n![unknown_filename.2.png](studienarbeit/_resources/Rotation_error_representation.resources/unknown_filename.2.png)\n\n[Quaternions](rotations/unit-quaternions.md)\n![unknown_filename.png](studienarbeit/_resources/Rotation_error_representation.resources/unknown_filename.png)\n\nEuler angles\n![unknown_filename.3.png](studienarbeit/_resources/Rotation_error_representation.resources/unknown_filename.3.png)\n\nUse upper sign  if {i,j,k} even permutation of {1,2,3}, lower sign otherwise\n\n[gibbs-rodrigues-parameter](rotations/gibbs-rodrigues-parameter.md). MRPs. etc.\n\n',title:"Untitled Page"},"/rotations/rotations-so3-group-index":{content:'---\ntitle: Rotations / SO(3) group index\ndate: "2021-08-16"\naliases:\n  - /rotations/_index/\n  - /rotations/index/\ntags:\n  - -master\n  - -sa/processed\n  - math/rotations\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-to-read\n  - -published\n---\n\n## Group theory\n* [SE(3) Special Euclidian Group](rotations/se3-special-euclidian-group.md)\n* [SO(3) 3D rotation group](rotations/so3-3d-rotation-group.md)\n* [Lie group, Lie algebra](rotations/lie-group-lie-algebra.md)\n	* [Exponential map](rotations/exponential-map.md)\n	* [Logarithm map](rotations/logarithm-map.md)\n\n## Ambiguities in rotation representations\n* [Active/passive or Alibi/alias rotation transformations](rotations/active-passive-or-alibi-alias-rotation-transformations.md)\n* [Intrinsic vs extrinsic rotations](rotations/intrinsic-vs-extrinsic-rotations.md)\n\n## Rotation representations  \n[Orientation parametrisations](orientation-parametrisations)  \n[Rotation error representation](rotations/rotation-error-representation.md)\n\n* [Which orientation parametrisation to choose?](rotations/20.4-which-orientation-parametrisation.md)  \n* [Linearisation of an orientation in SO(3)](studienarbeit/linearisation-of-an-orientation-in-so-3.md)\n\n### As Euler angles\n* [Euler angles](euler-angles.md)\n* [Rotations as xyz Bryan-Tait angles (Kardanwinkel)](rotations/bryan-tait-kardanwinkel.md)\n\n###  **Axis/angle**\n*   [Euler axis/angle representation](rotations/euler-axis-angle-representation.md)\n\n### **As quaternions**\n*   [Quaternion index](rotations/quaternion-index.md)\n*   [Quaternion to rotation matrix](rotations/quaternion-to-rotation-matrix.md)\n\n\n## Kinematics\n* [Kinematics primer](kinematics-primer.md)\n* [Chaining rotation matrices and angular velocities](chaining-rotation-matrices-and-angular-velocities.md)\n\n## IMU specific  \n[IMU preintegration on manifold](imu-preintegration-on-manifold.md)\n\n## To read\n* [ ] https://rip94550.wordpress.com/2010/08/09/quaternions-and-rotations-1/',title:"Untitled Page"},"/rotations/se3-special-euclidian-group":{content:'---\ntitle: SE(3) Special Euclidian Group\ndate: "2020-11-27"\ntags:\n  - -definitions\n  - -sa/processed\n  - math/rotations\n---\n\n**Source**: [Forster 2017 -- IMU Preintegration](forster-2017-imu-preintegration.md)\n\nGroup of rigid motion in 3D.  \nConsists of \n* a rotation in [SO(3)](rotations/so3-3d-rotation-group.md)\n* a translation in $\\mathbb{R}^3$\n\n$$\\begin{aligned}\n\\text{SE}(3) \\dot{=} \\left\\lbrace\n		\\left(\\mathbf{R}, \\mathbf{p}\n			\\right) :\n			\\mathbf{R} \\in \\text{SO}(3),\n			\\mathbf{p} \\in \\mathbb{R}^3\n	\\right\\rbrace\n\\end{aligned}$$\n\n$$\\begin{aligned}\n\\mathbf{T}_1\\mathbf{T}_2 \u0026= \\left(\n	\\mathbf{R}_1\\mathbf{R}_2,\n	\\mathbf{R}_1\\mathbf{p}_2 + \\mathbf{p}_1\n	\\right)\\\\\n\\mathbf{T}_1^{-1} \u0026= \\left(\n	\\mathbf{R}_1^\\text{T}, -\\mathbf{R}_1^{\\text{T}}\\mathbf{p}_1\n	\\right)\n\\end{aligned}$$\n\n\n',title:"Untitled Page"},"/rotations/so3-3d-rotation-group":{content:"---\ntitle: SO(3) 3D rotation group\ndate: \"2020-11-27\"\ntags:\n  - -definitions\n  - -sa/processed\n  - math/rotations\n  - -published\n---\n\n**Parent**: [Rotations / SO(3) group index](rotations-so3-group-index.md)  \n**See also**: [Orientation parametrisations](orientation-parametrisations.md), [Linearisation of an orientation](studienarbeit/linearisation-of-an-orientation-in-so-3.md), [Solà 2017 quaternion kinematics for eskf](solà-2017-quaternion-kinematics-for-eskf.md)\n\n**Source**: [MKok 2017](studienarbeit/mkok-2017.md)\n\n*   All orthogonal matrices with dim 3x3 have the property  \n		$$RR^\\text{T} = R^\\text{T}R = I_3$$\n*   They are part of the orthogonal group O(3)\n*   If, additionally, $\\det R = 1$, then the matrix belongs to SO(3) and is a rotation matrix\n\n* * *\n\n**Source**: [http://en.wikipedia.org/wiki/3D\\_rotation\\_group](http://en.wikipedia.org/wiki/3D_rotation_group)\n\nThe SO(3) group\n\n*   Group of all rotations about the origin of the 3D space (Euclidian space, $\\mathbb{R}^3$)\n*   Has a natural structure as a smooth [manifold](studienarbeit/manifolds.md).\n    *   Group operations on the manifold are smoothly differentiable\n    *   Is therefore a [Lie group](rotations/lie-group-lie-algebra.md)\n*   Compact, dim = 3\n\nRotations in general\n\n*   Rotations are linear transformations of $\\mathbb{R}^3$\n*   Can be represented as matrices using an orthonormal basis of $\\mathbb{R}^3$\n*   These matrices are called 'special orthogonal matrices', i.e. SO(3)\n\n* * *\n\n**Source**: [Forster 2017 IMU Preintegration](forster-2017-imu-preintegration.md)  \n**See**: [Lie group](rotations/lie-group-lie-algebra.md)\n\nUncertainty description in SO(3)\n\n*   Define a distribution in the tangent space (Lie algebra)\n*   Map the distribution in the tangent space to SO(3) via the exponential map\n*   We get the random variable $\\tilde{R} \\in \\text{SO}(3)$\n		$$\\begin{aligned}\n		\\tilde{R} \u0026= \\mathbf{R}~ \\text{Exp}(\\varepsilon)\n			\\quad \\varepsilon \\sim \\mathcal{N} (0, \\Sigma)\n		\\end{aligned}$$\n\nDistribution $p(\\tilde{R})$ of the random variable:\n![unknown_filename.4.png](studienarbeit/_resources/SO(3)_3D_rotation_group.resources/unknown_filename.4.png)\n\nWhen $\\Sigma$ is small, the normalisation factor can be approximated to\n![unknown_filename.3.png](studienarbeit/_resources/SO(3)_3D_rotation_group.resources/unknown_filename.3.png)\n\nIf the normalisation factor beta is approximated as a constant, the negative log-likelihood of a rotation R given its measurement is\n![unknown_filename.5.png](studienarbeit/_resources/SO(3)_3D_rotation_group.resources/unknown_filename.5.png)\n\n",title:"Untitled Page"},"/rotations/unit-quaternions":{content:'---\ntitle: Unit quaternions\ndate: "2021-05-14"\ntags:\n  - -sa/processed\n  - math/quaternions\n  - -published\n---\n\n**Parent**: [Quaternion index](rotations/quaternion-index.md), [orientation-parametrisations](orientation-parametrisations.md)  \n**See also**: [quaternion-conventions](studienarbeit/quaternion-conventions.md), [quaternion double cover](rotations/quaternion-double-cover.md)\n\n**Source**: [Solà 2017](solà-2017-quaternion-kinematics-for-eskf.md)\n\n## Properties\n$$\n\\begin{aligned}\n\\left\\lVert \\mathbf{q} \\right\\rVert \u0026= 1\\\\\n\\mathbf{q}^{-1} \u0026= \\mathbf{q}^*\n\\end{aligned}\n$$\n\nCan be written in the form\n$$\n\\mathbf{q} = \\left[ \\begin{array}{c}\n	\\cos\\theta \\\\ \\mathbf{u} \\sin\\theta\n	\\end{array} \\right]\n$$\n\n![](_img/quaternion_definition.png)\n\nwith\n* $\\mathbf{u}$ as a unit vector\n* $\\theta$ is the angle between $\\mathbf{q}$ and the\n	[identity quaternion](rotations/identity-quaternion.md)\n	$\\mathbf{q}_I = \\left[\\begin{array}{cccc}1 \u0026 0 \u0026 0 \u0026 0\\end{array}\\right]^\\text{T}$',title:"Untitled Page"},"/sensors/gyroscope":{content:"---\ntitle: Gyroscope/Gyrometer\ndate: \"2021-09-29\"\nmathjax: true\ntags:\n  - -sa/processing\n  - sensors\n---\n\n**Source**: [Phil's Lab](bibliography/phils-lab-sensor-fusion.md)\n\n## Gyroscope model\n$$\n\\begin{aligned}\n\\mathbf{\\omega}_B \u0026=\n	\\mathbf{\\omega}_\\text{true}\n	+ \\mathbf{\\beta}(t)\n	+ \\mathbf{\\eta}(t)\n\\end{aligned}\n$$\n\nWe need to transform these *body* rates to [Euler](euler-angles) rates!\n$$\n\\begin{aligned}\n\\left[ \\begin{array}{c} \\dot{\\phi} \\\\ \\dot{\\theta} \\end{array} \\right]\n\u0026= \\left[ \\begin{array}{ccc}\n	1 \u0026 \\sin\\phi \\tan\\theta \u0026 \\cos\\phi\\tan\\theta\\\\\n	0 \u0026 \\cos\\phi \u0026 -\\sin\\phi\n	\\end{array} \\right]\n	\\left[ \\begin{array}{c}p\\\\q\\\\r\\end{array} \\right]\n\\end{aligned}\n$$\n\n\nProblem: $\\phi$ and $\\theta$ need to be known!  \n--\u003e integrate?\n$$\\hat{\\phi} = \\int_0^T \\dot{\\phi}(t) ~dt ~?$$\n\nDirect integration not possible due to the presence of time-varying bias and noise; integration leads to gyroscope drift (we integrate the noise/error terms so we drift away from the true value)!\n$$\\hat{\\phi} = \\int_0^T  \\dot{\\phi}(t) + \\beta_\\phi(t) + \\eta_\\phi(t) ~dt$$\n\n## Conclusions\n* Using only the gyroscope provides a good estimate over short periods of time (due to integration of bias terms)\n* If using for a short period of time, the drift won't affect us too much (not had time to accumulate)",title:"Untitled Page"},"/sensors/imu":{content:'---\ntitle: IMU\ndate: "2021-04-23"\ntags:\n  - -sa/processed\n  - sensors/IMU\n  - -published\n---\n\n**Source**: [Mur-Artal 2017](bibliography/mur-artal-2017-vi-orb.md)\n\n*   measures acceleration (from accelerometer) and angular velocity (from gyrometer) of sensor at regular intervals\n*   measurements are affected by\n    *   sensor noise\n    *   accelerometer bias\n    *   gyrometer bias\n*   accelerometer is further affected by gravity --\u003e need to subtract effect of gravity',title:"Untitled Page"},"/sensors/laser-scanners":{content:'---\ntitle: Laser scanners\ndate: "2020-08-22"\ntags:\n  - sensors\n  - -sa/processed\n---\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\n*   Commonly used\n*   \\[+\\] Precise, efficient, not much processing work necessary\n*   \\[-\\] Expensive, bad readings with certain surfaces, bad for underwater applications\n\n',title:"Untitled Page"},"/sensors/monocular-cameras":{content:'---\ntitle: Monocular cameras\ndate: "2020-07-30"\ntags:\n  - sensors/cameras\n  - -sa/processed\n  - -published\n---\n\n**Source**: [Cometlabs](bibliography/cometlabs.md)    \n\n*   \\+ Simpler hardware implementation\n*   \\+ Smaller and cheapter systems\n*   \\- need complexer algos and software because of lack of direct depth information from a 2D image\n\n## How is the shape of the map generated?\n\n*   Integrating measurements in the chain of frames over time\n    *   Use triangulation method\n    *   As well as camera motion, if camera isn\'t stationary\n*   Depths of points are not oberved directly (s. [monocular depth perception](permanent/10-monocular-depth-perception.md))\n    *   estimated point and camera positions are related to the real positions by a common unknown scale factor (scale availability issue)\n    *   requires camera movement when depth is to be calculated\n\n## Adressing the scale availability issue\n\n*   Stereo cameras\n*   Introduce a real metric scale (via external scale reference: pre-specified object or sth w/ known size that can be recognised during mapping)\n\n',title:"Untitled Page"},"/sensors/position-acquisition":{content:'---\ntitle: Position acquisition (relative vs. absolute)\ndate: "2020-07-30"\ntags:\n  - sensors\n  - -sa/processed\n  - -published\n---\n\n**See also**: [SLAM hardware](sensors/slam-hardware.md)\n\n**Source**: [Cometlabs](cometlabs.md)\n\n![unknown_filename.png](studienarbeit/_resources/Position_acquisition_(relative_vs._absolute).resources/unknown_filename.png)\n\n*   relative (interoceptive sensors)\n    *   [odometry](definitions/odometry.md)\n*   [absolute](sensors/sensors-absolute.md) (exteroceptive sensors)\n    *   can be used alongside relative measurement sensors in order to correct odometry drift\n    *   beacons\n        *   direct measurement instead of integrating, therefore error in position does not grow unbounded\n        *   e.g. laser ranger finders, wifi (collect signal strength across field), GPS (bad for indoors)\n            *   Lidar, Ultra Wide Band (UWB), Wireless Fidelity, etc \\[[Wu](bibliography/wu-2018.md)\\]\n            *   Compared to these, cameras are flexible and low-cost \\[[Wu](bibliography/wu-2018.md)\\] (are also passive sensors) \\[[Cometlabs](bibliography/cometlabs.md)\\]\n\n',title:"Untitled Page"},"/sensors/rgb-d-cameras":{content:'---\ntitle: RGB-D cameras\ndate: "2020-07-30"\ntags:\n  - sensors/cameras\n  - -sa/processed\n---\n\n**Source**: [Cometlabs](bibliography/cometlabs.md)\n\n*   Provide depth information directly\n*   Employed by most of the SLAM systems\n*   Generate 3D images through structured light or time of flight technology\n    *   Structured light\n        *   camera projects a known pattern onto objects\n        *   Perceives deformation of pattern by an infrared camera\n        *   This lets depth and surface information of the objects be calculated\n    *   Time of flight\n        *   ToF of a light signal between camera and objects is measured --\u003e from this, depth is obtained\n*   Structured light sensors are sensitive to illumination -- not applicable in direct sunlight\n\n## Limitations\n\n*   Range data for semi-transparent or highly reflective surfaces are not reliable\n*   Limited effective range\n\n',title:"Untitled Page"},"/sensors/sensor-fusion":{content:"---\ntitle: Sensor fusion\ndate: \"2021-09-29\"\nmathjax: true\ntags:\n  - -sa/processing\n  - sensors\n  - -definitions\n---\n\n**Source**: [Phil's Lab](bibliography/phils-lab-sensor-fusion.md)\n\n## Definition\nCombine multiple sensor readings to form an improved estimate of a desired variable.\n\n## Why sensor fusion?\n* Overcome limitations of individual sensors (noise, uncertainty)\n* Estimate quantities that are not directly measurable  \n	e.g. gyroscope measures angular rates of change\n	but can't directly measure roll and pitch angle\n\n## Fusion in IMU\n* Accelerometer only: good estimate in non-accelerating conditions\n* Gyroscope only: good estimate over short periods of time (due to integration of bias terms)",title:"Untitled Page"},"/sensors/sensors-absolute":{content:'---\ntitle: Sensors (absolute measurements) for measuring distance to landmarks\ndate: "2020-07-30"\ntags:\n  - sensors\n  - -sa/processed\n  - -published\n---\n\nParents: [slam-hardware](sensors/slam-hardware.md)\n\n**Source**: [Cometlabs](bibliography/cometlabs.md)\n\n*   Acoustic\n    *   (Time of Flight) ToF technique\n    *   Surfaces need to have good acoustic reflection\n    *   Lack the ability to use surface properties for localisation\n    *   examples\n        *   Sonar\n        *   Ultrasonic, ultrasound\n*   Laser rangerfinders\n    *   ToF and phase-shift techniques\n    *   Lack the ability to use surface properties for localisation\n    *   e.g. Lidar\n*   [Visual sensors for localisation](sensors/visual-sensors-for-localisation.md)\n\n* * *\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\n*   [Laser scanners](sensors/laser-scanners.md)\n*   Sonar, usually polaroid sonar\n    *   \\[+\\] cheaper, good for underwater (s. dolphins)\n    *   \\[-\\] less precise\n*   [Visual sensors for localisation](sensors/visual-sensors-for-localisation.md)\n\n* * *\n\n**Source**: [Wikipedia Lokalisierung](wikipedia-lokalisierung.md)\n\n*   GPS (only for outdoors)\n*   Innenraumsensorik\n    *   Lidar, Ultra Wide Band (UWB), Wireless Fidelity, etc ([wu-2018](bibliography/wu-2018.md))\n    *   Compared to these, cameras are flexible and low-cost ([wu-2018](bibliography/wu-2018.md)) (are also passive sensors) ([cometlabs](bibliography/cometlabs.md))\n*   Radiobaken\n\n\n',title:"Untitled Page"},"/sensors/slam-hardware":{content:'---\ntitle: SLAM hardware\ndate: "2020-07-27"\ntags:\n  - sensors\n  - -sa/processed\n  - -published\n---\n\n**Parent**: [SLAM index](SLAM/slam_index.md)  \n**See also**: [Position acquisition (relative vs. absolute)](sensors/position-acquisition.md)\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\n1.  Robot parameters to consider\n    *   Ease of use\n    *   [Odometry](definitions/odometry.md) performance:  how well the robot can estimate its own position, just from the rotation of the wheels\n        *   Max errors: 2cm per meter moved, 2deg per 45deg turned\n        *   Bad odometry --\u003e bad estimation of current position --\u003e hard to implement SLAM\n2.  [Range measurement device options](sensors/sensors-absolute.md)\n',title:"Untitled Page"},"/sensors/stereo-cameras":{content:'---\ntitle: Stereo cameras\ndate: "2020-07-30"\ntags:\n  - sensors/cameras\n  - -sa/processed\n---\n\n**Source**: [Cometlabs](cometlabs.md)\n\n*   two cameras separated by a fixed distance (baseline)\n*   observations of the position of the same 3D point in both cameras allows depth to be calculated through triangulation (like humans do)\n*   depth measurement limited by baseline and resolution\n    *   generally, wider baseline --\u003e  better depth estimate (but occupies more physical space)\n\n',title:"Untitled Page"},"/sensors/visual-sensors-for-localisation":{content:'---\ntitle: Visual sensors for localisation\ndate: "2020-08-23"\ntags:\n  - sensors\n  - -sa/processed\n  - -published\n---\n\n**Source**: [Wikipedia Visual odometry](wikipedia-visual-odometry.md)\n\n*   Process of determining robot POSE by analysing the associated camera images\n*   Use sequential camera image to estimate the distance travelled\n\nApplications: robotics, computer vision\n\n* * *\n\n**Source**: [Cometlabs](cometlabs.md)\n\nTypes\n*   [Monocular cameras](sensors/monocular-cameras.md)\n*   [Stereo cameras](sensors/stereo-cameras.md)\n*   [RGB-D cameras](sensors/rgb-d-cameras.md)\n\nProvide rich visual information, but for that, higher computational cost\n\n* * *\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\n*   stereo or triclops sytem to measure distance\n*   \\[+\\] possibly more intuitive (bc humans use vision), more info. is contained in a picture compared to to laser/sonar scans\n*   \\[-\\] computation intensive (traditionally, might be less of a problem nowadays), error prone due to changes in light\n\n',title:"Untitled Page"},"/studienarbeit/40.1-imu-measurement-model":{content:'---\ntitle: 40.1 IMU measurement model\ndate: "2021-05-18"\ntags:\n  - -sa/processed\n  - sensors/IMU\n  - discussion/2021/2021-04\n  - -permanent\n---\n\nParent: [IMU index](imu index.md), [probabilistic models-for-imu](probabilistic-models-for-imu.md)\nBacklinks: [IMU motion model](imu motion model.md), [imu kinematic model using euler integration](imu-kinematic-model-using-euler-integration.md)\n\nAn IMU measures, relative to an inertial frame, acceleration and rotation rate.\n\n*   The measurements are corrupted by bias and noise (often assumed to be white Gaussian noise ![unknown_filename.3.png](./_resources/40.1_imu_measurement_model.resources/unknown_filename.3.png)). [mkok-2017](mkok-2017.md)\n*   Additionally, the acceleration measured is affected by gravity.\n*   Note the [assumptions in modelling the true angular velocity in IMUs](assumptions in modelling the-true-angular-velocity-in-imus.md)\n\nSensor readings in the body frame\nNotation in [Solà 2017](solà-2017.md):\n![unknown_filename.png](./_resources/40.1_IMU_measurement_model.resources/unknown_filename.png)\n\nNotation in [Forster 2017](forster-2017.md):\n![unknown_filename.2.png](./_resources/40.1_IMU_measurement_model.resources/unknown_filename.2.png)\n\nwhere![Image.png](./_resources/40.1_IMU_measurement_model.resources/Image.png) is the instantaneous angular velocity of the IMU (B) relative to the world (W), expressed in (B) coordinates\n\nAnother term for the acceleration is the specific force measured by the IMU. [MKok 2017](mkok-2017.md)\n![unknown_filename.4.png](./_resources/40.1_IMU_measurement_model.resources/unknown_filename.4.png)\n\nTrue acceleration and angular velocity, based on noisy and biased IMU measurements\nNotation in [Solà 2017](solà-2017.md):\n![unknown_filename.1.png](./_resources/40.1_IMU_measurement_model.resources/unknown_filename.1.png)\n\ntrue acceleration = acceleration due to inertial forces acting on B + acceleration due to gravity \\[[Source](http://matthewhampsey.github.io/blog/2020/07/18/mekf)\\]\n\nSee also:\n[Modelling noise and bias for IMU](modelling-noise-and-bias-for-imu.md)\n\n',title:"Untitled Page"},"/studienarbeit/40.1.1-assumptions-in-modelling-the-true-angular-velocity-in-imus":{content:'---\ntitle: 40.1.1 Assumptions in modelling the true angular velocity in IMUs\ndate: "2021-05-18"\ntags:\n  - -sa/processed\n  - sensors/IMU\n  - -permanent\n---\n\nParent: [IMU index](imu index.md), [imu-measurement-model](imu-measurement-model.md)\nSource: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)\n\nFor angular velocity, the term![unknown_filename.png](./_resources/40.1.1_Assumptions_in_modelling_the_true_angular_velocity_in_IMUs.resources/unknown_filename.png) should really be\n![unknown_filename.2.png](./_resources/40.1.1_Assumptions_in_modelling_the_true_angular_velocity_in_IMUs.resources/unknown_filename.2.png)\nwith\n\n*   negligible Earth rotation ![unknown_filename.1.png](./_resources/40.1.1_Assumptions_in_modelling_the_true_angular_velocity_in_IMUs.resources/unknown_filename.1.png) = 0\n*   stationary navigation frame, ![unknown_filename.3.png](./_resources/40.1.1_Assumptions_in_modelling_the_true_angular_velocity_in_IMUs.resources/unknown_filename.3.png) = 0\n\n',title:"Untitled Page"},"/studienarbeit/50.1-why-kalman-filters-":{content:'---\ntitle: 50.1 Why Kalman filters?\ndate: "2020-08-27"\ntags:\n  - -sa/processed\n  - filters/kalman-filter\n  - -permanent\n---\n\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nWe work with 2 sources of data:\n\n*   Sensor measurements\n*   Our own predictions (based on knowledge of system behaviour)\n\nSensors are noisy, don\'t give perfect information\n\n*   Simple solution: to average readings\n*   However, this doesn\'t work when\n    *   the sensor is too noisy\n    *   data collection not possible\n\nThe prediction, however, is also susceptible to noise (the world is noisy, outside/unaccounted for influences)\n\nIn short: "Knowledge is uncertain, and we \\[must\\] alter our beliefs based on the strength of the evidence."\n\nKeep in mind, though, that the Kalman filter math is based on an idealised model of the world (errors in sensor measurement are rarely truly Gaussian)\n\n*   Kalman filter equations assume normal distribution of noise\n*   Performance is suboptimal if this assumption is not true\n\n',title:"Untitled Page"},"/studienarbeit/50.1.1-aim-and-main-principle-of-kalman-filters":{content:'---\ntitle: 50.1.1 Aim and main principle of Kalman filters\ndate: "2020-08-27"\ntags:\n  - -sa/processed\n  - filters/kalman-filter\n  - -permanent\n---\n\n**Source**: [rlabbe-kalman-bayesian-filters-in-python](studienarbeit/rlabbe-kalman-bayesian-filters-in-python.md)\n\n## Aim\nAim of the Kalman/Bayesian filters: to accumulate (or to somehow blend)\n\n*   our noisy and limited knowledge (of system behaviour)\n*   noisy and limited sensor readings\n\nand with these, make the best possible prediction (estimate) of the system state.\n\n## Main principles:\n\n*   use past information to make predictions for the future\n*   [never throw away information](never-throw-away-information.md)\n*   predict/propagation step: calculate prediction based on process model and using previous state data (previous estimate)\n*   update step: calculate the estimates based on prediction and measurement\n\n![g+4zqmq9uTT2wAAAABJRU5ErkJggg==](data:image/png;base64,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)\n\nPrediction step a.k.a. evolution\nOne iteration of prediction + update is called an epoch\n\n',title:"Untitled Page"},"/studienarbeit/50.2.-bayes'-theorem":{content:"---\ntitle: 50.2. Bayes' Theorem\ndate: \"2020-08-31\"\ntags:\n  - -sa/processed\n  - math/statistics/bayesian-statistics\n  - -permanent\n---\n\nParent: [Bayesian Filter Update Step](bayesian-filter-update-step.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\n*   How do we compute the probability of an event given previous information?\n    (s. also [Frequentist vs Bayesian statistics](frequentist-vs-bayesian-statistics.md))\n    \n*   Formula to compute new information into existing information\n\nUsed in the update step of a Bayesian filter\n(valid for both probabilities as well as probability distributions)\n![87c061fe1c7430a5201eef3fa50f9d00eac78810](http://wikimedia.org/api/rest_v1/media/math/render/svg/87c061fe1c7430a5201eef3fa50f9d00eac78810)\n![unknown_filename.1.png](./_resources/50.2._Bayes'_Theorem.resources/unknown_filename.1.png)\n![unknown_filename.png](./_resources/50.2._Bayes'_Theorem.resources/unknown_filename.png)\nwhere || . || expresses normalisation\n\n|     |     |\n| --- | --- |\n| B   | Evidence (sensor measurements z) |\n| p(A) | Prior |\n| p(B\\|A) | Likelihood |\n| p(A\\|B) | Posterior |\n\nIn filtering systems, computing p(x|z) is nearly impossible, but computing p(z|x) is fairly straightforward, which then facilitates the computation of p(x|z) via the Bayes' theorem formula.\n\n",title:"Untitled Page"},"/studienarbeit/50.2.10-discrete-bayesian-filter":{content:'---\ntitle: 50.2.10 Discrete Bayesian filter\ndate: "2020-08-31"\ntags:\n  - -sa/processed\n  - filters/bayesian-filter\n  - -permanent\n---\n\n**Source**: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nThe Kalman filter is a subset of Bayesian filters\n\n*   Predict and update steps like in the g-h filter\n*   Here: error percentages are used to implicitly compute the g and h parameters\n\nSteps\n\n1.  \\[Initialise our belief in the state\\]\n2.  The [predict step](predict-step.md) always degrades our knowledge (belief/prior)\n3.  However, in the [update step](update-step.md), we add another measurement. This, will always improve our knowledge regardless of noise, enabling convergence\n\n![unknown_filename.png](./_resources/50.2.10_Discrete_Bayesian_filter.resources/unknown_filename.png)\n\n[Limitations of the discrete Bayes filter](limitations-of-the-discrete-bayes-filter.md)\n\n',title:"Untitled Page"},"/studienarbeit/50.2.10.1-discrete-bayesian-filter-predict-step":{content:'---\ntitle: 50.2.10.1 Discrete Bayesian Filter Predict Step\ndate: "2020-08-31"\ntags:\n  - -sa/processed\n  - to-do/missing-link\n  - filters/bayesian-filter\n  - -permanent\n---\n\nParent: [Discrete Bayesian filter](discrete-bayesian-filter.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nThe predict step uses the total probability theorem.\n\n*   Computes total probability of multiple possible events\n*   Uses the system model (propagates the states from prev. time step \\[posterior\\] to the next one); prediction\n*   Accounts for the uncertainty (kernel) in the prediction: produces a prior\n    *   Generalise the uncertainty using a kernel (distributes the uncertainty over a range around the prediction)\n    *   Integrate the kernel into the calculations by using convolution \\*\n    *   Convolving the "current probabilistic estimate" with the "probabilistic estimate of how much we think the position has changed" (from system model)\n    *   The prior is a \'degraded\' version of the belief\n*   i.e., produces a prior from the posterior and kernel\n\nTotal probability theorem\n"Total probability of an outcome which can be realised via several distinct events."\nX\\_i^t: X is at position i at time t\n![unknown_filename.1.png](./_resources/50.2.10.1_Discrete_Bayesian_Filter_Predict_Step.resources/unknown_filename.1.png)\nP(xi|xj) - Probability of moving from j to i (kernel)\n![unknown_filename.png](./_resources/50.2.10.1_Discrete_Bayesian_Filter_Predict_Step.resources/unknown_filename.png) - Probability of being at j at time (t-1)\n\nIs actually a convolution (summing up over all j is the same as doing the calculation for all kernel values\n![unknown_filename.2.png](./_resources/50.2.10.1_Discrete_Bayesian_Filter_Predict_Step.resources/unknown_filename.2.png)\n\n',title:"Untitled Page"},"/studienarbeit/50.2.10.2-bayesian-filter-update-step":{content:"---\ntitle: 50.2.10.2 Bayesian Filter Update Step\ndate: \"2020-08-31\"\ntags:\n  - -sa/processed\n  - filters/bayesian-filter\n  - -permanent\n---\n\nParent: [Discrete Bayesian filter](discrete-bayesian-filter.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nThe update step uses [Bayes' Theorem](bayes'-theorem.md)\n\n*   Produces the posterior by using the likelihood and the prior\n*   Also incorporates sensor data (measurements), as the measurements go into the likelihood calculation\n\nUpdate algorithm\n\n1.  Get a measurement, and associated belief about its accuracy\n2.  Compute likelihood from the measurement and the measurement accuracy assumption\n3.  Update the posterior using the likelihood and the prior\n\n",title:"Untitled Page"},"/studienarbeit/50.2.2-measurement-noise-r,-v-landmark":{content:'---\ntitle: 50.2.2 Measurement noise R, V (landmark)\ndate: "2020-07-29"\ntags:\n  - filters/EKF\n  - -sa/processed\n  - -permanent\n---\n\nParent: [Multivariate Kalman filter algorithm](multivariate-kalman-filter-algorithm.md)\n\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\n*   R models the noise in the sensors as a covariance matrix\n*   dim(R) = m x m (m: number of sensors)\n*   Possible complications\n    *   in multisensor systems, the correlation between the sensors might not be clear\n    *   sensor noise might not be pure Gaussian\n\n* * *\n\nSource: \u003chttp://www.linkedin.com/pulse/tuning-extended-kalman-filter-process-noise-training-alex-thompson\u003e\nWays to obtain R\n\n*   Using the variances given in the sensor specifications\n*   Comopare the measurements against a strong ground truth and derive the variance variable by variable\n*   Record the steady state measurements over a long period of time and measure the variance (look at the histogram)\n\n* * *\n\nSource: [Schneider 2013 How to not make the EKF fail](schneider 2013 how to-not-make-the-ekf-fail.md)\n\nIncreasing R\n\n*   filter is more robust against measurement errors/noise\n*   if too large, the measurements have no effect on the filter\n*   Kalman gain vanishes, filter becomes open loop\n*   if plant is unstable or in case of plant-model mismatch -\u003e possible offset or divergence of the filter states\n\n* * *\n\nSource: [SLAM for Dummies](slam-for-dummies.md)\n\nUsed in  [Step 2: Re-observation](studienarbeit/ekf-2-reobservation.md)\n\n*   Noise of range measurement device\n*   Assumed to have gaussian noise proportional to the range and bearing\n\nVRV^T, where V = I\\_2 (identity matrix)\nR = diag(rc, rd)\n![unknown_filename.png](./_resources/50.2.2_Measurement_noise_R,_V_(landmark).resources/unknown_filename.png)\n\nrc and rd represent the accuracy of the measurement device\ne.g.\n\n*   range 1cm variance --\u003e rc = 0.01\n    *   A good starting value for rc is 0.01\\*range value (i.e. 1% error in range)\n*   bearing error 1deg --\u003e bd = 1\n    *   A good starting value for bd is 1 (i.e. 1 degree error in measurements)\n    *   Error shouldn\'t be proportional to size of angle\n\n',title:"Untitled Page"},"/studienarbeit/50.2.20-1d-kalman-filters":{content:'---\ntitle: 50.2.20 1D Kalman filters\ndate: "2020-08-31"\nexternal_url: "http://nbviewer.jupyter.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/04-One-Dimensional-Kalman-Filters.ipynb"\ntags:\n  - filters/kalman-filter\n  - -permanent\n  - -sa/to-be-processed\n---\n\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\n[Deriving Kalman filter from Discrete Bayes using Gaussians](deriving kalman filter-from-discrete-bayes-using-gaussians.md)\n[1D Kalman filter algorithm](1d-kalman-filter-algorithm.md)\n[Kalman gain using Gaussians](kalman-gain-using-gaussians.md)\n[Variance of the 1D Kalman filter](variance-of-the-1d-kalman-filter.md)\n[Factors affecting Kalman filter performance](factors-affecting-kalman-filter-performance.md)\n\n',title:"Untitled Page"},"/studienarbeit/50.2.20.1-1d-kalman-filter-algorithm":{content:'---\ntitle: 50.2.20.1 1D Kalman filter algorithm\ndate: "2020-08-31"\ntags:\n  - filters/kalman-filter\n  - -permanent\n  - -sa/to-be-processed\n---\n\nParent: [1D Kalman filters](1d-kalman-filters.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nInitialisation:\n\n1.  Initialise the state of the filter\n2.  Initialise the belief in the state\n\nPredict step:\n\n*   Gaussian addition\n*   prior = predict(x, process\\_model)\n*   Incorporate process variance in order to prevent [smug filtering](smug-filtering.md)\n\nUpdate step:\n\n*   Gaussian multiplication\n*   likelihood = gaussian(z, sensor\\_var)\n    x = update(prior, likelihood)\n    \n\nThe output of both steps is a Gaussian probability distribution N(mean, var)\n\n![unknown_filename.png](./_resources/50.2.20.1_1D_Kalman_filter_algorithm.resources/unknown_filename.png)\n\n[Deriving the Kalman gain using Gaussians](deriving-the-kalman-gain-using-gaussians.md)\n\n',title:"Untitled Page"},"/studienarbeit/50.2.3-kalman-filter-initial-estimates":{content:'---\ntitle: 50.2.3 Kalman filter initial estimates\ndate: "2021-08-18"\ntags:\n  - filters/kalman-filter\n  - -permanent\n  - -sa/processed\n---\n\n**Source**: [Schneider 2013 How to not make the EKF fail](schneider 2013 how to-not-make-the-ekf-fail.md)\n\nInitial state estimate x0, P0\n\n*   Filter generally not badly affected by wrong initial state x0, but convergence will be slow if we are way off\n\n*   If P0 too small whereas x0 is way off\n    *   the gain K becomes small\n    *   filter relies on the model more than on the measurements\n\nThus: important to have a consistent pair x0, P0\n\nPossible to use\n![unknown_filename.png](./_resources/50.2.3_Kalman_filter_initial_estimates.resources/unknown_filename.png)\nusing ![unknown_filename.1.png](./_resources/50.2.3_Kalman_filter_initial_estimates.resources/unknown_filename.1.png)\n\n',title:"Untitled Page"},"/studienarbeit/50.2.30-multivariate-kalman-filter-algorithm":{content:"---\ntitle: 50.2.30 Multivariate Kalman filter algorithm\ndate: \"2020-09-01\"\ntags:\n  - filters/kalman-filter\n  - -permanent\n  - -sa/to-be-processed\n---\n\n**Parent**: [Multivariate Kalman filters](multivariate-kalman-filters.md)\n**Source**: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\n**Initialisation**\n\n1.  Initialise filter state\n2.  Initialise belief in the state\n\n**Predict**\n\n1.  Propagate state to the next time step using the system model \\[**prediction**\\]\n2.  Adjust belief to take into account the prediction uncertainty \\[**prior**\\]\n\n![unknown_filename.1.png](./_resources/50.2.30_Multivariate_Kalman_filter_algorithm.resources/unknown_filename.1.png)\n\n**Update**\n\n1.  Obtain measurement and associated belief about its accuracy\n2.  Calculate residual (prior - measurement)\n3.  Calculate scaling factor/Kalman gain\n4.  Set estimated state to be on the residual line based on the scaling factor\n5.  Update the belief in the state based on measurement certainty\n\n![unknown_filename.png](./_resources/50.2.30_Multivariate_Kalman_filter_algorithm.resources/unknown_filename.png)\n\nDesigning the measurement function\n\n*   The measurement function converts a state into a measurement (e.g. for unit conversion)\n*   We work in **measurement space** instead of state space because **most measurements aren't invertible**\n*   e.g. the state contains the hidden variable 'velocity', but it's not possible to convert a 'position' sensor measurement to a velocity.\n\nDesigning the measurement\nThe measurement noise matrix\n\n",title:"Untitled Page"},"/studienarbeit/50.2.40-kalman-filter-performance-metric":{content:'---\ntitle: 50.2.40 Kalman filter performance metric\ndate: "2021-08-18"\ntags:\n  - filters/kalman-filter\n  - -permanent\n  - -sa/processed\n---\n\n**Source**: [Schneider 2013 How to not make the EKF fail](schneider 2013 how to-not-make-the-ekf-fail.md)\n\n![unknown_filename.png](./_resources/50.2.40_Kalman_filter_performance_metric.resources/unknown_filename.png)\nk: time / step\nj: how many EKF runs? in tutorial: EKF was ran 1000 times (non-deterministic system due to noise)\n\n',title:"Untitled Page"},"/studienarbeit/50.3-modelling-imu-in-kf":{content:'---\ntitle: 50.3 IMU motion model in a Kalman filter\ndate: "2021-05-18"\ntags:\n  - -sa/processed\n  - sensors/IMU\n  - -permanent\n  - -published\n---\n\n**Parent**: [IMU index](imu-index.md)  \n**Source**: [Solà 2017 Quaternion kinematics for ESKF](solà-2017-quaternion-kinematics-for-eskf.md)\n\nWhich states do we use for the motion model?  \n[Choice of states for the IMU motion/kinematics model](studienarbeit/50.3.1-states-for-imu-motion-model.md)\n\nHow do we model the IMU motion?  \n[Choice of model for the IMU motion model](studienarbeit/50.3.2-imu-model-for-kf.md)\n\nThe kinematics (true state) can be partitioned into a nominal part and an error part, s. [variables in ESKF](studienarbeit/50.5.1.1-states-of-the-eskf-for-estimating-imu-pose.md).\nThe corresponding [nominal state dynamics and error state dynamics](nominal state-dynamics-and-error-state-dynamics.md) are given.\n\n**See also**:\n[IMU measurement model](studienarbeit/40.1-imu-measurement-model.md)\n\n',title:"Untitled Page"},"/studienarbeit/50.3.1-states-for-imu-motion-model":{content:'---\ntitle: 50.3.1 Choice of states for the IMU motion/kinematics model\ndate: "2021-05-18"\ntags:\n  - -sa/processed\n  - sensors/IMU\n  - -permanent\n  - -published\n---\n\n**Parent**: [IMU index](imu-index.md)  \n**See also**: [Choice of model for the IMU motion model](studienarbeit/50.3.2-imu-model-for-kf.md)\n\nAccording to [MKok 2017](mkok-2017.md), we can either\n\n1.  Use the full state vector\n    ![unknown_filename.1.png](./_resources/50.3.1_Choice_of_states_for_the_IMU_motion_kinematics_model.resources/unknown_filename.1.png)\n    *   \\[+\\] knowledge about sensor motion is included in model\n    *   \\[-\\] large state vector\n2.  Or the partial state vector, where the inputs are the inertial measurements from the IMU\n    ![unknown_filename.png](./_resources/50.3.1_Choice_of_states_for_the_IMU_motion_kinematics_model.resources/unknown_filename.png)\n    *   \\[+\\] process noise intuitively represents IMU noise.\n        This is useful when we have no knowledge about the motion model.\n        \n    *   \\[+\\] changes to acceleration and angular velocity will have a slightly faster effect (prediction not dependent on acc/ang.vel model)\n\nThe literature for EKF sensor fusion using IMUs mostly implement the latter option with IMU measurements as an input. [Solà 2017](studienarbeit/solà-2017-quaternion-kinematics-for-eskf.md) uses the following states, inputs and perturbations (white Gaussian noise)\n![unknown_filename.2.png](./_resources/50.3.1_Choice_of_states_for_the_IMU_motion_kinematics_model.resources/unknown_filename.2.png)\nin the [IMU kinematic equations](studienarbeit/50.3.2-imu-model-for-kf.md).\n\nExtending the equations to the error state dynamics, the error states are \n![unknown_filename.3.png](./_resources/50.3.1_Choice_of_states_for_the_IMU_motion_kinematics_model.resources/unknown_filename.3.png)\n\n',title:"Untitled Page"},"/studienarbeit/50.3.2-imu-model-for-kf":{content:'---\ntitle: 50.3.2 Choice of model for the KF using IMU readings\ndate: "2021-05-18"\ntags:\n  - -sa/processed\n  - sensors/IMU\n  - -permanent\n  - -published\n---\n\nParent: [IMU index](imu-index.md), [50.3-modelling-imu-in-kf](studienarbeit/50.3-modelling-imu-in-kf.md)\n\nAccording to [MKok 2017](mkok-2017.md), here are some models that assume either a constant acceleration or a constant angular velocity:\n\n*   Constant acceleration model\n    ![unknown_filename.png](./_resources/50.3.2_Choice_of_model_for_the_KF_using_IMU_readings.resources/unknown_filename.png)\n    \n*   Constant angular velocity model\n    ![unknown_filename.1.png](./_resources/50.3.2_Choice_of_model_for_the_KF_using_IMU_readings.resources/unknown_filename.1.png)\n    (Notation: angular velocity of the body with respect to world (n), expressed in body CS)\n    \n\nIf motion is unknown, there is also the option of modelling the states using random walk equations.\n\nA common model in the literature for EKF sensor fusion using IMUs use the IMU readings as inputs\n\n*   This results in a partial state vector for the pose, s. [50.3.1-states-for-imu-motion-model](studienarbeit/50.3.1-states-for-imu-motion-model.md)\n*   Variables such as biases can be appended to the states to be estimated and are modelled as a random walk\n*   If gravity is to be estimated, it also appears as a state in the estimator\n\nA such model is defined below in [Solà 2017](studienarbeit/solà-2017-quaternion-kinematics-for-eskf.md)\n\n![Image.png](./_resources/50.3.2_Choice_of_model_for_the_KF_using_IMU_readings.resources/Image.png)\n\nNote: [The initial gravity vector/orientation for the IMU ESKF](the initial gravity-vector_orientation-for-the-imu-eskf.md)\n\n',title:"Untitled Page"},"/studienarbeit/50.4.1-additive-quaternion-filtering":{content:'---\ntitle: 50.4.1 Additive quaternion filtering\ndate: "2021-08-17"\ntags:\n  - -sa/processed\n  - filters/kalman-filter\n  - math/quaternions\n  - -permanent\n---\n\n**Parents**: [Quaternion index](rotations/quaternion-index.md), [which orientation parametrisation to-choose?](rotations/20.4-which-orientation-parametrisation.md)\n\n**Source**: [Markley 2014](bibliography/markley-2014.md)\n\n## Additive quaternion filtering\nAdditive quaternion error\n![unknown_filename.2.png](./_resources/50.4.1_Additive_quaternion_filtering.resources/unknown_filename.2.png)\n\n### Methods of enforcing the normalisation\n\n1.  Renormalise the estimate by brute force\n    ![unknown_filename.1.png](./_resources/50.4.1_Additive_quaternion_filtering.resources/unknown_filename.1.png)\n    \n2.  Modify KF update equations to enforce a norm constraint using a Lagrange multiplier\n\n\\[1\\] and \\[2\\] yield biased estimates of the quaternion\n\n### Methods that don\'t enforce normalisation\n\n3.  Define the rotation matrix to be\n    ![unknown_filename.png](./_resources/50.4.1_Additive_quaternion_filtering.resources/unknown_filename.png)\n    *   guarantees orthogonality\n    *   introduces unobservable DOF: the quaternion norm\n4.  Use the above equation without the ||q||-2 factor --\u003e no orthogonality\n\n',title:"Untitled Page"},"/studienarbeit/50.4.2-multiplicative-quaternion-filtering-mekf":{content:'---\ntitle: 50.4.2 Multiplicative quaternion filtering (MEKF)\ndate: "2021-08-17"\ntags:\n  - -sa/processed\n  - -permanent\n  - filters/ESKF\n  - -published\n---\n\n**See also**: [Which orientation parametrisation to choose?](rotations/20.4-which-orientation-parametrisation.md)  \n**Source**: [Markley 2014](bibliography/markley-2014.md)\n\nMain idea is to use\n\n*   the quaternion as a global rotation representation\n*   a three component state vector as the local representation of rotation errors\n		$$\n		\\begin{aligned}\n		\\mathbf{q}_\\text{tr} \u0026= \\delta\\mathbf{q} (\\delta\\mathbf{\\theta}) \\otimes \\mathbf{\\hat{q}}\\\\\n		\\mathbf{R}(\\mathbf{q}_\\text{tr})\n			\u0026= \\mathbf{R} (\\delta\\mathbf{\\theta})\n			\\mathbf{R} (\\mathbf{\\hat{q}})\n		\\end{aligned}$$\n    \n*   each term $(\\mathbf{q}_\\text{tr},~\\delta\\mathbf{q},~ \\mathbf{\\hat{q}})$ is a normalised unit quaternion\n*   Any of the [rotation error representations](rotations/rotation-error-representation.md) can be used to calculate delta\\_theta, which is part of the error state of the MEKF.\n\n',title:"Untitled Page"},"/studienarbeit/50.5-error-state-kalman-filter":{content:'---\ntitle: 50.5 Error-State Kalman Filter\ndate: "2021-05-14"\nexternal_url: "http://notanymike.github.io/Error-State-Extended-Kalman-Filter/"\ntags:\n  - to-do/to-clarify\n  - filters/EKF\n  - -sa/processed\n  - -permanent\n---\n\nSource: [Markley](bibliography/markley-2014.md)\n\n*   An EKF propagates the expectation and covariance of the state\n*   The MEKF propagates the expectation and the covariance of the error state\n\n* * *\n\n**Source**: [Whampsey MEKF](whampsey-mekf.md)\n\n*   Previously: orientation is represented by one state\n*   Now: orientation is split up into ![unknown_filename.1.png](./_resources/50.5_Error-State_Kalman_Filter.resources/unknown_filename.1.png)\n    *   a large signal q\\_nom (nominal orientation) and\n    *   a small signal (perturbation angle alpha) -- parametrises an error quaternion ![unknown_filename.png](./_resources/50.5_Error-State_Kalman_Filter.resources/unknown_filename.png)\n*   This reformulates the error in terms of the group operation and so maintains the rotation invariance\n    (rotation preserves the origin, length, angle between two vectors, orientation, etc.)\n    \n\n* * *\n\nSource: [Solà 2017 Quaternion kinematics for ESKF](solà-2017-quaternion-kinematics-for-eskf.md)\n\nTrue state = nominal state (large signal) + error state (small signal)\n\nSome advantages of ESKF\n\n*   Orientation error-state is minimal (same number of parameters as the degrees of freedom)\n    *   avoids over-parametrisation, risk of singularity of covariance matrices\n    *   lower computational complexity due to the reduced dimension\n*   Error-state system always operates close to the origin\n    *   far from possible parameter singularities, gimbal lock issues etc. (s. [Which orientation parametrisation to choose?](rotations/20.4-which-orientation-parametrisation.md))\n    *   guarantees that the linearisation validity holds at all times\n    *   "One interesting common property of the errors is that they have less complex behaviour than the state itself" \\[[Source](http://notanymike.github.io/Error-State-Extended-Kalman-Filter/)\\] — pose behaviour is highly nonlinear, but the error has a much more linear behaviour\n*   Error-state is always small\n    *   all second order products are negligible\n    *   easy and fast computations of Jacobians\n*   Error dynamics are slow\n    *   large signal dynamics are computed in the nominal state\n    *   perform update/corrections at a slower rate compared to predictions [ ] why is this an advantage?\n*   in MEKF: the covariance of the rotation error angles has a transparent physical interpretation  [Markley](bibliography/markley-2014.md)\n\nStages\n\n1.  [Prediction of nominal states](prediction-of-nominal-states.md)\n    *   uses high frequency IMU data\n    *   does not consider noise, nor other model inaccuracies, hence it accumulate errors\n2.  [Prediction of error states](prediction-of-error-states.md) parallel to above\n    *   estimates the above errors, using error state evolution (linear dynamic system, linearised from nominal dynamics)\n    *   predicts a Gaussian estimate of the error state\n    *   covariance propagation\n3.  [ESKF Correction/update stage](eskf-correction_update-stage.md)\n    *   arrival of non-IMU measurements renders the errors observable, s. [Observation of the error state](observation-of-the-error-state.md)\n    *   calculates a posterior Gaussian estimate of the error state\n4.  Apply correction\n    *   add the mean of the error state to the nominal state\n    *   [Calculation of K and P in ESKF update](calculation of k-and-p-in-eskf-update.md)\n    *   [ESKF reset](eskf-reset.md)\n\n',title:"Untitled Page"},"/studienarbeit/50.5.1-imu-nominal-state-and-error-state-kinematics":{content:'---\ntitle: 50.5.1 IMU nominal-state and error-state kinematics\ndate: "2021-05-18"\ntags:\n  - -sa/processed\n  - sensors/IMU\n  - -permanent\n  - discussion/2021/2021-05\n---\n\nParents: [IMU index](imu index.md), [50.3 error-state-kalman-filter](50.3-error-state-kalman-filter.md)\n\nNote on discretisation [Solà 2017](solà-2017.md):\n\n*   Convert the differential equations to difference equations (use integration)\n*   Integration methods may vary\n    *   Closed form solutions\n    *   Numerical integration\n*   Integration is done for:\n    *   The nominal state\n    *   The error state (deterministic part): error state dynamics and control\n    *   The error state (stochastic part): noise and perturbations\n\n|     |     |     |\n| --- | --- | --- |\n|     | Nominal state | Error state |\n|     | Model without noise and perturbations |     |\n| Continuous | ![unknown_filename.png](./_resources/50.5.1_IMU_nominal-state_and_error-state_kinematics.resources/unknown_filename.png) | ![unknown_filename.1.png](./_resources/50.5.1_IMU_nominal-state_and_error-state_kinematics.resources/unknown_filename.1.png) |\n| Discrete | ![unknown_filename.2.png](./_resources/50.5.1_imu_nominal-state_and_error-state_kinematics.resources/unknown_filename.2.png) | ![unknown_filename.3.png](./_resources/50.5.1_imu_nominal-state_and_error-state_kinematics.resources/unknown_filename.3.png)\u003cbr\u003e\u003cbr\u003esummary:\u003cbr\u003e![unknown_filename.4.png](./_resources/50.5.1_imu_nominal-state_and_error-state_kinematics.resources/unknown_filename.4.png)\u003cbr\u003ewith the jacobians defined in [imu eskf-prediction-equations](imu-eskf-prediction-equations.md) |\n\n',title:"Untitled Page"},"/studienarbeit/50.5.1.1-states-of-the-eskf-for-estimating-imu-pose":{content:'---\ntitle: 50.5.1.1 States of the ESKF for estimating IMU pose\ndate: "2021-05-18"\ntags:\n  - -sa/processed\n  - sensors/IMU\n  - math/quaternions\n  - -permanent\n  - filters/ESKF\n  - -published\n---\n\n**Parent**: [IMU index](imu-index.md)\n\n**Source**: [Solà 2017 Quaternion kinematics for ESKF](solà-2017-quaternion-kinematics-for-eskf.md)\n\n## Full state\nVector with 19 elements\n![unknown_filename.1.png](./_resources/50.5.1.1_States_of_the_ESKF_for_estimating_IMU_pose.resources/unknown_filename.1.png)\n\nThe corresponding kinematics equations/motion model is given in [IMU kinematic equations/motion model](studienarbeit/50.3-modelling-imu-in-kf.md).\n\n### Notes\n1.  The angular error in 3D space is given by the notation $\\delta\\mathbf{\\theta}$.\n    \n2.  (s. [rotation-error-representation](rotations/rotation-error-representation.md))\n3.  The angular error $\\delta\\mathbf{\\theta}$ is defined locally w.r.t. the nominal orientation (classical approach used in most IMU-integration works).  \n    A more optimal approach may be to use a globally-defined angular error.\n    \n    A global definition of $\\delta\\mathbf{\\theta}$ would lead to a composition on the left hand side (Hamiltonian convention)!\n    ![unknown_filename.6.png](./_resources/50.5.1.1_States_of_the_ESKF_for_estimating_IMU_pose.resources/unknown_filename.6.png)\n    \n4.  The rotation estimate is not defined as an expectation [[markley-2014](bibliography/markley-2014.md)]\n\n## Inputs\nIMU inputs, 6 element vector\n![unknown_filename.png](./_resources/50.5.1.1_States_of_the_ESKF_for_estimating_IMU_pose.resources/unknown_filename.png)\n\n### Notes\n* The angular rate $\\mathbf{\\omega}$ is definned locally w.r.t. to the nominal quaternion\n* Enables direct incorporation of the gyrometer measurements $\\mathbf{\\omega}_m$ (which are in the body frame)\n\n',title:"Untitled Page"},"/studienarbeit/50.5.1.2-the-initial-gravity-vector-orientation-for-the-imu-eskf":{content:'---\ntitle: 50.5.1.2 The initial gravity vector/orientation for the IMU ESKF\ndate: "2021-05-18"\ntags:\n  - -sa/processed\n  - sensors/IMU\n  - -permanent\n---\n\nParent: [IMU index](imu index.md), [choice of model for the imu motion model](choice of model-for-the-imu-motion-model.md)\n\nNotes on the initial gravity vector/orientation for the IMU ESKF [Solà 2017](solà-2017.md)\n\n*   For simplicity, it is assumed that ![unknown_filename.1.png](./_resources/50.5.1.2_The_initial_gravity_vector_orientation_for_the_IMU_ESKF.resources/unknown_filename.1.png)\n*   The gravity vector g is estimated in terms of frame q0\n*   This puts the initial uncertainty on the gravity direction, rather than on the initial orientation.\n*   Doing this improves linearity, because now the equation\n    ![unknown_filename.png](./_resources/50.5.1.2_The_initial_gravity_vector_orientation_for_the_IMU_ESKF.resources/unknown_filename.png)\n    is linear in g and the initiial rotation R0 has no uncertainty\n    \n*   Optional: once g has been initialised, the real horizontal plane is known. If we wish, we could reorient the entire state to this horizontal plane\n\n',title:"Untitled Page"},"/studienarbeit/50.6-eskf-prediction-equations":{content:'---\ntitle: 50.6 ESKF prediction equations\ndate: "2021-05-18"\ntags:\n  - filters/EKF\n  - -sa/processed\n  - sensors/IMU\n  - -permanent\n---\n\nParents: [IMU index](imu index.md), [50.3 error-state-kalman-filter](50.3-error-state-kalman-filter.md)\nSource: [Solà 2017 Quaternion kinematics for ESKF](solà-2017-quaternion-kinematics-for-eskf.md)\n\nError state system equation becomes:\n![Image.png](./_resources/50.6_ESKF_prediction_equations.resources/Image.png)\nwhere ![unknown_filename.2.png](./_resources/50.6_ESKF_prediction_equations.resources/unknown_filename.2.png)\n(s. [IMU nominal-state and error-state kinematics](imu-nominal-state-and-error-state-kinematics.md) for an overview of the nonlinear kinematics equations)\n\nState propagation (without considering noise) — produces a state estimate (a priori)\n![unknown_filename.png](./_resources/50.6_ESKF_prediction_equations.resources/unknown_filename.png)\nNote: this always returns zero as the mean of the error ![unknown_filename.6.png](./_resources/50.6_ESKF_prediction_equations.resources/unknown_filename.6.png) initialises to zero!\n\nCovariance propagation (considers noise); a priori estimate\n![unknown_filename.1.png](./_resources/50.6_ESKF_prediction_equations.resources/unknown_filename.1.png)\n\nwith the Jacobians\n![unknown_filename.3.png](./_resources/50.6_ESKF_prediction_equations.resources/unknown_filename.3.png)\n(transition matrix approximated using first order Euler, more precise methods are available)\n\n![unknown_filename.4.png](./_resources/50.6_ESKF_prediction_equations.resources/unknown_filename.4.png)\n![unknown_filename.5.png](./_resources/50.6_ESKF_prediction_equations.resources/unknown_filename.5.png)\n\nwith the covariance matrices of the random impulses![unknown_filename.7.png](./_resources/50.6_ESKF_prediction_equations.resources/unknown_filename.7.png)\n![unknown_filename.8.png](./_resources/50.6_ESKF_prediction_equations.resources/unknown_filename.8.png)\nobtain the sigma values from the IMU datasheet or experimentally.\n\n',title:"Untitled Page"},"/studienarbeit/50.7-eskf-update-fusing-imu-with-complementary-sensory-data":{content:"---\ntitle: 50.7 ESKF update / Fusing IMU with complementary sensory data\ndate: \"2021-05-18\"\ntags:\n  - to-do/to-clarify\n  - -sa/processed\n  - sensors/IMU\n  - -permanent\n---\n\n**Parent**: [IMU index](imu index.md), [50.5-error-state-kalman-filter](studienarbeit/50.5-error-state-kalman-filter.md)  \n**Source**: [Solà 2017 Quaternion kinematics for ESKF](solà-2017-quaternion-kinematics-for-eskf.md)\n\n*   In the ESKF, the arrival of non-IMU sensor data triggers a correction stage.\n*   This correction makes the IMU biases observable [ ] , allows correct estimation of the biases\n\nThe correction stage is three-fold:\n\n1.  [observe the error state](observe-the-error-state.md) by way of filter correction\n2.  'add' the observed errors to the nominal state to get the supposed 'true' state according to the composition rules in [variables in ESKF using IMUs](studienarbeit/50.5.1.1-states-of-the-eskf-for-estimating-imu-pose.md)\n3.  [reset](studienarbeit/50.7.3-eskf-reset.md) the error state\n\n* * *\n\nSource: [Markley 2014](bibliography/markley-2014.md)\n\nWhat if several measurements come in without IMU / propagation in between (i.e. without a reset in between)?\n\n*   to avoid recalculating the nonlinear function h(x\\_true), the expectation of the measurement can be computed\n    ![unknown_filename.1.png](./_resources/50.7_ESKF_update___Fusing_IMU_with_complementary_sensory_data.resources/unknown_filename.1.png)\n    \n\nwhich makes the update equation\n![unknown_filename.png](./_resources/50.7_ESKF_update___Fusing_IMU_with_complementary_sensory_data.resources/unknown_filename.png)\n\nResidual becomes y\\_k - delta\\_theta\\_k, s. [50.5.1 Observation of the error state (filter correction)](50.5.1 observation of-the-error-state-(filter-correction).md)\n\nBut if a reset is done after each measurement update, the equation above simplifies to \n![unknown_filename.2.png](./_resources/50.7_ESKF_update___Fusing_IMU_with_complementary_sensory_data.resources/unknown_filename.2.png)\n\n",title:"Untitled Page"},"/studienarbeit/50.7.1-observation-of-the-error-state-filter-correction":{content:"---\ntitle: 50.7.1 Observation of the error state (filter correction)\ndate: \"2021-05-18\"\ntags:\n  - -sounding-board\n  - -sa/processed\n  - -permanent\n  - filters/ESKF\n  - discussion/2021/2021-08\n---\n\nParents: [50.3 Error-State Kalman Filter](50.3 error-state kalman filter.md), [50.5 eskf update / fusing imu with complementary sensory data](50.5 eskf update _ fusing-imu-with-complementary-sensory-data.md)\n\n**Source**: [Solà 2017 Quaternion kinematics for ESKF](solà-2017-quaternion-kinematics-for-eskf.md)\n\nGiven is a non-IMU sensor with the measurement function \\[[Solà](solà.md), [markley](bibliography/markley-2014.md)\\]\n![unknown_filename.png](./_resources/50.7.1_Observation_of_the_error_state_(filter_correction).resources/unknown_filename.png)\n\nwhere x\\_t is the true state and v is a white Gaussian noise\n![unknown_filename.1.png](./_resources/50.7.1_Observation_of_the_error_state_(filter_correction).resources/unknown_filename.1.png)\n\n---\n\n**Source**: [markley-2014](bibliography/markley-2014.md)\n\nIf the measurements are given in quaternion form:\n\n*   we can directly calculate the orientation error between measured orientation and estimated orientation\n*   this becomes our 'measured' angular error ![unknown_filename.3.png](./_resources/50.7.1_Observation_of_the_error_state_(filter_correction).resources/unknown_filename.3.png) which can be used to calculate the residual term\n\n![unknown_filename.2.png](./_resources/50.7.1_Observation_of_the_error_state_(filter_correction).resources/unknown_filename.2.png)\n\\[JPL convention\\] q\\_meas x (q\\_est).conj = ang\\_error\n\nNote: the measurement update should agree closely with the measurement residual calculation. therefore the measurement model should use the same parametrisation for delta\\_q as is used in the reset, s. [Rotation error representation](rotations/rotation-error-representation.md)\n\nUsing [Gibbs / Rodrigues parameter representation for rotations](rotations/gibbs-rodrigues-parameter.md):  \n![unknown_filename.4.png](./_resources/50.7.1_Observation_of_the_error_state_(filter_correction).resources/unknown_filename.4.png)\nAdvantage: the observation model is insensitive to the sign ambiguity in the measured quaternion\n![unknown_filename.5.png](./_resources/50.7.1_Observation_of_the_error_state_(filter_correction).resources/unknown_filename.5.png)\n\nResidual becomes $y_k - \\delta\\theta_k$\n\n",title:"Untitled Page"},"/studienarbeit/50.7.1.1-h-jacobian-matrix-in-the-eskf-filter-correction":{content:'---\ntitle: 50.7.1.1 H Jacobian matrix in the ESKF filter correction\ndate: "2021-05-18"\ntags:\n  - -sounding-board\n  - -sa/processed\n  - -permanent\n  - filters/ESKF\n---\n\nParent: [Filter correction](filter-correction.md), [eskf-update](eskf-update.md)\nSource: [Solà 2017 Quaternion kinematics for ESKF](solà-2017-quaternion-kinematics-for-eskf.md)\n\nEvaluation of the H Jacobian\n\n*   In the prediction stage, the filter estimates the error state![unknown_filename.png](./_resources/50.7.1.1_H_Jacobian_matrix_in_the_ESKF_filter_correction.resources/unknown_filename.png).\n*   Therefore, the Jacobian H needs to be defined w.r.t. the error state ![unknown_filename.png](./_resources/50.7.1.1_H_Jacobian_matrix_in_the_ESKF_filter_correction.resources/unknown_filename.png), and evaluated at the true state estimate ![unknown_filename.3.png](./_resources/50.7.1.1_H_Jacobian_matrix_in_the_ESKF_filter_correction.resources/unknown_filename.3.png)\n*   However, as the error state mean is zero (not yet observed), the true state is approximated to the nominal state ![unknown_filename.2.png](./_resources/50.7.1.1_H_Jacobian_matrix_in_the_ESKF_filter_correction.resources/unknown_filename.2.png)\n*   Thus we can use the nominal state as the evaluation point\n    ![unknown_filename.4.png](./_resources/50.7.1.1_H_Jacobian_matrix_in_the_ESKF_filter_correction.resources/unknown_filename.4.png)![unknown_filename.1.png](./_resources/50.7.1.1_H_Jacobian_matrix_in_the_ESKF_filter_correction.resources/unknown_filename.1.png)\n    \n\nThe first Jacobian\n![unknown_filename.5.png](./_resources/50.7.1.1_H_Jacobian_matrix_in_the_ESKF_filter_correction.resources/unknown_filename.5.png)\n\n*   Depends on the sensor\'s particular measurement function\n\nThe second Jacobian\n![unknown_filename.6.png](./_resources/50.7.1.1_H_Jacobian_matrix_in_the_ESKF_filter_correction.resources/unknown_filename.6.png)\nwith\n![unknown_filename.7.png](./_resources/50.7.1.1_H_Jacobian_matrix_in_the_ESKF_filter_correction.resources/unknown_filename.7.png)\n\n* * *\n\nSource: [Markley 2014](bibliography/markley-2014.md)\nMeasurement sensitivity matrix (Jacobian w.r.t. error states)\n![unknown_filename.9.png](./_resources/50.7.1.1_H_Jacobian_matrix_in_the_ESKF_filter_correction.resources/unknown_filename.9.png)\n\nExpressing the error quaternion and true quaternion states\n(making use of the fact that all the error representations are equivalent to first order in [rotation error representation](rotations/rotation-error-representation.md))\n![unknown_filename.8.png](./_resources/50.7.1.1_H_Jacobian_matrix_in_the_ESKF_filter_correction.resources/unknown_filename.8.png)\n![unknown_filename.11.png](./_resources/50.7.1.1_H_Jacobian_matrix_in_the_ESKF_filter_correction.resources/unknown_filename.11.png)\n\n![unknown_filename.10.png](./_resources/50.7.1.1_H_Jacobian_matrix_in_the_ESKF_filter_correction.resources/unknown_filename.10.png)\n\n',title:"Untitled Page"},"/studienarbeit/50.7.2-calculation-of-k-and-p-in-eskf-update":{content:'---\ntitle: 50.7.2 Calculation of K and P in ESKF update\ndate: "2021-08-17"\ntags: \n- -permanent\n- filters/ESKF\n- -sa/processed\n---\n\nParent: [50.3 Error-State Kalman Filter](50.3-error-state-kalman-filter.md), [eskf-update](eskf-update.md)\nSee also:[Evaluation of the H Jacobian](evaluation-of-the-h-jacobian.md) \n\nSource: [Solà 2017 Quaternion kinematics for ESKF](solà-2017-quaternion-kinematics-for-eskf.md)\n\nThe filter correction equations are\n![unknown_filename.6.png](./_resources/50.7.2_Calculation_of_K_and_P_in_ESKF_update.resources/unknown_filename.6.png)\n\n(yields a posteriori estimates)\n\nNotes:\n\n*   Here, the simplest form of the covariance update is used. This has poor numerical stability, however (no guarantee of symmetricity or positive definiteness)\n*   More stable forms are e.g.\n    *   ![unknown_filename.9.png](./_resources/50.7.2_Calculation_of_K_and_P_in_ESKF_update.resources/unknown_filename.9.png)\n    *   Joseph form (symmetric and positive)\n        ![unknown_filename.5.png](./_resources/50.7.2_Calculation_of_K_and_P_in_ESKF_update.resources/unknown_filename.5.png)\n        \n\nError correction?\n\n*   Timestamps: error correction is applied to the old state \\[[Source](http://notanymike.github.io/Error-State-Extended-Kalman-Filter/)\\]\n    Predict\n    ![unknown_filename.1.png](./_resources/50.7.2_Calculation_of_K_and_P_in_ESKF_update.resources/unknown_filename.1.png)\n    \n    Update\n    ![unknown_filename.10.png](./_resources/50.7.2_Calculation_of_K_and_P_in_ESKF_update.resources/unknown_filename.10.png)\n    \n*   Here: applied to current state (from the prediction) \\[[Source](http://www.coursera.org/lecture/state-estimation-localization-self-driving-cars/lesson-4-an-improved-ekf-the-error-state-extended-kalman-filter-7Nwfw)\\], also: \\[[Source](http://cggos.github.io/ekf-vs-eskf.html)\\]\n    *   Prediction\n        ![unknown_filename.7.png](./_resources/50.7.2_Calculation_of_K_and_P_in_ESKF_update.resources/unknown_filename.7.png)\n        ![unknown_filename.2.png](./_resources/50.7.2_Calculation_of_K_and_P_in_ESKF_update.resources/unknown_filename.2.png)\n        \n    *   Update\n        ![unknown_filename.8.png](./_resources/50.7.2_Calculation_of_K_and_P_in_ESKF_update.resources/unknown_filename.8.png)\n        ![unknown_filename.3.png](./_resources/50.7.2_Calculation_of_K_and_P_in_ESKF_update.resources/unknown_filename.3.png)\n        ![unknown_filename.png](./_resources/50.7.2_Calculation_of_K_and_P_in_ESKF_update.resources/unknown_filename.png)\n        ![unknown_filename.4.png](./_resources/50.7.2_Calculation_of_K_and_P_in_ESKF_update.resources/unknown_filename.4.png)\n        \n\nSee:\n[http://github.com/uoip/stereo\\_vio\\_eskf/blob/master/eskf.py](http://github.com/uoip/stereo_vio_eskf/blob/master/eskf.py)\n\n',title:"Untitled Page"},"/studienarbeit/50.7.3-eskf-reset":{content:'---\ntitle: 50.7.3 ESKF reset\ndate: "2021-05-18"\ntags:\n  - -sounding-board\n  - -sa/processed\n  - -permanent\n  - filters/ESKF\n---\n\nParent: [Fusing IMU with complementary sensory data](fusing-imu-with-complementary-sensory-data.md)\nBacklinks: [50.3 Error-State Kalman Filter](50.3-error-state-kalman-filter.md)\n\nSource: [Markley](markley.md)\n\n*   moves the rotation error to the global rotation\n*   this keeps the rotation error small and far from any singularities\n\nTo update the global state, the reset has to obey\n![unknown_filename.7.png](./_resources/50.7.3_ESKF_reset.resources/unknown_filename.7.png)\n\nThe reset has to preserve the quaternion norm, therefore an exact unit norm expression must be used, instead of an approximation.\nUsing the [Rodrigues parameter](rotations/gibbs-rodrigues-parameter.md), the reset becomes \n![unknown_filename.6.png](./_resources/50.7.3_ESKF_reset.resources/unknown_filename.6.png)\n\nwhich leads to a two step update (1. linear Kalman update, 2. brute force normalisation) of \n![unknown_filename.8.png](./_resources/50.7.3_ESKF_reset.resources/unknown_filename.8.png)\n\nNotes:\n\n*   MEKF using Rodrigues provides a theoretical justification for the brute force update\n*   \\[+\\] Avoids the possibility of accumulated errors in the quaternion norm\n*   \\[+\\] Rodrigues parameters map the rotation group into three dimensional Euclidian space (180 degree rotation errors are mapped to infinity),\n    *   therefore, probability distributions with infinitely long tails (e.g. Gaussians) are compatible with the Rodrigues parameter space\n\n* * *\n\nSource: [Solà 2017 Quaternion kinematics for ESKF](solà-2017-quaternion-kinematics-for-eskf.md)\n\n*   Reset of the error state mean ![unknown_filename.png](./_resources/50.7.3_ESKF_reset.resources/unknown_filename.png) after the true state is calculated from the nominal state and the error state\n*   This is important for the orientation, because the new orientation error will be expressed locally w.r.t. orientation frame of the new nominal state\n*   Also, need to update the covariance of the error\n\nDefine an error reset function ![unknown_filename.1.png](./_resources/50.7.3_ESKF_reset.resources/unknown_filename.1.png)\n\nThe reset operation:\n![unknown_filename.2.png](./_resources/50.7.3_ESKF_reset.resources/unknown_filename.2.png)\n\nwith\n![unknown_filename.3.png](./_resources/50.7.3_ESKF_reset.resources/unknown_filename.3.png) ![unknown_filename.4.png](./_resources/50.7.3_ESKF_reset.resources/unknown_filename.4.png)\n\nNote:\n\n*   In most cases, ![unknown_filename.5.png](./_resources/50.7.3_ESKF_reset.resources/unknown_filename.5.png) can be ignored, making G an 18x18 identity matrix.\n*   For more precise results, i.e. for reducing long term error drift, ![unknown_filename.5.png](./_resources/50.7.3_ESKF_reset.resources/unknown_filename.5.png) should not be neglected.\n\n',title:"Untitled Page"},"/studienarbeit/atsushisakai-pythonrobotics":{content:'---\ntitle: (AtsushiSakai) PythonRobotics\ndate: "2020-08-27"\ntags:\n  - software/python\n  - -sa/processed\n  - -resources/tutorials\n---\n\n\u003chttp://nbviewer.jupyter.org/github/AtsushiSakai/PythonRobotics/\u003e\n\n',title:"Untitled Page"},"/studienarbeit/b1-modelling-of-tissue-and-sensor,-navigation-of-multimodal-sensors":{content:'---\ntitle: B1 Modelling of tissue and sensor, navigation of multimodal sensors\ndate: "2020-08-09"\ntags: \n- -master/project-background \n- -sa/processed \n- grk-2543\n---\n\nSource: \u003chttp://www.isys.uni-stuttgart.de/forschung/medizintechnik/intraoperative-multisensorische-gewebedifferenzierung/\u003e\nBacklinks: [GRK 2543: Intraoperative Multi-sensor Tissue Differentiation in Oncology](grk 2543_ intraoperative-multi-sensor-tissue-differentiation-in-oncology.md)\n\n![unknown_filename.png](./_resources/B1__Modelling_of_tissue_and_sensor,_navigation_of_multimodal_sensors.resources/unknown_filename.png)\n\n*   cross-domain modelling (tissue and sensor)\n*   tissue parameters change depending on status (benign/malignant), obtained or derived from sensor signals\n*   sensor signal must be synchronised with the current position of the sensor probe on the tissue\n\n',title:"Untitled Page"},"/studienarbeit/back-end-optimisation":{content:'---\ntitle: Back-end optimisation\ndate: "2020-08-03"\ntags:\n  - to-do/to-clarify\n  - SLAM/VSLAM\n  - -sa/processed\n---\n\nParent: [Visual SLAM Implementation Framework](SLAM/vslam-framework.md)\n\nSource: [cometlabs](bibliography/cometlabs.md)\n\n(Camera pose optimisation)\n\n*   To compensate for drift of pose estimation\n*   Traditionally using EKF ([filter-based](http://www.evernote.com/shard/s484/nl/217355218/48ab2536-eadf-4a7f-8dfa-f377bfbe3839))\n    *   simple implementation\n    *   therefore good for small scale estimations\n*   Alternative: bundle adjustment (graph optimisation)\n    *   joint optimisation of the camera pose and the 3D structure parameters\n    *   combines numerical methods and graph theory\n    *   increasingly favoured over filtering, due to the latter\'s inherent inconsistency\n    *   more efficient when combined with sub-mapping\n\n',title:"Untitled Page"},"/studienarbeit/badias-2020-morph-dslam":{content:'---\ntitle: Badias 2020 MORPH-DSLAM\ndate: "2020-10-20"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-to-read\n  - -sa/to-be-processed\n---\n\n**URL**: \u003chttp://arxiv.org/pdf/2009.00576.pdf\u003e\n**Video**: [http://www.youtube.com/watch?v=P\\_QN8Nv--\\_g](http://www.youtube.com/watch?v=P_QN8Nv--_g)\n\nTo read!\n\n',title:"Untitled Page"},"/studienarbeit/bag-of-words":{content:'---\ntitle: Bag of words\ndate: "2020-11-19"\ntags:\n  - -sa/processed\n  - vision\n  - discussion/2020/2020-12\n---\n\n**Parent**: [SLAM Index](SLAM/slam_index.md)\n\n**Source**: \u003chttp://towardsdatascience.com/bag-of-visual-words-in-a-nutshell-9ceea97ce0fb\u003e\n\nHas its origins in natural language processing (NLP), information retrieval\n\n*   A text can be seen as a bag of words, with each word having different frequencies from one another\n*   This can be used to compare and classify texts (similar histograms)\n\n## In vision\n\n*   Instead of words we have features (identifying pattern in an image)\n*   An image is represented as a set of features\n*   Features consist of\n    *   Keypoints: points that are invariant to transformation\n    *   [Descriptors](studienarbeit/descriptors-in-feature-detection-extraction.md): description of the keypoint, for feature representation\n*   Construct a frequency histogram of features in the image\n\n## Workflow\nFeature detection/extraction --\u003e build vocabulary/codewords --\u003e make histogram = BoW\n\n## Feature extractor algorithms\n\n*   Feature detection --\u003e desriptor extraction\n*   e.g. SIFT, SURF, ORB etc. are algorithms for feature identification/description\n\n![unknown_filename.png](./_resources/Bag_of_words.resources/unknown_filename.png)\n\n## Vocabulary building\n\n*   Clusters are made from the [descriptors](studienarbeit/descriptors-in-feature-detection-extraction.md)\n*   Clustering algorithms, e.g. k-means, DBSCAN, etc.\n*   Vocabulary (codewords) consists of the centres of each cluster\n    "i.e. 1 vocab word is summarised from a group of descriptors"\n    \n\n![unknown_filename.2.png](./_resources/Bag_of_words.resources/unknown_filename.2.png)\n(descriptor space)\n\n## Bag of words and comparison to other images\n![unknown_filename.1.png](./_resources/Bag_of_words.resources/unknown_filename.1.png)![Image.jpg](./_resources/Bag_of_words.resources/Image.jpg)\ns.  [Feature matching](feature-matching.md)\n\n',title:"Untitled Page"},"/studienarbeit/barycentric-coordinates":{content:'---\ntitle: Barycentric coordinates\ndate: "2020-08-09"\ntags:\n  - software/SOFA/data-types\n  - -sa/processed\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\nBaclinks:  [Top-down mapping (master to slave)](top-down mapping (master to slave).md), [lamarca-2019-defslam](lamarca-2019-defslam.md)\n\n*   Barycentre: centre of mass\n*   A coordinate system, in which the location of point of a [simplex](simplex.md) (line, triangle, tetrahedron, etc) is specified as the centre of mass of the masses placed at its vertices\n\n![unknown_filename.png](./_resources/Barycentric_coordinates.resources/unknown_filename.png)\n\n|     |     |\n| --- | --- |\n| x\\_i | vertices of a simplex |\n| p   | a point in space |\n\n*   The a\\_i coefficients are the barycentric coordinates of p w.r.t. x\\_1 ... x\\_n\n*   Non-unique, e.g. (ba1 ... ban) are also the barycentric coordinates of p, unless the coordinate values are restricted with a relation like ![unknown_filename.2.png](./_resources/Barycentric_coordinates.resources/unknown_filename.2.png) (absolute barycentric coordinates)\n*   Negative coordinates lie outside of the simplex\n\nBarycentric coordinates of the vertices: ![unknown_filename.1.png](./_resources/Barycentric_coordinates.resources/unknown_filename.1.png)\n\n',title:"Untitled Page"},"/studienarbeit/basic-python-script-in-sofa":{content:"---\ntitle: Basic python script in Sofa\ndate: \"2020-07-15\"\ntags:\n  - software/SOFA\n  - software/python\n  - -sa/processed\n---\n\nParent: [SofaPython Index](sofapython-index.md)\n\nImports\nimport Sofa\n\nGeneral functions\n\n*   createGraph(self, root)\n*   reset()\n*   onKeyPressed()\n*   ...\n\nRequired in every script:\ncreateScene(rootNode)\n\nCreate a child from a node\nnode.createChild('Name')\n\nAdd an object component to the node:\nnode.createObject(type in string, kwargs\\*\\*)\n\n",title:"Untitled Page"},"/studienarbeit/baumgarte-stabilisation-over-the-so-3-rotation-group-for-control":{content:'---\ntitle: Baumgarte stabilisation over the SO(3) rotation group for control\ndate: "2021-08-18"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-read\n  - math/rotations\n  - math/quaternions\n  - -sa/to-be-processed\n---\n\n**Author:** Sebastien Gros\n\n',title:"Untitled Page"},"/studienarbeit/besprechung-2021-03-08":{content:'---\ntitle: Besprechung 2021-03-08\ndate: "2021-03-08"\ntags:\n  - -sa/processed\n  - discussion/2021/2021-03\n---\n\nAgenda\n\n*   DefSLAM + OS3\n    *   Up till DefTracking::MonocularInitialization()\n    *   on hold, working on Kalman stuff for the time being\n*   DefSLAM + Kalman\n    *   new plot (monocular trajectory without any pose updating)\n    *   [ ] to do: use noisy stereo data, plug into update step while discarding images\n    *   functions in System.cc: read data, update pose\n*   DefSLAM + sockets\n\n* * *\n\nMeeting notes:\n\n*   next step: implement the Kalman filter. When that is done, discuss next steps e.g. how to test/validate\n    - [ ] KIV: Mon 15.03.2021\n    *   try model-free Kalman filter with random walk (xnext = I\\*x + noise)\n*   discussed, but out of scope for the moment: outsource the optimisation e.g. to Python/MATLAB, then feed the results back into DefSLAM\n*   we are getting a new Linux laptop\n*   if there is time: interface/bindings for reading/writing input into DefSLAM\n*   new narrative for SA presentation: worked on Kalman filter, then worked on trying to incorporate an optimisation-based SLAM\n*   sockets with C++ as a Hiwi task maybe\n\n',title:"Untitled Page"},"/studienarbeit/besprechung-2021-03-15":{content:"---\ntitle: Besprechung 2021-03-15\ndate: \"2021-03-15\"\ntags:\n  - -sa/processed\n  - discussion/2021/2021-03\n  - discussion/2021/2021-04\n---\n\nStatus\n\n*   Last week: generated noisy IMU data (pose) from stereo trajectory, 'offline' Kalman\n*   This week: Kalman + 'live' DefSLAM\n*   Still to do: design KF (EKF, or other methods...)\n\n* * *\n\n'Offline' Kalman\n\n*   To learn how to use openCV's Kalman filter without having to rebuild DefSLAM every time\n    Aim was to figure out the update/correction workflow and implement it in live DefSLAM run\n    \n*   Uses pre-extracted trajectories (mono and stereo)\n*   Only testing update/correction of xyz coordinates, not the orientation\n*   Update procedure ...\n\n~~KF + 'live' DefSLAM~~\n\n*   ~~Still have got to try out updating the pose while no images are received~~\n    *   ~~- [ ] set state to LOST, or do System::Reset, or completely not carry out SLAM.TrackMonocular depending current timestamp~~\n*   ~~at present, Kalman update from Frame 220 to Frame 260,, crash around frame 272~~\n*   ~~angles look weird~~\n\n* * *\n\nAfter meeting notes\n\n*   Continue working on offline Kalman until it works properly, then only implement it live\n*   Make Kalman filter use raw IMU measurements\n*   Incorporate DefSLAM pose (from optimisation) as an extra measurement to the KF\n    *   Method I: freeze covariance matrix for the DefSLAM measurements\n        maybe P -\u003e infinity for DS measurements between frames\n        or: use old DS measurements to fill in the gaps (between frames)\n        \n    *   ~~Method II: use MHE~~, scrapped because solver needed -- makes it harder to achieve real-time capability\n*   Get pie at 12 pm\n\n",title:"Untitled Page"},"/studienarbeit/besprechung-2021-04-01":{content:"---\ntitle: Besprechung 2021-04-01\ndate: \"2021-04-01\"\ntags:\n  - -sa/processed\n  - discussion/2021/2021-04\n---\n\nAgenda\n\n*   Recap: [last meeting (2021-03-15)](last-meeting-(2021-03-15).md)\n    *   Offline Kalman — after this works, do a 'live' implementation on DefSLAM run\n    *   IMU measurements as \\[acc, gyro\\] readings\n        \n        *   With noise, but without considering bias, IMU-cam transformation, gravity\n        \n    *   Two sets of measurements for filter: IMU measurements, DefSLAM (camera) measurements\n    *   KF prediction using random walk\n*   Recap: SLAM +filtering terminology (loose coupling, tight coupling)\n*   Literature\n*   Original suggestion using random walk model\n*   Interface (DefSLAM, Python)\n    *   read: OK\n    *   write: to do\n*   [x] Offline Kalman as a separate repo\n*   Good papers?\n\nMisc\n\n*   python3 plot\\_noisy\\_traj.py\n*   traj\\_parser.py\n*   Weiss PhD thesis\n*   IMU tutorial\n*   github msf-ekf\n*\n\n",title:"Untitled Page"},"/studienarbeit/bladder-cancer-surgery":{content:"---\ntitle: Bladder cancer surgery procedure\ndate: 2021-10-05\ntags:\n  - -sa/processed\n  - -permanent\n  - medical/surgery\n  - -published\n---\n\n**See also**: [related-types-of-surgery](studienarbeit/related-types-of-surgery.md)\n\n# BCS procedure\n(according to my understanding)\n1.  [Cystocopy](studienarbeit/cystocopy.md) to inspect the bladder\n2.  If tumour is found, do resection --\u003e [cryosection](studienarbeit/cryosection.md)\n3.  Cancer detected --\u003e tumour removal",title:"Untitled Page"},"/studienarbeit/bottom-up-mapping-slave-to-master":{content:'---\ntitle: Bottom-up mapping (slave to master)\ndate: "2020-08-09"\ntags:\n  - software/SOFA/mappings\n  - -sa/processed\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\nParent: [Mappings](mappings.md)\n\nMapping of a slave forces to the master forces\nNewton\'s law f\\=Ma applies\n\nEquivalence of power\n![unknown_filename.png](./_resources/Bottom-up_mapping_(slave_to_master).resources/unknown_filename.png)\n![unknown_filename.2.png](./_resources/Bottom-up_mapping_(slave_to_master).resources/unknown_filename.2.png) using the kinematic relation ![unknown_filename.1.png](./_resources/Bottom-up_mapping_(slave_to_master).resources/unknown_filename.1.png)\n![unknown_filename.3.png](./_resources/Bottom-up_mapping_(slave_to_master).resources/unknown_filename.3.png)using the principle of virtual work\n\n',title:"Untitled Page"},"/studienarbeit/building-sofa-on-windows":{content:'---\ntitle: Building SOFA on Windows\ndate: "2020-07-17"\ntags:\n  - software/SOFA\n  - -sa/processed\n---\n\nParent: [SofaPython Index](sofapython-index.md)\n\n\u003chttp://www.sofa-framework.org/community/doc/getting-started/build/windows\u003e/\n\n',title:"Untitled Page"},"/studienarbeit/cadena-2016":{content:'---\ntitle: Cadena 2016 Past, Present, and Future of SLAM\ndate: "2020-08-25"\ntags:\n  - -resources/-bibliography\n  - SLAM\n  - to-do/go-through-literature-later\n  - -resources/-bibliography/bib-to-read\n  - -sa/to-be-processed\n---\n\nAuthors Cadena et al\n\nAbstract\n\n*   cited by 1.2k people\n*   "This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM"\n\nRecommended other works\ns. [Works of possible interest](bibliography/works-of-interest.md)\n\nContents/Chapters\nTakeaway\n\n',title:"Untitled Page"},"/studienarbeit/calculating-the-estimated-state-in-the-gh-filter":{content:'---\ntitle: Calculating the estimated state in the GH-filter\ndate: "2020-08-27"\nexternal_url: "http://nbviewer.jupyter.org/github/feudalism/Kalman-and-Bayesian-Filters-in-Python/blob/master/01-g-h-filter.ipynb"\ntags:\n  - -sa/processed\n  - filters/gh-filter\n---\n\nParent: [g-h filter or α-β filter](g-h-filter-or-α-β-filter.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\n![unknown_filename.png](./_resources/Calculating_the_estimated_state_in_the_GH-filter.resources/unknown_filename.png)\nUsing [a gain](http://www.evernote.com/shard/s484/nl/217355218/a7509361-eda5-4c69-a509-eb5c9d213ab2), the estimate therefore always falls between the measurements (circles) and the predictions (in red).\nThe prediction is dependent on the previous filter output (i.e. last estimate). Here it is modelled to increase by 1 from the previous estimate.\n\nThe estimates are not a straight line, but definitely closer in shape to the ground truth than the measurements alone.\nMoreover, they seem to improve with time.\n\n[Effects of varying g](effects-of-varying-g.md)\n\nCaveat: This worked well because we had a good prediction model (gain +1 lb/day), which is close to the ground truth behaviour (also +1 lb/day).\nIf we had derived the prediction model with a bad gain, e.g. -1 lb/day, the esimates won\'t be that good anymore (but nevertheless an improvement over the prediction alone).\n\n![unknown_filename.1.png](./_resources/Calculating_the_estimated_state_in_the_GH-filter.resources/unknown_filename.1.png)\n\nImprovement: [Improving the gh-filter by using h](improving-the-gh-filter-by-using-h.md)\n\n',title:"Untitled Page"},"/studienarbeit/camera-calibration":{content:'---\ntitle: Camera calibration\ndate: "2020-11-20"\ntags:\n  - sensors/cameras\n  - -sa/processed\n---\n\nParent: [SLAM Index](SLAM/slam_index.md)\nSource: \u003chttp://de.mathworks.com/help/vision/ug/camera-calibration.html\u003e\n\n*   estimates lens/sensor parameters\n*   e.g. to correct lens distortion, determine position, measurement etc\n*   there are several camera models, e.g. fisheye, [pinhole](pinhole.md)\n\nCamera parameters\n\n*   [intrinsic](intrinsic.md)\n*   [extrinsic](extrinsic.md)\n*   distortion coefficients\n\nHow to solve for camera parameters?\n\n1.  Need to have 3D world points and the corresponding 2D image points\n2.  Take multiple images of a calibration pattern to obtain these correspondences\n3.  With the mapping 3Dp -\u003e 2Dp, solve for camera parameters\n\nEvaluate accuracy of estimated camera parameters:\n\n*   plot relative locations of the camera and the calibration pattern\n*   calculate reprojection error\n*   calculate parameter estimation error\n\n',title:"Untitled Page"},"/studienarbeit/camera-views-as-seen-by-slam-at-distal-end-of-probe-scope":{content:'---\ntitle: Camera views as seen by SLAM at distal end of probe/scope\ndate: "2021-06-16"\ntags:\n  - to-do/orphan\n  - -sa/processed\n  - medical/surgery/endoscope\n---\n\n![unknown_filename.jpeg](./_resources/Camera_views_as_seen_by_SLAM_at_distal_end_of_probe_scope.resources/unknown_filename.jpeg)\n\n',title:"Untitled Page"},"/studienarbeit/cancer-biopsy":{content:'---\ntitle: Cancer biopsy\ndate: "2020-08-08"\ntags:\n  - medical/cancer\n  - medical/surgery\n  - -sa/processed\n  - -definitions\n---\n\n**Source**: \u003chttp://en.wikipedia.org/wiki/Biopsy\u003e\n\n# Biopsy\ntype of medical test in which a cell/tissue sample is extracted in order to detect disease\n\n## Types of biopsy:\n*   excisional biopsy: removal of entire suspicious area to be diagnosed\n*   incisional biopsy: removal of only samples of the abnormal tissue\n*   needle aspiration biopsy: removal of cells via needle\n\n## Diagnosing / Pathological examination\n*   to determine whether the abnormality is benign or malignant (classification of the cancer)\n*   to determine how far it has spread\n	*   negative margins: no disease found at the edge of specimen\n	*   positive margins: disease was found at edge, further excision may be in order\n\nIn bladders: usually done using a  [cystocopy](cystocopy.md)\n\n',title:"Untitled Page"},"/studienarbeit/central-limit-theorem":{content:'---\ntitle: Central Limit Theorem\ndate: "2020-08-31"\ntags:\n  - -definitions\n  - -sa/processed\n  - math/statistics\n---\n\nParent: [Gaussian distribution](gaussian-distribution.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nIf we make many measurements, the measurements will be normally distributed.\n(only applies under certain conditions)\n\n',title:"Untitled Page"},"/studienarbeit/chaining-rotation-matrices-and-angular-velocities":{content:'---\ntitle: Chaining rotation matrices and angular velocities\ndate: "2021-05-24"\ntags:\n  - -sa/processed\n  - math/rotations\n  - math/kinematics\n  - discussion/2021/2021-06\n---\n\nSource: [Woernle Mehrkörpersysteme](woernle-mehrkörpersysteme.md)\nBacklinks: [Kinematics primer](kinematics primer.md), [obtaining imu measurements from camera by forward kinematics](obtaining imu measurements-from-camera-by-forward-kinematics.md)\n**See also**: [Rotations / SO(3) group index](rotations-_-so(3)-group-index.md)\n\nChaining rotation matrices\n![unknown_filename.png](./_resources/Chaining_rotation_matrices_and_angular_velocities.resources/unknown_filename.png)\nT\\_02 transforms a point in CS2 to CS0\n\ncompare:\n![unknown_filename.3.png](./_resources/Chaining_rotation_matrices_and_angular_velocities.resources/unknown_filename.3.png)\n\nChaining homogeneous transformation matrices\n![unknown_filename.4.png](./_resources/Chaining_rotation_matrices_and_angular_velocities.resources/unknown_filename.4.png)\n\nChaining angular velocities (in same CS)\n![unknown_filename.1.png](./_resources/Chaining_rotation_matrices_and_angular_velocities.resources/unknown_filename.1.png)\n![unknown_filename.2.png](./_resources/Chaining_rotation_matrices_and_angular_velocities.resources/unknown_filename.2.png)\n\nang.vel. of 2 rel to 0 = ang.vel. of 1 rel. to 0 + ang.vel. of 2 rel. to 1\n\n',title:"Untitled Page"},"/studienarbeit/cheatsheet":{content:'---\ntitle: Cheatsheet\ndate: "2020-09-30"\ntags:\n  - -sa/processed\n  - software/SOFA/SofaPython3\n---\n\nParent: [SofaPython Index](SofaPython Index.md)\n\n**Imports/Plugins**\nimport SofaRuntime\nfrom SofaRuntime import PluginRepository\nPluginRepository.addFirstPath(SOFA\\_INSTALL\\_DIR)\nSofaRuntime.importPlugin("SofaComponentAll")\nSofaRuntime.importPlugin("SofaPython3")\nSofaRuntime.importPlugin("SofaOpenglVisual")\n\nFrom root\nroot = Sofa.Core.Node("root")\nc = root.createObject("MechanicalObject", name="t", position=\\[\n                              \\[0, 0, 0\\], \\[1, 1, 1\\], \\[2, 2, 2\\]\\])\nc1 = root.addObject("MechanicalObject", name="c1")\n\nFrom nonroot\nnonroot\\_node = Sofa.Core.Node("a\\_node")\nnonroot\\_node.addObject("MechanicalObject", name="c1")\n.addData("d", value="coucou", type="string")\n.addData("data1", value="@/c1.d") # @ is root\n\nAdd data from relative/absolute paths\nc4.addData(\'data1\', value="@/n1/c3.data1") # absolute path (chained)\nc4.addData(\'data2\', value="@../n1/c3.data1") # relative path (down, chained)\nc1.addData(\'data4\', value="@n1/c3.data2") # relative (up, chained)\n\nAdd data from parent\n        c1 = root.addObject("MechanicalObject", name="c1")\n        c1.addData("d", value="coucou", type="string")\n        c1.addData("d2", value="@c1.d")\n\nGet\nroot.getClassName()\nc.getTemplateName()\nc,getDataFields()\n\n**GUI**\nimport Sofa.Gui\n\nSofa.Simulation.init(root)\nSofa.Gui.GUIManager.Init("simple\\_scene", "qglviewer") # alt: \'qt\' instead of qglviewer\nSofa.Gui.GUIManager.createGUI(root)\nSofa.Gui.GUIManager.MainLoop(root)\nSofa.Gui.GUIManager.closeGUI()\n\n',title:"Untitled Page"},"/studienarbeit/chen-2018-mis-slam":{content:'---\ntitle: Chen 2018 SLAM-based dense surface reconstruction in MIS with AR\ndate: "2020-08-24"\ntags:\n  - to-do/to-clarify\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-to-read\n  - -sa/to-be-processed\n---\n\n**Authors** Chen et al\n\n## Abstract\n\n*   Intra-operative dense surface reconstruction framework to provide geometry information from only monocular videos\n*   The proposed framework works well with rapid camera movements, however is not suitable for large deformations\n*   Only tweaks ORBSLAM to adjust between point density and computational performance\n\n## Contents/Chapters\nProblems in medical AR:\n\n*   tissue surface illumination\n*   tissue deformation\n*   rapid movements of the medical tool e.g. endoscope (s. also [kidnapped robot problem](kidnapped-robot-problem.md) for relocalisation, tracking mus therefore be robust)\n*   field of view often very small\n\n"A typical human uses 14 visual cues to perceive depth, only 3/14 are binocular vision related."\ns. also: [Monocular depth perception in humans](permanent/10-monocular-depth-perception.md)\n\nTraditional feature-tracking for AR in MIS:\n\n*   SIFT\n*   SURF\n*   Optical flow tracking\n*   Other approaches for soft tissues\n\nHowever, these are designed for 2D tracking\n\n\\--\u003e ORB descriptor\n\n*   binary feature descriptor\n*   faster than SURF and SIFT with better accuracy\n*   invariant to rotation, illumination and scale\n\nORB-SLAM uses Bag of Words algo. for relocalisation\n\nTask of the SLAM algo in this framework:\n\n*   Recovers camera POSE\n*   Generates a sparse point cloud, based on which 3D geometric information is generated\n\nInialisation is problematic for monocular SLAM because the depth isn\'t recoverable from a single image\n\\--\u003e automatic approach in ORBSLAM:\n\n*   for planar scenes: calculate homography\n*   for non-planar scenes: calculate a fundamental matrix dynamically\n\n## Takeaway\n\nNot suitable for large deformations\n\n',title:"Untitled Page"},"/studienarbeit/classification-of-image-based-camera-localization-approaches":{content:'---\ntitle: Classification of image-based camera localization approaches\ndate: "2020-07-30"\ntags:\n  - SLAM/VSLAM\n  - -sa/processed\n---\n\nParent: [SLAM Index](SLAM/slam_index.md)\nSource: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)\n\n![unknown_filename.png](./_resources/Classification_of_image-based_camera_localization_approaches.resources/unknown_filename.png)\n\nUnknown environment (must be reconstructed from image data)\nOnline/real-time mapping (SLAM)\n\n*   [geometric metric SLAM](http://www.evernote.com/shard/s484/nl/217355218/41c3a019-d86e-7e60-40a5-d4680f7d668b?title=Geometric%20metric%20SLAM) (accurate computations, therefore still widely used in practice)\n    *   monocular\n    *   multiocular\n    *   [multi-kind sensor](http://www.evernote.com/shard/s484/nl/217355218/0df3e4ab-19f4-4b5f-bd9f-eb689af9d991?title=Multi-kind%20sensors%20SLAM) (active)\n        *   loosely-coupled\n        *   closely-coupled\n*   learning SLAM (active)\n    *   needs a prior dataset to train NN -- dataset determines performanace of the SLAM\n    *   low generalisation capability, therefore not as flexible as geom. SLAM\n    *   Has a 3D map\n*   [topological SLAM](http://www.evernote.com/shard/s484/nl/217355218/fe42bb48-04ca-333c-346e-c694bd94e354?title=Topological%20SLAM) (getting more unpopular)\n*   marker SLAM (semi-known environment)\n\n\\[GEOM\\] Filter-based SLAM\n\\[GEOM\\] Keyframe-based SLAM(active)\n\n*   Feature-based\n*   Direct\n\n',title:"Untitled Page"},"/studienarbeit/collision-detection-and-response":{content:'---\ntitle: Collision detection and response\ndate: "2020-08-22"\ntags:\n  - -sa/processed\n  - software/SOFA/simulation-algos\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\nParent: [Simulation algorithms in SOFA](simulation-algorithms-in-sofa.md)\n\n*   split into several phases\n*   each phase is scheduled by a CollisionPipeline component\n*   an object which can potentially collide is associated with a collision geometry\n    *   returns pairs of colliding bounding volumes (broad phase component)\n    *   returns pairs of geometric primitives + contact points (narrow phase component)\n*   the returned pairs are passed to the contact manager\n*   the contact manager creates contact interactions\n\n... (skimmed)\n\n',title:"Untitled Page"},"/studienarbeit/collision-model":{content:'---\ntitle: Collision model\ndate: "2020-07-15"\ntags:\n  - software/SOFA/models\n  - -sa/processed\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\nParent: [Models in SOFA](models-in-sofa.md)\n\nPrimitives coming into contact — we need\n\n*   collision detection\n*   collision response\n\nCollision detection approaches:\n\n*   Distances between pairs of geometric primitives\n*   Points in distance fields\n*   Distances between colliding meshes using ray-tracing\n*   Intersection volume using images\n\nCollision model\n\n*   Similar to internal model\n*   Topology/geometry is different (geometry specially for contact model), can be stored in a data structure dedicated to collision detection\n    e.g. TriangleModel: computes collision detection on triangular mesh surfaces\n    \n*   Has best trade-off between precision and speed in collision detection, compared to the [internal model](internal-model.md)\n*   When the contact models collide, pairs of contact points are created, each point a slave of a contact model.\n\nTime take for collision detection depends on collision mesh\n\n*   If too long, set collision mesh to be coarser\n*   If precision required, detailed collision mesh\n\n',title:"Untitled Page"},"/studienarbeit/components-of-the-internal-model":{content:'---\ntitle: Components of the internal model\ndate: "2020-08-09"\ntags:\n  - software/SOFA/simulation-components\n  - -sa/processed\n---\n\nSource:  [SOFA extended documentation](sofa-extended-documentation.md)\nParent: [Internal model as a scene graph in SOFA](internal model as-a-scene-graph-in-sofa.md)\n\n![Image.png](./_resources/Components_of_the_internal_model.resources/Image.png)\n\n*   MeshLoader: topology, geometry\n*   [MechanicalState](mechanicalstate.md)\n*   TetrahedronSetTopologyContainer\n    *   tetrahedral connectivity\n    *   passed on to other components\n*   [Forces](forces.md)\n    \n*   Mass\n    *   DiagonalMass\n    *   UniformMass: less accurate, but allows faster computation\n*   FixedConstraint: P (cancels displacements)\n*   EulerSolver: integration scheme\n\n',title:"Untitled Page"},"/studienarbeit/computational-properties-of-gaussian-distributions":{content:'---\ntitle: Computational properties of Gaussian distributions\ndate: "2020-08-31"\ntags:\n  - -sa/processed\n  - math/statistics\n---\n\nParent: [Gaussian distribution](gaussian-distribution.md)\n**Backlinks**: [Showing correlation using error ellipses](showing-correlation-using-error-ellipses.md)\n\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\n*   g1 + g2 = g3; all are Gaussians\n*   g1 \\* g2 = g3; g3 is not Gaussian, but proportional to a Gaussian\n\nSum of two Gaussians\n![unknown_filename.1.png](./_resources/Computational_properties_of_Gaussian_distributions.resources/unknown_filename.1.png)\n\nProduct of two Gaussians:\n![unknown_filename.png](./_resources/Computational_properties_of_Gaussian_distributions.resources/unknown_filename.png)\n\nProduct of multidimensional Gaussians:\n![unknown_filename.2.png](./_resources/Computational_properties_of_Gaussian_distributions.resources/unknown_filename.2.png)\n\n',title:"Untitled Page"},"/studienarbeit/conditional-independence":{content:'---\ntitle: Conditional independence\ndate: "2020-08-25"\nexternal_url: "http://en.wikipedia.org/wiki/Conditional_independence"\ntags:\n  - -definitions\n  - -sa/processed\n  - math/probability-theory\n---\n\n**Source**: [http://en.wikipedia.org/wiki/Conditional\\_independence](http://en.wikipedia.org/wiki/Conditional_independence)\n\nA and B are conditionally independent given C\nIf and only if, given the knowledge that C occurs,\nthe knowledge of whether A occurs provides no information whatsoever on the likelihood of B occurring, and vice versa\n\n**Example**\nWeather and delay\n\n*   Let the two events be the probabilities of persons A and B getting home in time for dinner\n*   The third event C is the fact that a snow storm hit the city.\n*   While both A and B have a lower probability of getting home in time for dinner, the lower probabilities will still be independent of each other.\n    i.e. the knowledge that A is late does not tell you whether B will be late.\n    \n*   However, if you have information (other than C) that they live in the same neighborhood, use the same transportation, and work at the same place, then the two events are NOT conditionally independent.\n    \n\nHeight and vocabulary\nHeight and vocabulary are independent; but they are conditionally not independent if you add age.\n\n',title:"Untitled Page"},"/studienarbeit/constraint-solvers":{content:'---\ntitle: Constraint solvers\ndate: "2020-08-22"\ntags:\n  - -sa/processed\n  - software/SOFA/simulation-algos\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\nParent: [Simulation algorithms in SOFA](simulation-algorithms-in-sofa.md)\nBacklinks: [Scene graph in SOFA](scene-graph-in-sofa.md)\n\n*   Lagrange multipliers to handle complex constraints (which aren\'t handle-able using [projection matrices](projection-matrices.md))\n*   May be combined with explicit or implicit integration\n\n![unknown_filename.png](./_resources/Constraint_solvers.resources/unknown_filename.png)\nphi: bilateral interaction laws (attachments, sliding joints, ...)\npsi: unilateral interaction laws (contact, friction, ...)\n\nThe Lagrange multipliers add force terms to the equation [A\\*dv = b](http://www.evernote.com/shard/s484/nl/217355218/e3d45207-3f91-4941-8a1d-0573043b4546)\n![unknown_filename.1.png](./_resources/Constraint_solvers.resources/unknown_filename.1.png)\n![unknown_filename.2.png](./_resources/Constraint_solvers.resources/unknown_filename.2.png)\n\nThe H matrices are stored in the MechanicalState of each node.\n\nSolving the constraints\n\n1.  Free motion (lam = 0; A\\*dv = b)\n    *   Solve interacting objects independently\n    *   Obtained: free motion ![unknown_filename.3.png](./_resources/Constraint_solvers.resources/unknown_filename.3.png) for each object (here: 2 objects)\n    *   Integrate to get: ![unknown_filename.4.png](./_resources/Constraint_solvers.resources/unknown_filename.4.png)\n2.  Constraint solving\n    ![unknown_filename.5.png](./_resources/Constraint_solvers.resources/unknown_filename.5.png)\n    with ![unknown_filename.6.png](./_resources/Constraint_solvers.resources/unknown_filename.6.png)\n    *   Together with the interaction laws above, this poses a mixed complementarity problem\n    *   Compute the value of lambda\n3.  Corrective motion\n    ![unknown_filename.7.png](./_resources/Constraint_solvers.resources/unknown_filename.7.png)\n    \n\nCompliance computation\n\n*   Compliance: inv(A), changes at every time step (in a nonlinear model)\n*   Computation of the compliance is time-consuming for RT applications\n*   There are strategies to improve the speed of the algorithm, implemented in ConstraintCorrections\n*   ConstraintCorrections: computed ![unknown_filename.8.png](./_resources/Constraint_solvers.resources/unknown_filename.8.png)\n\n',title:"Untitled Page"},"/studienarbeit/converting-imu-data-to-inertial-frame":{content:"---\ntitle: Converting IMU data to inertial frame\ndate: \"2021-07-23\"\ntags:\n  - -sa/processed\n  - sensors/IMU\n  - math\n---\n\nParent: [IMU index](imu-index.md)\nSource: [http://redshiftlabs.com.au/wp-content/uploads/2018/02/an-1005\\_-\\_understanding\\_euler\\_angles.pdf](http://redshiftlabs.com.au/wp-content/uploads/2018/02/an-1005_-_understanding_euler_angles.pdf)\n\nIMU outputs are in the body frame of the sensor.\n\n*   Convention used in the article: yaw/psi (z) - pitch/theta (y) - roll/phi (x) around momentary axes\n*   Momentary coordinate systems: W -\u003e W' -\u003e W'' -\u003e B\n\nBody acceleration to inertial acceleration\nW\\_a = R\\_WB @ B\\_a\n\nBody angular rate to inertial angular rate\n\n*   Each angular rate must be converted to the corresponding frame\n    *   p: gyro\\_z -\u003e rotated into W: R\\_w\\_w' @ R\\_w'\\_w'' @ R\\_w''\\_B @ q\n    *   q: gyro\\_y -\u003e rotated into W': R\\_w'\\_w'' @ R\\_w''\\_B @ q\n    *   r: gyro\\_x -\u003e rotated into W'': R\\_w''\\_B @ r\n\n![unknown_filename.1.png](./_resources/Converting_IMU_data_to_inertial_frame.resources/unknown_filename.1.png)\nwith\n![unknown_filename.png](./_resources/Converting_IMU_data_to_inertial_frame.resources/unknown_filename.png)\n\nGimbal lock: pitch approaches +-90, terms divided by cos90\n\n",title:"Untitled Page"},"/studienarbeit/converting-velocity-from-cs1-to-cs0":{content:'---\ntitle: Converting velocity from CS1 to CS0\ndate: "2021-05-24"\ntags:\n  - -sa/processed\n  - math/kinematics\n---\n\nSource: [Woernle Mehrkörpersysteme](woernle-mehrkörpersysteme.md)\nBacklinks: [Kinematics primer](kinematics primer.md), [poisson equation for skew symmetric matrix of angular velocity](poisson equation for skew-symmetric-matrix-of-angular-velocity.md)\n\nLinear velocity\n![unknown_filename.png](./_resources/Converting_velocity_from_CS1_to_CS0.resources/unknown_filename.png)\nThis is the derivative of r relative to CS0, as depicted in CS0 coordinates\n\n![unknown_filename.1.png](./_resources/Converting_velocity_from_CS1_to_CS0.resources/unknown_filename.1.png)\nWhere the expression in square brackets means:\nthe derivative of r relative to CS0, as depicted in CS1 coordinates\n\nAngular velocity\n![unknown_filename.2.png](./_resources/Converting_velocity_from_CS1_to_CS0.resources/unknown_filename.2.png)\n\nwith ![unknown_filename.3.png](./_resources/Converting_velocity_from_CS1_to_CS0.resources/unknown_filename.3.png): ang.vel. of D relative to B, given in E coordinates\n\n[Note](http://en.wikipedia.org/wiki/Cross_product):  ![506480ff74ac8093f4e4a1c9faccdbe3b8b5eb1c](http://wikimedia.org/api/rest_v1/media/math/render/svg/506480ff74ac8093f4e4a1c9faccdbe3b8b5eb1c)\n\n',title:"Untitled Page"},"/studienarbeit/correlation-and-independence":{content:'---\ntitle: Correlation and independence\ndate: "2020-08-31"\ntags:\n  - -sa/processed\n  - math/statistics\n---\n\nParent: [Gaussian distribution](gaussian-distribution.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nIndependent variables are uncorrelated.\nBut the reverse is not always true: uncorrelated variables may be dependent on one another\ne.g. y=x^2 has no \\[linear\\] correlation, but y depends on x nonetheless\n\n',title:"Untitled Page"},"/studienarbeit/covisible-keyframes":{content:'---\ntitle: Covisible keyframes\ndate: "2020-09-15"\ntags:\n  - localisation/keyframes\n  - -definitions\n  - -sa/processed\n---\n\nSource: Palafox 2019 ([thesis](http://vision.cs.tum.edu/_media/members/demmeln/palafox2019ma.pdf))\n**Backlinks**: [Lamarca 2019 DefSLAM](lamarca-2019-defslam.md)\n\nTwo keyframes are covisible if they share several common landmarks.\n\n![unknown_filename.png](./_resources/Covisible_keyframes.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/cryosection":{content:'---\ntitle: Cryosection\ndate: "2020-08-09"\nexternal_url: https://en.wikipedia.org/wiki/Frozen_section_procedure\ntags:\n  - medical/cancer\n  - -definitions\n  - -sa/processed\n  - -published\n---\n\n**Source**: [https://en.wikipedia.org/wiki/Frozen\\_section\\_procedure](https://en.wikipedia.org/wiki/Frozen_section_procedure)  \n\n*   aka frozen section procedure\n*   allows rapid analysis of a dissected/resected specimen during the course of [surgery](studienarbeit/related-types-of-surgery.md)\n*   the specimen is frozen rapidly and brought to a lab for analysis\n*   the results are relayed to the surgeon by intercom\n    *   benign or malignant\n    *   when operating on a previously confirmed malignant tissue, information on whether residual cancer was found on the [resection margin](resection margin.md) of the tissue\n*   the surgeon makes his decision on how to continue the operation based on the results\n\n',title:"Untitled Page"},"/studienarbeit/cyril-stachniss-ekf-slam":{content:'---\ntitle: Cyril Stachniss EKF-SLAM\ndate: "2021-05-13"\ntags:\n  - -resources\n  - filters/EKF\n  - -sa/to-be-processed\n---\n\n**Links**:\n\n*   Course material: \u003chttp://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/\u003e\n*   Lectures: [http://www.youtube.com/playlist?list=PLgnQpQtFTOGQrZ4O5QzbIHgl3b1JHimN\\_\u0026feature=g-list](http://www.youtube.com/playlist?list=PLgnQpQtFTOGQrZ4O5QzbIHgl3b1JHimN_\u0026feature=g-list)\n\n',title:"Untitled Page"},"/studienarbeit/cystocopy":{content:'---\ntitle: Cystocopy\ndate: "2020-08-08"\ntags:\n  - medical/surgery\n  - -definitions\n  - -sa/processed\n  - -published\n---\n\n**Source**: \u003chttps://en.wikipedia.org/wiki/Cystoscopy\u003e  \n**Backlinks**:  [Some questions](Some questions.md)\n\n*   [Endoscopy](permanent/30.1.1-endoscopy.md) of the bladder via the urethra\n*   Tool involved: cystoscope\n\n\u003chttps://www.youtube.com/watch?v=ybhzlW7ivro\u003e  \nVideo of cystocopy and bladder [biopsy](studienarbeit/cancer-biopsy.md)\n![unknown_filename.png](./_resources/Cystocopy.resources/unknown_filename.png)\n\nModern Latin for bladder: _cystis_\n\n',title:"Untitled Page"},"/studienarbeit/data-association-in-defslam":{content:'---\ntitle: Data association in DefSLAM\ndate: "2020-11-20"\ntags:\n  - to-do/go-through-literature-later\n  - localisation/data-association\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\n**Source**: [Lamarca 2020 DefSLAM](lamarca-2020.md)\n**See also**: [Data association](SLAM/data-association.md)\n\n*   Goal: match keypoints in current frame (newly extracted) with map points (already in map/system)\n*   Use the active matching strategy proposed in \\[Agudo 2015\\]: “Simultaneous pose and non-rigid shape with particle dynamics,”\n\n**Steps** \n\n1.  ORB points (keypoints) are detected in current frame\n2.  Camera pose Tcw is predicted\n    *   using camera motion model\n    *   camera motion model: function of past camera poses\n3.  Predict where map points (existing in map) would be imaged, based on last estimated template\n    i.e. now we know the camera movement, project (predict) the previous points to the next frame\n    \n4.  Define a region around each predicted point\n    *   One/some/no keypoints would now lie within these regions\n    *   Search the region around the predicted points for keypoints to be matched with the projected map point\n5.  Predicted point gets matched to one of the keypoints\n    *   the one with the most similar ORB descriptor\n    *   Hamming distance as similarity criterion (between ORB descriptors)\n\n',title:"Untitled Page"},"/studienarbeit/data-structure-in-sofa":{content:'---\ntitle: Data structure in SOFA\ndate: "2020-07-15"\ntags:\n  - software/SOFA\n  - -sa/processed\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\n\nThree different solutions for three relevant levels \\[of organisation of simulation data\\].\n\n1.  [Scenegraph](http://www.evernote.com/shard/s484/nl/217355218/bf18694a-5cea-4dbe-8ba2-95b6c187a636?title=Scene%20in%20SOFA) ( directed acyclic graphs)\n    s. also: [Visitors](visitors.md)\n    \n2.  Component attributes\n    *   Component params stored using Data\u003c...\u003e containers\n    *   e.g. list of particle indices Data\u003cvector\u003cunsigned\u003e\u003e\n    *   Engines\n        *   Create connections between Data instances, for synchronisation of values\n        *   Compute a value from several others (input/output processor)\n        *   Are only used to apply straightforward relations between model parameters\n        *   State update algorithms are implemented using [visitors](http://www.evernote.com/shard/s484/nl/217355218/8e1b1539-2194-4c70-b3b5-c3d4328fa151)\n3.  [Mesh geometry](mesh-geometry.md) and [mesh-topology](mesh-topology.md)\n\n',title:"Untitled Page"},"/studienarbeit/dbow2-weighing-and-scoring":{content:'---\ntitle: DBoW2 weighing and scoring\ndate: "2020-12-15"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-to-read\n  - vision\n  - discussion/2020/2020-12\n  - -sa/to-be-processed\n---\n\nSource: \u003chttp://github.com/dorian3d/DBow\u003e\n\n![unknown_filename.png](./_resources/DBoW2_weighing_and_scoring.resources/unknown_filename.png)\n\n![unknown_filename.1.png](./_resources/DBoW2_weighing_and_scoring.resources/unknown_filename.1.png)\n\n',title:"Untitled Page"},"/studienarbeit/defoptimizer-defposeoptimization":{content:'---\ntitle: DefOptimizer::DefPoseOptimization\ndate: "2020-10-22"\ntags:\n  - -sa/processed\n  - discussion/2020/2020-10\n  - SLAM/algos/DefSLAM\n---\n\nParent: [DefTracking::Track](deftracking__track.md)\n\nAs far as I understand it:\n\n*   Uses g2o library for the optimisation (graph-based SLAM)\n*   cost function terms are converted to edges and nodes\n*   each cost function term seems to correspond to an edge in the graph\n*   in g2o paper/tutorial: [an edge is fully characterised by its error function and its information matrix](http://www.evernote.com/shard/s484/nl/217355218/ed9e53f1-76f2-48bd-a0c1-3b59ebf0326c)\n\nint DefPoseOptimization(Frame \\*pFrame, Map \\*mMap, double RegLap, double RegInex,\n  double RegTemp, uint NeighboursLayers)\n// define optimiser, set solver\noptimizer = new ...\noptimizer.setSolver(...)\n\n// create node for current frame (value = camera pose Tcw)\nvSE3-\u003esetEstimate(Converter::toSE3Quat(pFrame\\-\u003emTcw));\nvSE3-\u003esetId(0);\noptimizer.addVertex(vSE3);\n\n// add mesh points to graph (as nodes)\nsetMeshNodes(optimizer, mMap);\n\n// set MapPoint (feature) vertices\nconst int N = pFrame\\-\u003eN; // how many features in the current frame\nvpEdgesMono.reserve(N);\nvnIndexEdgeMono.reserve(N);\n\n// iterate over each feature map point in the current frame (camera observation)\n![unknown_filename.3.png](./_resources/DefOptimizer__DefPoseOptimization.resources/unknown_filename.3.png)\n{\n MapPoint \\*pMP = pFrame\\-\u003emvpMapPoints\\[i\\];\n\n // observation: 2x1 matrix (2D image point), x and y coords\n cv::KeyPoint \u0026kpUn = pFrame\\-\u003emvKeysUn\\[i\\];\n Eigen::Matrix\u003cdouble, 2, 1\u003e obs \u003c\u003c kpUn.pt.x, kpUn.pt.y;\n\n // define edge (defined by error function and info. matrix)\n \\*e = new g2o::EdgeNodesCamera();\n\n // get the 3 nodes of the facet, in which the mappoint/feature is embedded\n std::set\u003cNode \\*\u003e NodesMp =\n  static\\_cast\u003cDefMapPoint \\*\u003e(pMP)-\u003egetFacet()-\u003egetNodes();\n e-\u003eresize(NodesMp.size() + 1);\n\n // set 0-th vertex of edge\n e-\u003esetVertex(0, dynamic\\_cast\u003cg2o::OptimizableGraph::Vertex \\*\u003e(\n optimizer.vertex(0)));\n\n // iterate over NodesMp (each node is \'it\')\n {\n e-\u003esetVertex(ui, dynamic\\_cast\u003cg2o::OptimizableGraph::Vertex \\*\u003e(\n optimizer.vertex((\\*it)-\u003egetIndex())));\n ViewedNodes.insert(\\*it);\n }\n\n // now e has 4 vertices\n // set 2D observation associated with the edge\n e-\u003esetMeasurement(obs);\n // set information vector of the edge, Huber\n e-\u003esetInformation(Eigen::Matrix2d::Identity() \\* invSigma2 / N);\n e-\u003esetRobustKernel(rk);\n\n // compute error (between observation/meas and projected point?)\n e-\u003efx = pFrame\\-\u003efx;\n ... // camera parameters (from calibration matrix)\n e-\u003ecomputeError();\n\n optimizer.addEdge(e);\n}\n\n// temporal smoothness ![unknown_filename.1.png](./_resources/DefOptimizer__DefPoseOptimization.resources/unknown_filename.1.png)\n// iterate over all viewednodes (roughly num. features x 3 facet nodes?)\n{\n OptLap \\= ViewedNodes;\n\n \\*e = new g2o::EdgesReference();\n e-\u003esetVertex(0, dynamic\\_cast\u003cg2o::OptimizableGraph::Vertex \\*\u003e(\n optimizer.vertex((\\*it)-\u003egetIndex())));\n\n e-\u003esetMeasurement(v) // v from initial pose of viewednodes\n e-\u003esetInformation(...)\n optimizer.addEdge(e);\n e-\u003ecomputeError();\n\n // add neighbouring nodes to OptLap\n}\n\n// Laplacian regulariser ![unknown_filename.png](./_resources/DefOptimizer__DefPoseOptimization.resources/unknown_filename.png)\n{\n // iterate over OptLap\n ...\n e-\u003esetMeasurement(InitialMeanCurvature);\n e-\u003esetInformation(RegLap \\* Eigen::Vector1D::Identity() / OptLap.size());\n optimizer.addEdge(e);\n}\n\n// Inextensibility ![unknown_filename.2.png](./_resources/DefOptimizer__DefPoseOptimization.resources/unknown_filename.2.png)\n{\n // iterate over medges (derived from OptLap) \n s \u003c\u003c (\\*ite)-\u003egetDist();\n e-\u003esetMeasurement(s);\n e-\u003esetInformation(RegInex \\* Eigen::Vector1D::Identity() / medges.size());\n e-\u003ecomputeError();\n optimizer.addEdge(e);\n}\n\noptimizer.optimize(50);\n\n// Reprojection error? ![unknown_filename.3.png](./_resources/DefOptimizer__DefPoseOptimization.resources/unknown_filename.3.png)\n{\n // Iterate over vpEdgesMono\n \\*e = vpEdgesMono\\[i\\];\n ...\n e-\u003ecomputeError();\n auto a = e-\u003eerrorData();\n double er = sqrt(pow(a\\[0\\], 2) + pow(a\\[1\\], 2));\n sumError += er;\n\n cout \u003c\u003c "Reprojection error: " \u003c\u003c sumError / n \u003c\u003c endl;\n}\n\n// new vertex that seems to return the pose from optimiser\nvSE3\\_recov = static\\_cast\u003cg2o::VertexSE3Expmap \\*\u003e(optimizer.vertex(0));\nSE3quat\\_recov = vSE3\\_recov-\u003eestimate();\npose = Converter::toCvMat(SE3quat\\_recov);\n\n',title:"Untitled Page"},"/studienarbeit/defoptimizer-poseoptimization":{content:'---\ntitle: DefOptimizer::poseOptimization\ndate: "2020-10-21"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\nParent: [DefTracking:TrackWithMotionModel()](deftracking_trackwithmotionmodel().md)\n\nint DefOptimizer::poseOptimization(Frame \\*pFrame)\n\n// Set estimate of solution to current camera pose\ng2o::VertexSE3Expmap \\*vSE3 = new g2o::VertexSE3Expmap();\n vSE3-\u003esetEstimate(Converter::toSE3Quat(pFrame\\-\u003emTcw));\n vSE3-\u003esetId(0);\n vSE3-\u003esetFixed(false);\n optimizer.addVertex(vSE3);\n\n// Set MapPoint vertices (num. nodes in opt. graph?)\n  const int N = pFrame\\-\u003eN;\n\n',title:"Untitled Page"},"/studienarbeit/defslam-and-discontinuous-areas-classical-datasets":{content:'---\ntitle: DefSLAM and discontinuous areas (classical datasets)\ndate: "2021-01-15"\nexternal_url: "http://github.com/UZ-SLAMLab/DefSLAM/issues/1"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\n**Parent**: [Lamarca 2020 DefSLAM](studienarbeit/lamarca-2020.md)\n**Source**: \u003chttp://github.com/UZ-SLAMLab/DefSLAM/issues/1\u003e\n\nJoseLamarca:\nDefSLAM is suitable for rigid areas, proof of that is the abdominal sequence that is kind of rigid.\nThe problem for these sequences is the discontinuous areas. For the monocular case, we are assuming that the surface is smooth that is not usually valid for the classical datasets. Apart from complexity issues that algorithms with RGB-D and stereo cameras could have in those scenes \\[1\\] and \\[2\\].\n\nIn our decision to choose an algorithm for the mapping, there are two main state-of-the-art non-rigid reconstruction approaches:\n\n1.  going for orthogonal cameras and assuming that you can see the real size of the object in your image (there are a vast of good works in the literature, very recommended \\[3\\])\n2.  going for perspective cameras and impose isometry or inextensibility.\n\nWe decide to go for perspective cameras due to the nature of our scenarios with strong perspective effects.\n\nThe problem using perspective cameras appears when you try to reconstruct independently each point in a non-rigid environment, you always have gauge freedom in the scale for each point, i.e. you can track a small fly very close to the camera or a large elephant further away having exactly the same measurements in the camera. Without any restrictions, It would be equivalent to do a SLAM independently for every single point, so you have to constrain them in some way.\n\nWhat you can estimate independently of this effect is the normals of the points assuming isometry \\[3,4\\] (in the paper \\[2\\] sec. 5.B). That is why you have to impose a regularization to recover the depth. Following the work of Shaifali in \\[3\\] and Chhatkuli in \\[4\\], we estimate the surface by imposing minimum bending that is a kind of a minimal surface.\n\nIn our case, working inside the body it is a valid assumption for many cases of laparoscopies like the ones presented in the Hamlyn dataset.\nIn contrast, assuming that the points are connected by a smooth surface does not fit with the scenes in the classical datasets like EuroC or TUM at least that you follow a single object. To generalize monocular deformable SLAM in those scenes, we should find another regularization.\n\nThis papers should be enough for a smooth introduction to deformable reconstruction:\n\n\\[1\\]R. A. Newcombe, D. Fox, and S. M. Seitz. Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time. In CVPR, 2015.\n\\[2\\] J. Song, J. Wang, L. Zhao, S. Huang, and G. Dissanayake. Misslam: Real-time large-scale dense deformable slam system in minimally invasive surgery based on heterogeneous computing.\n\\[1\\] Dai, Yuchao, Hongdong Li, and Mingyi He. "A simple prior-free method for non-rigid structure-from-motion factorization."\n\\[2\\] "Defslam: Tracking and mapping of deforming scenes from monocular sequences"\n\\[3\\] S. Parashar, D. Pizarro, and A. Bartoli. Isometric non-rigid shape-from motion with Riemannian geometry solved in linear time.\n\\[4\\] A. Chhatkuli, D. Pizarro, and A. Bartoli. Non-rigid shape-from-motion for isometric surfaces using infinitesimal planarity.\n\n',title:"Untitled Page"},"/studienarbeit/defslam-branch-overview":{content:'---\ntitle: DefSLAM branch overview\ndate: "2021-02-19"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n  - discussion/2021/2021-03\n---\n\nParent: [SA TODO](sa-todo.md)\n\nRepo\n\u003chttp://github.com/feudalism/DefSLAM\u003e\n\nDormant\n\n*   master\n*   sa\n\nDeprecated\n\n*   windows - deprecated, changes made for building on Windows\n*   imu - deprecated, has Imu tracking functions but dependencies not resolved\n*   obs\\_tuple - initial attempt to incorporate Atlas, attempt to use OS3\'s structure for MapPoint observations : \u003cKeyFrame, tuple\u003cint, int\u003e\u003e as opposed to \u003cKeyframe, int\u003e in DefSLAM+OS2\n\nTemporary/Experimental\n\n*   s. to do list\n*   [debugging the segfault](http://github.com/feudalism/DefSLAM/tree/db_segf_nsp) that seemingly appears in Surface::getNormalSurfacePoint\n    seems to happen after System reset\n    \n*   [sockets](http://github.com/feudalism/DefSLAM/commits/sockets)\n\nTo-do\n\n*   [x] code to [read out DefSLAM pose](http://github.com/feudalism/DefSLAM/tree/extractpose) and write\n    *   [x] visualisation\n        *   [x] post process using trajectory.txt\n        *   [ ] [live vis. of trajectory using sockets](http://github.com/feudalism/DefSLAM/commits/sockets) — for later!\n    *   [x] [write access to camera-pose](write-access-to-camera-pose.md)\n        done, but map points in optimisation step push Tcw back to its original trajectory before the update\n        \n*   [ ] [DefSLAM + Kalman](http://github.com/feudalism/DefSLAM/commits/kalman)\n    *   [x] load DefSLAMGT poses to be used as measurements\n    *   [ ] implement filter (prereq: make sure write function works)\n*   [ ] DefSLAM + ORBSLAM3\n    *   [x] System ctor for IMU\\_MONOCULAR case\n        *   [x] Map::SetInertialSensor()\n        *   [x] DefTracking::ctor\n        *   [x] DefLocalMapping::ctor\n        *   [x] ? LoopCloser?\n    *   [ ] System::TrackMonocularIMU\n        *   - [x] Tracker::ResetActiveMap\n        *   [x] Tracker::GrabImuData\n        *   [ ] DefTracker::DefGrabImageMonocular\n            *   [x] ImuFrame ctor\n                *   [x] ImuFrame::ExtractORB (vLapping for stereo)\n            *   [ ] [DefTracker::TrackWithImu](http://github.com/feudalism/DefSLAM/commits/dt_trackwimu)\n                *   [x] Map::IsImuInitialised\n                *   [x] System::Reset\n                    might need to change some stuff here to prevent a segfault occurring\n                    *   [x] Tracker::Reset\n                        *   [x] localMapper.requestReset\n                        *   [x] Atlas::ClearAtlas\n                        *   [x] Atlas::CreateNewMap (new map ptr in OS3, defSLAM reuses old ptr)\n                *   [x] System::ResetActiveMap\n                    *   [x] Tracker::ResetActiveMap\n                        *   [x] localMapper::RequestResetAM\n                        *   [x] Atlas::ClearMap\n                            *   [x] Map::Clear\n                *   [x] PreintegrateImu\n                *   [x] Tracker::MonocularInitialization\n                *   [x] change mCurrentFrame in Tracking from pointer to object\n                *   [ ] TrackWithMotionModel\n                *   [ ] ...\n                *   [ ] TrackLocalMap (main tracking function?)\n                *   [ ] ...\n\nNotes\nMandala dataset: 30 fps (from DefSLAM paper), 640x480 px\n1560936003.993-1560936003.113 (first 30 frames à 1s), i.e. \\*1e-3 to get real timestamp\n\n',title:"Untitled Page"},"/studienarbeit/defslam-dependency-inheritance-diagram":{content:'---\ntitle: DefSLAM dependency/inheritance diagram\ndate: "2020-11-17"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\n![defslam-dependencies.png](./_resources/DefSLAM_dependency_inheritance_diagram.resources/defslam-dependencies.png)\n\n',title:"Untitled Page"},"/studienarbeit/defslam-errors-encountered":{content:'---\ntitle: DefSLAM errors encountered\ndate: "2021-01-13"\nexternal_url: "http://www.google.com/search?client=firefox-b-d\u0026q=segmentation+fault"\ntags:\n  - -sa/processed\n  - software/cpp\n  - SLAM/algos/DefSLAM\n---\n\nRebuilding DefSLAM in Debug mode\nError: "Virtual memory exhausted: Cannot allocate memory"\n![unknown_filename.png](./_resources/DefSLAM_errors_encountered.resources/unknown_filename.png)\nSolution: reduce degree of make -j\n\nSegmentation fault in Defslam debug mode\n![unknown_filename.1.png](./_resources/DefSLAM_errors_encountered.resources/unknown_filename.1.png)\n\nBased on \u003chttp://stackoverflow.com/questions/19615371/segmentation-fault-due-to-vectors\u003e\nChanged: surfacePoints\\_\\[ind\\] to surfacePoints\\_.at(ind)\n\nNew error\n![unknown_filename.2.png](./_resources/DefSLAM_errors_encountered.resources/unknown_filename.2.png)\nsurfacePoints\\_ appears to be NULL? \nWas it instantiated in another thread?\n\u003chttp://stackoverflow.com/questions/11645857/debugging-with-gdb-why-this-0x0\u003e\n\nUsing core dumps with gdb\n\u003chttp://jvns.ca/blog/2018/04/28/debugging-a-segfault-on-linux/\u003e\n\n',title:"Untitled Page"},"/studienarbeit/defslam-framework":{content:'---\ntitle: DefSLAM framework\ndate: "2020-11-20"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n  - -published\n---\n\n**Source**: [Lamarca 2020 DefSLAM](lamarca-2020.md)  \n**See also**: [template-substitution-in-defslam](studienarbeit/template-substitution-in-defslam.md)\n\n"Fusion of the methods available for processing non-rigid monocular scenes"\n\n1.  Deformation tracking \\[front end\\]\n    *   estimates/recovers/optimises:\n        *   camera pose\n        *   scene deformation / deformation of map points (observations)\n    *   the map points are then embedded into the template Tk (to compute their position on the surface)\n    *   operates at frame rate\n    *   SFT-based ([shape from template](studienarbeit/sft.md)), requires prior geometry (template of scene at rest) for the currently being viewed map\n    *   Map points are deformed (updated) by solving an optimisation problem\n        min { reprojection error + deformation energy } per frame\n        \n2.  [Deformation mapping](studienarbeit/mapping-step-by-step-in-defslam.md) \\[back end\\]\n    *   periodically re-estimate template to adapt it to current observation (deformed map points from tracking)\n    *   extends the map as it provides the updated geometry for when new regions are visited\n    *   operates at keyframe rate\n    *   based on NRSfM ([non-rigid structure from motion](studienarbeit/nrsfm.md)), s. also [NRSfM in DefSLAM](studienarbeit/nrsfm-in-defslam.md)\n    *   recovers a surface for the last keyframe, by processing the map points of several keyframes from the same scene region\n    *   from the surface, a deformed map is generated and passed to the tracking thread\n\n![Image.png](./_resources/DefSLAM_framework.resources/Image.png)\n\n',title:"Untitled Page"},"/studienarbeit/defslam-simple-camera":{content:'---\ntitle: DefSLAM simple_camera\ndate: "2020-10-19"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\nWithout ground truth\n\nApp run\n// Create defSLAM::system, which initializes all threads (local mapping, ~~loop closing~~, viewer)\n[defSLAM::System](http://www.evernote.com/shard/s484/nl/217355218/c460b4e1-e0ae-4d47-8423-6d840e453d24) SLAM(orbVocab, calibFile, bUseViewer);\n\n// Timestamp\nuint timestamp := 0;\n\n// Process frames from video capture\nwhile (capture.isOpened())\n{\n // Get the capture as a matrix;\n // SLAM\n SLAM.[TrackMonocular](defslam-system-trackmonocular.md)(img\\_matrix, timestamp);\n timestamp++;\n}\n\n',title:"Untitled Page"},"/studienarbeit/defslam-system-constructor":{content:'---\ntitle: defSLAM::System constructor\ndate: "2020-10-21"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\nParent: [DefSLAM simple\\_camera](defslam-simple__camera.md)\n\ndefSLAM::System::System(const string \u0026strVocFile, const string \u0026strSettingsFile,\n const bool bUseViewer)\n:   mSensor(MONOCULAR),\n mpLoopCloser(NULL),\n mpViewer(static\\_cast\u003cViewer  \\*\u003e(nullptr)),\n mbReset(false),\n mbActivateLocalizationMode(false),\n mbDeactivateLocalizationMode(false)\n\nConstructor\n// initialise mpVocabulary from file\n// create mpKeyFrameDatabase from mpVocabulary\n// create map DefMap()\n// create drawers for viewer DefFrameDrawer DefMapDrawer\n// initialise tracking, mapping, viewer threads; loop closing not implemented in DefSLAM\nmpTracker = new DefTracking(...);\nmpLocalMapper = new DefLocalMapping(...); \nmpViewer = new DefViewer(...);\n\nAttributes\neSensor mSensor\nORBVocabulary \\*mpVocabulary\nKeyFrameDatabase \\*mpKeyFrameDatabase\nMap \\*mpMap // stores pointers to all KFs, all MapPoints\n\nTracking \\*mpTracker         // receives Frame, computes camera pose Tcw, KF insertion\nLocalMapping \\*mpLocalMapper // local map management, local BA\nLoopClosing \\*mpLoopCloser   // search for loop every KF. if there is a loop, do PGO, full BA\n\nViewer \\*mpViewer\nFrameDrawer \\*mpFrameDrawer;\nMapDrawer \\*mpMapDrawer;\n// Threads\n// tracking thread in main execution\nstd::thread \\*mptLocalMapping;\nstd::thread \\*mptLoopClosing;\nstd::thread \\*mptViewer;\nstd::mutex mMutexReset;\nbool mbReset;\nstd::mutex mMutexMode;\nbool mbActivateLocalizationMode;\nbool mbDeactivateLocalizationMode;\nstd::mutex mMutexState;\nstd::mutex mMutexdata;\n\na\n\n',title:"Untitled Page"},"/studienarbeit/defslam-system-trackmonocular":{content:'---\ntitle: defSLAM::System::TrackMonocular\ndate: "2020-10-21"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\nParent: [DefSLAM simple\\_camera](defslam-simple-camera.md)\n\ncv::Mat defSLAM::System::TrackMonocular\ncv::Mat Tcw = mpTracker-\u003e[GrabImageMonocular](tracking-grabimagemonocular)(im, timestamp);\n\n// get information from mpTracker:\n// get states mTrackingState\n// get map points mTrackedMapPoints\n// get key points mTrackedKeyPointsUn\n\n// return camera pose\nreturn Tcw;\n\n',title:"Untitled Page"},"/studienarbeit/deftracking-monocularinitialization":{content:'---\ntitle: DefTracking::MonocularInitialization\ndate: "2020-10-28"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\nParent: [DefTracking::Track](deftracking__track.md)\n\nInitialises\n\n*   surface\n*   points in the surface\n\nIf num. features in current frame \u003e 100\n\n*   set frame pose to origin\n*   make new KF (GroundTruthKeyFrame) pKFini\n*   add the KF to the map mpMap\n*   iterate over the N features\n    *   get feature kp\n    *   convert kp to 3d point\n    *   make new DefMapPoint(3dp, pKFini, mpMap)\n    *   set pointers between DefMapPoint, GroundTruthKeyFrame, DefMap\n*   Save surface using bbs\n*   Set mLastFrame := mCurrentFrame\n*   **Local window**: Add KF to local KFs vector, add MapPoints to local MP vector, mpLocalMapper\n*   Calculate Tcr from Tcw\n*   **Initialise SLAM**: Set reference KF, reference MapPoints\n*   set mState to OK\n\n',title:"Untitled Page"},"/studienarbeit/deftracking-track":{content:'---\ntitle: DefTracking::Track\ndate: "2020-10-21"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\nParent: [Tracking::GrabImageMonocular](tracking__grabimagemonocular.md)\n\nvoid DefTracking::Track\n// if: not initialised, do: [monocular initialisation](http://www.evernote.com/shard/s484/nl/217355218/b32ef55f-1e31-486d-8a80-9dfd4226618e)\n// elseif: already initialised, do: track frame\n{\n // if: tracking and mapping, do:\n bOK = [TrackWithMotionModel](http://www.evernote.com/shard/s484/nl/217355218/b44c4be7-f66b-4739-90da-6710241bae2e)();\n\n // if bOK (there exists camera pose estimate and matching), track local map\n // if template is updated (keyframe-rate update)\n set reference KF from new template\n do [DefPoseOptimization](http://www.evernote.com/shard/s484/nl/217355218/9a19739f-c04f-4008-8822-d782ed4f5e2a)(...);\n bOK = TrackLocalMap();\n\n // if: bOK, update motion model (update mVelocity); clean VO matches\n // check if we should insert a new KF, delete outliers for BA\n\n}\n\n',title:"Untitled Page"},"/studienarbeit/deftracking-trackwithmotionmodel-":{content:'---\ntitle: DefTracking:TrackWithMotionModel()\ndate: "2020-10-21"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\nParent: [DefTracking::Track](deftracking__track.md)\n\n// Initial tracking to locate rigidly the camera and discard outliers.\nbool DefTracking::TrackWithMotionModel()\n\n// Update last frame relative pose according to its reference keyframe\nUpdateLastFrame();\n\n// Project points seen in prev frames\nint th = 15;\nint nmatches = Defmatcher.SearchByProjection(\\*mCurrentFrame, mLastFrame, th,\n mSensor == System::MONOCULAR);\n\n// Optimise frame pose with all matches to initialise camera pose\nOptimizer::[poseOptimization](http://www.evernote.com/shard/s484/nl/217355218/c3625cbe-ddbe-4cbd-b447-00913cf6370d)(mCurrentFrame, myfile);\n\n// Discard outliers\n\n// return: sufficient number of matches?\nreturn nmatchesMap \u003e= 10;\n\n',title:"Untitled Page"},"/studienarbeit/dense-direct-vslam":{content:'---\ntitle: Dense/direct VSLAM\ndate: "2020-07-30"\ntags:\n  - SLAM/sparse-vs-dense/direct-SLAM\n  - -sa/processed\n  - -published\n---\n\nParent: [Visual SLAM Implementation Framework](SLAM/vslam-framework.md), [slam_index](SLAM/slam_index.md)\nSee also: [Feature-based vs direct SLAM workflow](feature-based-vs-direct-slam-workflow.md)\n\nSource: [cometlabs](bibliography/cometlabs.md)\n\n*   Front-end part of the [Visual SLAM Implementation Framework](SLAM/vslam-framework.md)\n*   Use most or all of the pixels in each received frame\n    *   Provide more information about the environment\n*   Many more pixels, require GPUs\n*   [Feature-based vs direct SLAM workflow](feature-based-vs-direct-slam-workflow.md)\n*   Disadvantages:\n    *   Don\'t handle outliers very well (outliers will be processed and implemented into the final map)\n    *   Slower than [feature-based](studienarbeit/sparse-feature-based-vslam.md) variants\n*   Aims to minimise photometric error (intensity differences)\n\n![unknown_filename.1.png](./_resources/Dense_direct_VSLAM.resources/unknown_filename.1.png)\nSemi-dense\n\n![unknown_filename.png](./_resources/Dense_direct_VSLAM.resources/unknown_filename.png)\nDense map\n\n',title:"Untitled Page"},"/studienarbeit/deriving-kalman-filter-from-discrete-bayes-using-gaussians":{content:'---\ntitle: Deriving Kalman filter from Discrete Bayes using Gaussians\ndate: "2020-08-31"\ntags:\n  - -sa/processed\n  - filters/kalman-filter\n  - filters/bayesian-filter\n---\n\nParent: [1D Kalman filters](1d-kalman-filters.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nReplacing discrete Bayes with Gaussian distributions\n![unknown_filename.png](./_resources/Deriving_Kalman_filter_from_Discrete_Bayes_using_Gaussians.resources/unknown_filename.png)\nwhere the operators in the circles are as of yet undetermined\n\n',title:"Untitled Page"},"/studienarbeit/descriptors-in-feature-detection-extraction":{content:"---\ntitle: Descriptors in feature detection/extraction\ndate: \"2020-12-08\"\ntags:\n  - -definitions\n  - -sa/processed\n  - vision\n---\n\n**Source**: \u003chttp://medium.com/data-breach/introduction-to-feature-detection-and-matching-65e27179885d\u003e  \n**Backlinks**: [Bag of words](bag-of-words.md)\n\n## Descriptors\n*   A description of the local appearance around each feature point (keypoint)\n*   The descriptor encodes 'interesting' information from the image into numbers and act as an identifier ('fingerprint') to differentiate between features\n*   The description should ideally be invariant to changes (such as illumination, translation, scale, in-plane rotation) so that the feature can be found again, even if the image is transformed\n*   Typically: for each feature point, there is a descriptor vector\n\n### Classes of descriptors:\n\n*   Local descriptor\n    *   represents the point's local neighbourhood\n*   Global descriptor\n    *   describes the whole image\n    *   generally not very robust—changes in parts of the image may cause the descriptor to fail\n\n### Some algorithms for feature detection/descriptor generation\n\n*   SIFT (scale-invariant feature transform)\n*   SURF (speeded up robust feature)\n*   BRISK (binary robust invariant scalable keypoints)\n*   BRIEF (binary robust independent elementary features)\n*   [ORB](studienarbeit/orb-descriptor.md) (oriented [FAST](studienarbeit/fast-keypoint-detector.md) and rotated BRIEF)\n\n---\n\n**Source**: [http://en.wikipedia.org/wiki/Bag-of-words\\_model\\_in\\_computer\\_vision](http://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision)\n\nFeature representation using feature descriptors\n\n*   Descriptors: vectors that represent local patches in the image\n*   A good descriptor should be able to handle transformations such as rotation, intensity change, scale, etc.\n*   e.g. SIFT: Scale-invariant feature transform\n    *   converts a patch to a vector in 128 dimensions\n    *   image as a bag of words -- image as a collection of SIFT vectors -- image as a collection of sparse numerical vector\n\n",title:"Untitled Page"},"/studienarbeit/diagnosis-bladder-cancer":{content:'---\ntitle: Diagnosis bladder cancer\ndate: "2021-07-30"\ntags:\n  - -resources\n  - -sa/to-be-processed\n---\n\n\u003chttp://www.cancer.org/cancer/bladder-cancer/detection-diagnosis-staging/how-diagnosed.html\u003e\n\n',title:"Untitled Page"},"/studienarbeit/diagram-imu-on-cystoscope":{content:'---\ntitle: IMU on cystoscope\ndate: "2021-05-21"\ntags: \n- -sa/processed \n- math/kinematics \n- discussion/2021/2021-05 \n- medical/surgery/endoscope\n---\n\nBacklinks: [Discussion 2021-05-21](discussion-2021-05-21.md)\n\n![unknown_filename.jpeg](./_resources/Diagram__IMU_on_cystoscope.resources/unknown_filename.jpeg)\n\n',title:"Untitled Page"},"/studienarbeit/difference-between-haptic-feedback-and-vibration-alerting":{content:'---\ntitle: Difference between haptic feedback and vibration alerting\ndate: "2020-06-19"\nexternal_url: "http://www.precisionmicrodrives.com/haptic-feedback/introduction-to-haptic-feedback/"\ntags:\n  - misc/haptic-feedback\n  - -sa/processed\n---\n\n**Source**: \u003chttp://www.precisionmicrodrives.com/haptic-feedback/introduction-to-haptic-feedback/\u003e\n**Parent**:  [Scope of Studienarbeit](scope-of-studienarbeit.md)\n\n![unknown_filename.jpeg](./_resources/Difference_between_haptic_feedback_and_vibration_alerting.resources/unknown_filename.jpeg)\n\n',title:"Untitled Page"},"/studienarbeit/differentiation-in-different-coordinate-systems":{content:'---\ntitle: Differentiation in different coordinate systems\ndate: "2021-06-27"\ntags:\n  - -sa/processed\n  - math/kinematics\n---\n\nSource: [Woernle Mehrkörpersysteme](woernle-mehrkörpersysteme.md)\nBacklinks: [Kinematics primer](kinematics-primer.md)\n\n![unknown_filename.2.png](./_resources/Differentiation_in_different_coordinate_systems.resources/unknown_filename.2.png)\n\n![unknown_filename.png](./_resources/Differentiation_in_different_coordinate_systems.resources/unknown_filename.png)\n\n![unknown_filename.1.png](./_resources/Differentiation_in_different_coordinate_systems.resources/unknown_filename.1.png)\n\n',title:"Untitled Page"},"/studienarbeit/discretising-a-state-space-equation":{content:'---\ntitle: Discretising a state space equation\ndate: "2021-03-31"\ntags:\n  - math\n  - -sa/to-be-processed\n---\n\n**Source**: [http://en.wikibooks.org/wiki/Control\\_Systems/State-Space\\_Equations\n- Discretization](http://en.wikibooks.org/wiki/Control_Systems/State-Space_Equations\n- Discretization)\n\nDiscretising a state space equation\n![unknown_filename.png](./_resources/Discretising_a_state_space_equation.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/discussion-2020-11-03":{content:'---\ntitle: Discussion 2020-11-03\ndate: "2020-11-02"\ntags:\n  - -sa/processed\n  - discussion/2020/2020-11\n---\n\nCurrent progress\n- [x] Find out how g2o works, how the DefSLAM implementation of the tracking optimisation works\n- [ ] Incorporate IMU data (s. ORBSLAM3 implementation) \n- [ ] IMU initialisation\n- [ ] IMU preintegration\n- [ ] IMU terms in cost function (which function?)\n\nTopics\n\n*   How g2o works\n*   IMU preintegration\n*   DefSLAM + IMU cost function\n*   Implementation in DefSLAM using g2o\n*   \\[tbd\\] ORBSLAM3\'s implementation\n    *   the IMU cost function terms\n    *   initialisation\n    *   preintegration\n*   Kalman idea for IMU integration\n*   Compile + run\n\n',title:"Untitled Page"},"/studienarbeit/discussion-2021-05-10":{content:'---\ntitle: Discussion 2021-05-10\ndate: "2021-05-10"\ntags:\n  - -resources\n  - filters/EKF\n  - -sa/processed\n  - discussion/2021/2021-05\n---\n\nAgenda\n\n*   Change/Reduction of scope of SA (from fusing IMU with camera) to using sensor fusion to determine transformation parameters between IMU and camera\n*   Camera and IMU setup involves kinematic modelling (not fixed transformation as previously assumed!)\n*   Offline implementation in Python/MATLAB (scripting language)\n*   HiWi tasks can include\n    *   DefSLAM bindings / interface\n    *   C++ bindings of skrogh EKF implementation?\n*   HiWi prioritises Versuchsstand for now\n\nTasks\n\n1.  [x] Find an EKF implementation that works well and can be used with DefSLAM + IMU data\n2.  [x] implement kinematic model equations in the prediction-step, s.  [discussion-2021-05-25](discussion-2021-05-25.md)\n\n![unknown_filename.jpeg](./_resources/Discussion_2021-05-10.resources/unknown_filename.jpeg)\n\nLinks related to Task 1\n[http://github.com/search?o=desc\u0026q=ekf+slam\u0026s=stars\u0026type=Repositories](http://github.com/search?o=desc\u0026q=ekf%2Bslam\u0026s=stars\u0026type=Repositories)\n\n*   [http://github.com/ydsf16/aruco\\_ekf\\_slam](http://github.com/ydsf16/aruco_ekf_slam) : uses ROS? C++\n*   \u003chttp://github.com/JzHuai0108/ekfmonoslam\u003e : MATLAB\n*   \u003chttp://github.com/AtsushiSakai/PythonRobotics\u003e : doc \u003chttp://pythonrobotics.readthedocs.io/en/latest/modules/localization.html\n- extended-kalman-filter-localization\u003e — hard coded pred. eqns (vel model?) (can be overloaded?)\n*   \u003chttp://filterpy.readthedocs.io/en/latest/kalman/ExtendedKalmanFilter.html\u003e : nbviewer \u003chttp://nbviewer.jupyter.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/11-Extended-Kalman-Filters.ipynb\u003e\n\n\u003chttp://github.com/micropython-IMU/micropython-fusion\u003e\n\u003chttp://github.com/enginBozkurt/Error-State-Extended-Kalman-Filter\u003e\n\n',title:"Untitled Page"},"/studienarbeit/discussion-2021-05-21":{content:'---\ntitle: Discussion 2021-05-21\ndate: "2021-05-21"\ntags:\n  - -sa/processed\n  - discussion/2021/2021-05\n---\n\nNotes — Before\n\n*   Got the ESKF implementation (Solà) to work with my fake IMU data\n    *   \\[non-noisy IMU\\] results look ok for low process noise (trust the prediction more) with relatively high measurement noise\n    *   \\[noisy IMU\\] ok?\n    *   current assumptions/simplifications:\n        *   fake data assumes IMU is sitting right on top of camera\n        *   fake data, as of yet, does not take into account: biases, gravity\n        *   simplified state vector (no scale estimate, no gravity estimate, no bias estimate etc)\n        *   TBD: [x] modify equations/states to fit the problem, i.e. to obtain the imu-camera trafo as an estimate instead of the imu/cam-position, s. [discussion-2021-05-25](discussion-2021-05-25.md)\n    *   changes to code\n        *   propagation function explicitly partitioned into: \\_predict\\_nominal, \\_predict\\_error, \\_predict\\_cov\n        *   used the update equations/Jacobians given in the tutorial (Solà)\n*   Had a look at FilterPy\n\nNotes — After\n[Endoscope/cystoscopy pics/videos](endoscope_cystoscopy-pics_videos.md)\n[IMU on cystoscope](imu-on-cystoscope.md)\n\nNext step\n- [x] come up with equations for this, where the transformation for l1 is initially fixed (to be estimated later using ekf). l2 is assumed to be a rigid transformation. (s. [probe-forward-kinematics](probe-forward-kinematics.md))\n![unknown_filename.png](./_resources/Discussion_2021-05-21.resources/unknown_filename.png)\n\n![unknown_filename.1.jpeg](./_resources/Discussion_2021-05-21.resources/unknown_filename.1.jpeg)\n\nStuff I need\n\n*   equations for calibration parameters (for predict\\_nominal), maybe random walk?\n*   Jacobians for predict\\_error\n*   corresponding process noise ... from actors?\n*   noise values\n\n',title:"Untitled Page"},"/studienarbeit/discussion-2021-05-25":{content:'---\ntitle: Discussion 2021-05-25\ndate: "2021-05-25"\ntags:\n  - -sa/processed\n  - discussion/2021/2021-05\n---\n\nBacklinks: [Discussion 2021-05-10](discussion-2021-05-10.md), [discussion-2021-05-21](discussion-2021-05-21.md)\n\nNotes\n\n*   IMU-rod transformation: rotation part (spherical joint), translation part\n*   predict and update equations?\n*   maybe change variables in states vector to local coordinates\n*   add gravity later\n*   Ausblick: Einfluss der IMU auf verbesserte Lokalisierung --\u003e evtl eine IMU Koordinate weglassen\n\nNext:\n- [x] ~~generate fake imu data~~ (delegated, s. [obtaining imu measurements from camera by forward kinematics](obtaining imu measurements-from-camera-by-forward-kinematics.md))\n- [x] look for existing literature on IMU fusion/EKF which uses kinematic relations\n- [x] Massenmatrix, Koriolisterme etc.\n![unknown_filename.2.png](./_resources/Discussion_2021-05-25.resources/unknown_filename.2.png)\n\nGedachtjes\nmisschien zouden de onbekende hoeken theta\\_1:3 uit de spherical joint direct in de toestandsvector terrible idea, nonlinearity\n\nStuff to read:\n\n*   [Jeon 2009 Kinematic Kalman Filter for Robot End-Effector Sensing](jeon 2009 kinematic kalman-filter-for-robot-end-effector-sensing.md)\n*   Calibration of DH-params (skimmed, scrapped)\n    *   Du2014 - Online robot calibration based on hybrid sensors using Kalman Filters\n    *   Lee 2012 - Geometrical Derivation  of Differential Kinematics  to Calibrate Model Parameters  of Flexible Manipulator\n*   Misc\n    *   Du2013 - IMU-based online kinematic calibration of robot manipulator\n    *   Du2013 - Online robot calibration based on vision measurement\n    *   Kim 2004 - Initial calibration of an inertial measurement unit using an optical position tracking system\n\nAnhang\n\n![unknown_filename.1.png](./_resources/Discussion_2021-05-25.resources/unknown_filename.1.png)\n\n![unknown_filename.jpeg](./_resources/Discussion_2021-05-25.resources/unknown_filename.jpeg)\n\n',title:"Untitled Page"},"/studienarbeit/discussion-2021-06-01":{content:'---\ntitle: Discussion 2021-06-01\ndate: "2021-06-07"\ntags:\n  - -sa/processed\n  - discussion/2021/2021-06\n---\n\nNotes\n\n*   IMU data to be generated using kinematic relations, not via numerical differentiation\n    *   Reduce loss of data, model for prediction, noise propagation\n*   Forward kinematics B --\u003e C (everything in terms of SLAM coordinates), s. [Probe forward kinematics](probe-forward-kinematics.md)\n*   For visualisation: IMU data in W coordinates\n*   Monday: real probe\n*   Python robotics toolboxes for generating of forward kinematics matrices, velocity expressions (symbolic differentiation)\n\nPredict step\n\n1.  Kinematics ([equations of motion IMU to camera](equations-of-motion-imu-to-camera.md))  = f(DOF)\n    p\\_BC = f(l1, l2)\n    v\\_BC = f(ang\\_vel)\n    a\\_BC = f(acc)\n    \n    OR\n    \n    Forward kinematics\n    \n2.  Get IMU data from above step\n    IMU = (ang\\_vel, acc)\n    s.  [Chaining rotation matrices and angular velocities](chaining-rotation-matrices-and-angular-velocities.md) to get om\\_B from om\\_C, om\\_BC\n    \n3.  Prediction equation\n    x\\_dot = f(ang\\_vel, acc)\n    \n    x = \\[p\\_B, v\\_B, rot\\_B, DOF\\]\n    \n\n![unknown_filename.png](./_resources/Discussion_2021-06-01.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/distal-and-proximal-ends":{content:'---\ntitle: Distal and proximal ends\ndate: "2021-07-21"\ntags:\n  - -definitions\n  - -sa/processed\n  - medical/surgery/endoscope\n  - -published\n---\n\n**Source**: [Leiner](leiner.md)\n\ndistal: far from the surgeon  \nproximal: near the surgeon\n\n',title:"Untitled Page"},"/studienarbeit/durrant-whyte-2006-slam-tutorial-part-i":{content:'---\ntitle: Durrant-Whyte 2006 SLAM Tutorial Part I\ndate: "2020-08-25"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-to-read\n  - -sa/to-be-processed\n---\n\n**Backlinks**: [Works of possible interest](bibliography/works-of-interest.md)\n\nAuthors: Bailey, Durrant-Whyte\n\nAbstract:\n\nContents/Chapters\n\nTakeaway\n\n',title:"Untitled Page"},"/studienarbeit/dynamic-bayesian-network-formulation-of-slam":{content:'---\ntitle: Dynamic Bayesian Network formulation of SLAM\ndate: "2020-10-24"\ntags:\n  - to-do/missing-tag\n  - -sa/processed\n---\n\nSource: [Grisetti 2011 - Tutorial graph-based SLAM](grisetti-2011---tutorial-graph-based-slam.md)\n\nDynamic Bayesian Network\n\n![unknown_filename.png](./_resources/Dynamic_Bayesian_Network_formulation_of_SLAM.resources/unknown_filename.png)\nSolution of full SLAM problem:  ![unknown_filename.1.png](./_resources/Dynamic_Bayesian_Network_formulation_of_SLAM.resources/unknown_filename.1.png)\nTransition model: ![unknown_filename.3.png](./_resources/Dynamic_Bayesian_Network_formulation_of_SLAM.resources/unknown_filename.3.png)\nObservation model: ![unknown_filename.2.png](./_resources/Dynamic_Bayesian_Network_formulation_of_SLAM.resources/unknown_filename.2.png)\n\n*   The observation model is usually multimodal: a single observation may result in multiple edges (in the spatial graph)\n*   Therefore, the Gaussian assumption does not hold\n\n',title:"Untitled Page"},"/studienarbeit/edges-in-g2o":{content:'---\ntitle: Edges in g2o\ndate: "2020-10-22"\ntags:\n  - -sa/processed\n  - discussion/2020/2020-10\n  - software/g2o\n---\n\n![unknown_filename.png](./_resources/Edges_in_g2o.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/effects-of-varying-g":{content:'---\ntitle: Effects of varying g\ndate: "2020-08-27"\ntags:\n  - -sa/processed\n  - filters/gh-filter\n---\n\nParent: [Calculating the estimated state in the GH-filter](calculating the-estimated-state-in-the-gh-filter.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\n![unknown_filename.png](./_resources/Effects_of_varying_g.resources/unknown_filename.png)\n\nThe greater the g value, the more we follow the measurements rather than rely on our \\[model-based\\] predictions.\n\n',title:"Untitled Page"},"/studienarbeit/effects-of-varying-h":{content:"---\ntitle: Effects of varying h\ndate: \"2020-08-27\"\ntags:\n  - -sa/processed\n  - filters/gh-filter\n---\n\nParent [Improving the gh-filter by using h](improving-the-gh-filter-by-using-h.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\n![unknown_filename.png](./_resources/Effects_of_varying_h.resources/unknown_filename.png)\n\nThe greater the h value, the more we trust the rate of change that we can derive from the measurement data.\n\n*   a larger h enables us to react to transient (initial condition dependent) changes more rapidly.\n    Because if we have a large difference between our chosen IC and the measurement, this results in a huge residual velocity\n    \n*   the h term responds to changes in velocity that may not be present in the mathematical model we choose (change in velocity is then 'seen' in the sensor data)\n\n",title:"Untitled Page"},"/studienarbeit/ekf-2-reobservation":{content:'---\ntitle: Step 2 Re-observation\ndate: "2020-07-29"\ntags: \n- filters/EKF \n- -sa/processed\n---\n\n**Parent**: [Basic EKF for SLAM](SLAM/basic-ekf-for-slam.md)\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\nSecond step in the three-step EKF — overview\n\n*   In this step we update the robot position that we got in [step 1]](studienarbeit/step-1-odometry-update-prediction-step.md)\n    *   Compensate for errors due to odometry\n        pos\\_est (odometry-based) - pos\\_actual (LM-based) = Innovation, (based on the LM that the robot can see)\n        \n    *   Use this to update robot position\n*   Update the uncertainty of each observed LM to reflect recent changes e.g.\n    *   very low uncertainty of a LM.\n    *   Re-observing the LM from the same position with low uncertainty will increase the LM certainty (s. [loop closure](SLAM/loop-closure-detection.md))\n    *   (variance of LM w.r.t. current robot position)\n*   This step is run for each re-observed landmark\n\n1.  Calculate range and bearing to the landmark (range and bearing in current measurements) using the [measurement model](studienarbeit/measurement-model.md)\n    This can be compared to the range and bearing obtained from data association (denoted as z)\n    \n2.  Calculate the [Jacobian H](jacobian-h.md) of the measurement model\n3.  Update the [error matrix R](http://www.evernote.com/shard/s484/nl/217355218/fc4f284b-8751-4c90-9253-94898f905f97) to reflect range and bearing in current measurements\n4.  Compute Kalman gain\n    1.  ![unknown_filename.png](./_resources/Step_2__Re-observation.resources/unknown_filename.png)\n    2.  term in bracket is S (innovation covariance)\n5.  Compute new [state vector](http://www.evernote.com/shard/s484/nl/217355218/9090a6f9-c3ba-4927-be3a-496e4aa93d4c) (range and bearing) using Kalman gain\n    1.  ![unknown_filename.1.png](./_resources/Step_2__Re-observation.resources/unknown_filename.1.png)\n    2.  this updates robot pose along with landmark positions\n    3.  z - h = v (displacement in range and bearing)\n\n',title:"Untitled Page"},"/studienarbeit/ekf-3-new-landmarks":{content:'---\ntitle: Step 3 New landmarks\ndate: "2020-07-29"\ntags: \n- filters/EKF \n- localisation/landmarks \n- -sa/processed\n---\n\n**Parent**: [Basic EKF for SLAM](SLAM/basic-ekf-for-slam.md)\n\n**Source**: [SLAM for dummies](bibliography/riisgaard-slam-for-dummies.md)  \n\nOverview\n\n*   Landmarks that are new are not dealt with until step 3.\n    *   Delaying the incorporation of new landmarks until the will decrease the computation cost needed for this step\n    *   the covariance matrix, P, and the system state, X, are smaller by then.\n*   Update state vector x and covariance matrix P with new landmarks\n\n1.  Add new landmark to state vector X\n2.  Add new row and column to [covariance matrix](SLAM/covariance-matrix-p.md)\n    1.  Covariance for new landmark\n    2.  Robot-landmark covariance\n\n',title:"Untitled Page"},"/studienarbeit/ekf-matrices-vectors":{content:'---\ntitle: EKF matrices/vectors\ndate: "2020-07-27"\ntags:\n  - filters/EKF\n  - -sa/processed\n---\n\n**Source**: [SLAM for Dummies](bibliography/riisgaard-slam-for-dummies.md)\n\n1.  [System state X](http://www.evernote.com/shard/s484/nl/217355218/9090a6f9-c3ba-4927-be3a-496e4aa93d4c)\n2.  [Estimate of POSE](estimate-of-pose.md)  \n    [Jacobian of prediction model](SLAM/prediction-model.md)\n    \n3.  [Landmark range and bearing](http://www.evernote.com/shard/s484/nl/217355218/54e21e37-2184-46fe-b494-01ef63b3b2eb)\n    [Jacobian of measurement model](http://www.evernote.com/shard/s484/nl/217355218/54e21e37-2184-46fe-b494-01ef63b3b2eb)\n    \n4.  [Covariance matrix P](SLAM/covariance-matrix-p.md)\n5.  [Kalman gain K](http://www.evernote.com/shard/s484/nl/217355218/0803ff71-43b4-4605-85fd-361715d8345e)\n6.  [SLAM-specific jacobians](slam-specific-jacobians.md)\n\n',title:"Untitled Page"},"/studienarbeit/ekf-system-state":{content:'---\ntitle: EKF System State\ndate: "2020-08-06"\ntags:\n  - filters/EKF\n  - -sa/processed\n---\n\nSource: [SLAM for Dummies](slam-for-dummies.md)\nBacklinks:  [EKF matrices](ekf-matrices.md), [step-2:-re-observation](studienarbeit/ekf-2-reobservation.md)\n\n*   Contains robot POSE and landmark position\n*   POSE: (x y theta)\\_r\n*   LM: (x, y)\\_l1 ... (x,y)\\_ln; n = num. of landmarks\n*   Size: 3+2n rows\n\n',title:"Untitled Page"},"/studienarbeit/empirical-rule-68-95-99.7":{content:'---\ntitle: Empirical rule 68/95/99.7\ndate: "2020-08-31"\ntags:\n  - -definitions\n  - -sa/processed\n  - math/statistics\n---\n\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nEmprical rule, a.k.a. [68–95–99.7 rule](http://en.wikipedia.org/wiki/68%25E2%2580%259395%25E2%2580%259399.7_rule)\nAbout 68% of all values lie within one [standard deviation](http://www.evernote.com/shard/s484/nl/217355218/4f00112b-f7b3-46f3-84ca-cd033e23a5c2) of the mean.\n![unknown_filename.png](./_resources/Empirical_rule_68_95_99.7.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/endoscope-cystoscopy-pics-videos":{content:'---\ntitle: Endoscope/cystoscopy pics/videos\ndate: "2021-05-21"\nexternal_url: "http://www.maestro-portal.eu/procedure/detail/4"\ntags:\n  - -resources\n  - -sa/processed\n  - discussion/2021/2021-05\n  - medical/surgery/endoscope\n---\n\nBacklinks: [Discussion 2021-05-21](discussion-2021-05-21.md)\n\nRigid endoscope for cystoscopy\n\nSource: \u003chttp://www.ebay.com/itm/113780645426\u003e\n![unknown_filename.png](./_resources/Endoscope_cystoscopy_pics_videos.resources/unknown_filename.png)\n\nSource: [http://www.researchgate.net/figure/Intraoperative-image-of-the-rigid-cystoscope-entering-the-bladder-through-the-screw-tip\\_fig2\\_322897289](http://www.researchgate.net/figure/Intraoperative-image-of-the-rigid-cystoscope-entering-the-bladder-through-the-screw-tip_fig2_322897289)\n![unknown_filename.1.png](./_resources/Endoscope_cystoscopy_pics_videos.resources/unknown_filename.1.png)\n\nSource: [http://www.youtube.com/watch?v=1gEpz9wijoY](http://www.youtube.com/watch?v=1gEpz9wijoY\u0026feature=youtu.be)\n![unknown_filename.2.png](./_resources/Endoscope_cystoscopy_pics_videos.resources/unknown_filename.2.png)![unknown_filename.3.png](./_resources/Endoscope_cystoscopy_pics_videos.resources/unknown_filename.3.png)\n\n\u003chttp://www.maestro-portal.eu/procedure/detail/4\u003e\nVideos: Semi-Rigid Ureteroscopy and Laser Lithotripsy for Ureter Stones\n\nSource: [http://www.medicinenet.com/how\\_painful\\_is\\_a\\_cystoscopy/article.htm](http://www.medicinenet.com/how_painful_is_a_cystoscopy/article.htm)\n![cystoscopy-and-ureteroscopy.jpg](http://images.medicinenet.com/images/article/main_image/cystoscopy-and-ureteroscopy.jpg)\n\n',title:"Untitled Page"},"/studienarbeit/endoscope-tip":{content:'---\ntitle: Endoscope tip\ndate: "2021-06-14"\ntags:\n  - medical/surgery/endoscope\n  - -sa/to-be-processed\n---\n\n**Source**: \u003chttp://www.osapublishing.org/ao/fulltext.cfm?uri=ao-43-1-113\u0026id=78236\n- F2\u003e\nNote: this is a mini endoscope, probably not the standard construction\n\n![unknown_filename.png](./_resources/Endoscope_tip.resources/unknown_filename.png)\n\n![unknown_filename.1.png](./_resources/Endoscope_tip.resources/unknown_filename.1.png)\n\n',title:"Untitled Page"},"/studienarbeit/equations-for-obtaining-omega-angular-velocity-and-acceleration-of-imu-from-camera":{content:'---\ntitle: Equations for obtaining omega (angular velocity) and acceleration of IMU\n  from camera\ndate: "2021-06-11"\ntags:\n  - -sa/processed\n  - math/kinematics\n  - discussion/2021/2021-06\n---\n\n**Parent**: [Update 2021-06-11](update-2021-06-11.md)\n\n![unknown_filename.png](./_resources/Equations_for_obtaining_omega_(angular_velocity)_and_acceleration_of_IMU_from_camera.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/error-ellipse-confidence-ellipse":{content:'---\ntitle: Error ellipse/Confidence ellipse\ndate: "2020-09-01"\ntags:\n  - -definitions\n  - -sa/processed\n  - math/statistics\n---\n\nParent: [Multivariate Gaussian distributions](multivariate-gaussian-distributions.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nAny slice through a multivariate Gaussian is an ellipse\n![unknown_filename.png](./_resources/Error_ellipse_Confidence_ellipse.resources/unknown_filename.png)\nPlots show the slice for 3 standard deviations\n\n[Showing correlation using error ellipses](showing-correlation-using-error-ellipses.md)\n\n',title:"Untitled Page"},"/studienarbeit/eskf-repos":{content:'---\ntitle: ESKF repos\ndate: "2021-08-12"\ntags:\n  - -resources\n  - -sa/processed\n  - discussion/2021/2021-08\n---\n\nC++\n[http://github.com/skrogh/msf\\_ekf](http://github.com/skrogh/msf_ekf)\n\u003chttp://github.com/je310/ESKF\u003e\n[http://github.com/hobbeshunter/IMU\\_EKF](http://github.com/hobbeshunter/IMU_EKF) (only IMU)\n\nPython\n\u003chttp://github.com/enginBozkurt/Error-State-Extended-Kalman-Filter\u003e \n[http://github.com/uoip/stereo\\_vio\\_eskf](http://github.com/uoip/stereo_vio_eskf) (unsuccessful) -- uses average IMU readings\n[http://github.com/aipiano/ESEKF\\_IMU](http://github.com/aipiano/ESEKF_IMU)\n\n',title:"Untitled Page"},"/studienarbeit/euler-angles":{content:"---\ntitle: Euler angles\ndate: \"2021-08-15\"\nexternal_url: \"http://en.wikipedia.org/wiki/Davenport_chained_rotations\"\ntags:\n  - -sa/processed\n  - math/rotations\n  - -published\n---\n\n**Parents**: [rotations-so3-group-index](rotations/rotations-so3-group-index.md), [orientation-parametrisations](orientation-parametrisations.md)  \n\n**Source**: [Phil's Lab](bibliography/phils-lab-sensor-fusion.md)\n* Three angles that describe the orientation of an object w.r.t. a *fixed* coordinate system\n* Roll $\\phi$, Pitch $\\theta$, Yaw $\\psi$\n\n---\n\n**Source**: [http://en.wikipedia.org/wiki/Euler\\_angles](http://en.wikipedia.org/wiki/Euler_angles)\n\n## Possible representations\n\n*   Proper Euler angles (e.g. $zxz$) vs Tait-Bryan (e.g. $xyz$, $zyx$)\n*   [Intrinsic vs. extrinsic rotations](rotations/intrinsic-vs-extrinsic-rotations.md)\n	*   Extrinsic rotations (around fixed CS $xyz$)\n	*   Intrinsic rotations (around body CS $XYZ = x''' y''' z'''$)\n\n## As a rotation matrix\n$$R = X(\\alpha) Y(\\beta) Z(\\gamma)$$\n\nThis means either: (s. [Intrinsic vs extrinsic rotations](rotations/intrinsic-vs-extrinsic-rotations.md))\n\n*   extrinsic rotations about z -\u003e y -\u003e x / yaw pitch roll\n*   intrinsic rotations about x -\u003e y' -\u003e z'' = Z = z'''\n\n**Note**:  Any extrinsic rotation is equivalent to an intrinsic rotation by the same angles but with inverted order of elemental rotations, and vice versa. \\[[Source](http://en.wikipedia.org/wiki/Davenport_chained_rotations)\\]\n\nIntrinsic rotations $x-y’-z″$ by angles $\\alpha, \\beta, \\gamma$ are equal to extrinsic rotations $z-y-x$ by angles $\\gamma, \\beta, \\alpha$.\n\n![unknown_filename.2.png](./_resources/Euler_angles.resources/unknown_filename.2.png)\n\nDefinition of X(alp), Y(beta), Z(gamma) \\[elemental rotation matrix\\] depends on convention chosen\n\nTable of Euler rotation matrices (RH, active, intrinsic):\n\n| Proper Euler | Tait-Bryan |\n| --- | --- |\n|     | ![unknown_filename.1.png](./_resources/euler_angles.resources/unknown_filename.1.png)\u003cbr\u003es. derivation here:  [bryan-tait-kardanwinkel](rotations/bryan-tait-kardanwinkel.md) |\n|     | ![unknown_filename.png](./_resources/Euler_angles.resources/unknown_filename.png)\u003cbr\u003eintrinsic yaw pitch roll |\n\n---\n\n**Source**: [Markley 2014](bibliography/markley-2014.md)\n\nTable of Euler rotation matrices (RH, passive, intrinsic):\n1: phi, 2:theta, 3:psi\n\n| Proper Euler | Tait-Bryan |\n| --- | --- |\n|     | ![unknown_filename.3.png](./_resources/Euler_angles.resources/unknown_filename.3.png)\u003cbr\u003e(transpose of the active version) |\n|     | ![unknown_filename.4.png](./_resources/Euler_angles.resources/unknown_filename.4.png)\u003cbr\u003e(transpose of the active version) |\n\nEuler to quaternion conversions (note: these are all passive transformations??)\n1: phi, 2:theta, 3:psi\n![unknown_filename.5.png](./_resources/Euler_angles.resources/unknown_filename.5.png)\n\n",title:"Untitled Page"},"/studienarbeit/expected-value":{content:"---\ntitle: Expected value\ndate: \"2020-08-27\"\ntags:\n  - to-do/to-clarify\n  - -sa/processed\n  - math/statistics\n---\n\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nExample: if we take a thousand sensor readings, the readings won't always be the same (due to the inherent noise).\n\n*   The expected value 'averages' all of the readings into a single value.\n*   This can be a mean (probabilities of all values assumed equal)\n*   Or if incorporating individual and different probabilities, the expectation isn't the mean of the range of values\n\n![unknown_filename.png](./_resources/Expected_value.resources/unknown_filename.png)\n![unknown_filename.1.png](./_resources/Expected_value.resources/unknown_filename.1.png)\n\nProven: the average of a large number of measurements will be very close to the actual weight\n- [ ] What is the name for this theorem?\n\nCaveat:\n\n*   Using a normal mean calculation assumes that all measurements are equally likely.\n*   However, real sensors tend to return readings close to the actual value (barring any offset disturbances).\n*   They can still return readings further away from the actual value, just with a reduced probability compared to values nearer the actual value\n*   The probability \\[distribution\\] must be factored in somehow\n\n",title:"Untitled Page"},"/studienarbeit/extern-c++":{content:'---\ntitle: extern c++\ndate: "2021-01-11"\nexternal_url: "http://en.cppreference.com/w/cpp/language/storage_duration"\ntags:\n  - -sa/processed\n  - software/cpp\n---\n\n[http://en.cppreference.com/w/cpp/language/storage\\_duration](http://en.cppreference.com/w/cpp/language/storage_duration)\n\nis a storage class specifier that controls storage duration and its linkage.\n\nextern - static or thread storage duration and external linkage\n\n**Storage duration**\n\n*   static:\n    *   storage for the object is allocated when the program begins and deallocated when the program ends.\n    *   only one instance of the object exists\n*   thread\n    *   storage for the object is allocated when the thread begins and deallocated when the thread ends\n    *   each thread has its own instance of the object\n        \n\n**Linkage**\n\n*   If a name has **linkage**, it refers to the **same entity** as the same name introduced by a declaration in another scope\n*   If a variable, function, or another entity with the same name is declared in several scopes, but does not have sufficient linkage, then several instances of the entity are generated.\n    \n\nexternal linkage:\nvariables and functions with external linkage also have [language linkage](http://en.cppreference.com/w/cpp/language/language_linkage), which makes it possible to link translation units written in different programming languages.\n\n',title:"Untitled Page"},"/studienarbeit/external-data-in-sofa":{content:'---\ntitle: External data in SOFA\ndate: "2020-07-24"\ntags:\n  - software/SOFA/communication\n  - -sa/processed\n---\n\n**Parent**: [SofaPython Index](sofapython-index.md)\n\n\u003chttp://www.sofa-framework.org/community/forum/topic/how-to-use-external-data-in-sofa/\u003e\n\u003chttp://www.sofa-framework.org/community/forum/topic/how-to-send-data-to-sofa-through-socket/\u003e\n\u003chttp://www.sofa-framework.org/community/forum/topic/connecting-sofa-to-an-external-data-com-port/\u003e\n\n',title:"Untitled Page"},"/studienarbeit/factors-affecting-kalman-filter-performance":{content:'---\ntitle: Factors affecting Kalman filter performance\ndate: "2020-08-31"\ntags:\n  - -sa/processed\n  - filters/kalman-filter\n---\n\nParent: [1D Kalman filters](1d-kalman-filters.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nDifficulties of creating a well-performing Kalman filter:\nIncludes modeling the sensor performance (what variance most accurately represents the reality? Which probability distribution?)\n\nFactors affecting the performance of the Kalman filter\n\n*   [On modelling the process noise/variance](on-modelling-the-process-noise_variance.md)\n*   Bad initial estimate\n    *   Filter can recover from this, because we have a certain belief in the sensor measurements\n    *   Typically the initial value is set to the first sensor measurement\n*   [Nonlinearity of the system](nonlinearity-of-the-system.md)\n\n',title:"Untitled Page"},"/studienarbeit/fast-keypoint-detector":{content:'---\ntitle: FAST keypoint detector\ndate: "2020-12-08"\ntags:\n  - -sa/processed\n  - vision\n  - discussion/2020/2020-12\n---\n\nSource: \u003chttp://medium.com/data-breach/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf\u003e\n**Parent**: [ORB descriptor](orb-descriptor.md)\n\nFAST (Features from Accelerated and Segments Test)\n\nHow it works\n\n*   Given: pixel p, surrounded by other pixels in the image\n*   Take the surrounding pixels that are in a small circle around p\n    ![unknown_filename.jpeg](./_resources/FAST_keypoint_detector.resources/unknown_filename.jpeg)\n    \n*   If more than half of the surrounding pixels are darker/brighter than p, p is selected as a keypoint\n*   Good for edge detection\n\n**Drawbacks**\n\n*   Noninvariant with respect to orientation and scaling\n\n',title:"Untitled Page"},"/studienarbeit/feature-based-vs-direct-slam-workflow":{content:'---\ntitle: Feature-based vs direct SLAM workflow\ndate: "2020-08-03"\ntags:\n  - SLAM/sparse-vs-dense\n  - -sa/processed\n  - -published\n---\n\nParent: [SLAM Index](SLAM/slam_index.md)\n\nSource: [cometlabs](bibliography/cometlabs.md)\n\n| Feature-based (aka [sparse](studienarbeit/sparse-feature-based-vslam.md)) | direct (aka [dense](studienarbeit/dense-direct-vslam.md)) |\n| --- | --- |\n| Extraction of features required | No abstraction necessary |\n| Aims to minimise error between point location estimate (from odometry) and location based on camera | Tracks objects by minimising photometric error (intensity differences) |\n\n![unknown_filename.png](./_resources/Feature-based_vs_direct_SLAM_workflow.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/feature-maps":{content:'---\ntitle: Feature maps\ndate: "2020-07-30"\ntags: \n- SLAM/sparse-vs-dense/feature-based \n- -sa/processed\n---\n\nParents: [Mapping representations in robotics](mapping representations-in-robotics.md), [sparse/feature-based-vslam](sparse_feature-based-vslam.md)\n\nSource: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)\n\nUses a limited number of sparse objects to represent a map\ne.g. points, lines\n\n*   Low computation cost because of the sparsity\n    *   Map management solutions are  good solutions for current applications [x] What\'s map management (probably storing maps in databases and recognising an existing map)\n*   \\[-\\] Sensitivity to false data association\n\n',title:"Untitled Page"},"/studienarbeit/feature-matching":{content:"---\ntitle: Feature matching\ndate: \"2020-12-08\"\ntags:\n  - to-do/to-clarify\n  - -sa/processed\n  - vision\n---\n\n**Source**: \u003chttp://medium.com/data-breach/introduction-to-feature-detection-and-matching-65e27179885d\u003e\n**Backlinks**: [Bag of words](bag-of-words.md), [sparse/feature-based-vslam](sparse_feature-based-vslam.md)\n\nFor matching between images, i.e. to establish a relationship ('correspondence') between two images of the same scene or object.\n\n**Basic algorithm**\n\n*   Find/detect a set of identifying ('distinctive') keypoints from all images to be matched\n*   Define a search region around each keypoint\n*   Extract and normalise the region content\n*   Compute a local descriptor from the normalised region\n*   Match local descriptors between the images\n\nPerformance of matching methods depend on\n\n*   characteristic of the interest points themselves\n*   choice of the feature detectors / image descriptors\n    *   e.g. corner detector for man-made surfaces, blob detector for more organic sturctures\n    *   the detector/descriptor should be able to handle image degradation\n\n**Some algorithms**\n\n*   Brute-Force Matcher\n*   FLANN (Fast Lirbary for Approximate Nearest Neighbours) Matcher\n\n",title:"Untitled Page"},"/studienarbeit/filter-based-vs-optimisation-based-slam":{content:'---\ntitle: Filter-based vs optimisation-based SLAM\ndate: "2020-08-23"\ntags:\n  - SLAM/filter-vs-optim\n  - -sa/processed\n---\n\nParent: [SLAM Index](SLAM/slam_index.md)\n\nSource: [Scaradozzi 2018 SLAM application in surgery](studienarbeit/scaradozzi-2018.md)\n\nMain paradigms of SLAM\n\n1.  [Filters](http://www.evernote.com/shard/s484/nl/217355218/48ab2536-eadf-4a7f-8dfa-f377bfbe3839) — [Kalman filters](http://www.evernote.com/shard/s484/nl/217355218/bb893af4-28b5-484b-b110-cdcb4eb91477), Particle filters\n2.  Graph-based SLAM\n    Estimate the entire trajectory and the map from the full set of measurements (full SLAM)\n    \n\nWhich SLAM algorithm to use?\nDepends on application\n\n*   map resolution\n*   update time (real time or not)\n*   type of environment\n*   type of sensors available\n\n',title:"Untitled Page"},"/studienarbeit/force-classes-in-sofa":{content:'---\ntitle: Force classes in SOFA\ndate: "2020-08-09"\ntags:\n  - software/SOFA/simulation-components\n  - -sa/processed\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\nParent: [Components of the internal model](components-of-the-internal-model.md)\n\nMore than 30 classes available in SOFA\n\n*   FEM\n    \n    *   for deformable volumes/surfaces\n        *   volume: tetrahedron/hexahedron\n        *   surface: shell/membrane\n    \n    *   TetrahedralCorotationalFEMForceField: forces based on FEM\n    \n    *   corotational/hyperelastic formulations\n    *   wire/tubular objects\n*   Springs\n    *   SpringForceField: forces generated by the surface (alternative: TriangleFEMFroceField)\n*   ConstantForceField: external forces\n\n',title:"Untitled Page"},"/studienarbeit/forster-2017-imu-preintegration":{content:"---\ntitle: Forster 2017 IMU Preintegration\ndate: \"2020-11-17\"\ntags:\n  - to-do/to-clarify\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-to-read\n  - -sa/processed\n  - sensors/IMU/preintegration\n---\n\nAuthors: Forster et al\n\nAbstract:\n\n*   First contribution: preintegration theory (building up on Lupton's work)\n    *   what's different from Lupton's:\n        *   addresses manifold structure of the rotation group, analytic derivation of all Jacobians\n        *   Lupton's work uses Euler angles\n        *   Using Euler angles and techniques of Euclidian spaces for state propagation/covariance estimation is not properly invariant under rigid transformations\n        *   uncertainty propagation, a-posteriori bias correction\n    *   same as Lupton: integration performed in local frame, eliminating need for reintegrating when linearisation point changes\n*   Second contribution: integration of the preintegrated IMU model into a visual-inertial pipeline\n*   The system presented uses incremental smoothing for fast computation of the optimal MAP estimate\n*   Uses structureless model (3D landmarks are not part of the variables to be estimated)\n*   for visual measurements --\u003e allows eliminating large numbers of variables\n\nMotivation:\n\n*   optimisation-based approaches are more accurate than filtering-based ones, but come at the cost of high computational complexity\n*   preintegration theory allows reduction of the computational complexity by accurately summarising multiple inertial measurements into a single relative motion constraint\n\nIMU preintegration over frames\n![unknown_filename.png](./_resources/[Forster_2017]_IMU_Preintegration.resources/unknown_filename.png)\n\nIntroduction\n[Why use the visual-inertial sensor combination?](why-use-the-visual-inertial-sensor-combination_.md)\n[Handling the computational complexity of optimisation-based SLAM](handling the-computational-complexity-of-optimisation-based-slam.md)\n\nPreliminaries\n[SO(3) 3D rotation group](so(3) 3d rotation group.md), [lie group,-lie-algebra](rotations/lie-group-lie-algebra.md)\n  [Exponential map](rotations/exponential-map.md)\n\n  [Logarithm map](rotations/logarithm-map.md)\n\n[SE(3) Special Euclidian Group](se(3)-special-euclidian-group.md)\n[Gauss-Newton Method on Manifold](gauss-newton-method-on-manifold.md)\n\nMAP VI state estimation\n[System in a VIN problem with IMU preintegration](system in a-vin-problem-with-imu-preintegration.md)\n[MAP estimation](map-estimation.md)\n\nIMU model and motion integration\n[IMU kinematic model using Euler integration](imu-kinematic-model-using-euler-integration.md)\n[IMU preintegration on manifold](imu-preintegration-on-manifold.md)\n\nPreintegration math\n- [ ] To do...\n\n",title:"Untitled Page"},"/studienarbeit/forward-kinematics-imu-to-camera":{content:'---\ntitle: Forward kinematics IMU to camera\ndate: "2021-06-07"\ntags:\n  - -sa/processed\n  - math/kinematics\n  - discussion/2021/2021-06\n---\n\nBacklinks: [Discussion 2021-05-21](discussion 2021-05-21.md), [discussion-2021-06-01](discussion-2021-06-01.md), [update-2021-06-11](update-2021-06-11.md)\n\n![unknown_filename.jpeg](./_resources/Forward_kinematics_IMU_to_camera.resources/unknown_filename.jpeg) \n\nSimplified model (rigid)\n![unknown_filename.1.png](./_resources/Forward_kinematics_IMU_to_camera.resources/unknown_filename.1.png)![unknown_filename.3.png](./_resources/Forward_kinematics_IMU_to_camera.resources/unknown_filename.3.png)![unknown_filename.2.png](./_resources/Forward_kinematics_IMU_to_camera.resources/unknown_filename.2.png)\n\n',title:"Untitled Page"},"/studienarbeit/frequentist-vs-bayesian-statistics":{content:'---\ntitle: Frequentist vs Bayesian statistics\ndate: "2020-08-31"\ntags:\n  - -definitions\n  - -sa/processed\n  - math/statistics\n---\n\nParent: [Discrete Bayesian filter](discrete-bayesian-filter.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nFrequentist vs Bayesian statistics\n\n*   Probability of flipping a fair coin infinitely many times is 50% - frequentist\n*   Probability of flipping a fair coin one more time (which way do I believe it landed?), single event - Bayesian\n    *   Bayesian statistics takes past information (prior) into account\n    *   If finding the prior is tricky, frequentist techniques are sometimes used\n\ne.g. Weather\n\n*   Frequentist: If it rains 4 out of 100 days, the chance of rain today is 25%\n*   Bayesian: If we know that it\'s raining today and the storm front is stalled (prior information), it is likely to rain tomorrow\n\n',title:"Untitled Page"},"/studienarbeit/g-h-filter-or-α-β-filter":{content:'---\ntitle: g-h filter or α-β filter\ndate: "2020-08-27"\ntags:\n  - -sa/processed\n  - filters/gh-filter\n---\n\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nA filter that uses two scaling factors:\n\n*   [g or \\\\alpha](http://www.evernote.com/shard/s484/nl/217355218/a7509361-eda5-4c69-a509-eb5c9d213ab2) for the measurement\n*   h or \\\\beta for the rate of change of measurement\n\n[GH filter algorithm](gh-filter-algorithm.md)\n\n[Calculating the estimated state in the GH-filter](calculating the-estimated-state-in-the-gh-filter.md)\n[Improving the gh-filter by using h](improving-the-gh-filter-by-using-h.md)\n\n[Several unwanted effects using gh filters](several-unwanted-effects-using-gh-filters.md)\n\nBasis for many other filters, e.g.\n\n*   Kalman filter\n*   Least squares filter\n*   Benedict-Bordner filter\n*   etc.\n\nwhere each different filter has a different way of determining g and h (whether constant values or varying them dynamically with time)\n\nStatic choices for g and h do not perform very well — tied to only one particular dataset.\n\n',title:"Untitled Page"},"/studienarbeit/gain-g-of-the-gh-filter":{content:'---\ntitle: Gain g of the gh-filter\ndate: "2020-08-27"\ntags:\n  - -sa/processed\n  - filters/gh-filter\n---\n\nParent: [Calculating the estimated state based on measurements and predictions](calculating the estimated state-based-on-measurements-and-predictions.md)\nBacklinks: [GH filter algorithm](gh-filter-algorithm.md)\n\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nWhich one do we trust more, the meaasurement z or the prediction x?\nApplying corresponding weights to both, we obtain the estimate x\\_est\n\n![unknown_filename.png](./_resources/Gain_g_of_the_gh-filter.resources/unknown_filename.png)\n\nThe prediction is nothing other than a propagated state estimate.\n\n\\[Me\\] The prediction is basesd on the model (a priori knowledge)\nIf the model also depends on previous states (which are themselves an output of the filter, i.e. the state estimates), then the prediction is dependent on filter output too (up till the last time step).\n\n\\# Prediction step\nprediction = system\\_model(previous\\_state\\_estiamte)\n\n\\# Update step\nresidual = measurement z - prediction x\nestimate = prediction + g \\* residual\n\nHere we have applied a scale to the measurement, but we can also do this to the rate of change of measurement.\ngain\\_rate = gain\\_rate + h \\* (residual/time\\_step)\n\n',title:"Untitled Page"},"/studienarbeit/gauss-newton-method-on-manifold":{content:'---\ntitle: Gauss-Newton Method on Manifold\ndate: "2020-11-27"\ntags:\n  - -sa/processed\n  - sensors/IMU/preintegration\n  - math\n---\n\nSource: [Forster 2017 IMU Preintegration](forster-2017-imu-preintegration.md)\n\nStandard approach for optimization on manifold\n\n*   define a retraction to reparametrise the problem (lifting)\n*   retraction ![unknown_filename.4.png](./_resources/Gauss-Newton_Method_on_Manifold.resources/unknown_filename.4.png)\n    *   bijective map\n    *   map between an element of the tangent space at x and a neighbourhood of x on the manifold\n*   i.e. we work in the tangent space (locally like a Euclidian space) and apply standard optimisation techniques\n*   for Gauss-Newton specifically:\n    *   \\[ts\\] squared cost around current estimate\n    *   \\[ts\\] solve the quadratic approximation --\u003e we get vector ![unknown_filename.2.png](./_resources/Gauss-Newton_Method_on_Manifold.resources/unknown_filename.2.png) in tangent space\n    *   \\[m\\] update the current guess on the manifold ![unknown_filename.3.png](./_resources/Gauss-Newton_Method_on_Manifold.resources/unknown_filename.3.png) \n\nConsider: ![unknown_filename.png](./_resources/Gauss-Newton_Method_on_Manifold.resources/unknown_filename.png)\nReparametrised: ![unknown_filename.1.png](./_resources/Gauss-Newton_Method_on_Manifold.resources/unknown_filename.1.png)\n\nRetraction for SE(3)\nThe exponential map of SE(3) as a retraction is possible, but may not be convenient (computationally)\n\nThe following retraction is proposed (at ![unknown_filename.5.png](./_resources/Gauss-Newton_Method_on_Manifold.resources/unknown_filename.5.png)):\n![unknown_filename.6.png](./_resources/Gauss-Newton_Method_on_Manifold.resources/unknown_filename.6.png)        ![unknown_filename.7.png](./_resources/Gauss-Newton_Method_on_Manifold.resources/unknown_filename.7.png)\n\nUsing this rectraction, we don\'t need to compute the exponential map for SE(3)\n\n',title:"Untitled Page"},"/studienarbeit/gaussian-distribution.1":{content:'---\ntitle: Gaussian distribution\ndate: "2020-08-31"\ntags:\n  - -definitions\n  - math/statistics\n  - -sa/to-be-processed\n---\n\nBacklinks: [Limitations of the discrete Bayes filter](limitations-of-the-discrete-bayes-filter.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\na.k.a. Normal distribution\nUnimodal, continuous probability distribution function (pdf)\n\nThe probability of a range of measurements is the area under the graph of the probability distribution between the end values of the range -- cumulative distribution function (cdf)\n\nBackground statistics\n[Variance, standard deviation, covariances](variance,-standard-deviation,-covariances.md)\n[Central Limit Theorem](central-limit-theorem.md)\n[Correlation and independence](correlation-and-independence.md)\n\nTypes\n[Univariate Gaussian distribution](univariate-gaussian-distribution.md)\n[Multivariate Gaussian distributions](multivariate-gaussian-distributions.md)\n\n[Computational properties of Gaussian distributions](computational-properties-of-gaussian-distributions.md)\n[Pros and cons of Gaussian distributions](pros-and-cons-of-gaussian-distributions.md)\n\n',title:"Untitled Page"},"/studienarbeit/general-ekf-implementation-non-slam":{content:'---\ntitle: General EKF implementation (non-SLAM)\ndate: "2020-08-22"\ntags:\n  - filters/EKF\n  - -sa/processed\n---\n\nParent: [Extended Kalman Filter](SLAM/extended-kalman-filter.md)\n\nSource: [SLAM for Dummies](slam-for-dummies.md)\nGeneral (non-SLAM) implementation of EKF:\n\n*   only state estimation\n*   robot is given a perfect map\n*   no map update necessary\n\nSLAM implementations of EKF requires map update and therefore the matrices are changed.\n\nSource: [Scaradozzi 2018 SLAM application in surgery](studienarbeit/scaradozzi-2018.md)\nEKF [vs KF](http://www.evernote.com/shard/s484/nl/217355218/bb893af4-28b5-484b-b110-cdcb4eb91477)\n\n*   circumvents linearity assumption\n*   uses nonlinear functions to describe\n    *   the next state probability\n    *   measurement probability\n*   approximates the state distribution with a Gaussian Random Variable\n\n',title:"Untitled Page"},"/studienarbeit/general-kalman-filter":{content:'---\ntitle: General Kalman Filter\ndate: "2020-08-23"\ntags:\n  - -sa/processed\n  - filters/kalman-filter\n---\n\nBacklinks: [Main paradigms of SLAM](main-paradigms-of-slam.md)\nSource: [Scaradozzi 2018 SLAM application in surgery](studienarbeit/scaradozzi-2018.md)\n\nKF original algorithm assumes linearity (rarely ever the case)\n\n*   Variations of the Kalman filter:\n    *   [Extended Kalman Filter](http://www.evernote.com/shard/s484/nl/217355218/a3417515-123a-4310-ac2f-937cd4878942) (EKF)\n    *   [Unscented Kalman Filter](http://www.evernote.com/shard/s484/nl/217355218/b19e0595-dbcb-4e34-8ea9-78069a850ca3) (UKF)\n*   [Information filtering](http://www.evernote.com/shard/s484/nl/217355218/bf412e27-aa5a-41b0-986e-1ba17317747a) (IF) — dual to KF\n*   Combination of EKF and IF: CF-SLAM, with the goal to be more efficient w.r.t. computational complexity\n\n',title:"Untitled Page"},"/studienarbeit/geometric-metric-slam":{content:'---\ntitle: Geometric metric SLAM\ndate: "2020-07-30"\ntags:\n  - SLAM\n  - SLAM/sparse-vs-dense\n  - -sa/processed\n---\n\nSource: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)\nParent: [Classification of image-based camera localization approaches](classification-of-image-based-camera-localization-approaches.md)\n\nComputes 3D maps with accurate mathematical equations\n\nClassification according to sensors\n\n*   monocular\n*   multiocular (most studies focus on binocular vision)\n*   [multisensor fusion](multisensor-fusion.md) (e.g. vision and IMU -- vision and IMU fusion gaining in popularity)\n\nClassification according to techniques used\n\n*   [Filter-based SLAM](http://www.evernote.com/shard/s484/nl/217355218/2429fd5f-f22a-acc6-9f7d-d09e63206c25)\n*   [Keyframe](http://www.evernote.com/shard/s484/nl/217355218/6f2315b3-3897-623d-3285-77bc7d51be76)\\-based SLAM (active)\n    *   Feature-based (keyframe-based feature SLAM) / sparse\n    *   Direct / dense\n*   Grid-based SLAM (mainly deals with laser data, deals only a bit with image data)\n\nAccording to \\[76\\] keyframe-based can provide more accurate results compared to filter-based\n\n*   Filter-based was common before 2010\n*   Solutions after 2010 mostly based on non-filter, keyframe-based architecture\n\nClassification according to sparse/dense (how detailed are the extracted features)\n\n*   Feature SLAM/sparse: use points, edges etc (I think)\n*   Direct SLAM/dense:\n    *   detailed textured dense depth maps are produced (also: semi-dense approaches)\n    *   camera pose is tracked at frame rate by entire image alignment against the dense textured model\n\n',title:"Untitled Page"},"/studienarbeit/gh-filter-algorithm":{content:'---\ntitle: GH filter algorithm\ndate: "2020-08-27"\ntags:\n  - -sa/processed\n  - filters/gh-filter\n---\n\nParent: [g-h filter or α-β filter](g-h-filter-or-α-β-filter.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nInitialisation\n\n1.  Initialise the state of the filter\n2.  Initialise our belief in the state\n\nPrediction\n\n1.  Use system model to propagate the state to the next time step\n2.  Adjust our belief to account for uncertainty in the prediction\n\nUpdate\n\n1.  Get a measurement and an associated belief about its accuracy\n2.  Calculate residual = measurement - estimated state\n3.  Using [a certain gain](http://www.evernote.com/shard/s484/nl/217355218/a7509361-eda5-4c69-a509-eb5c9d213ab2), our updated state estimate is somewhere on the residual line\n\n',title:"Untitled Page"},"/studienarbeit/grisetti-2011":{content:'---\ntitle: Grisetti 2011 - Tutorial graph-based SLAM\ndate: "2020-10-24"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-to-read\n  - -sa/to-be-processed\n---\n\ntemp\n\nBacklinks: [What is SLAM?](what-is-slam_.md)\n\nAbstract\n\n*   formulate SLAM using a graph\n    *   nodes: poses of the robot (as well as landmark postiions) at different points in time\n    *   edges: constraints between poses\n        *   come from\n            *   sensor measurements/observations\n            *   robot movement/control input\n        *   constraints can contradict each other, due to effect of noise in sensor readings\n*   solve the graph, i.e. compute the map: find the spatial configuration of the nodes that best satisfy the constraints/edges\n*   tutorial for back-end (optimisation) part of graph-based SLAM\n\n:: Navigation task: requires a map and knowledge of current position relative to locations in the map\n\nThe need for an accurate map\n\n*   systems navigating the map don\'t have to rely on external reference systems like GPS sensors\n    *   GPS isn\'t suitable for indoor environments\n*   can operate in complex environments while relying only on their on-board sensors\n\nFiltering vs. smoothing\nFiltering: on-line state (robot pose, map) estimation; "on-line" because incremental in nature\nSmoothing: full trajectory estimation, typically rely on LSQ minimisation\n\nHistory of graph-based formulation\n\n*   initially unpopular due to high solving complexity (high dimensional state spaces)\n*   later:\n    *   use structure of SLAM problem\n        *   static world assumption\n        *   [Markov assumption](markov-assumption.md)\n    *   advancements in sparse linear algebra\n*   now: graph-based SLAM is state-of-the-art (w.r.t. accuracy, speed)\n\nModelling the probabilistic SLAM formulation\n\n*   [DBN](http://www.evernote.com/shard/s484/nl/217355218/9dc7aed8-e237-41d5-90b6-1ed08781529c) (shows temporal structure)\n*   Graph-based (shows spatial structure)\n\nTemporal vs spatial pose graphs\n![Image.png](./_resources/Grisetti_2011_-_Tutorial_graph-based_SLAM.resources/Image.png)![unknown_filename.png](./_resources/Grisetti_2011_-_Tutorial_graph-based_SLAM.resources/unknown_filename.png)\n\nGraph-based SLAM\n\n*   node:\n    *   robot pose, labelled according to location (as opposed to time as in DBN)\n    *   measurement acquired at that position\n*   edges: spatial constraints (in the form of probability distributions) between poses\n    *   from odometry/observations\n    *   :: g2o edges are constraints, which are the optim. function terms\n    *   edges are like an abstraction of raw measurements into \'virtual measurements\' (using probability distributions, observation model etc.)\n*   main tasks of graph-based SLAM:\n    *   \\[graph construction\\] constructing the graph from raw measurements\n        *   front-end\n        *   data association\n        *   very sensor-dependent\n        *   "How to interpret sensor data to obtain the graph constraints?"\n    *   \\[graph optimisation\\] finding best pose configuration from graph\n        *   back-end\n        *   relies on abstract representation of data, is sensor agnostic\n        *   "How to compute the map given the constraints?"\n*   front- and back-end executions are interleaved (front end requires prior term which comes from back-end)\n\nObservation model\n![unknown_filename.1.png](./_resources/Grisetti_2011_-_Tutorial_graph-based_SLAM.resources/unknown_filename.1.png)\n\n*   The observation model is usually multimodal:\n    *   a single observation may result in multiple edges (in the spatial graph)\n    *   "a feature can be observed from multiple view points/camera poses"\n    *   the graph connectivity\n*   Due to this multimodality, the Gaussian assumption does not hold\n\n',title:"Untitled Page"},"/studienarbeit/grk-2543":{content:'---\ntitle: (GRK 2543) Intraoperative Multi-sensor Tissue Differentiation in Oncology\ndate: "2020-06-19"\ntags:\n  - -master/project-background\n  - -master\n  - -sa/processed\n  - grk-2543\n---\n\n**Sources**:\n*   [Deutsche Forschungsgemeinschaft project page](http://gepris.dfg.de/gepris/projekt/409474577?language=en)\n*   [ISYS project page](http://www.isys.uni-stuttgart.de/forschung/medizintechnik/intraoperative-multisensorische-gewebedifferenzierung/)\n\n## Background: \n\n*   Cooperation between Uni Tübingen and Uni Stuttgart\n*   Gynelogical and urological application scenarios\n\n## Aims:\n\n*   Minimise invasiveness and duration of surgical [cancer treatment](studienarbeit/cancer-biopsy.md), while at the same time maximising effectiveness of the treatment\n    *   Minimise damage to surrounding tissue during tumour [resection](studienarbeit/resection-margin.md)\n    *   Aid decision-making during surgery (intraoperative)\n        *   Reliable differentiation between cancerous tissue and the surrounding healthy tissue\n        *   Decide whether to preserve tissue or continue with [surgery](studienarbeit/related-types-of-surgery.md)\n*   Complement existing techniques\n    *   Histological\n    *   Imaging\n    *   Optical (IR, Raman spectroscopy)\n\nCurrent standard of intraoperative tissue identification: [frozen section diagnostics](studienarbeit/cryosection.md) (takes about 30 minutes)\n\n## Means of achieving project goals\n\n*   Multimodal (multitype) sensor fusion to increase gainable knowledge, compared to using separate sensor data\n*   Machine learning\n*   Model-based analysis\n*   Real-time communication\n\n![unknown_filename.png](./_resources/GRK_2543__Intraoperative_Multi-sensor_Tissue_Differentiation_in_Oncology.resources/unknown_filename.png)\n\n## Research focuses of the ISYS Medizintechnik group\n\n*   Focus A: Sensor development (optic, mechanical, electrical basis)\n    *   A5: Tissue differentiation using electrical impedance spectroscopy\n*   Focus B: Modelling and classification\n    *   [B1: Modelling of tissue and sensor, navigation of multimodal sensors](b1_ modelling of tissue and-sensor,-navigation-of-multimodal-sensors.md)\n    *   B3: Multimodal data-driven sensor fusion\n*   Focus C: Surgery and pathology\n\n',title:"Untitled Page"},"/studienarbeit/handling-the-computational-complexity-of-optimisation-based-slam":{content:'---\ntitle: Handling the computational complexity of optimisation-based SLAM\ndate: "2020-11-17"\ntags:\n  - SLAM/filter-vs-optim/opt-based\n  - -sa/processed\n---\n\n**Parent**: [SLAM Index](SLAM/slam_index.md)\n**Source:** [Forster 2017 IMU Preintegration](forster-2017-imu-preintegration.md)\n\nComplexity of nonlinear batch optimisation\n\n*   The trajectory and the map, which comprise the states, grow with time\n*   The larger the SLAM problem, the less feasible it is to perform the optimisation in real-time\n\nSolutions to improve computational efficiency\n\n*   Keyframe-based methods: discard frames except for a few selected keyframes\n*   Run the optimisation parallelly (e.g. tracking and mapping threads)\n*   Fixed-lag smoothing: Use of a local map of fixed size, with marginalisation of the old states (summarise the old states into a prior term)\n    *   Filtering is a special case of this: window of size 1, i.e. only the latest sensor state is optimised\n    *   \\[-\\] commits to a linearisation point when marginalising\n    *   linearisation errors will eventually accumulate, can lead to drift or other inconsistencies\n*   Incremental smoothing\n    *   In between filtering and batch optimisation\n    *   Usage of factor graphs\n    *   Identifies and updates only a small subset of variables affected by a new measurement ("new constraint?")\n\n',title:"Untitled Page"},"/studienarbeit/haptic-rendering":{content:'---\ntitle: Haptic rendering\ndate: "2020-07-15"\ntags:\n  - software/SOFA\n  - misc/haptic-feedback\n  - -sa/processed\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\n\nThe main interest of interactive simulation is that\n\n*   the user can modify the course of the computations in real-time\n*   when a virtual medical instrument comes into contact with some models of a soft-tissue, instantaneous deformations must be computed\n    *   This visual feedback of the contact can be enhanced by haptic rendering so that the surgeon can really feel the contact."\n\nTwo main issues in SOFA for providing haptics\n\n1.  Haptic forces need to be computed at 1 kHz\n    However: real-time visual feedback (w/o haptic) is obtained at 30 Hz\n    \n2.  Haptic feedback could artificially add some energy inside the simulation\n    1.  may create instabilities if the control is not passive\n\nThus\nTwo different approaches:\n\n1.  Virtual Coupling\n2.  Constraint-based rendering\n    COMMENT: possibly what I need  \n    \n\n... (skimmed)\n\n',title:"Untitled Page"},"/studienarbeit/hibbeler-dynamics":{content:'---\ntitle: (Hibbeler) Dynamics\ndate: "2021-05-24"\ntags:\n  - -resources/-bibliography\n  - -sa/processed\n  - -resources/-bibliography/bib-skimmed\n---\n\nAuthor: Russell Hibbeler\nContents\n\n*   Kinematics, kinetics of\n    *   particle\n    *   \\[planar\\] rigid body\n    *   \\[3D\\] rigid body\n*   Vibrations\n\n[Kinematics primer](kinematics-primer.md)\n\n',title:"Untitled Page"},"/studienarbeit/hidden-variables-in-a-multivariate-kalman-filter":{content:'---\ntitle: Hidden variables in a multivariate Kalman filter\ndate: "2020-09-01"\ntags:\n  - filters/kalman-filter\n  - -sa/to-be-processed\n---\n\nParent: [Multivariate Kalman filters](multivariate-kalman-filters.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nExample:\n![unknown_filename.1.png](./_resources/Hidden_variables_in_a_multivariate_Kalman_filter.resources/unknown_filename.1.png)\nBlue error ellipse:\n\n*   Certainty in position x=0\n*   No idea about the velocity (long in y-axis)\n\nWe know that position and velocity are correlated,\ni.e. the next position depends on the current velocity value\n(red error ellipse — likelihood/prediction for the next step)\ne.g. if v=5m/s, the next position is 5m +- position uncertainties\n\nWe get a position update (new blue error ellipse)\n![unknown_filename.png](./_resources/Hidden_variables_in_a_multivariate_Kalman_filter.resources/unknown_filename.png)\n\n*   The new covariance (posterior) is obtained by multiplying the previous two covariances —\u003e intersection\n*   The posterior\'s tilt implies that there is some correlation between position and velocity\n*   Not only are we now more certain about the velocity, but our position certainty also increases (compared to not considering the velocity at all)!\n\nObserved variable: can be obtained from sensor\nHidden variable: inferrable from observedvariables\nUnobserved variables: not sensable and also not inferrable from sensor measurements\n\n**Takeaway**: Hidden variables can significantly increase the accuracy of the filter (due to their correlation with the observed variables)\n\n**Caveat**: When inferring a state from an observed variable, do not always assume that the derived state is accurate. Possible error sources here:\n\n*   We assume a correlation between the hidden variable and the observed variable, but in fact there is none\n*   The hidden variable might be oscillating at a certain frequency F, but we sample the observed variable at a frequency less than 2F (s. Nyquist-Shannon theory) and get a bad representation of the velocity signal\n*   Bad initialisation. An observed state may recover from bad IC, but a hidden state may not.\n\nValidation necessary!\n\n',title:"Untitled Page"},"/studienarbeit/https--www.sofa-framework.org-applications-gallery-percutaneous-liver-surgery-":{content:'---\ntitle: http://www.sofa-framework.org/applications/gallery/percutaneous-liver-surgery/\ndate: "2020-07-23"\ntags:\n  - misc/haptic-feedback\n  - -resources/videos\n  - -sa/processed\n---\n\n\u003chttp://www.sofa-framework.org/applications/gallery/percutaneous-liver-surgery/\u003e\nConstraint-based haptic rendering\n![unknown_filename.png](./_resources/http___www.sofa-framework.org_applications_gallery_percutaneous-liver-surgery_.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/importing-a-fork-in-python-instead-of-installed-package":{content:'---\ntitle: Importing a fork in Python instead of installed package\ndate: "2021-07-23"\ntags:\n  - software/python\n  - -sa/processed\n---\n\n\u003chttp://stackoverflow.com/questions/23075397/python-how-to-edit-an-installed-package\u003e\n\nRun this in repo that uses the fork (this installs the package as a submodule):\npython3 -m pip install -e git+[ssh://git@github-feudalism/feudalism/spatialmath-python.git\n- egg=f-spatialmath](ssh://git@github-feudalism/feudalism/spatialmath-python.git\n- egg=f-spatialmath) --upgrade\n\n![unknown_filename.png](./_resources/Importing_a_fork_in_Python_instead_of_installed_package.resources/unknown_filename.png)\n\nInstructions:\n\n1.  Fork the package repo\n2.  cd to own repo where you want to use the package\n3.  Install the fork using the above pip install command. This creates ./src/submodule\n4.  When making changes to fork:\n    make changes in either\n    *   the submodule folder (for immediate effect), or\n    *   in the fork subdirectory + push + reinstall\n\n',title:"Untitled Page"},"/studienarbeit/improving-the-gh-filter-by-using-h":{content:"---\ntitle: Improving the gh-filter by using h\ndate: \"2020-08-27\"\ntags:\n  - -sa/processed\n  - filters/gh-filter\n---\n\nParent: [g-h filter or α-β filter](g-h-filter-or-α-β-filter.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\n[Implementing the g value without h](implementing-the-g-value-without-h.md)\n\nWe improve the estimation, previously by only predicting the state, by now predicting the rate of change of state.\ni.e. Also predict the weight gain per day instead of setting it at a constant value.\nWe use the sensor information for this! Even if it's noisy, there's information in there somewhere, and data is always better than a guess.\ne.g.![unknown_filename.png](./_resources/Improving_the_gh-filter_by_using_h.resources/unknown_filename.png) \nwhere 1/3 is an arbitrary, tunable value.\n\nWith this approach, not only do we predict a state, but we additionally predict its rate of change\n gain\\_rate = gain\\_rate + gain\\_scale \\* (residual/time\\_step)\n\n![unknown_filename.1.png](./_resources/Improving_the_gh-filter_by_using_h.resources/unknown_filename.1.png)\nDue to bad prediction model (-1 lb/day), we have some bad early estimates and it takes the filter some time to achieve accuracy, but once it does, it remains accurate.\n\n[Effects of varying h](effects-of-varying-h.md)\n\n",title:"Untitled Page"},"/studienarbeit/imu-data-generation-from-camera-visual-data":{content:'---\ntitle: IMU data generation from camera/visual data\ndate: "2021-05-13"\ntags:\n  - -sa/processed\n  - sensors/IMU\n  - math/kinematics\n---\n\nParent: [IMU index](imu-index.md)\nSource: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)\n\n![IMG_4418.HEIC](./_resources/IMU_data_generation_from_camera_visual_data.resources/IMG_4418.HEIC)\n\n',title:"Untitled Page"},"/studienarbeit/imu-index":{content:'---\ntitle: IMU index\ndate: "2020-10-20"\ntags:\n  - -master\n  - -sa/processed\n  - sensors/IMU\n---\n\nParent: [SLAM Index](SLAM/slam_index.md)  \n\n**General**\n* [IMU](sensors/imu.md)\n* [Gyroscope](sensors/gyroscope.md)\n* [Odometry](definitions/odometry.md)\n* [Why use the visual-inertial sensor combination?](why-use-the-visual-inertial-sensor-combination_.md)\n* [IMU to camera coordinate transformations](imu-to-camera-coordinate-transformations.md)\n* [phils-lab-sensor-fusion](bibliography/phils-lab-sensor-fusion.md)\n	* [sensor-fusion](sensors/sensor-fusion.md)\n\n**Practical**  \n[Converting IMU data to inertial frame](converting-imu-data-to-inertial-frame.md)\n\n**Modelling**  \n* [Probabilistic models for IMU](probabilistic-models-for-imu.md)\n	* [Choice of model for the IMU motion model](choice of model-for-the-imu-motion-model.md)\n    * [Choice of states for the IMU motion/kinematics model](choice of states-for-the-imu-motion_kinematics-model.md)\n    * [Variables in ESKF using IMUs](variables-in-eskf-using-imus.md)\n* [IMU states, dynamics equations](imu states, dynamics equations.md) (kfs and preintegration) / [(kok) imu motion model (discrete)]((kok) imu motion model (discrete).md) /  [(forster) imu kinematic model using euler]((forster) imu kinematic model using euler.md) / [(sola) imu-motion-model]((sola)-imu-motion-model.md)\n     [The initial gravity vector/orientation for the IMU ESKF](the initial gravity-vector_orientation-for-the-imu-eskf.md)\n     [IMU nominal-state and error-state kinematics](imu-nominal-state-and-error-state-kinematics.md)\n* [IMU measurement model](imu-measurement-model.md)  \n    * [Assumptions in modelling the true angular velocity in IMUs](assumptions in modelling the-true-angular-velocity-in-imus.md)  \n    * [Modelling noise and bias for IMU](modelling-noise-and-bias-for-imu.md)\n\n**ESKF**\n* [IMU ESKF prediction equations](imu-eskf-prediction-equations.md)\n* [Fusing IMU with complementary sensory data](fusing-imu-with-complementary-sensory-data.md)\n\n**Preintegration**\n* [Preintegration of IMU](preintegration-of-imu.md)\n* [IMU preintegration on manifold](imu-preintegration-on-manifold.md)\n\n**Fake data generation**  \n[IMU data generation from camera/visual data](imu-data-generation-from-camera_visual-data.md)\n\n**Literature**\n* [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)\n* [Forster 2017 IMU Preintegration](forster-2017-imu-preintegration.md)\n* [resource IMU common specifications, error models etc](resource imu-common-specifications,-error-models-etc.md)\n* http://docs.openvins.com/propagation.html\n\n',title:"Untitled Page"},"/studienarbeit/imu-kinematic-model-using-euler-integration":{content:'---\ntitle: IMU kinematic model using Euler integration\ndate: "2020-11-27"\ntags:\n  - -sa/processed\n  - sensors/IMU\n  - sensors/IMU/preintegration\n---\n\nParent: [IMU index](imu-index.md)\nSource: [Forster 2017 IMU Preintegration](forster-2017-imu-preintegration.md)\nBacklinks: [IMU preintegration on manifold](imu preintegration on-manifold.md), [imu-measurement-model](imu-measurement-model.md)\n\nKinematic model\n![unknown_filename.png](./_resources/IMU_kinematic_model_using_Euler_integration.resources/unknown_filename.png)\n\nUsing Euler integration assuming acc and angVel are constant in the time interval:\n![unknown_filename.1.png](./_resources/IMU_kinematic_model_using_Euler_integration.resources/unknown_filename.1.png)\n\nUsing the measurement equations:\n![unknown_filename.2.png](./_resources/IMU_kinematic_model_using_Euler_integration.resources/unknown_filename.2.png)\n\n',title:"Untitled Page"},"/studienarbeit/imu-motion-model-discrete":{content:'---\ntitle: IMU motion model (discrete)\ndate: "2021-03-23"\ntags:\n  - sensors/IMU\n  - discussion/2021/2021-04\n  - -sa/to-be-processed\n---\n\nParent: [IMU index](imu index.md), [probabilistic models-for-imu](probabilistic-models-for-imu.md)\nSource: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)\n\nPosition dynamics\n![unknown_filename.png](./_resources/IMU_motion_model_(discrete).resources/unknown_filename.png)\n\nOrientation dynamics (either quaternion or rotation matrix representation)\n![unknown_filename.1.png](./_resources/IMU_motion_model_(discrete).resources/unknown_filename.1.png)\nwith\n![unknown_filename.2.png](./_resources/IMU_motion_model_(discrete).resources/unknown_filename.2.png)\n\n',title:"Untitled Page"},"/studienarbeit/imu-preintegration-on-manifold":{content:'---\ntitle: IMU preintegration on manifold\ndate: "2020-11-27"\ntags:\n  - to-do/to-clarify\n  - -sa/processed\n  - sensors/IMU\n  - sensors/IMU/preintegration\n---\n\nParent: [IMU index](imu-index.md)\nSource: [Forster 2017 IMU Preintegration](forster-2017-imu-preintegration.md)\nBacklinks: [IMU model](imu-model.md)\n\nPreintegration on manifold\n\n*   Summarising all measurements between the keyframes i and j into a single measurement\n*   This preintegrated IMU measurement constrains the motion between two consecutive keyframes\n*   Assume IMU is synchronised with the camera\n\n![unknown_filename.11.png](./_resources/IMU_preintegration_on_manifold.resources/unknown_filename.11.png)\n\n*   The above equations already provide the summarised IMU measurements,\n*   however, the integration has to be repeated whenever the linearisation point at t=t\\_i changes\n    *   i.e., change in R\\_i impolies change in all future rotations\n    *   would imply reevaluating the summations and products for each step!\n    *   we want to avoid this (repeated integrations) --\u003e try to define the relative motion increments without using pose or velocity at t\\_i\n\n![unknown_filename.3.png](./_resources/IMU_preintegration_on_manifold.resources/unknown_filename.3.png)\n![unknown_filename.6.png](./_resources/IMU_preintegration_on_manifold.resources/unknown_filename.6.png)\n![unknown_filename.png](./_resources/IMU_preintegration_on_manifold.resources/unknown_filename.png)\n\nThis is better, but there is still a dependency on the biases\n\n*   Solve the problem in two steps\n*   First assume b\\_i is known\n\nPreintegrated IMU measurements\n![unknown_filename.8.png](./_resources/IMU_preintegration_on_manifold.resources/unknown_filename.8.png)![unknown_filename.5.png](./_resources/IMU_preintegration_on_manifold.resources/unknown_filename.5.png)\n(using the properties of the exponential map: perturbations, adjoint)\nWith\n\n*   ![unknown_filename.2.png](./_resources/IMU_preintegration_on_manifold.resources/unknown_filename.2.png) preintegrated rotation measurement\n*   ![unknown_filename.1.png](./_resources/IMU_preintegration_on_manifold.resources/unknown_filename.1.png) noise of the preintegrated rotation measurement\n\nAlso,\n![unknown_filename.7.png](./_resources/IMU_preintegration_on_manifold.resources/unknown_filename.7.png)![unknown_filename.9.png](./_resources/IMU_preintegration_on_manifold.resources/unknown_filename.9.png)\n![unknown_filename.10.png](./_resources/IMU_preintegration_on_manifold.resources/unknown_filename.10.png)![unknown_filename.4.png](./_resources/IMU_preintegration_on_manifold.resources/unknown_filename.4.png)\n\nSubbing these back into the equations above, we obtain the preintegrated measurements\n![unknown_filename.12.png](./_resources/IMU_preintegration_on_manifold.resources/unknown_filename.12.png)\n(function of the to-be estimated state and a random noise)\n\n',title:"Untitled Page"},"/studienarbeit/imu-states-dynamics-equations":{content:'---\ntitle: IMU states, dynamics equations\ndate: "2021-04-23"\ntags:\n  - -sa/processed\n  - sensors/IMU\n---\n\nParent: [IMU index](imu-index.md)  \n**Source**: [Mur-Artal 2017 VI-ORB](bibliography/mur-artal-2017-vi-orb.md)\n\nEvolution of IMU states (world frame to IMU: orientation R, position p, velocity v) between consecutive keyframes\n\n![unknown_filename.1.png](./_resources/IMU_states,_dynamics_equations.resources/unknown_filename.1.png)\n\nEvolution of IMU states (world frame to IMU: orientation R, position p, velocity v) between consecutive frames\n\nUsing the [preintegration](preintegration.md) terms ![unknown_filename.png](./_resources/IMU_states,_dynamics_equations.resources/unknown_filename.png) \n![unknown_filename.2.png](./_resources/IMU_states,_dynamics_equations.resources/unknown_filename.2.png)\n\nPreintegration (delta) terms and the Jacobians can be computed iteratively as IMU measurements arrive (s. Forster\'s paper on preintegration)\n\n',title:"Untitled Page"},"/studienarbeit/imu-to-camera-coordinate-transformations":{content:'---\ntitle: IMU to camera coordinate transformations\ndate: "2021-05-13"\ntags:\n  - sensors/IMU\n  - math/kinematics\n  - discussion/2021/2021-05\n  - -sa/to-be-processed\n---\n\nParent: [IMU index](imu-index.md)\nSource: Weiss 2011\n\n![IMG_4417.HEIC](./_resources/IMU_to_camera_coordinate_transformations.resources/IMG_4417.HEIC)\n\n',title:"Untitled Page"},"/studienarbeit/in-vivo":{content:'---\ntitle: In vivo\ndate: "2020-08-24"\nexternal_url: "http://en.wikipedia.org/wiki/In_vivo"\ntags:\n  - -definitions\n  - -sa/processed\n---\n\n*   Latin for "within the living"\n*   studies in which the effects of various biological entities are tested on whole, living organisms or cells, as opposed to a tissue extract/dead organism\n\n',title:"Untitled Page"},"/studienarbeit/information-filter":{content:'---\ntitle: Information Filter\ndate: "2020-08-23"\ntags:\n  - to-do/to-clarify\n  - -sa/processed\n  - filters/kalman-filter\n---\n\nParent: [General Kalman Filter](general-kalman-filter.md)\n\nSource: [Scaradozzi 2018 SLAM application in surgery](studienarbeit/scaradozzi-2018.md)\n\n*   also same assumptions as the [EKF](http://www.evernote.com/shard/s484/nl/217355218/a3417515-123a-4310-ac2f-937cd4878942)\n*   main difference: how the Gaussian belief is represented\n*   est. cov. — replaced by information matrix (IM)\n*   est. state — replaced by information vector (IV)\n*   superior to KF in the following ways\n    *   data is filtered by summing up the IMs and IVs\n    *   often numerically more stable\n\nDual character of KF and IF\n\n*   in IF: prediction step requires two matrix inversions\n    *   higher computational complexity\n    *   high-dimension state space\n    *   while for KF, update in the prediction step is additive\n*   roles are reversed in measurement step\n\nVariants of IF\n\n*   EIF (extended IF)\n*   SEIF (sparse extended IF)\n\nInspired by SLAM filters that are used to represent relative distances\n\n',title:"Untitled Page"},"/studienarbeit/initialising-graph-in-sp3":{content:'---\ntitle: Initialising graph in SP3\ndate: "2020-09-23"\nexternal_url: "http://github.com/SofaDefrost/plugin.SofaPython3.deprecated/pull/110"\ntags:\n  - -sa/processed\n  - software/SOFA/SofaPython3\n---\n\n**Parent:** [SofaPython Index](sofapython-index.md)\n\n\u003chttp://github.com/SofaDefrost/plugin.SofaPython3.deprecated/pull/110\u003e\n\n"Can you share an example of a scene and a component you have in mind ?\nBecause currently to summary the discussion during sofa-meeting the problem with init/bdwInit/reinit is that it that this is severely ill defined and we are considering to totally remove that from Sofa and use alternatives pattern among which:\n\n*   have an onSimulationStart / onSimulationStop event to detect when the simulation is on or not\n*   avoid using getContext() to fetch other components unless you store them in SingleLink.\n*   systematic use of SingleLinks to exhibit inter component dependency\n*   use ComponentState tracker to detect the change of state of one component from another\n    [sofa-framework/sofa\n- 1168](http://github.com/sofa-framework/sofa/pull/1168)\n    \n*   use the node notification API to detect node adding/removal."\n\n',title:"Untitled Page"},"/studienarbeit/install-rosconnector-in-sofa":{content:'---\ntitle: Install ROSConnector in SOFA\ndate: "2020-07-17"\ntags:\n  - software/SOFA/plugins\n  - -sa/processed\n---\n\nParent: [SofaPython Index](sofapython-index.md)\n\n\u003chttp://github.com/sofa-framework/SofaROSConnector\u003e\nDocumentation (outdated for current SOFA version 20.06.00)\n\n\u003chttp://www.sofa-framework.org/community/forum/topic/error-configuring-cmake-sofarosconnector\u003e/\nPending answer. Last reply 10th July 2020.\n\nAlternative using SoftRobots:\n\u003chttp://www.sofa-framework.org/community/forum/topic/error-with-plugins-with-sofarosconnector/\n- post-15665\u003e\n\u003chttp://project.inria.fr/softrobot\u003e/\n\n',title:"Untitled Page"},"/studienarbeit/internal-model":{content:'---\ntitle: Internal model\ndate: "2020-07-15"\ntags:\n  - software/SOFA/models\n  - -sa/processed\n---\n\nSource:  [SOFA extended documentation](sofa-extended-documentation.md)\nParent: [Models in SOFA](models-in-sofa.md)\n\nFor the internal deformable mechanics\n\n*   Contains the independent DOFs, mass and physical laws\n*   Mechanical behaviour modelled e.g. by FEM\n*   Geometry of this model is optimised for the computation of internal forces\n    *   usually by using a reduced number of well-shaped tetrahedra\n    *   this increases speed and stability\n    *   however not accurate enough for [collision detection](http://www.evernote.com/shard/s484/nl/217355218/3d43d987-12c2-40e5-bf8e-6754c868ea4d)\n    *   nor is it smooth enough for [visuals](http://www.evernote.com/shard/s484/nl/217355218/8446d918-bc51-46f5-822a-d8405da17ade)\n\n|     |     |\n| --- | --- |\n| ![unknown_filename.png](./_resources/Internal_model.resources/unknown_filename.png) | *   Boxes: fixed nodes\u003cbr\u003e*   Arrows: external forces |\n\n[Mathematical model of the internal model in SOFA](mathematical model of-the-internal-model-in-sofa.md)\n[Internal model as a scene graph in SOFA](internal model as-a-scene-graph-in-sofa.md)\n\n',title:"Untitled Page"},"/studienarbeit/internal-model-as-a-scene-graph-in-sofa":{content:'---\ntitle: Internal model as a scene graph in SOFA\ndate: "2020-08-09"\ntags:\n  - software/SOFA/models\n  - -sa/processed\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\nParent: [Internal model](internal-model.md)\n\nScene graph of the internal model\n\n*   Consists of components which are connected to a common scenegraph node (root of the internal model)\n*   Each component is responsible for a set of tasks\n    Examples: solver, mass, constraints, ...\n    \n*   Each component can query its parent node to get access to the its sibling components such as [MechanicalState](http://www.evernote.com/shard/s484/nl/217355218/d4586073-9cf7-40f7-9750-a8736a94457f), topology\n*   Components are independent of one another — modularity\n\n[Components of the internal model](components-of-the-internal-model.md)\n![unknown_filename.png](./_resources/Internal_model_as_a_scene_graph_in_SOFA.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/intro-to-bladder-cancer":{content:'---\ntitle: Intro to bladder cancer\ndate: "2020-12-13"\ntags:\n  - -resources\n  - medical/cancer\n  - -sa/to-be-processed\n---\n\n\u003chttp://www.cancer.net/cancer-types/bladder-cancer/introduction\u003e\n\n',title:"Untitled Page"},"/studienarbeit/introduction-to-the-kalman-filter":{content:"---\ntitle: Introduction to the Kalman Filter\ntags:\n- -resources\n- filters/kalman-filter\n- -sa/to-be-processed \n---\n\n[http://resourcium.org/journey/introduction-kalman-filter](http://resourcium.org/journey/introduction-kalman-filter)\n",title:"Untitled Page"},"/studienarbeit/inverse-of-a-homogeneous-transformation-matrix":{content:'---\ntitle: Inverse of a homogeneous transformation matrix\ndate: "2021-06-01"\ntags:\n  - -sa/processed\n  - math/kinematics\n---\n\n**Parent**: [Kinematics primer](kinematics-primer.md)\n**Source**: \u003chttp://mathematica.stackexchange.com/questions/106257/how-do-i-get-the-inverse-of-a-homogeneous-transformation-matrix\u003e\n\n![unknown_filename.png](./_resources/Inverse_of_a_homogeneous_transformation_matrix.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/inverse-quaternion":{content:'---\ntitle: Inverse quaternion\ndate: "2021-05-14"\ntags:\n  - -sa/processed\n  - math/quaternions\n---\n\nParent: [Quaternion index](rotations/quaternion-index.md)\nSource: [Solà 2017 Quaternion kinematics for ESKF](solà-2017-quaternion-kinematics-for-eskf.md)\n\n![unknown_filename.1.png](./_resources/Inverse_quaternion.resources/unknown_filename.1.png)\nThe inverse is the conjugate in case of [unit quaternions](rotations/unit-quaternions.md)\n\n![unknown_filename.png](./_resources/Inverse_quaternion.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/jeon-2009-kinematic-kalman-filter-for-robot-end-effector-sensing":{content:'---\ntitle: Jeon 2009 Kinematic Kalman Filter for Robot End-Effector Sensing\ndate: "2021-05-26"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-read\n  - discussion/2021/2021-05\n  - -sa/to-be-processed\n---\n\nBacklinks: [Discussion 2021-05-25](discussion-2021-05-25.md)\n\nAuthors: Jeon and Tomizuka\n\nAbstract\n\n*   inaccuracies in estimation of EE motion can come from kinematic error (error in parameters in kinematic equations)\n    *   to overcome this: take direct measurements e.g. using vision, but vision has high latency\n    *   IMUs are used to provide interframe data\n*   fuse camera and IMU in a kinematic Kalman filter (KKF) framework.\n    Note: uses ESKF\n    \n*   effect of camera measurement delay, augmenting the KF states to also estimate the time delay\n\n|     |     |\n| --- | --- |\n| ![unknown_filename.1.png](./_resources/[Jeon_2009]_Kinematic_Kalman_Filter_for_Robot_End-Effector_Sensing.resources/unknown_filename.1.png) | ![unknown_filename.png](./_resources/[Jeon_2009]_Kinematic_Kalman_Filter_for_Robot_End-Effector_Sensing.resources/unknown_filename.png) |\n\n',title:"Untitled Page"},"/studienarbeit/kalman-gain-for-ekf":{content:'---\ntitle: Kalman gain for EKF\ndate: "2020-08-06"\ntags:\n  - filters/EKF\n  - -sa/processed\n---\n\nSource: [SLAM for Dummies](slam-for-dummies.md)\nBacklinks:  [EKF matrices](ekf-matrices.md)\n\nHow much we will trust the observed landmarks\n\n*   compromise between odometry and landmark correction\n*   uses\n    *   uncertainty of observed landmarks\n    *   measure of quality of the range measurement device\n    *   odometry performance\n\nGains for range and brearing (3+2n x 2)\n\n![unknown_filename.png](./_resources/Kalman_gain_for_EKF.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/kalman-gain-using-gaussians":{content:'---\ntitle: Kalman gain using Gaussians\ndate: "2020-08-31"\ntags:\n  - filters/kalman-filter\n  - -sa/to-be-processed\n---\n\nParent: [1D Kalman filters](1d-kalman-filters.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nKalman gain in the [update step](http://www.evernote.com/shard/s484/nl/217355218/830f0fa2-05fa-4c1a-a183-a6cbf0bdff4e)\n![unknown_filename.1.png](./_resources/Kalman_gain_using_Gaussians.resources/unknown_filename.1.png)\n\n*   Basically a scaling term that chooses a value between the sensor distr. mean and the posterior distr. mean\n*   Gives greater weight to the term with lower variance (we trust this data more!)\n\nMean and variance in terms of the Kalman gain\n![unknown_filename.png](./_resources/Kalman_gain_using_Gaussians.resources/unknown_filename.png)\n\nVariance of the filter (i.e., what variance is show by the estimated output/posterior?)\n\n*   Always converges to a fixed value if the sensor and process variances are constant\n*   We can run simulations to determine the value to which the filter variance converges\n*   Then hard code this value into the filter (+ with first sensor measurement as initial value, the filter should have good performance)\n*   Alternative: instead of using the variance value, use the calculated Kalman gain\n\nExample implementation using the Kalman gain\nHowever, using the Kalman gain obscures the Bayesian approach\n![unknown_filename.2.png](./_resources/Kalman_gain_using_Gaussians.resources/unknown_filename.2.png)\n\n',title:"Untitled Page"},"/studienarbeit/kalman-vs.-nonlinear-systems":{content:'---\ntitle: Kalman vs. nonlinear systems\ndate: "2020-08-31"\ntags:\n  - -sa/processed\n  - filters/kalman-filter\n---\n\nParent: [Factors affecting Kalman filter performance](factors-affecting-kalman-filter-performance.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nKalman filter equations are linear\n![unknown_filename.png](./_resources/Kalman_vs._nonlinear_systems.resources/unknown_filename.png)\n\nExample: approximating a sine-wave signal\n![unknown_filename.1.png](./_resources/Kalman_vs._nonlinear_systems.resources/unknown_filename.1.png)\n\nExplanation:\n\n*   Back to the basic g-h filter structure: the filter output chooses a value on the residual line\n*   The process model in the underlying filter assumes constant velocity (0 acceleration), whereas in the sine example above, the signal is always accelerating\n\n',title:"Untitled Page"},"/studienarbeit/kf-kinematics":{content:'---\ntitle: KF kinematics\ndate: "2021-07-09"\ntags:\n  - to-do/orphan\n  - -sounding-board\n  - -sa/processing\n  - filters/ESKF\n  - discussion/2021/2021-07\n---\n\nOverview of KF states (true, nominal, error)\n![unknown_filename.1.png](./_resources/KF_kinematics.resources/unknown_filename.1.png)\n\nNominal state kinematics\n![unknown_filename.2.png](./_resources/KF_kinematics.resources/unknown_filename.2.png)\n\nError state kinematics\n![unknown_filename.3.png](./_resources/KF_kinematics.resources/unknown_filename.3.png)\n\nOld stuff:\n![unknown_filename.jpeg](./_resources/KF_kinematics.resources/unknown_filename.jpeg)\n\n',title:"Untitled Page"},"/studienarbeit/kidnapped-robot-problem":{content:'---\ntitle: Kidnapped robot problem\ndate: "2020-07-30"\ntags:\n  - localisation\n  - SLAM/robustness-of-SLAM\n  - -sa/processed\n---\n\nSource: [Wikipedia Lokalisierung](wikipedia-lokalisierung.md)\n**Backlinks**: [Lamarca 2020 DefSLAM](studienarbeit/lamarca-2020.md)\n\n*   Position initially known\n*   Then robot is repositioned without knowing it\n*   Robot has to be able to realise that the initial successful localisation isn\'t valid any more -- a new global localisation must be carried out\n*   Realise this via unplausible sensor measurements (huge contradiction to prev. measurements)\n*   Has to do with the measure of robustness of the localisation method carried out\n\n',title:"Untitled Page"},"/studienarbeit/kinematics-equations-of-motion-imu-to-camera":{content:'---\ntitle: Equations of motion IMU to camera\ndate: "2021-06-07"\ntags: \n- -sa/processed \n- math/kinematics \n- discussion/2021/2021-06\n---\n\nBacklinks: [Discussion 2021-06-01](discussion-2021-06-01.md)\n\n![unknown_filename.png](./_resources/Kinematics__equations_of_motion_IMU_to_camera.resources/unknown_filename.png)\n\nR\\_WB = R\\_WC \\* R\\_CB\n\n**Notes**: ref \u003chttp://docs.sympy.org/latest/modules/physics/vector/vectors.html\u003e for vector calculus (symbolic)\n\n',title:"Untitled Page"},"/studienarbeit/kinematics-primer":{content:'---\ntitle: Kinematics primer\ndate: "2021-05-24"\ntags:\n  - -master\n  - -sa/processed\n  - math/kinematics\n  - discussion/2021/2021-06\n---\n\nSource: [Hibbeler Dynamics](hibbeler-dynamics.md), [woernle-mehrkörpersysteme](woernle-mehrkörpersysteme.md)\nSee also: [Reversed kinematics relations](reversed-kinematics-relations.md), [denavit-hartenberg-convention](denavit-hartenberg-convention.md)\nBacklinks: [Obtaining IMU measurements from camera by forward kinematics](obtaining imu measurements from camera by forward kinematics.md), [rotations / so(3) group-index](rotations-_-so(3)-group-index.md)\n\nPrereqs:\n\n*   [Chaining rotation matrices and angular velocities](chaining-rotation-matrices-and-angular-velocities.md)\n*   [Converting velocity from CS1 to CS0](converting-velocity-from-cs1-to-cs0.md)\n*   [Poisson equation for skew symmetric matrix of angular velocity](poisson equation for skew-symmetric-matrix-of-angular-velocity.md)\n*   [Inverse of a homogeneous transformation matrix](inverse-of-a-homogeneous-transformation-matrix.md)\n*   [Differentiation in different coordinate systems](differentiation-in-different-coordinate-systems.md)\n\n|     |     |\n| --- | --- |\n| Position (in world coordinates)\u003cbr\u003e![unknown_filename.png](./_resources/kinematics_primer.resources/unknown_filename.png)\u003cbr\u003e\u003cbr\u003e![unknown_filename.5.png](./_resources/kinematics_primer.resources/unknown_filename.5.png)\u003cbr\u003e\u003cbr\u003evelocity (in world coordinates)\u003cbr\u003e![unknown_filename.1.png](./_resources/kinematics_primer.resources/unknown_filename.1.png)\u003cbr\u003e![unknown_filename.2.png](./_resources/kinematics_primer.resources/unknown_filename.2.png)\u003cbr\u003e\u003cbr\u003e![unknown_filename.7.png](./_resources/kinematics_primer.resources/unknown_filename.7.png)\u003cbr\u003ewhere the skew symmetric matrix is the angular velocity of cs1 relative to cs0,\u003cbr\u003e(~~given in cs1 coordinates? ~~ cs whatever - [x] )\u003cbr\u003e\u003cbr\u003eacceleration (in world coordinates)\u003cbr\u003e![unknown_filename.3.png](./_resources/kinematics_primer.resources/unknown_filename.3.png)\u003cbr\u003e![unknown_filename.4.png](./_resources/kinematics_primer.resources/unknown_filename.4.png)\u003cbr\u003e\u003cbr\u003e![unknown_filename.9.png](./_resources/kinematics_primer.resources/unknown_filename.9.png) |-![unknown_filename.8.png](./_resources/kinematics_primer.resources/unknown_filename.8.png)\u003cbr\u003e[hibbeler-dynamics](hibbeler-dynamics.md)\u003cbr\u003e\u003cbr\u003e![unknown_filename.6.png](./_resources/kinematics_primer.resources/unknown_filename.6.png)\u003cbr\u003e[woernle-mehrkörpersysteme](woernle-mehrkörpersysteme.md) |\n\n',title:"Untitled Page"},"/studienarbeit/lamarca-2020":{content:'---\ntitle: Lamarca 2020 DefSLAM\ndate: "2020-08-06"\ntags:\n  - -resources/-bibliography\n  - to-do/go-through-literature-later\n  - -resources/-bibliography/bib-read\n  - discussion/2020/2020-08\n  - -sa/processed\n  - discussion/2020/2020-09\n  - SLAM/algos/DefSLAM\n  - SLAM/deformable\n---\n\n**URL**: \u003chttp://arxiv.org/abs/1908.08918\u003e  \n**Authors**: Lamarca et al  \n**Code**: \u003chttp://github.com/UZ-SLAMLab/DefSLAM\u003e  \n**Results** (video): [http://www.youtube.com/watch?v=6mmhD2\\_t6Gs](http://www.youtube.com/watch?v=6mmhD2_t6Gs)\n\n## Summary\n\n*   First monocular SLAM for deformable environments in real-time\n    *   Most other SLAM implementations assume rigidity\n*   Main techniques used (techniques for monocular non-rigid scenes):\n    *   isometric shape from template ([SfT](studienarbeit/sft.md))\n    *   non-rigid structure from motion ([NRSfM](studienarbeit/nrsfm.md))\n*   Main principle: computation in [two parallel threads (s. DefSLAM framework)](studienarbeit/defslam-framework.md)\n    *   Deformation tracking \\[front end\\]\n    *   Deformation mapping \\[back end\\]\n*   The map from the mapping thread defines the shape-at-rest template used by deformation tracking\n*   Validation: compare with ORBSLAM (rigid)\n*   Assumes isometric deformation\n*   Future work: relocalisation (s. [kidnapped robot problem](kidnapped-robot-problem.md)), loop closure for robustness\n\n## Contents\n1. Introduction\n2. [Initialisation of monocular SLAM](SLAM/initialisation-of-monocular-slam.md)\n3. Most SLAM algorithms exploit the rigidity assumption!\n4. [NRSfM](studienarbeit/nrsfm.md) and [SfT](studienarbeit/sft.md)\n5. [DefSLAM framework](defslam-framework.md)\n6. [DefSLAM and discontinuous areas (classical datasets)](studienarbeit/defslam-and-discontinuous-areas-classical-datasets.md)\n\n## Related work\n* in visual SLAM\n	*   Existing deformable visual SLAM\n		*   mostly using RGB-D or stereo cameras\n		*   the ones mentioned optimise the whole map, but hier DefSLAM only optimises the observed map zone (local zone)\n	*   Rigid methods used in (semi-)deformable environments\n		*   assume negligible deformation\n		*   circumvent deformable situations by excluding any deformable regions from the map\n\nIn DefSLAM: [energy-based SfT](studienarbeit/sft.md) and [isometric NRSfM](studienarbeit/nrsfm.md) are used.\n\n## Notation\n\n|     |     |\n| --- | --- |\n| Map | Set of 3D map points |\n| ![unknown_filename.6.png](./_resources/[Lamarca_2020]_DefSLAM.resources/unknown_filename.6.png) | j-th map point in frame t |\n| ![unknown_filename.1.png](./_resources/[Lamarca_2020]_DefSLAM.resources/unknown_filename.1.png) | camera pose in frame t |\n| ![unknown_filename.2.png](./_resources/[Lamarca_2020]_DefSLAM.resources/unknown_filename.2.png) | surfaced observed in keyframe k |\n| ![unknown_filename.3.png](./_resources/[Lamarca_2020]_DefSLAM.resources/unknown_filename.3.png) | template (has map points embedded into it), based initially on keyframe k (deformations can occur after k) -- already explored areas |\n| ![unknown_filename.4.png](./_resources/[Lamarca_2020]_DefSLAM.resources/unknown_filename.4.png) | local zone (currently being viewed area) |\n| ![unknown_filename.5.png](./_resources/[Lamarca_2020]_DefSLAM.resources/unknown_filename.5.png) | image |\n| ![unknown_filename.7.png](./_resources/[Lamarca_2020]_DefSLAM.resources/unknown_filename.7.png) | feature keypoint in image |\n| ![unknown_filename.8.png](./_resources/[Lamarca_2020]_DefSLAM.resources/unknown_filename.8.png) | *   warp between the keyframes k and k\\*\u003cbr\u003e*   image transformation between Ik to Ik\\* |\n| ![unknown_filename.9.png](./_resources/[Lamarca_2020]_DefSLAM.resources/unknown_filename.9.png) | *   embedding of an image point onto the scene surface\u003cbr\u003e*   transforms an image point (2D) into a point on a 3D surface |\n| Anchor keyframe \\[of a map point\\] | *   Keyframe in which a map point is initialised\u003cbr\u003e*   After each new KF processing, one of the anchor KFs is selected as the reference KF |\n| Reference keyframe | Defines the template used by the deformation tracking |\n\n## Tracking\nComponents: map/template, local zone, camera pose\n\n## Stages of tracking:\n\n1.  Data association\n2.  Camera pose estimation\n3.  Template deformation\n4.  New keyframe selection: as soon as mapping finishes one run\n    *   If new KF of a new map: this KF becomes an anchor KF, the ref. KF, and a new template is created\n    *   If new KF of known region: this KF is a regular KF, the most covisible anchor KF is selected as ref KF, template is refined (deformed)\n\n[Template in DefSLAM](template-in-defslam.md)\n[Pinhole camera projection function](pinhole-camera-projection-function.md)\n[Tracking optimisation in DefSLAM](tracking-optimisation-in-defslam.md)\n[Data association in DefSLAM](data-association-in-defslam.md)\n\n## Keyframe selection\nA new keyframe is selected when the mapping finishes the last estimation, i.e. new keyframe at the end of each deformation mapping run\n\n## Mapping\nIn a nutshell:\n\n*   recovers observed map as a surface for the next keyframe k\n*   the surface Sk is the shape-at-rest of template Tk for the next frames\n\n[Mapping step-by-step in DefSLAM](mapping-step-by-step-in-defslam.md)\n\n*   [NRSfM in DefSLAM](nrsfm-in-defslam.md)\n*   [Surface alignment in DefSLAM](surface-alignment-in-defslam.md)\n*   [Template substitution in DefSLAM](template-substitution-in-defslam.md)\n\n[Non-Rigid Guided Matching (b/w KFs) in DefSLAM](non-rigid guided-matching-(b_w-kfs)-in-defslam.md)\n\n## Validation results\nIn the experiments, DefSLAM was run sequentially in a single thread (for repeatability)!\n\n![unknown_filename.png](./_resources/[Lamarca_2020]_DefSLAM.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/laparoscopy":{content:'---\ntitle: Laparoscopy\ndate: "2020-08-08"\ntags:\n  - medical/surgery\n  - -definitions\n  - -sa/processed\n---\n\nSource: \u003chttps://en.wikipedia.org/wiki/Laparoscopy\u003e\nBacklinks: [Related types of surgery](Related types of surgery.md), [Some questions](Some questions.md), [Trocar](Trocar.md)\n\n*   minimally invasive surgery (MIS) / keyhole surgery\n*   make a small incision in the abdomen area and operate through it\n\n',title:"Untitled Page"},"/studienarbeit/leiner":{content:'---\ntitle: (Leiner) Digital Endoscope Design\ndate: "2021-05-21"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-read\n  - -sa/processed\n  - medical/surgery/endoscope\n---\n\n**Backlinks**: [Endoscopes](permanent/30-endoscopes-index.md)\n**URL**: \u003chttp://www.spiedigitallibrary.org/ebooks/SL/Digital-Endoscope-Design/1/Digital-Endoscope-Design/10.1117/3.2235283.ch1?SSO=1\u003e\n\nNotes\n[Insertion of an endoscope](insertion-of-an-endoscope.md)\n[Types of endoscopes](permanent/30.1.2.1-types-of-endoscopes.md)\n[Endoscope system components](permanent/30.1.2.2-endoscope-system-components.md)\n[Endoscope specification](permanent/30.1.2.3-endoscope-specification.md)\n\n',title:"Untitled Page"},"/studienarbeit/limitations-of-the-discrete-bayes-filter":{content:'---\ntitle: Limitations of the discrete Bayes filter\ndate: "2020-08-31"\ntags:\n  - -sa/processed\n  - filters/bayesian-filter\n---\n\nParent: [Discrete Bayesian filter](discrete-bayesian-filter.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nLimitations of the discrete Bayes filter\n\n*   Scaling\n    *   Dog tracking example is one-dimensional, but in real life we often want to track more things (e.g. 2D coordinates, velocities)\n    *   Multidimensional case: store probabilities in a grid\n    *   4 tracked variables: O(n^4) per time step\n    *   High computational cost with high dimensionality\n*   Filter is discrete and therefore gives discrete output\n    *   But a lot of applications require continuous output\n    *   Discretising a solution space can lead to lots of data (depending on accuracy required) --\u003e calculations for lots of different probabilities!\n*   Filter is multimodal\n    *   Not always a problem, e.g. particle filters are multimodal\n    *   But not always a good things either, sometimes not a realistic representation of the reality (e.g. 40% in this location, 30% in the other location)\n*   Requires measurement of the change in state (dog tracking example assumes movement by 1 unit)\n\nWe want a unimodal, continuous filter --\u003e achieve this by using [Gaussian distributions](http://www.evernote.com/shard/s484/nl/217355218/12fcf819-bea6-4160-9bf0-fd22e8f59cb1).\n\n',title:"Untitled Page"},"/studienarbeit/linear-solvers":{content:'---\ntitle: Linear solvers\ndate: "2020-08-22"\ntags:\n  - -sa/processed\n  - software/SOFA/simulation-algos\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\nParent: [Simulation algorithms in SOFA](simulation-algorithms-in-sofa.md)\n\nConjugate gradient\n![unknown_filename.png](./_resources/Linear_solvers.resources/unknown_filename.png)\nJ: first-order mapping of a node to its parent\npath(i): list of mappings from the independent DOFs to the node the force applies to\n\nComputation using a visitor:\n![unknown_filename.1.png](./_resources/Linear_solvers.resources/unknown_filename.1.png)\n\n*   Top down visitor: propagates the given displacement, clears force vector\n*   Bottom up visitor: accumulates forces, maps them up to the independent DOFs\n\nDirect solvers\n\n*   can be used as preconditioners of the conjugate gradient algorithm\n*   can be used to solve [the equation system A\\*dv=b](http://www.evernote.com/shard/s484/nl/217355218/e3d45207-3f91-4941-8a1d-0573043b4546)\n*   implementations are based external libraries\n\n',title:"Untitled Page"},"/studienarbeit/linearisation-of-an-orientation-in-so-3":{content:'---\ntitle: Linearisation of an orientation in SO(3)\ndate: "2021-03-23"\ntags:\n  - -sa/processed\n  - math/rotations\n  - math/quaternions\n  - -published\n---\n\n**Parents**: [Rotations / SO(3) group index](rotations/rotations-so3-group-index.md), [Quaternion index](rotations/quaternion-index.md), [orientation-parametrisations](orientation-parametrisations.md)  \n**Source**: [MKok 2017](mkok-2017.md)\n\nRotation of a vector in SO(3)\n\n*   The [SO(3)](rotations/so3-3d-rotation-group.md) group is a [Lie group](rotations/lie-group-lie-algebra.md), so there exists\n    *   an [exponential map](rotations/exponential-map.md) from a corresponding Lie algebra to the SO(3) group\n    *   a reverse [logarithm map](rotations/logarithm-map.md)\n*   Possible to represent\n    *   orientations using unit quaternions or rotation matrices in SO(3) — linearisation point\n        ![unknown_filename.1.png](./_resources/Linearisation_of_an_orientation_in_SO(3).resources/unknown_filename.1.png)\n    *   orientation deviations $\\eta_t$\n\n![unknown_filename.2.png](./_resources/Linearisation_of_an_orientation_in_SO(3).resources/unknown_filename.2.png)\n\n- [x] I think this is a global representation\n\n',title:"Untitled Page"},"/studienarbeit/literature-management":{content:'---\ntitle: Literature management\ndate: "2020-08-04"\ntags:\n  - -resources/-bibliography/meta\n  - -sa/processed\n---\n\n*   Only focus on one paper per topic at a time\n*   Skim through and take notes on only the important chapters\n    *   Link and backlink\n*   Note which topics were skimmed\n*   Come back later for further literature review\n\n',title:"Untitled Page"},"/studienarbeit/localisation":{content:"---\ntitle: Localisation\ndate: \"2020-07-30\"\nexternal_url: https://de.wikipedia.org/wiki/Lokalisierung_(Robotik)\ntags:\n  - localisation\n  - -sa/processed\n---\n\nParent: [SLAM Index](SLAM Index.md)\n\nSource: [Wikipedia Lokalisierung](Wikipedia Lokalisierung.md)\nThe positioning of an autonomous mobile robot relative to its environment\n\n*   The position of a mobile robot is seldom known exactly\n*   An unknown initial position / measurement uncertainties while moving\n*   Becomes a SLAM problem when neither the position nor the map is known\n\nGoal/Output: POSE\n\n*   Due to uncertainties etc, it's good to have a POSE representation that also shows these uncertainties\n*   e.g. probability densities, particle clouds\n\nApproaches mostly fusion-based (odometry/sensors + landmarks)\n\n*   Cross-bearing known landmarks\n*   Template-matching current sensor measurements (auch Scan-Matching)\n*   Probabilistic methods\n\nLocal and global localisation\n\n*   Local\n    *   current POSE in the environment is known\n    *   correct the incremental odometry error that occurs every step\n*   Global\n    *   current POSE in the environment is unknown\n    *   position errors aren't negligible\n    *   error of the initial estimated position can be arbitrarily huge\n    *   the robot has to determine its position first through finding significant landmarks, only then can a local localisation be carried out\n*   [Kidnapped robot problem](https://www.evernote.com/shard/s484/nl/217355218/0bf37f21-1c54-6e11-7680-fefa5f5044df?title=Kidnapped%20robot%20problem) (check robustness)\n\n",title:"Untitled Page"},"/studienarbeit/loose-vs-tight-coupling":{content:'---\ntitle: Loose vs Tight coupling\ndate: "2020-07-30"\ntags:\n  - SLAM/multisensor-SLAM\n  - -sa/processed\n---\n\nParent: [SLAM Index](SLAM/slam_index.md)\nBacklinks: [Multisensor fusion](multisensor-fusion.md)\nSource: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)\n\nIn loosely-coupled systems: all sensor states are independently estimated and optimized\n\n*   easier to process frame and IMU data\n*   less accurate/robust compared to tight coupling\n*   e.g. Integrated IMU data can be incorporated as independent measurements in stereo vision optimization\n*   e.g. Vision-only pose estimates are used to update an EKF so that IMU propagation can be performed\n\nIn tightly-coupled systems: all sensor states are jointly estimated and optimized\n\n*   more difficult to process frame and IMU data\n*   more accurate/robust compared to loose coupling\n\n',title:"Untitled Page"},"/studienarbeit/maley-2013-mekf-for-nonspinning-guided-projectiles":{content:'---\ntitle: Maley 2013 MEKF for Nonspinning Guided Projectiles\ndate: "2021-08-16"\nexternal_url: "http://apps.dtic.mil/sti/citations/ADA588831"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-to-read\n  - filters/MEKF\n  - -sa/to-be-processed\n---\n\n**Source**: \u003chttp://apps.dtic.mil/sti/citations/ADA588831\u003e\n\n',title:"Untitled Page"},"/studienarbeit/manifolds":{content:'---\ntitle: Manifolds\ndate: "2020-11-27"\ntags:\n  - -definitions\n  - -sa/processed\n  - math\n---\n\n**Source**: \u003chttps://www.euclideanspace.com/maths/geometry/space/surfaces/manifold/index.htm\u003e\n\n*   Like a surface in $n$-dimensions (hypersurface)\n*   An $n$-dim manifold looks like $\\mathbb{R}^n$ locally (locally Euclidian)\n    *   Circle: 1-dim manifold. If we zoom around a point on the circle, it looks like a line ($\\mathbb{R}^1$)\n    *   Sphere: 2-dim manifold. Zooming onto a point, it looks like a plane ($\\mathbb{R}^2$)\n\n---\n\n**Source**: \u003chttps://www.seas.upenn.edu/~meam620/slides/kinematicsI.pdf\u003e\n\nAn $n$-dim manifold is a a set $M$ which is locally homeomorphic to $\\mathbb{R}^n$\n\n* Homeomorphism: a map $f$ from $M$ to $N$ and its inverse are both continuous\n* Smooth map: all partial derivatives of $f$, of all orders, exist and are continuous\n* Diffeomorphism: smooth map and with all partial derivatives of $\\text{inv}(f)$, of all orders, exist and are continuous\n\nA group that is a differentiable manifold is called a [Lie group](rotations/lie-group-lie-algebra.md).\n',title:"Untitled Page"},"/studienarbeit/map-estimation":{content:"---\ntitle: MAP estimation\ndate: \"2020-11-27\"\ntags:\n  - sensors/IMU/preintegration\n  - -sa/to-be-processed\n---\n\nSource: [Forster 2017 IMU Preintegration](forster-2017-imu-preintegration.md)\n\nFactor graph: way of representing the posterior probability of the states ![unknown_filename.2.png](./_resources/MAP_estimation.resources/unknown_filename.2.png) given the available measurements ![unknown_filename.png](./_resources/MAP_estimation.resources/unknown_filename.png) and priors ![unknown_filename.1.png](./_resources/MAP_estimation.resources/unknown_filename.1.png)\n![Image.png](./_resources/MAP_estimation.resources/Image.png)\n\n![unknown_filename.3.png](./_resources/MAP_estimation.resources/unknown_filename.3.png)\n\nThe terms in the equation above are called 'factors'\n\n*   MAP: maximum a posteriori\n*   We want to maximise the probability derived above --\u003e MAP estimate\n*   (aka minimum of negative log posterior)\n*   The negative log posterior can be written as a sum of squared residuals, **assuming** zero-mean Gaussian noise\n\n![unknown_filename.4.png](./_resources/MAP_estimation.resources/unknown_filename.4.png)\n\n|     |     |\n| --- | --- |\n| ![unknown_filename.5.png](./_resources/MAP_estimation.resources/unknown_filename.5.png) | residual errors (prior, IMU, camera) |\n| ![unknown_filename.6.png](./_resources/MAP_estimation.resources/unknown_filename.6.png) | covariance matrices |\n\nHow do we define these residuals?\n\n",title:"Untitled Page"},"/studienarbeit/mapping-in-viorb":{content:'---\ntitle: Mapping in VIORB\ndate: "2020-10-20"\ntags:\n  - -sa/processed\n  - SLAM/algos/VIORB\n---\n\nSource: [Mur-Artal 2017 VI-ORB](bibliography/mur-artal-2017-vi-orb.md)\n\nMapping in VIORB\nPreviously in ORBSLAM (only poses are optimised):\n![unknown_filename.png](./_resources/Mapping_in_VIORB.resources/unknown_filename.png)\n\nNow in VIORB, more states to optimise:\n![unknown_filename.1.png](./_resources/Mapping_in_VIORB.resources/unknown_filename.1.png)\n\nIncrease in complexity\n\n*   more states to optimise (v, b)\n*   IMU measurements creates constraints between keyframes\n\nOriginal ORBSLAM discards redundants KFs (poses a problem with IMU constraints!)\nWorkaround: in local BA, only allow discarding of KF if, after discarding, the time between two consecutive KFs is short enough (\u003c= 0.5s)\nFurther: in full BA (after loop closure, or any time for map refinement), only allow time between KFs to be \u003c= 3s (to reduce IMU noise integration)\n\n',title:"Untitled Page"},"/studienarbeit/mapping-representations-in-robotics":{content:'---\ntitle: Mapping representations in robotics\ndate: "2020-07-30"\ntags:\n  - localisation/mapping-in-robotics\n  - -sa/processed\n---\n\nSource: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)\n\n1.  [Feature maps](http://www.evernote.com/shard/s484/nl/217355218/1cfa1788-a515-ae4e-54e1-128eb2a23841?title=Feature%20maps)\n2.  Occupancy grids\n    *   Grids containing occupancy probability information\n    *   Useful for path planning, exploration\n    *   Drawback: computational complexity\n\n',title:"Untitled Page"},"/studienarbeit/mapping-step-by-step-in-defslam":{content:'---\ntitle: Mapping step-by-step in DefSLAM\ndate: "2020-11-20"\ntags:\n  - to-do/to-clarify\n  - -sa/processed\n  - to-do/missing-link\n  - SLAM/algos/DefSLAM\n---\n\n**Source**: [lamarca-2020](studienarbeit/lamarca-2020.md)  \n**Parent**: [DefSLAM framework](defslam-framework.md)\n\n![unknown_filename.1.png](./_resources/Mapping_step-by-step_in_DefSLAM.resources/unknown_filename.1.png)\n\n## Steps\n\n1.  Recover warps between k and k\\* (s.  [Non-Rigid Guided Matching (b/w KFs) in DefSLAM](non-rigid guided-matching-(b_w-kfs)-in-defslam.md))\n    *   with k: anchor keyframes, i.e. KFs where one of the observed map points was initialised\n    *   with k\\*: set of best [covisible keyframes](covisible-keyframes.md)\n    *   warps: transformation between the images Ik to Ik\\*\n        *   In DefSLAM, Schwarps (a family of warps using 2D Schwarzian equation regularisers) is used\n        *   Schwarps has something to do with the infinitesimal planarity assumption of NRSfM\n2.  \\[[NRSfM](studienarbeit/nrsfm.md)\\] Process k\\* to get estimate of an up-to-scale surface \n    ![unknown_filename.2.png](./_resources/Mapping_step-by-step_in_DefSLAM.resources/unknown_filename.2.png)\n    *   Input of NRSfM: warps\n3.  \\[[Surface alignment](studienarbeit/surface-alignment-in-defslam.md)\\] Up-to-scale surface (\\\\hat{S}\\_k) is aligned with the whole map in order to obtained the scaled surface Sk w.r.t. the old map in keyframe k\n4.  \\[[SfT template substitution](studienarbeit/template-substitution-in-defslam.md)\\] A new template (mesh with embedded map points) is created from the surface\n\n![unknown_filename.png](./_resources/Mapping_step-by-step_in_DefSLAM.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/mappings":{content:'---\ntitle: Mappings\ndate: "2020-07-15"\ntags:\n  - software/SOFA/mappings\n  - -sa/processed\n---\n\nSource: [SOFA extended documentation](SOFA extended documentation.md)\nBacklinks: [Models in SOFA](models-in-sofa.md), [Visual model](visual-model.md)\n\nEnforces consistency between the many model representations of an object, by propagating information (such as positions, velocities, forces) in a top-down and bottom-up approach. \n\n![unknown_filename.png](./_resources/Mappings.resources/unknown_filename.png)\n**Figure**: Mappings between liver and grasper models\n\n*   Master model imposes its displacements to the slave models ([top-down mapping](top-down mapping.md))\n*   Slaves, depending on model type, can also pass information (e.g. forces) back to the master (bottom-up)\n*   A mapped model can be master of another model\n*   Also used to connect generalised coordinates (e.g. joint angles) to world-space geometry\n\n',title:"Untitled Page"},"/studienarbeit/markley-2003-attitude-error-representations-for-kalman-filtering":{content:'---\ntitle: Markley 2003 Attitude Error Representations for Kalman Filtering\ndate: "2021-08-16"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-to-read\n  - -sa/to-be-processed\n---\n\nSource: [http://scholar.google.com/scholar?cluster=9266330323139560128\u0026hl=en\u0026as\\_sdt=0,5](http://scholar.google.com/scholar?cluster=9266330323139560128\u0026hl=en\u0026as_sdt=0,5)\nAuthor: FL Markley\n\nMotivation\n\n*   Quaternion as an attitude representation\n    *   Good: lowest dimensionality while being a globally nonsingular representation\n    *   Not so good: must obey a unit norm constraint\n\n*   In research, various methods for either getting around the norm constraint, or to enforce it\n*   Most successful method employs the global attitude as a unit quaternion with a 3-comp attitude error representation\n\n*   MEKF doesn\'t estimate the quaternion state. Instead, it estimates the 3-comp error, with the quaternion as a reference about which the errors are defined\n\n**Structure of the paper**\n\n*   Attitude representations\n*   Alternative quaternion estimation schemes\n*   Basic MEKF equations\n*   Second order attitude filter\n\nContents/Chapters\nTakeaway\n\n',title:"Untitled Page"},"/studienarbeit/markov-assumption":{content:'---\ntitle: Markov assumption\ndate: "2021-03-26"\ntags:\n  - -definitions\n  - -sa/processed\n---\n\nSource: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)\nBacklinks: [Grisetti 2011 - Tutorial graph-based SLAM](grisetti 2011 - tutorial graph-based slam.md), [probabilistic models-for-imu](probabilistic-models-for-imu.md)\n\nModels with state x which have the Markov property:\n\n*   all information up till time t is contained in xt\n*   enables marginalisation of state xt at time t+1\n\n',title:"Untitled Page"},"/studienarbeit/mathematical-model-of-the-internal-model-in-sofa":{content:'---\ntitle: Mathematical model of the internal model in SOFA\ndate: "2020-08-09"\ntags:\n  - software/SOFA/models\n  - -sa/processed\n---\n\nSource:  [SOFA extended documentation](sofa-extended-documentation.md)\nParent: [Internal model](internal-model.md)\nBacklinks: [ODE solvers](ode-solvers.md), [constraint-solvers](constraint-solvers.md)\n\nDynamic/quasi-static system of particles (nodes)\nIndependent DOFs: node coordinates, governed by \n![unknown_filename.png](./_resources/Mathematical_model_of_the_internal_model_in_SOFA.resources/unknown_filename.png)\n\n*   f: different force functions, e.g. volume, surface and external forces)\n*   M: mass matrix\n*   P: constraints (projection matrix)\n*   each operator corresponds to a simulation component\n\n',title:"Untitled Page"},"/studienarbeit/measurement-model":{content:'---\ntitle: Measurement model\ndate: "2020-07-29"\ntags:\n  - filters/EKF\n  - -sa/processed\n---\n\nSource: [SLAM for Dummies](slam-for-dummies.md)\nBacklinks:  [EKF matrices/vectors](ekf-matrices_vectors.md)\n\nEstimate of the range and bearing (from landmark) in  [Step 2: Re-observation](studienarbeit/ekf-2-reobservation.md)\n\n![unknown_filename.png](./_resources/Measurement_model.resources/unknown_filename.png)\n\nx, y, theta - current position estimate\nlambdax, y - landmark position\n\nJacobian H w.r.t. x, y, theta (here for regular EKF, not for extended)\n![unknown_filename.1.png](./_resources/Measurement_model.resources/unknown_filename.1.png)\n\nIn SLAM we need additional values for the landmarks\nhere for landmark number two in extended EKF\n![unknown_filename.2.png](./_resources/Measurement_model.resources/unknown_filename.2.png)\n\n*   Upper row is for information, not part of matrix\n*   First three columns are regular H\n*   Landmarks don\'t have any rotation\n\n',title:"Untitled Page"},"/studienarbeit/mechanicalstate":{content:'---\ntitle: MechanicalState\ndate: "2020-08-09"\ntags:\n  - software/SOFA/simulation-components\n  - -sa/processed\n  - to-do/missing-link\n---\n\nSource: [SOFA extended documentation](SOFA extended documentation.md)\nParents: [Components of the internal model](Components of the internal model.md), [Internal model as a scene graph in SOFA](Internal model as a scene graph in SOFA.md)\nBacklinks: [VecId](VecId.md), [Scene graph in SOFA](Scene graph in SOFA.md), [Mesh topology](Mesh topology.md)\n\n*   Contains state vectors of each mesh node\n    *   Coordinates x\n    *   Velocities v\n    *   Net force f\n*   n nodes: n entries of the state vector\n*   Each entry has the same size of the node type (3 for 3D particles)\n*   Nodes of different types belong to different MechanicalStates\n    *   the other MechanicalStates are attached to other scene graph nodes\n    *   they might be connected with one another using interaction forces\n\n',title:"Untitled Page"},"/studienarbeit/mesh-geometry":{content:'---\ntitle: Mesh geometry\ndate: "2020-07-15"\ntags:\n  - software/SOFA/meshing\n  - -sa/processed\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\nParent: [Data structure in SOFA](data-structure-in-sofa.md)\nSee also: [Mesh topology](mesh-topology.md)\n\nMesh geometry: location of vertices in space\n\nMeshes\n\n*   k-simplices (triangles)\n*   k-cubes (quads)\n\n\\--\u003e decomposition into k-cells\n\n*   1-cell: edges\n*   2-cells: triangles, quads\n*   3-cells: tetrahedron, hexahedron\n\nMesh data:\n\n*   containers, similar to STL std::vector classes\n*   there are as many data structures for mesh data as [topological elements](topological-elements.md),\n    e.g. vertices, edges, triangles, quads, tetras, hexas\n    \n\ne.g. in spring-mass models: edge container -- stores spring stiffness, damping value, ith element for ith edge\n\n',title:"Untitled Page"},"/studienarbeit/mesh-topology":{content:'---\ntitle: Mesh topology\ndate: "2020-08-22"\ntags:\n  - software/SOFA/meshing\n  - -sa/processed\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\nParent: [Data structure in SOFA](data-structure-in-sofa.md)\nSee also: [Mesh geometry](mesh-geometry.md)\n\nMesh topology: how the vertices are connected to each other (using what element?)\n\nHierarrchy of mesh topology:\n![unknown_filename.png](./_resources/Mesh_topology.resources/unknown_filename.png)\n\nTopology objects consist of four functional members which creates/modifies/gets topology arrays/geometrical information:\n\n*   Container\n*   Modifier\n*   Algorithms\n*   Geometry\n\n![unknown_filename.1.png](./_resources/Mesh_topology.resources/unknown_filename.1.png)\n\nTopological mapping:\n\n*   Define a new mesh topology from an existing one, using the same DOFs\n*   e.g. for subsetting a set of nodes, edges, or to split quads into 2 triangles each\n    *   these topologies are therefore assigned to the same [MechanicalState](http://www.evernote.com/shard/s484/nl/217355218/d4586073-9cf7-40f7-9750-a8736a94457f)\n\n![unknown_filename.2.png](./_resources/Mesh_topology.resources/unknown_filename.2.png)\n\n',title:"Untitled Page"},"/studienarbeit/mkok-2017":{content:'---\ntitle: MKok 2017 Using inertial sensors for position and orientation estimation\ndate: "2021-03-23"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-read\n  - sensors/IMU\n  - discussion/2021/2021-04\n  - -sa/to-be-processed\n---\n\nSource: \u003chttp://arxiv.org/abs/1704.06053\u003e\nAuthors: M Kok, JD Hol, TB Schön\n\nAbstract\n\nContents/Chapters\n[Quaternions](quaternions.md)\n[Probabilistic models for IMU](probabilistic-models-for-imu.md)\n    [Orientation parametrisations](orientation-parametrisations.md)\n        [Which orientation parametrisation to choose?](rotations/20.4-which-orientation-parametrisation.md)\n     [Linearisation of an orientation in SO(3)](linearisation-of-an-orientation-in-so(3).md)\n    [IMU measurement model](imu-measurement-model.md)\n         [Modelling noise and bias for IMU](modelling-noise-and-bias-for-imu.md)\n    [IMU motion models](imu-motion-models.md)\n    IMU prior models\n\n',title:"Untitled Page"},"/studienarbeit/modelling-noise-and-bias-for-imu":{content:'---\ntitle: Modelling noise and bias for IMU\ndate: "2021-04-23"\ntags:\n  - -sa/processed\n  - sensors/IMU\n---\n\nParent: [IMU index](imu index.md), [imu-measurement-model](imu-measurement-model.md)\nSource: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)\n\nModelling the noise\nThe noise not only represents measurement noise, but also model uncertainty.\n\nWith proper calibration, the three gyroscope axes are independent:\n![unknown_filename.1.png](./_resources/Modelling_noise_and_bias_for_IMU.resources/unknown_filename.1.png)\nSame for accelerometer — assume ![unknown_filename.3.png](./_resources/Modelling_noise_and_bias_for_IMU.resources/unknown_filename.3.png) diagonal for a properly calibrated sensor\n\nModelling the biases — two approaches\n\n*   treat bias as constant (due to short experiment times)\n    *   pre-calibrate in a separate experiment, or\n    *   make part of the parameters vector\n*   treat as slowly time-varying (due to long experiment times or shorter bias stability)\n    *   make the bias part of the state vector\n    *   model the bias as a random walk\n        ![unknown_filename.2.png](./_resources/Modelling_noise_and_bias_for_IMU.resources/unknown_filename.2.png)         ![unknown_filename.png](./_resources/Modelling_noise_and_bias_for_IMU.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/models-in-sofa":{content:"---\ntitle: Models in SOFA\ndate: \"2020-07-15\"\ntags:\n  - software/SOFA/models\n  - -sa/processed\n---\n\nSource:  [SOFA extended documentation](sofa-extended-documentation.md)\nBacklinks: [SOFA Introduction](sofa-introduction.md)\n\nA simulation object can have several models\n\n*   Each model is 'predestined' for a certain task\n*   Each model is independent of the other\n*   Synchronisation of models: via a [mapping](mappings.md) mechanism\n\nThree typical models for a physical object\n\n*   [Internal mechanical model](http://www.evernote.com/shard/s484/nl/217355218/ca7f5186-b72e-49f3-a775-84a7bf394f30?title=Internal%20model/Solid%20mechanics)\n*   [Collision model](http://www.evernote.com/shard/s484/nl/217355218/3d43d987-12c2-40e5-bf8e-6754c868ea4d?title=Collision%20model)\n*   [Visual model](visual-model.md)\n\n![unknown_filename.png](./_resources/Models_in_SOFA.resources/unknown_filename.png)\n\nOne of the models acts as the master\n\n*   typically the internal model\n*   imposes its displacements to slaves using [mappings](mappings.md) (synchronisation of the models)\n\n",title:"Untitled Page"},"/studienarbeit/modes-of-operation-of-the-scope":{content:'---\ntitle: Modes of operation of the scope\ndate: "2021-06-30"\ntags:\n  - to-do/orphan\n  - -sa/processing\n  - medical/surgery/endoscope\n---\n\n![modes-operation-scope](_img/modes-operation-scope.png)\n',title:"Untitled Page"},"/studienarbeit/modified-denavit-hartenberg-convention":{content:'---\ntitle: Modified Denavit-Hartenberg convention\ndate: "2021-05-24"\ntags:\n  - to-do/orphan\n  - -sa/processed\n  - math/kinematics\n---\n\n**Source**: Craig - Introduction to Robotics\n**Backlinks**: [Kinematics primer](kinematics-primer.md)\nNote: s. \u003chttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1083.6428\u0026rep=rep1\u0026type=pdf\u003e for comparison (Lipkin)\n\n**Link-frame attachment**\n\n1.  Identify joint axes\n2.  For joint axes i and i+1, identify the \n    *   common perpendicular + where it meets axis i, or\n    *   point of intersection\n        and let this be the link-frame origin\n        \n3.  Let Z\\_i point along the i-th joint axis\n4.  Let X\\_i\n    *   point along common perpendicular, or\n    *   be normal to the plane containing the two axes\n5.  Assign Y\\_i (right hand coordinate system)\n6.  Let CS0 match CS1 when the first joint variable is zero.\n7.  Choose CS{N} (origin and direction of X\\_N) such that as many parameters as possible become zero.\n\n|     |     |\n| --- | --- |\n| **Link parameters**\u003cbr\u003e![unknown_filename.png](./_resources/Modified_Denavit-Hartenberg_convention.resources/unknown_filename.png)\u003cbr\u003e\u003cbr\u003eNote: does not result in a unique attachment of frames to links\u003cbr\u003e\u003cbr\u003e![unknown_filename.2.png](./_resources/Modified_Denavit-Hartenberg_convention.resources/unknown_filename.2.png) | ![unknown_filename.1.png](./_resources/Modified_Denavit-Hartenberg_convention.resources/unknown_filename.1.png) |\n\n**Examples**\n\n|     |     |\n| --- | --- |\n| ![unknown_filename.4.png](./_resources/Modified_Denavit-Hartenberg_convention.resources/unknown_filename.4.png) | ![unknown_filename.3.png](./_resources/Modified_Denavit-Hartenberg_convention.resources/unknown_filename.3.png) |\n| Spherical wrist (note: this doesn’t follow modified DH)\u003cbr\u003eSource: [robotics.usc.edu/~aatrash/cs545/Lecture8.pdf](http://robotics.usc.edu/~aatrash/cs545/Lecture8.pdf)\u003cbr\u003e![unknown_filename.6.png](./_resources/Modified_Denavit-Hartenberg_convention.resources/unknown_filename.6.png) | ![unknown_filename.5.png](./_resources/Modified_Denavit-Hartenberg_convention.resources/unknown_filename.5.png) |\n\n',title:"Untitled Page"},"/studienarbeit/modified-vs-original-dh":{content:'---\ntitle: Modified vs original DH\ndate: "2021-05-25"\ntags:\n  - to-do/orphan\n  - math/kinematics\n  - -sa/to-be-processed\n---\n\nWikipedia\n\n|     |     |\n| --- | --- |\n| Modified DH (proximal) | Original DH (distal?) |\n| a: offset in x (from old origin)\u003cbr\u003ealpha: twist of z around old x axis\u003cbr\u003ed: offset in z (to next origin)\u003cbr\u003etheta: rotation around current z | d: offset in z (from prev origin)\u003cbr\u003etheta: rotation around prev z\u003cbr\u003er / a: offset in x from prev origin\u003cbr\u003ealp: twist of z around current x |\n\n',title:"Untitled Page"},"/studienarbeit/multisensor-fusion":{content:'---\ntitle: Multisensor fusion\ndate: "2020-07-30"\ntags:\n  - SLAM/multisensor-SLAM\n  - sensors\n  - -sa/processed\n---\n\nParent: [SLAM Index](SLAM/slam_index.md), [geometric-metric-slam](geometric-metric-slam.md)\n\nSource: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)\n\n*   Avoid limitations of using only one sensor\n    *   Relative measurements: provide precise positioning information constantly\n    *   At certain times absolute measurements are made to correct potential errors (correct drift)\n*   several approaches (for localisation), e.g.\n    *   merge sensor feeds at the lowest level before being processed homogeneously\n    *   hierarchical approaches (fuse state estimates derived independently from multiple sensors)\n    *   s. also [loose vs tight coupling](loose-vs-tight-coupling.md)\n*   combine pos measurements in a formal probabilistic framework (e.g. Markov Localisation Framework)\n    *   localisation problem consists of estimating the probability density over the space of all locations\n    *   MLF: combines info from sensors to form a combined belief in location\n\nSource: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)\n\n*   recently, vision and IMU fusion has attracted attention \n*   universality and complementarity of visual-inertial sensors(s. [why use the visual-inertial sensor combination?](why-use-the-visual-inertial-sensor-combination_.md))\n*   main distinctions: [loose vs tight coupling](loose-vs-tight-coupling.md)\n\n',title:"Untitled Page"},"/studienarbeit/multivariate-gaussian-distributions":{content:'---\ntitle: Multivariate Gaussian distributions\ndate: "2020-08-31"\ntags:\n  - -definitions\n  - math/statistics\n  - -sa/to-be-processed\n---\n\nParent: [Gaussian distribution](gaussian-distribution.md)\n\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\nSee also: [Probability distribution](probability-distribution.md)\n\n![unknown_filename.1.png](./_resources/Multivariate_Gaussian_distributions.resources/unknown_filename.1.png)\n\nN means for N dimensions\n![unknown_filename.png](./_resources/Multivariate_Gaussian_distributions.resources/unknown_filename.png)\n\n[Variances are now also combined with covariances](http://www.evernote.com/shard/s484/nl/217355218/4f00112b-f7b3-46f3-84ca-cd033e23a5c2) (to take into account correlation between different dimensions)\n\n*   Variance: how does a population vary amongst themselves?\n*   Covariance: how much do two variables change relative to each other?\n\nThe correlation helps prediction!\n\nHere: only linear correlation considered; however nonlinear correlations also exist.\n\n[Error ellipse/Confidence ellipse](error-ellipse_confidence-ellipse.md)\n\n',title:"Untitled Page"},"/studienarbeit/multivariate-kalman-filters":{content:'---\ntitle: Multivariate Kalman filters\ndate: "2020-09-01"\ntags:\n  - filters/kalman-filter\n  - -sa/to-be-processed\n---\n\nParent: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\n[Hidden variables in a multivariate Kalman filter](hidden variables-in-a-multivariate-kalman-filter.md)\n\nHere:\n\n*   Focus is on a subset of problems describable using Newton\'s equations of motion\n*   Discretised continuous-time kinematic filters\n\n[Multivariate Kalman filter algorithm](multivariate-kalman-filter-algorithm.md)\n\nDesigning the filter\n\n*   State (x, P)\n*   Process (F, Q)\n*   Measurement (z, R)\n*   Measurement function H\n*   Control inputs (B, u)\n\nAssumptions of the Kalman filter\n\n*   The sensors and motion model have Gaussian noise\n*   Everything is linear\n\nIf the assumptions are true, then the Kalman filter is optimal in a least squares sense\n\n',title:"Untitled Page"},"/studienarbeit/non-rigid-guided-matching-b-w-kfs-in-defslam":{content:'---\ntitle: Non-Rigid Guided Matching (b/w KFs) in DefSLAM\ndate: "2020-11-25"\ntags:\n  - to-do/to-clarify\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\n**Source**: [lamarca-2020](studienarbeit/lamarca-2020.md)\n\n*   Matching between keyframes (used in deformation mapping in DefSLAM)\n*   Use an estimated warp as a reference\n    *   To increase number of matches in the covisible keyframes\n\n## Process\n\n1.  Matches are given by deformation tracking\n2.  Estimate an initial warp between k and k\\* (covisible keyframes) [ ] how?\n3.  Using this initial warp, estimate where a point would be seen in k\\*\n4.  Define a search region around thesse estimated positions. The search regions may contain several keypoints\n5.  Select the keypoint with the smallest distance between the ORB descriptors (criterion: Hamming distance, apply threshold) --\u003e these are the new matches\n6.  Add them to the initial matches and estimate the final warp\n\n',title:"Untitled Page"},"/studienarbeit/notch-positions-due-to-scope-rotation":{content:'---\ntitle: Notch positions due to scope rotation\ndate: "2021-06-11"\ntags:\n  - -sa/processed\n  - discussion/2021/2021-06\n  - medical/surgery/endoscope\n---\n\n**Backlinks**: [Update 2021-06-11](update-2021-06-11.md)\n\n![unknown_filename.jpeg](./_resources/Notch_positions_due_to_scope_rotation.resources/unknown_filename.jpeg)\n\n',title:"Untitled Page"},"/studienarbeit/note-kf-with-different-sampling-rate":{content:'---\ntitle: note KF with different sampling rate\ndate: "2021-03-24"\nexternal_url: "http://stackoverflow.com/questions/59566384/kalman-filter-with-different-sampling-rate"\ntags: \n- to-do/orphan \n- filters/kalman-filter \n- -sa/to-be-processed\n---\n\nSource: \u003chttp://stackoverflow.com/questions/59566384/kalman-filter-with-different-sampling-rate\u003e\n\nApproach 1: KF with variable dt\nApproach 2: KF with static dt\n\n\'Sub\' updates? e.g.\n\n1.  predict()\n2.  update() with sensor A\n3.  skip update() for sensor B since no measurement arrived\n4.  update() with sensor c\n5.  repeat\n\nGenerally discouraged:\n\n1.  If not predicting before each update, there is the risk of the filter lagging behind real world dynamics. The update step at t=k compares a measurement zk to the projected (predicted) state xk.\n    This would however be ok if the real world dynamics are slower than the filter\n    \n2.  Separate updates --\u003e redundant matrix operations. Split into different KFs if states are independent (purely diagonal matrices).\n\n* * *\n\nSource: \u003chttp://math.stackexchange.com/questions/694504/kalman-filter-with-sensors-having-different-sampling-rate\u003e\nNormally, what you should do when the rate of sensors do not match, is that\n\n*   you should propagate states at a base sample rate (say 10Hz)\n*   depending on your implementation you may propagate covariance only between measurements or all the time.\n*   Once a measurement arrives, you perform the Kalman correction, updating your covariance and your state.\n\n* * *\n\nSource: \\[Mirzaei 2008\\] A Kalman Filter-Based Algorithm for IMU-Camera Calibration: Observability Analysis and Performance Evaluation -- 462\nIMU sampling 100 Hz, i.e. T = 0.01s\nWhen new IMU measurement received\n\n*   state estimate propagated using 4th order RK integration of system dynamics\n*   covariance propagation\n    ![unknown_filename.png](./_resources/note__KF_with_different_sampling_rate.resources/unknown_filename.png)\n    \n\nFurther reading\n\u003chttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC6339169/\u003e\nFusion of High-Dynamic and Low-Drift Sensors Using Kalman Filters\n\n* * *\n\nSource: \u003chttp://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/\u003e\nIntuition covariance matrix\n![unknown_filename.1.png](./_resources/note__KF_with_different_sampling_rate.resources/unknown_filename.1.png)\n\n* * *\n\n**Source:** Onur Sari 2020, Sensor Fusion for Localization of Automated Guided Vehicles\n\n**Handling asynchronous measurements**\n\n*   predict and update steps of the Kalman Filter are performed whenever a measurement is available\n*   varying time interval between timesteps k and k + 1 in each KF cycle\n*   F and Q are interval dependent matrices (recalculate in each prediction step)\n\n![unknown_filename.2.png](./_resources/note__KF_with_different_sampling_rate.resources/unknown_filename.2.png)\n\n',title:"Untitled Page"},"/studienarbeit/note-kf-with-missing-measurements":{content:'---\ntitle: note KF with missing measurements\ndate: "2021-03-26"\nexternal_url: "http://math.stackexchange.com/questions/982982/kalman-filter-with-missing-measurement-inputs"\ntags: \n- to-do/orphan \n- filters/kalman-filter \n- discussion/2021/2021-04 \n- -sa/to-be-processed\n---\n\n**Sources**\n\u003chttp://math.stackexchange.com/questions/982982/kalman-filter-with-missing-measurement-inputs\u003e\n\u003chttp://opencv-users.1802565.n2.nabble.com/Kalman-filters-and-missing-measurements-td2886593.html\u003e\n\nFor a missing measurement:\n\n*   use the last state estimate as a measurement\n*   set the covariance matrix of the measurement to essentially infinity.\n\nThis would cause a Kalman filter to essentially ignore the new measurement since the ratio of the variance of the prediction to the measurement is zero.\nThe result will be a new prediction that maintains velocity/acceleration but whose variance will grow according to the process noise.\n\n',title:"Untitled Page"},"/studienarbeit/note-quaternions":{content:'---\ntitle: note quaternions\ndate: "2021-03-31"\nexternal_url: "http://math.stackexchange.com/questions/1896379/how-to-use-the-quaternion-derivative"\ntags: \n- math/quaternions \n- -sa/to-be-processed\n---\n\nConverting from quaternion to angular velocity then back to quaternion\n\u003chttp://math.stackexchange.com/questions/2282938/converting-from-quaternion-to-angular-velocity-then-back-to-quaternion\u003e\n\n',title:"Untitled Page"},"/studienarbeit/nrsfm":{content:'---\ntitle: Non-rigid Surface from Motion\ndate: "2020-11-20"\ntags:\n  - to-do/go-through-literature-later\n  - SLAM/deformable-SLAM\n  - to-do/not-good-enough\n  - -sa/to-be-processed\n  - -published\n---\n\n**Notes**:\n* Original NRSFM paper?  \n	https://www.cs.dartmouth.edu/~lorenzo/Papers/TorrHertzBreg-pami08.pdf\n* A Phd thesis [Kumar] https://openresearch-repository.anu.edu.au/handle/1885/164278?mode=full\n\n\n---\n\n**Source**: Kumar\n* The problem with dynamic or non-rigid scenes:  \n	if we project a scene point into a camera image plane, there will be several possible 3D configurations!\n* Allowing arbitrary deformations makes the 3D reconstruction an ill posed problem (underconstrained) --\u003e need to make additional assumptions about the object or scene (make more constraints)!\n\n\n\n---\n\n\n**Source**: [lamarca-2020](studienarbeit/lamarca-2020.md)\n**See also**: [nrsfm-in-defslam](studienarbeit/nrsfm-in-defslam.md), [sfm](studienarbeit/sfm.md)\n\n## NRSfM (non-rigid structure from motion)\n\n*   batch processing of images to recover deformation\n*   computationally demanding — slower than [SfT](studienarbeit/sft.md)\n\n### Orthographic NRSfM\n\n*   usually fails with very large deformations\n*   uses an orthographic camera projection/model\n*   (weak approximation to the perspective camera) — a limitation, as many vision-related applications have a significant perspective effect\n*   exploits\n    *   spatial constraints\n    *   temporal constraints\n    *   spatiotemporal constraints\n*   usually ok for small deformations, but not for very large deformations\n\n### Perspective NRSfM\n\n*   the perspective camera model is more accurate than the orthographic one\n*   also uses the isometry assumption (as in SfT methods), which has produced good results in NRSfM\n*   Parashar 2018 "Isometric NRSfM" \\[6\\] local method that handles occlusions and missing data well',title:"Untitled Page"},"/studienarbeit/nrsfm-in-defslam":{content:'---\ntitle: NRSfM in DefSLAM\ndate: "2020-11-20"\ntags:\n  - to-do/to-clarify\n  - -sa/processed\n  - to-do/missing-link\n  - SLAM/algos/DefSLAM\n  - -published\n---\n\n**Parent**: [Mapping step-by-step in DefSLAM](mapping-step-by-step-in-defslam.md)\n**Source**: [lamarca-2020](studienarbeit/lamarca-2020.md)\n\n**See also**: [NRSfM](studienarbeit/nrsfm.md)\n\n## Assumptions\n\n*   Isometric deformation\n*   Infinitesimal planarity \\[DEF\\]: any surface can be approximated as a plane at infinitesimal level, all the while maintaining its curvature at a global level\n\n## Locality\nThe method used here is a local method --\u003e implies that it handles missing data and occlusions inherently\n\n*   surface deformation is modelled locally for each point, under the above assumptions\n\n## Embedding, $\\phi_k$ of the scene surface\n\n*   is a parametrisation — transforms an image point to a point on a 3D surface\n*   uses the normalised coordinates of the image Ik (xhat, yhat)\n\n|     |\n| --- |\n| ![unknown_filename.png](./_resources/NRSfM_in_DefSLAM.resources/unknown_filename.png) |\n| ![unknown_filename.1.png](./_resources/NRSfM_in_DefSLAM.resources/unknown_filename.1.png) |\n| ![Image.png](./_resources/NRSfM_in_DefSLAM.resources/Image.png) |\n\n## Procedure\n\n1.  A point is matched in more than two keyframes (warps are used in the [matching process](studienarbeit/non-rigid-guided-matching-b-w-kfs-in-defslam.md))\n    *   We can calculate its normal in its anchor keyframe k\n    *   The normal is defined by the variables K below\n2.  Perform nonlinear optimisation to recover the variables K (which appear in the vector of the normal to surface)\n    ![unknown_filename.2.png](./_resources/NRSfM_in_DefSLAM.resources/unknown_filename.2.png)\n    *   P and Q are cubic polynomial equations that are derived from the metric tensors and Christoffel symbols of Sk and Sk\\*\n    *   P and Q contain p and q coefficients which depend on\n        *   normalised coordinates\n        *   1st order derivative of warp\n        *   2nd order derivative of warp\n    *   Initial solution: normals of the deformed template \n        ![unknown_filename.3.png](./_resources/NRSfM_in_DefSLAM.resources/unknown_filename.3.png)\n         when the keyframe k was inserted\n        \n3.  Calculate normal, nj to surface for all points j in last keyframe k\n4.  Get an initial surface from the normals (SfN: shape from normals)\n    *   model the initial surface as a bicubic spline (like a spline in 3D)\n    *   the spline is parametrised by its control nodes depth\n5.  Fit node depth to get a surface orthogonal to the estimated normals; use a regulariser in terms of bending energy\n6.  The final depth estimation is then up-to-scale\n    *   the embedding $\\phi_k$\n    *   the up-to-scale surface \n        ![Image.1.png](./_resources/NRSfM_in_DefSLAM.resources/Image.1.png)\n        \n\n## NRSfM math\n![unknown_filename.4.png](./_resources/NRSfM_in_DefSLAM.resources/unknown_filename.4.png)\n\n',title:"Untitled Page"},"/studienarbeit/obtaining-imu-measurements-from-camera-by-forward-kinematics":{content:'---\ntitle: Obtaining IMU measurements from camera by forward kinematics\ndate: "2021-06-11"\ntags:\n  - -sa/processed\n  - math/kinematics\n  - discussion/2021/2021-06\n  - discussion/2021/2021-07\n---\n\nParent: [SA TODO](sa-todo.md)\nBacklinks: [Thesis restructure](private/thesis-sa/thesis.md)\n\nDone:\n- [x] [reverse-fwkin](reverse-fwkin.md) (scrapped)\n- [x] omega\\_B symbolic\n- [x] check links in BC and CB config — new diagrams (split up into \u003e=2 bodies?)\n- [x] check om\\_B = om\\_C + om\\_CB (s. [http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg\\_parameters\n- Kinematics](http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters\n- Kinematics), [Woernle](woernle.md))\n- [x] save om\\_B to container\n- [x] obtain accel. (s. [http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg\\_parameters\n- Kinematics](http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters\n- Kinematics),  [Kinematics primer](kinematics-primer.md))\n    ![6323bb657b9f82d520ad13e7d86fc612160d7a78](http://wikimedia.org/api/rest_v1/media/math/render/svg/6323bb657b9f82d520ad13e7d86fc612160d7a78)\n    where  ![e502f568baa4e4d91f1733ea1f5f2ec0d0d41b42](http://wikimedia.org/api/rest_v1/media/math/render/svg/e502f568baa4e4d91f1733ea1f5f2ec0d0d41b42) (ang. vel of body j w.r.t. body i, expressed in CS k)\n    ![581c4001a22f55cf57844a8861d0901827e4f267](http://wikimedia.org/api/rest_v1/media/math/render/svg/581c4001a22f55cf57844a8861d0901827e4f267)\n\n    Chaining velocities and accelerations:\n    ![unknown_filename.png](./_resources/Obtaining_IMU_measurements_from_camera_by_forward_kinematics.resources/unknown_filename.png)\n- [x] \\[validation\\] reconstruct rot\\_B from om\\_B, compare with camera\n- [x] debug first reconstruction of IMU traj, if that doesn\'t work debug the fake data generation\n    - [x] update readme\n- [x] update robot model with simplification around pivot point\n- [x] validate updated model\n\nAnhang\n![unknown_filename.1.png](./_resources/Obtaining_IMU_measurements_from_camera_by_forward_kinematics.resources/unknown_filename.1.png)\n\n',title:"Untitled Page"},"/studienarbeit/ode-solvers":{content:'---\ntitle: ODE solvers\ndate: "2020-08-22"\ntags:\n  - -sa/processed\n  - software/SOFA/simulation-algos\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\nParent: [Simulation algorithms in SOFA](simulation-algorithms-in-sofa.md)\nBacklinks: [Linear solvers](linear-solvers.md), [constraint-solvers](constraint-solvers.md)\n\n*   implement animation algorithms at each time step\n*   integrate and compute positions and velocities one time step ahead\n*   uses state vectors (e.g. for position or force), denoted by symbolic identificators called [VecId](vecid.md)s\n*   this allows the solver to be implemented completely independently of the physical model\n\n![unknown_filename.png](./_resources/ODE_solvers.resources/unknown_filename.png)\nEach statement in the example above is implemented using a visitor\n\n*   Explicit solvers: variants of the above\n*   Implicit solvers (consider the derivative somewhere)\n    *   Solution of the equation system\n        ![unknown_filename.1.png](./_resources/ODE_solvers.resources/unknown_filename.1.png)\n        (also solvable using [direct solvers](direct-solvers.md))\n        *   M - mass matrix\n        *   K = ![unknown_filename.2.png](./_resources/ODE_solvers.resources/unknown_filename.2.png) stiffness matrix\n        *   B = damping matrix\n        *   if beta and gamma = 0 --\u003e explicit\n    *   Use a [projection matrix](projection-matrix.md) to apply simple displacement constraints\n        ![unknown_filename.3.png](./_resources/ODE_solvers.resources/unknown_filename.3.png)\n        \n    *   [Lagrange multipliers](http://www.evernote.com/shard/s484/nl/217355218/d3ac75ba-fe8e-4ef2-bc5f-2fe53d54d4c0) are used for more complex constraints\n        ![unknown_filename.4.png](./_resources/ODE_solvers.resources/unknown_filename.4.png)\n        \n    *   More stable than explicit schemes for stiff forces or large time steps\n\n',title:"Untitled Page"},"/studienarbeit/on-quaternions-and-rotation-matrices":{content:'---\ntitle: On quaternions and rotation matrices\ndate: "2021-03-15"\nexternal_url: "http://stackoverflow.com/questions/8919086/why-are-quaternions-used-for-rotations"\ntags:\n  - to-do/orphan\n  - math\n  - math/quaternions\n  - -sa/to-be-processed\n---\n\n\u003chttp://stackoverflow.com/questions/8919086/why-are-quaternions-used-for-rotations\u003e\n\nIt\'s worth bearing in mind that all the properties related to rotation are not truly properties of Quaternions: they\'re properties of Euler-Rodrigues Parameterisations, which is the actual 4-element structure used to describe a 3D rotation.\nTheir relationship to Quaternions is purely due to a paper by Cayley, "On certain results related to Quaternions", where the author observes the correlation between Quaternion multiplication and combination of Euler-Rodrigues parameterisations. This enabled aspects of Quaternion theory to be applied to the representation of rotations and especially to interpolating between them.\nYou can read the paper here: \u003chttp://archive.org/details/collmathpapers01caylrich\u003e . But at the time, there was no connection between Quaternions and rotation and Cayley was rather surprised to find there was:\n\n\u003e In fact the formulae are precisely those given for such a transformation by M. Olinde Rodrigues Liouville, t. v., "Des lois géométriques qui régissent les déplacements d\'un système solide \\[...\\]" (or Comb. Math. Journal, t. iii. p. 224 \\[6\\]). It would be an interesting question to account, a priori, for the appearance of these coefficients here.\n\nHowever, there is nothing intrinsic about Quaternions that gives any benefit to rotation. Quaternions do not avoid gimbal lock; Euler-Rodrigues parameterisations do. Very few computer programs that perform rotation are likely to truly implement Quaternion types that are first-class complex mathematical values. Unfortunately, a misunderstanding of the role of Quaternions seems to have leaked out somewhere resulting in quite a few baffled graphics students learning the details of complex math with multiple imaginary constants and then being baffled as to why this solves the problems with rotation.\n\n',title:"Untitled Page"},"/studienarbeit/oncology":{content:'---\ntitle: Oncology\ndate: "2020-08-09"\ntags:\n  - medical/cancer\n  - -definitions\n  - -sa/processed\n---\n\nSource: \u003chttps://en.wikipedia.org/wiki/Oncology\u003e\n**Backlinks**:  [GRK 2543: Intraoperative Multi-sensor Tissue Differentiation in Oncology](GRK 2543_ Intraoperative Multi-sensor Tissue Differentiation in Oncology.md)\n\nPrevention, diagnosis and treatment of cancer.\n\n',title:"Untitled Page"},"/studienarbeit/opencv-kalman-filter-pre-post-states":{content:'---\ntitle: OpenCV Kalman filter pre/post states\ndate: "2021-04-23"\ntags:\n  - -sa/processed\n  - software/cpp\n---\n\n![unknown_filename.png](./_resources/OpenCV_Kalman_filter_pre_post_states.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/optical-flow":{content:"https://en.wikipedia.org/wiki/Optical_flow\nhttps://nanonets.com/blog/optical-flow/",title:"Untitled Page"},"/studienarbeit/optitrack-in-sofa":{content:"---\ntitle: OptiTrack in SOFA\ndate: \"2020-10-01\"\ntags:\n  - software/SOFA/SofaPython3\n  - software/OptiTrack\n  - -sa/to-be-processed\n---\n\n*   Using OptiTrackNatNet\n    *   C++ implementation? \u003chttp://www.sofa-framework.org/community/doc/programming-with-sofa/create-your-scene-in-cpp/\u003e\n    *   XML scenes?\n*   OptiTrack + Python\n\notnn\\_client = root.addObject('OptiTrackNatNetClient',\n        name='otnnClient')\n\u003cSofa.Core.Object\u003e\ndir(otnn\\_client)\n\\['bbox', 'clientName', 'componentState', 'drawOtherMarkersColor', 'drawOtherMarkersSize', 'drawTrackedMarkersColor', 'drawTrackedMarkersSize', 'listening', 'name', 'otherMarkers', 'printLog', 'scale', 'serverName', 'tags', 'trackedMarkers'\\] bold: not in API\n[http://www.sofa-framework.org/api/master/plugins/OptiTrackNatNet/html/class\\_sofa\\_opti\\_track\\_nat\\_net\\_1\\_1\\_opti\\_track\\_nat\\_net\\_client.html](http://www.sofa-framework.org/api/master/plugins/OptiTrackNatNet/html/class_sofa_opti_track_nat_net_1_1_opti_track_nat_net_client.html)\n\n- [ ] difference between client name and server name\n\n",title:"Untitled Page"},"/studienarbeit/orb-descriptor":{content:"---\ntitle: ORB descriptor\ndate: \"2020-12-08\"\ntags:\n  - -sa/processed\n  - vision\n  - discussion/2020/2020-12\n---\n\nSource: \u003chttp://medium.com/data-breach/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf\u003e\n**Backlinks**: [Descriptors in feature detection/extraction](descriptors-in-feature-detection_extraction.md)\n\n*   Oriented FAST and Rotated BRIEF, developed 2011\n*   Was developed as an alternative to SIFT and SURF, and ended up being better/faster than both\n*   Build on\n    *   [FAST keypoint detector](fast-keypoint-detector.md)\n    *   BRIEF descriptor\n\nORB using FAST, but with (partial) scale invariance\n\n*   Use a multiscale image pyramid\n*   Each level of the pyramid is the same image, but scaled at different resolutions (reduced size as you go higher up)\n    ![unknown_filename.png](./_resources/ORB_descriptor.resources/unknown_filename.png)\n    \n*   Once the pyramid is computed, FAST is used to detect keypoints \n\nORB detection\n\n*   Compute scale pyramid\n*   Use FAST to detect keypoints using the scale pyramid\n*   An orientation is assigned to each keypoint (left/right facing)\n    *   orientation depends on how the intensity changes around the keypoint\n    *   intensity detection: intensity centroid\n\nBrief (binary robust independent elementary feature)\n\n*   Converts all keypoints into a binary feature vector, in order to be able to represent an object with this vector\n*   Binary feature vector = binary feature descriptor\n*   Each keypoint is assigned a feature vector (128 to 512 bit-string)\n    ![unknown_filename.1.png](./_resources/ORB_descriptor.resources/unknown_filename.1.png)\n    \n\nHow it works\n\n*   Image smoothing (Gaussian blur) to reduce descriptor's sensitivity to high-frequency noise\n*   For one bit within the feature vector, perform a binary test:\n    *   select a random pair of pixels in a neighbourhood (patch) around the keypoint\n        *   p1 drawn from a Gaussian distribution centred around the KP, std = sigma\n        *   p2 drawn from a Gaussian distribution centred around p1, std = 2sigma\n    *   If p1 brighter than p2, bit is set to 1, else 0\n*   If 128-bit vector, this process is repeated 128 times, with 128 random pairs\n\n![unknown_filename.4.png](./_resources/ORB_descriptor.resources/unknown_filename.4.png)\ntau: binary test\np: patch\np(x): intensity at x\n\nResulting binary vector of one feature (one keypoint) using n binary tests\n![unknown_filename.5.png](./_resources/ORB_descriptor.resources/unknown_filename.5.png)\n\nDrawbacks\nBRIEF isn't invariant to rotation --\u003e rBRIEF (rotation-aware BRIEF)\n\nrBRIEF\nFor one feature with n binary tests we have the matrix S\n![unknown_filename.2.png](./_resources/ORB_descriptor.resources/unknown_filename.2.png)\n\nORB steers BRIEF according to the keypoint orientation, via\n![unknown_filename.3.png](./_resources/ORB_descriptor.resources/unknown_filename.3.png)\nWith theta = patch orientation, Rtheta = rotation matrix\n\nResulting rBRIEF feature vector\n![unknown_filename.6.png](./_resources/ORB_descriptor.resources/unknown_filename.6.png)\n\n",title:"Untitled Page"},"/studienarbeit/orbslam-frame-constructor-monocular":{content:'---\ntitle: ORBSLAM::Frame constructor (monocular)\ndate: "2020-10-28"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\nSource: [Tracking::GrabImageMonocular](tracking__grabimagemonocular.md)\n\n*   Set scale level info from ORB extractor\n*   Extract ORB features\n    *   mvKeys (vector of keypoints/features)\n*   Set N number of features\n*   Make mvpMapPoints (null, but with size N), mvbOutlier (all entries false, size N)\n*   If first frame or calibration change:\n    *   ComputeImageBounds\n*   AssignFeaturesToGrid()\n\n',title:"Untitled Page"},"/studienarbeit/orbslam2-mods":{content:'---\ntitle: ORBSLAM2 mods\ndate: "2020-11-20"\ntags:\n  - software/python\n  - -sa/processed\n  - SLAM/algos/ORBSLAM\n---\n\nPatch to work with opencv4\n[http://github.com/Windfisch/ORB\\_SLAM2](http://github.com/Windfisch/ORB_SLAM2)\n\nORBSLAM2 Python bindings\n[http://github.com/jskinn/ORB\\_SLAM2-PythonBindings](http://github.com/jskinn/ORB_SLAM2-PythonBindings)\n\n',title:"Untitled Page"},"/studienarbeit/orbslam2-unofficial-documentation":{content:'---\ntitle: ORBSLAM2 unofficial documentation\ndate: "2021-02-17"\ntags:\n  - -resources\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\nPartially done, abandonned: [http://github.com/raulmur/ORB\\_SLAM2/compare/master...AlejandroSilvestri:master](http://github.com/raulmur/ORB_SLAM2/compare/master...AlejandroSilvestri:master)\nIn Spanish: \u003chttp://alejandrosilvestri.github.io/os1/doc/html/\u003e\n\n',title:"Untitled Page"},"/studienarbeit/orientation-parametrisations":{content:'---\ntitle: Orientation parametrisations\ndate: "2021-03-23"\ntags:\n  - -sa/processed\n  - discussion/2021/2021-04\n  - math/rotations\n  - math/quaternions\n  - -published\n---\n\n**Parents**: [rotations-so3-group-index](rotations/rotations-so3-group-index.md), [quaternion index](rotations/quaternion-index.md), [probabilistic models-for-imu](probabilistic-models-for-imu.md)  \n**See also**: [Rotation error representation](rotations/rotation-error-representation.md)\n\n**Source**: [MKok 2017](mkok-2017.md), [markley-2014](bibliography/markley-2014.md)\n\nOrientation parametrisations\n\n*   Note: CCW rotation of a vector $x_v$ to $x_u$ corresponds to a CW rotation of the CS $v$ to CS $u$ (see also [active/passive transformations](rotations/active-passive-or-alibi-alias-rotation-transformations.md)).\n*   Rotations are a member of [SO(3)](rotations/so3-3d-rotation-group.md)\n\n| rotation matrix | unique description of orientation |\n| --- | --- |\n| [Euler axis/angle](rotations/euler-axis-angle-representation.md)\u003cbr\u003e[Rotation vector](studienarbeit/rotation-vector-representation.md) | not unique, due to wrapping |\n| [Euler angles](euler-angles.md) | not unique, due to wrapping and gimbal lock |\n| [Unit quaternions](rotations/unit-quaternions.md) | not unique, -q and q depict the same orientation\u003cbr\u003eProof: \u003chttp://math.stackexchange.com/questions/2016282/negative-quaternion\u003e |\n| [Gibbs / Rodrigues parameter](rotations/gibbs-rodrigues-parameter.md) | infinite for 180 degree rotations, but 1:1 mapping between itself and unit quaternions |\n\nNext: [Which orientation parametrisation to choose?](rotations/20.4-which-orientation-parametrisation.md)\n\n',title:"Untitled Page"},"/studienarbeit/particle-filters":{content:'---\ntitle: Particle filters\ndate: "2020-08-23"\ntags:\n  - SLAM/filter-vs-optim/filter-based\n  - -sa/processed\n---\n\nParent: [Filter localisation methods](SLAM/filter-localisation-methods.md)\n\nSource: [Wikipedia Lokalisierung](wikipedia-lokalisierung.md)\nParticle filter / Monte Carlo localisation / sequential Monte Carlo methods\n\n*   allow solution of all three localisation problems\n*   POSE represented by a particle cloud\n*   Each particle : possible POSE\n*   The filter checks the plausibility of each particle\n    *   Increases and decreases the probabilities of each particle accordingly\n    *   When a lower probability threshold is exceeded, the particle is not considered any longer\n\nSource: [Scaradozzi 2018 SLAM application in surgery](studienarbeit/scaradozzi-2018.md)\n**Particle filters (sequential Monte Carlo)**\n\n*   Posterior representation: particles (set of random state samples)\n*   Suitable for implementation with any probabilistic robot model with Markov chain formulation\n*   Easy to implement\n    *   no need to linearise\n    *   closed-form solutions of the conditional probability are irrelevant (as in KF)\n*   Poor performance in higher dimensional spaces\n\n*   Rao-Blackwellized particle filters\n    *   more efficient solutions, however, prone to significant estimation inconsistencies\n    *   therefore, as estimation consistency is quite important, led to the adoption of different sampling strategies\n*   FastSLAM\n    *   integrates particle filters and EKF\n        *   particles filters for estimating robot path\n        *   for each particle, EKF for estimating feature locations\n    *   mapping problem split\n        *   one mapping problem for each feature in the map\n    *   particle approximation doesn\'t converge uniformly in time\n\n',title:"Untitled Page"},"/studienarbeit/perceptual-aliasing":{content:'---\ntitle: Perceptual aliasing\ndate: "2020-08-25"\nexternal_url: "http://arxiv.org/abs/1810.11692"\ntags:\n  - -definitions\n  - -sa/processed\n---\n\nSource: [http://en.wikipedia.org/wiki/Robotic\\_mapping](http://en.wikipedia.org/wiki/Robotic_mapping)\n\nTwo different places are perceived as the same\n\n**Source**: \u003chttp://arxiv.org/abs/1810.11692\u003e\nModeling Perceptual Aliasing in SLAM via Discrete-Continuous Graphical Models \\[Lajoie 2018\\]\n(from the abstract)\n\n*   Phenomenon where different places generate a similar visual footprint\n*   Leads to spurious measurements being fed into the SLAM estimator\n*   Result: incorrect localisation and map\n\n',title:"Untitled Page"},"/studienarbeit/pinhole-camera-model":{content:'---\ntitle: Pinhole camera model\ndate: "2020-10-20"\ntags:\n  - sensors/cameras\n  - -sa/processed\n---\n\nParent: [SLAM Index](SLAM/slam_index.md)\nBacklinks: [Camera calibration](camera calibration.md), [pinhole camera projection function](pinhole camera projection function.md), [weiss thesis vision based navigation for micro helicopters](weiss thesis vision-based-navigation-for-micro-helicopters.md)\nSee also: [World to camera trafo](world-to-camera-trafo.md)\n\nSource: \u003chttp://de.mathworks.com/help/vision/ug/camera-calibration.html\u003e\nDoes not account for lens distortion (ideal pinhole camera doesn\'t have a lens)\nTo represent a real camera, the full camera model to be used should include (radial and tangential) lens distortion, (such as the one used in the MATLAB computer vision toolbox)\n\n![unknown_filename.png](./_resources/Pinhole_camera_model.resources/unknown_filename.png)\n\nCamera matrix P\n\n*   Parameters are represented by a 4x3 camera matrix\n*   Maps 3Dp -\u003e 2Dp (image plane)\n*   Extrinsic parameters\n    *   location of the camera in the 3D world\n    *   maps 3Dp (world) -\u003e camera coordinates\n*   Intrinsic parameters\n    *   non-positional parameters\n    *   maps camera coords -\u003e 2Dp (image)\n    *   optical centre (principal point), focal length, skew coefficient\n\n![unknown_filename.1.png](./_resources/Pinhole_camera_model.resources/unknown_filename.1.png)\n![unknown_filename.2.png](./_resources/Pinhole_camera_model.resources/unknown_filename.2.png)  ![unknown_filename.3.png](./_resources/Pinhole_camera_model.resources/unknown_filename.3.png)\n\nIntrinsic parameters\nK = ![unknown_filename.4.png](./_resources/Pinhole_camera_model.resources/unknown_filename.4.png)   ![unknown_filename.5.png](./_resources/Pinhole_camera_model.resources/unknown_filename.5.png)\n![unknown_filename.6.png](./_resources/Pinhole_camera_model.resources/unknown_filename.6.png)\n\n',title:"Untitled Page"},"/studienarbeit/pinhole-camera-projection-function":{content:'---\ntitle: Pinhole camera projection function\ndate: "2020-11-20"\ntags:\n  - sensors/cameras\n  - -sa/processed\n---\n\nBacklinks: [Pinhole camera model](pinhole-camera-model.md)\nSee also: [World to camera trafo](world-to-camera-trafo.md)\n\nSource: [Mur-Artal 2017 VI-ORB](bibliography/mur-artal-2017-vi-orb.md)\n3D points\n![unknown_filename.2.png](./_resources/Pinhole_camera_projection_function.resources/unknown_filename.2.png)\n\nProjection function ![unknown_filename.3.png](./_resources/Pinhole_camera_projection_function.resources/unknown_filename.3.png)\n\n*   Transforms 3D points into 2D points on image plane\n    ![unknown_filename.4.png](./_resources/Pinhole_camera_projection_function.resources/unknown_filename.4.png)\n    \n*   Focal length: ![unknown_filename.1.png](./_resources/Pinhole_camera_projection_function.resources/unknown_filename.1.png)\n*   Principal point: ![unknown_filename.6.png](./_resources/Pinhole_camera_projection_function.resources/unknown_filename.6.png)\n*   The projection does not consider the distortion due to the lens\n    *   therefore when extracting image features, first undistort their coordinates\n    *   only then match to projected points (existing features which have undergone projection from 3D to 2D)\n\nSource: [Lamarca 2019 DefSLAM](lamarca-2019-defslam.md)\n3D point:\n![unknown_filename.png](./_resources/Pinhole_camera_projection_function.resources/unknown_filename.png)\n\nProjection function maps\n\n*   [extrinsic camera parameters](extrinsic-camera-parameters.md) SE(3)\n*   world map point\n\nto a 2D image point (becomes a projected map point)\n\n![unknown_filename.9.png](./_resources/Pinhole_camera_projection_function.resources/unknown_filename.9.png)\n![unknown_filename.7.png](./_resources/Pinhole_camera_projection_function.resources/unknown_filename.7.png)\n![unknown_filename.8.png](./_resources/Pinhole_camera_projection_function.resources/unknown_filename.8.png) from camera transformation\n\nImage\n\n*   Set of observations in image: set of keypoints which are matched to map points\n*   Normalised keypoint coordinates\n    ![unknown_filename.5.png](./_resources/Pinhole_camera_projection_function.resources/unknown_filename.5.png)\n    \n\nSource: \u003chttp://www.cse.psu.edu/~rtc12/CSE486/lecture12.pdf\u003e\n![unknown_filename.10.png](./_resources/Pinhole_camera_projection_function.resources/unknown_filename.10.png)\nDerivation of the perspective projection\n\n',title:"Untitled Page"},"/studienarbeit/pipenv-venv-in-project-folder":{content:'---\ntitle: Pipenv venv in project folder\ndate: "2020-09-06"\nexternal_url: "http://github.com/pypa/pipenv/issues/259"\ntags:\n  - -sa/processed\n  - software/python/pipenv\n---\n\nPIPENV\\_VENV\\_IN\\_PROJECT=1\n\nPowershell:\n$Env:PIPENV\\_VENV\\_IN\\_PROJECT="1"\n\n',title:"Untitled Page"},"/studienarbeit/pizarro-2016-schwarps":{content:"---\ntitle: Pizarro 2016 Schwarps\ndate: \"2020-12-20\"\ntags:\n  - to-do/to-clarify\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-to-read\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\n**Author**: Daniel Pizarro et al.\n\n**Abstract**\n\n*   Warp between two images of a deforming surface:\n    *   a transformation that depict the geometric deformation between the two\n    *   'maps points between images of a deforming surface'\n*   Current approach to enforce a warp's smoothness: penalise its second order partial derivatives\n    *   However this favours locally affine warps\n    *   Does not capture the local projective component of the image deformation\n*   Propose: **novel penalty to smooth the warp while capturing the deformation's local projective structure**\n    *   Proposed penalty is based on equivalents to the Schwarzian derivatives\n        *   Schwarzian derivatives: projective differential invariants exactly preserved by homographies\n    *   Methodology to derive a set of PDEs with only homographies as the solutions\n*   Validation: Schwarps outperform existing warps in modeling and extrapolation power: perform better in deformable reconstruction methods\n\n**Introduction/Related work**\n\n*   Projective geometry: study of the geometric properties of projective transformations\n    *   Uses in CV: image stitching, image registration, SfM (which all assume rigid scenes)\n    *   The above are insufficient forfor problems like NRSfM, SfT and non-rigid image registration\n*   In a deformable environment, one of the big problems is the modeling of the image warp\n*   Representation of a warp: generally via linear basis expansion (but there are also other warp models)\n    *   e.g. Thin-Plate Spline (TPS), tensor-product BSpline (BS), finite elements, etc.\n*   Fitting a warp to measurements made requires priors\n    *   \\`Registration measurements are discrete and generally scattered in the image'\n    *   General assumption: warp is smooth or piecewise smooth\n*   Current approach to enforce a warp's smoothness: penalise its derivatives\n    *   penalising second order derivatives leads to **bending energy**; this forces the warp to be locally affine\n        *   BE = 0 implies warp is a global affine transformation\n        *   this ignores a fundamental prior that images are formed by perspective projection of surfaces\n            *   implies that the warp is a rational function whose derivatives _do not vanish at any order_\n            *   hence, using the BE penalty causes the infinitesimal projective information [ ] of the warp to be discarded\n            *   affects methods that require high accuracy in the warp's derivatives, such as NRSfM and SfT\n*   Thus, we need to find another penalty\n    *   Penalties that force the warp to behave locally as a homography\n        *   Led to Generalised TPS warps, NURBS warp, which used 3D bending energy expressed in homogeneous coordinates\n    *   Using local homographies to explicitly model the warp — has limitations ...\n    *   Proposed penalty: able to smooth a warp while capturing the infinitesimal projective structure\n        *   applicable to all warp models (including linear basis expansions; does not require the use of rational warps)\n            *   Rational warps, in theory, are smooth and capture IP structure\n            *   However, they are non-convex and may be unstable due to their rational structure\n        *   based on projective differential geometry (PDG) --\u003e Schwarzian derivative\n\n**Contributions**\n\n1.  New derivation framework for 1D Schwarzian derivative which extends to higher dimensions (we want 2D, for images)\n2.  Schwarp: image warp which is estimated while penalising the residual of the 2D Schwarzian equations\n\n",title:"Untitled Page"},"/studienarbeit/poisson-equation-for-skew-symmetric-matrix-of-angular-velocity":{content:'---\ntitle: Poisson equation for skew symmetric matrix of angular velocity\ndate: "2021-05-24"\ntags:\n  - -sa/processed\n  - math/rotations\n---\n\nSource: [Woernle Mehrkörpersysteme](woernle-mehrkörpersysteme.md)\nBacklinks: [Kinematics primer](kinematics primer.md), [converting velocity from cs1 to cs0](converting-velocity-from-cs1-to-cs0.md)\n\nSkew-symmetric angular velocity: Poisson equation\n![unknown_filename.png](./_resources/Poisson_equation_for_skew_symmetric_matrix_of_angular_velocity.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/possible-plugins":{content:'---\ntitle: Possible plugins\ndate: "2020-07-16"\ntags:\n  - software/SOFA\n  - software/SOFA/plugins\n  - -sa/processed\n---\n\nParent: [Scope of Studienarbeit](scope-of-studienarbeit.md), [sofapython-index](sofapython-index.md)\n\nCommunication\n\n*   [ZMQCommunication](http://www.sofa-framework.org/community/forum/topic/calling-header-files-between-sofa-plugins/) someone\'s own plugin\n\nOptical system\n\n*   OptiTrackNatNet\n\nMesh geometry/topology\n\n*   CGALPlugin (computational geometry algorithms)\n\nHaptic\n\n*   Haptics with Geomagic -- requires Geomagic probe, but code/intro may be useful\n*   SofaHaptics\n*   Haption\n*   Flexible - for deformations\n*   Sensable\n\nRobot arm\n\n*   SoftRobots\n*   ~~[ROS Connector](ros-connector.md)~~ too complicated\n\nScenes\n\n*   [STLIB (Sofa Template Library)](stlib-(sofa-template-library).md)\n\nRegistration\n\n*   [Registration](registration.md)\n\n',title:"Untitled Page"},"/studienarbeit/precision-recall-curve":{content:'---\ntitle: Precision recall curve\ndate: "2020-08-03"\ntags:\n  - -sa/processed\n  - SLAM/loop-detection\n---\n\nParent: [Loop closure detection](SLAM/loop-closure-detection.md)\nSource: [cometlabs](http://www.evernote.com/shard/s484/nl/217355218/1a11af29-6862-53e6-3b33-4608a7c87c15?title=What%20You%20Need%20to%20Know%20About%20SLAM)\n\n*   used to better quantify the performance (balance between false positives and false negatives in [loop closure detection](SLAM/loop-closure-detection.md))\n*   highlights tradeoff between precision and recall\n    *   precision (absence of false positives) [ ] but may lead to the appearance of false negatives\n    *   recall (prediction power)\n*   e.g. tweaking to improve recall\n    *   increases sensitivity to similarities in the image\n    *   thus increases possibility of false positives\n\n![unknown_filename.png](./_resources/Precision_recall_curve.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/preintegration-of-imu":{content:"---\ntitle: Preintegration of IMU\ndate: \"2020-10-20\"\ntags:\n  - sensors/IMU\n  - sensors/IMU/preintegration\n  - -sa/to-be-processed\n---\n\nParent: [IMU](sensors/imu.md)\nBacklinks: [IMU states, dynamics equations](studienarbeit/imu-states-dynamics-equations.md)\n\n*   IMU measurements arrive at a higher frequency (frame rate) compared to camera captures (keyframe rate)\n*   IMU measurements constrain consecutive states\n*   We want to summarise these 'in-between' IMU measurements into one single relative motion constraint between keyframes\n\n",title:"Untitled Page"},"/studienarbeit/principle-of-never-throwing-away-information":{content:'---\ntitle: Principle of never throwing away information\ndate: "2020-08-27"\ntags:\n  - -sa/processed\n---\n\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\n"Two sensors are better than one, even if one is less accurate than the other."\n\nExample 1\nGiven:\n\n*   A: a more accurate sensor\n*   B: a less accurate sensor\n\nShould we therefore choose A as the estimate and discard B?\nNo, because B can improve our knowledge when combined with A.\n\n![ScreenClip.1.png](./_resources/Principle_of_never_throwing_away_information.resources/ScreenClip.1.png)\n\nUsing the measurements from B further narrows the range of estimates (overlap between the error bars).\n\nExample 2\nAssume:\n\n*   Scale A has an error of +- 1kg\n*   Scale B has an error of +- 9 kg\n\nWe measure:\nA = 160 kg\nB = 170 kg\n\n![ScreenClip.png](./_resources/Principle_of_never_throwing_away_information.resources/ScreenClip.png)\n\nEven though B is highly inaccurate, it still helps to narrow down the range of estimates.\n\n',title:"Untitled Page"},"/studienarbeit/probabilistic-models-for-imu":{content:'---\ntitle: Probabilistic models for IMU\ndate: "2021-03-23"\ntags:\n  - -sa/processed\n  - sensors/IMU\n---\n\nParent: [IMU index](imu-index.md)\nSource: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)\n\nThree main components to the probabilistic models\n\n1.  [IMU measurement model](imu-measurement-model.md) (infer knowledge about pose from measurements) ![unknown_filename.5.png](./_resources/Probabilistic_models_for_IMU.resources/unknown_filename.5.png)\n2.  [Prediction model](SLAM/prediction-model.md) (how sensor pose changes over time)\n3.  Models of the initial pose (prior)\n\nKnowledge we are interested in: pose of the sensor\n\n*   time-varying variables: states ![unknown_filename.png](./_resources/Probabilistic_models_for_IMU.resources/unknown_filename.png)\n*   constants: parameters ![unknown_filename.1.png](./_resources/Probabilistic_models_for_IMU.resources/unknown_filename.1.png)\n\nKnowledge available to us: sensor dynamics, available sensor measurements ![unknown_filename.2.png](./_resources/Probabilistic_models_for_IMU.resources/unknown_filename.2.png)\n\nConditional probability distribution\n\n*   Smoothing problem (uses all measurements)\n    ![unknown_filename.3.png](./_resources/Probabilistic_models_for_IMU.resources/unknown_filename.3.png)\n    \n*   Filtering problem (only up to t-th measurement)\n    ![unknown_filename.4.png](./_resources/Probabilistic_models_for_IMU.resources/unknown_filename.4.png)\n    Parameters are treated as slowly time-varying —\u003e incorporated into state vector x\n    \n*   Variations: fixed-lag smoothing, MHE\n\nAssumption: our models have the [Markov property](markov-property.md)\n\nThe complexity of pose estimation can be mainly attributed to\n\n*   the nonlinear nature of orientation —\u003e [linearise the orientations](linearise-the-orientations.md)\n*   the different available ways of [parametrising orientation](parametrising-orientation.md)\n\nResulting probabilistic models\nPose estimation\n![unknown_filename.6.png](./_resources/Probabilistic_models_for_IMU.resources/unknown_filename.6.png)\n![unknown_filename.7.png](./_resources/Probabilistic_models_for_IMU.resources/unknown_filename.7.png)\n\nInitial conditions\n![unknown_filename.8.png](./_resources/Probabilistic_models_for_IMU.resources/unknown_filename.8.png)\n![unknown_filename.9.png](./_resources/Probabilistic_models_for_IMU.resources/unknown_filename.9.png)\n\nOrientation at t=1 from QUEST algo\n![unknown_filename.10.png](./_resources/Probabilistic_models_for_IMU.resources/unknown_filename.10.png)\n\nOrientation estimation\n![unknown_filename.11.png](./_resources/Probabilistic_models_for_IMU.resources/unknown_filename.11.png)\n![unknown_filename.12.png](./_resources/Probabilistic_models_for_IMU.resources/unknown_filename.12.png)\n![unknown_filename.7.png](./_resources/Probabilistic_models_for_IMU.resources/unknown_filename.7.png)\n\n',title:"Untitled Page"},"/studienarbeit/probability-distribution":{content:"---\ntitle: Probability distribution\ndate: \"2020-08-31\"\ntags:\n  - -definitions\n  - -sa/processed\n  - math/probability-theory\n---\n\nParent: [Discrete Bayesian filter](discrete-bayesian-filter.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nProbability distribution:\n\n*   collection of all possible probabilities for an event\n*   the distribution lists  all possible events and the probability of each\n*   sum up to 1\n\nPrior probability distribution: probability prior to incorporating any measurements or other information\n\nJoint probability P(x,y):\n\n*   probability of both events happening\n*   the multivariate Gaussian distribution is already already a joint probability distribution\n\nMarginal probability:\n\n*   probability of an event happening, without considering the occurrence of other events\n*   'projecting' the variances onto the walls\n\n![unknown_filename.png](./_resources/Probability_distribution.resources/unknown_filename.png)\n\n",title:"Untitled Page"},"/studienarbeit/program-outline":{content:"---\ntitle: Program outline\ndate: \"2021-07-07\"\ntags:\n  - to-do\n  - to-do/orphan\n  - -sounding-board\n  - -sa/processed\n  - discussion/2021/2021-07\n---\n\nCurrent assumptions (to take care of later!)\n\n*   [x] Probe is rigid — DOFs are either 0 or constant — switch to rotating scope later\n*   ~~No gravity~~\n*   ~~No bias/offset, no noise in IMU~~\n\n* * *\n\nNote: Stuff marked with checkboxes are either to-dos or things I'm not sure that I implemented correctly\n\n\u003chttp://github.com/feudalism/dvi-ekf/tree/eskf\u003e; [projects](http://github.com/feudalism/dvi-ekf/projects)\n\ngenerate\\_data.py\nData generation (is called from main.py)\n\nMain objects\n\n*   Generate camera data (from DefSLAM mono trajectory)\n*   Make RigidSimpleProbe (for now, all DOFs are 0 or constant)\n*   Make IMU object, generate first (om, acc) values from interpolated camera data ( - [x] should generate it from stereo data instead)\n\nVariables\n\n*   Initial states x0 (initial IMU pose, initial IMU velocity initial IMU dofs, initial camera pose)\n    *   Generated from camera data at t=0\n    *   Initial IMU pose = initial camera pose + relative pose B-\u003eC from probe (dependent on DOFs)\n*   Initial covariance matrix P0\n*   Noise matrices Q and R (to do: [x] perform generation inside Filter itself, so that the entries can be updated with Filter.dt)\n\n* * *\n\nmain.py\nInitialisation of Kalman filter\n\n*   Set initial states and covariance: x0, P0\n*   Set values in buffer: imu.om, imu.acc, R\\_WB (- [x] maybe I should include p\\_B in the buffer, for p\\_cam prediction)\n*   Append initial states to Trajectory (for plotting)\n*   Set noise matrices from stdev values ( [x] as stated above, to be moved to an internal Filter function!)\n\nFilter main loop\nIterate over camera images\n\n*   Propagate stage\n    *   Get queue of interpolated camera data  (from old\\_t up till current t, i.e. from previous 'real' camera data up till current 'real' camera data)\n        (- [x] would probably be faster to pre-generate the fake IMU data to a text file and read from it for the current test case using RigidSimpleProbe — I only implemented it this way because I might want to update the probe DOFs later on, which would affect the IMU data to be generated, s. next checkbox below)\n        \n    *   Iterate over queue of interpolated camera data\n        *   Fake data gen: evaluate (om, acc) values based on:\n            *   joint dofs ([x] here I use the initial DOFs for the entire loop; later to update according to the estimated DOFs?)\n            *   interpolated camera data\n        *   Perform state propagation\n            *   Predict nominal state, ref. eqns in [IMU nominal-state and error-state kinematics](imu-nominal-state-and-error-state-kinematics.md)\n                x\\_next = f(t, x, u)\n                \n            *   Predict error state\n                *   calculate Fx, ref.  [IMU ESKF prediction equations](imu-eskf-prediction-equations.md)\n                *   redundant: err\\_x\\_next = Fx \\* err\\_x = 0 (because initial err\\_x is zero)\n            *   Predict covariance\n                ![Image.png](./_resources/Program_outline.resources/Image.png)\n                for calculating Fi matrix, see [IMU ESKF prediction equations](imu-eskf-prediction-equations.md)\n                \n        *   Update buffer\n        *   Append current state to Trajectory (for plotting)\n\n*   Update stage\n    *   Get current camera data (pos, rot)\n    *   [x] to do for later when using a non-rigid probe: compensate for notch angular displacement \n    *   Compute gain using H matrix, ref. [H Jacobian matrix in the ESKF filter correction](h jacobian matrix-in-the-eskf-filter-correction.md)\n    *   Compute residuals\n    *   Compute error states: err = K @ residuals\n    *   Correct the nominal states according to the respective composition rules, ref. [Variables in ESKF](variables-in-eskf.md)\n    *   Calculate covariance, ref. [Filter correction](filter-correction.md)\n    *   \\--\u003e next loop iteration\n\nPlot\n- [x] I still need to make a proper plot function where we can see all the states\n- [x] Maybe also a 3D plot\n\n",title:"Untitled Page"},"/studienarbeit/programmatic-implementations-of-monoslam":{content:'---\ntitle: Programmatic implementations of MonoSLAM\ndate: "2020-08-04"\ntags:\n  - -resources\n  - SLAM/VSLAM/monoslam\n  - -sa/processed\n---\n\nParent: [SLAM resources](slam-resources.md)\n\nPython\n\u003chttp://github.com/agnivsen/Py-M-SLAM\u003e\n\u003chttp://github.com/agnivsen/LibMonoSLAM\u003e\n\nMATLAB\n[http://perso.ensta-paris.fr/~filliat/Courses/2011\\_projets\\_C10-2/BRUNEAU\\_DUBRAY\\_MURGUET/monoSLAM\\_bruneau\\_dubray\\_murguet\\_en.html](http://perso.ensta-paris.fr/~filliat/Courses/2011_projets_C10-2/BRUNEAU_DUBRAY_MURGUET/monoSLAM_bruneau_dubray_murguet_en.html)\n\n',title:"Untitled Page"},"/studienarbeit/pros-and-cons-of-gaussian-distributions":{content:"---\ntitle: Pros and cons of Gaussian distributions\ndate: \"2020-08-31\"\ntags:\n  - -sa/processed\n  - math/statistics\n---\n\nParent: [Gaussian distribution](gaussian-distribution.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nBig advantage of using Gaussian distributions (as opposed to discrete ones w/ histogram bins): less data, b/c a Gaussian distribution is represented fully using only two values: the mean and the variance\n![unknown_filename.png](./_resources/Pros_and_cons_of_Gaussian_distributions.resources/unknown_filename.png)\n\nLimitations of using Gaussian distributions to model the world\ni.e. deviations from the central limit theorem\n\n*   Not all situations are describable by Gaussian distributions\n    *   e.g. sensors in the real world have fat tails (kurtosis) — don't extend to infinity and skew\n*   Can't depict any arbitrary probability distributions like in e.g. discrete probability distributions (used by particle filters)\n\n![unknown_filename.1.png](./_resources/Pros_and_cons_of_Gaussian_distributions.resources/unknown_filename.1.png)\nWith enough bins, an arbitrary error characteristic \\[of a sensor\\] can be modelled.\n\n",title:"Untitled Page"},"/studienarbeit/qin-2019-general-optimization-based-framework-multisensor":{content:'---\ntitle: Qin 2019 General Optimization-based Framework (Multisensor)\ndate: "2020-08-07"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-read\n  - discussion/2020/2020-08\n  - -sa/to-be-processed\n---\n\nAuthors: Qin et al\nCode: \u003chttp://github.com/HKUST-Aerial-Robotics/VINS-Fusion\u003e (uses ROS)\n\n![unknown_filename.3.png](./_resources/[Qin_2019]_General_Optimization-based_Framework_(Multisensor).resources/unknown_filename.3.png)\n\nAbstract:\n\n*   odometry estimation with multiple sensors, general framework which is optimisation-based\n*   demonstrated combinations:\n    *   stereo cameras\n    *   monocular cam + IMU\n    *   stereo cams + IMU\n*   sensor = factor in the framework\n*   comparison with other state-of-the-art algos\n\nAim:\n\n*   to create a general algo which supports different multisensor suites\n*   also for redundancy: in case of sensor failure, it can be switched out easily\n\nRelated work:\n\n*   multiple sensors to increase robustness of state estimation\n*   two trends for multisensor fusion\n    *   filter-based\n    *   optimisation-based/bundle adjustment — graph-based\n        *   outperform the filter-based algorithms in term of accuracy at the cost of computational complexity.\n\nFramework\n![unknown_filename.1.png](./_resources/[Qin_2019]_General_Optimization-based_Framework_(Multisensor).resources/unknown_filename.1.png)\nFactors (sensors) constrain a state or several states\n\nOptimisation problem\n\n*   Solve the graph: find states which satisfy the constraints (edges)\n*   MLE (Maximum Likelihood Estimation) problem: joint probability distribution of robot poses over a period of time\n    ![unknown_filename.png](./_resources/[Qin_2019]_General_Optimization-based_Framework_(Multisensor).resources/unknown_filename.png)\n    \n*   Marginalization procedure summarises past measurements into one prior term (to reduce complexity due to huge number of states)\n\nResults\n![unknown_filename.2.png](./_resources/[Qin_2019]_General_Optimization-based_Framework_(Multisensor).resources/unknown_filename.2.png)\n\n*   IMU integration works successfully in all sequences; improves motion tracking by bridging the gap when visuals are not so good\n    *   stereo-only method performed worst in most sequences\n*   stereo + IMU didn’t always perform best, because it requires a more accurate calibration compared to mono + IMU\n*   results outperform [OKVIS](http://www.evernote.com/shard/s484/nl/217355218/ec22bc95-10ac-46e4-ae59-60c67cd8501f)\n\n',title:"Untitled Page"},"/studienarbeit/quaternion-conjugate":{content:'---\ntitle: Quaternion conjugate\ndate: "2021-05-05"\ntags:\n  - -sa/processed\n  - math/quaternions\n---\n\nParent: [Quaternion index](rotations/quaternion-index.md)\nSource: \u003chttp://en.wikipedia.org/wiki/Quaternion\u003e\n\nFlip signs of vector part\n\n![25b7285b430db4cfe99a65bbf6327c549707ca63](http://wikimedia.org/api/rest_v1/media/math/render/svg/25b7285b430db4cfe99a65bbf6327c549707ca63)\n\n![708ac96c1741cb088eddaa1fa8629ae45402d755](http://wikimedia.org/api/rest_v1/media/math/render/svg/708ac96c1741cb088eddaa1fa8629ae45402d755)\n\nSource: [Solà 2017 Quaternion kinematics for ESKF](solà-2017-quaternion-kinematics-for-eskf.md)\n\nMultiplying with own conjugate\n![unknown_filename.png](./_resources/Quaternion_conjugate.resources/unknown_filename.png) (scalar!)\n\nConjugate operation on quaternion products\n![unknown_filename.1.png](./_resources/Quaternion_conjugate.resources/unknown_filename.1.png)\n\n',title:"Untitled Page"},"/studienarbeit/quaternion-conventions":{content:'---\ntitle: Quaternion conventions\ndate: "2021-05-14"\ntags:\n  - -sa/processed\n  - math/quaternions\n  - -published\n---\n\n**Parent**: [Quaternion index](rotations/quaternion-index.md)  \n**Source**: [Solà 2017 Quaternion kinematics for ESKF](solà-2017-quaternion-kinematics-for-eskf.md)\n\n![unknown_filename.png](./_resources/Quaternion_conventions.resources/unknown_filename.png)\n\n---\n\n**Source:** \\[[Wikipedia](http://en.wikipedia.org/wiki/quaternions_and_spatial_rotation)\\], [markley-2014](bibliography/markley-2014.md)\n\nFor [quaternion multiplication](rotations/quaternion-multiplication.md): change the order to transform between conventions\n![unknown_filename.1.png](./_resources/Quaternion_conventions.resources/unknown_filename.1.png)\n\n| Hamilton | Shuster |\n| --- | --- |\n| ![c47a68dc8f066725fc1c396eb75e41c0a29ae073](http://wikimedia.org/api/rest_v1/media/math/render/svg/c47a68dc8f066725fc1c396eb75e41c0a29ae073) | ![5261e79fcaea90cfc58979041db6fa325a4dfe5a](http://wikimedia.org/api/rest_v1/media/math/render/svg/5261e79fcaea90cfc58979041db6fa325a4dfe5a) |\n| ![Image.1.png](./_resources/Quaternion_conventions.resources/Image.1.png) | ![Image.png](./_resources/Quaternion_conventions.resources/Image.png) |\n|     | ![98588a43a484a30aee1a363e8c66eea187f16d22](http://wikimedia.org/api/rest_v1/media/math/render/svg/98588a43a484a30aee1a363e8c66eea187f16d22) |\n| ![e7f45da9aa0beec9f7b83351ccbfaedf98e6bee0](http://wikimedia.org/api/rest_v1/media/math/render/svg/e7f45da9aa0beec9f7b83351ccbfaedf98e6bee0) | ![085cc99ad35d09041f082ffab67f86f5f0cfe337](http://wikimedia.org/api/rest_v1/media/math/render/svg/085cc99ad35d09041f082ffab67f86f5f0cfe337)\u003cbr\u003eTranspose of the Hamiltonian version |\n\n',title:"Untitled Page"},"/studienarbeit/quaternion-differentiation":{content:'---\ntitle: Quaternion differentiation\ndate: "2021-05-14"\ntags:\n  - -sa/processed\n  - math/quaternions\n---\n\n**Parent**: [Quaternion index](rotations/quaternion-index.md)  \n**Source**: J. D. Hol — Sensor fusion and calibration of inertial sensors, vision, ultra-wideband and GPS\n\n![unknown_filename.3.png](./_resources/Quaternion_differentiation.resources/unknown_filename.3.png)\n\n![unknown_filename.png](./_resources/Quaternion_differentiation.resources/unknown_filename.png)\n\n![unknown_filename.2.png](./_resources/Quaternion_differentiation.resources/unknown_filename.2.png)\n\nUsing the identities:\n![unknown_filename.1.png](./_resources/Quaternion_differentiation.resources/unknown_filename.1.png)\n\n---\n\n\u003chttp://math.stackexchange.com/questions/189185/quaternion-differentiation\u003e    \nNumerical differentiation (Euler)\n\n---\n\n\u003chttp://math.stackexchange.com/questions/1896379/how-to-use-the-quaternion-derivative\u003e\n\n$$\n\\begin{aligned}\nq(t+dt) \u0026= q(t) \\otimes dq\\\\\n\\dfrac{dq}{dt} \u0026= \\frac{1}{2} \\omega \\otimes q\n	\\quad \\text{with }\n	\\omega = \\left[ \n		\\begin{array}{cccc}\n			0 \u0026 \\omega_x \u0026 \\omega_y \u0026 \\omega_z\n		\\end{array}\n		\\right]^\\text{T}\n\\end{aligned}$$\n\nIntegrating this, assuming $\\omega=\\text{const.}$ from $t_0$ to $t_0 + dt$:\n$$\n\\begin{aligned}\nq(t) \u0026= q(t_0) \\exp \\left( \\frac{1}{2} \\omega \\cdot \\left( t - t_0\\right) \\right)\\\\\n\\rightarrow dq \u0026= \\exp \\left( \\frac{1}{2} \\omega \\cdot dt \\right)\n\\end{aligned}\n$$\n\n',title:"Untitled Page"},"/studienarbeit/quaternion-norm":{content:'---\ntitle: Quaternion norm\ndate: "2021-05-14"\ntags:\n  - -sa/processed\n  - math/quaternions\n---\n\nParent: [Quaternion index](rotations/quaternion-index.md)\nSource: [Solà 2017 Quaternion kinematics for ESKF](solà-2017-quaternion-kinematics-for-eskf.md)\n\n![unknown_filename.png](./_resources/Quaternion_norm.resources/unknown_filename.png)\n\nWith the property\n![unknown_filename.1.png](./_resources/Quaternion_norm.resources/unknown_filename.1.png)\n\n',title:"Untitled Page"},"/studienarbeit/random-variable":{content:'---\ntitle: Random variable\ndate: "2020-08-31"\ntags:\n  - -definitions\n  - -sa/processed\n  - math/statistics\n---\n\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\n*   Combination of values + associated probabilities.\n*   "Event" e.g. die toss, height of students\n\ne.g. in a fair die\nvalues = {1, 2, ..., 6} (range of values = sample space)\nprobabilities = {1/6} \\* 6\n\n',title:"Untitled Page"},"/studienarbeit/registration":{content:'---\ntitle: Registration\ndate: "2020-07-23"\nexternal_url: https://www.sofa-framework.org/applications/marketplace/registration/\ntags:\n  - software/SOFA/plugins\n  - -definitions\n  - -sa/processed\n  - registration\n  - -published\n---\n\n* Aligning two points, each in different spaces respectively, together\n\n## Examples\n* aligning an endoscope coordinate frame to CT data (based on a similarity metric between endoscopic image and CT image) -- from https://pubmed.ncbi.nlm.nih.gov/25991876/\n* match virtual surface to corresponding endoscopic video -- https://ieeexplore.ieee.org/document/958638\n\n\n---\n\n**Parent**: [SofaPython Index](studienarbeit/sofapython-index.md)\n\n*   allows a matching between deformable surfaces\n*   finds spatial transformations to align two point sets or two meshes\n*   done based on:\n	*   either target surfaces (ClosestPointRegistrationForceField , RegistrationContactForceField)\n	*   or target images (IntensityProfileRegistrationForceField), which requires the use of the [image plugin](https://www.sofa-framework.org/applications/marketplace/image-manipulation/)\n\n',title:"Untitled Page"},"/studienarbeit/related-types-of-surgery":{content:'---\ntitle: Related types of surgery\ndate: "2020-08-09"\ntags:\n  - medical/surgery\n  - -definitions\n  - -sa/processed\n---\n\n**Source**: [http://en.wikipedia.org/wiki/Resection\\_(surgery)](http://en.wikipedia.org/wiki/Resection_(surgery))\n\n### By procedure\n\n*   Resection: remove all parts or a key part of an internal organ\n    *   s. also:  [resection margin](resection-margin.md)\n*   Excision: cut out only a part of an organ/tissue\n\n### By degree of invasiveness\n\n*   minimally-invasive surgery (-scopy)\n    *   [laparoscopy](laparoscopy.md)\n\n',title:"Untitled Page"},"/studienarbeit/resection-margin":{content:'---\ntitle: Resection margin\ndate: "2020-08-09"\nexternal_url: "http://en.wikipedia.org/wiki/Resection_margin"\ntags:\n  - medical/cancer\n  - -definitions\n  - -sa/processed\n---\n\nSource: [http://en.wikipedia.org/wiki/Resection\\_margin](http://en.wikipedia.org/wiki/Resection_margin)\nBacklinks:  [Cryosection](cryosection.md),  [related types-of-surgery](related-types-of-surgery.md)\n\nMargin on non-cancerous tissue around a tumour that has been removed.\n\n*   Negative margin: no tumour\n*   Microscopic positive: tumour identified microscopically\n*   Macroscopic positive: tumour significantly present\n\n',title:"Untitled Page"},"/studienarbeit/resource-imu-common-specifications,-error-models-etc":{content:'---\ntitle: resource IMU common specifications, error models etc\ndate: "2021-03-27"\nexternal_url: "http://www.vectornav.com/resources/imu-specifications"\ntags:\n  - -resources\n  - sensors/IMU\n  - -sa/to-be-processed\n---\n\nParent: [IMU](sensors/imu.md)\nSource: \u003chttp://www.vectornav.com/resources/imu-specifications\u003e\n\nIMU common specifications, bias, scale factor, orthogonality errors, and acceleration sensitivity for gyroscopes.\n\n* * *\n\nSource: Woodman - An introduction to inertial navigation\n![unknown_filename.png](./_resources/[resource]_IMU_common_specifications,_error_models_etc.resources/unknown_filename.png)\n\n* * *\n\nSource:  Quinchia - A Comparison between Different Error Modeling of MEMS Applied to GPS/INS Integrated Systems\n\n3.2. State-Space Representation for Different Bias Models\n\n1.  First order Gauss-Markov (GM)\n    ![unknown_filename.1.png](./_resources/[resource]_IMU_common_specifications,_error_models_etc.resources/unknown_filename.1.png)\n    ![unknown_filename.2.png](./_resources/[resource]_IMU_common_specifications,_error_models_etc.resources/unknown_filename.2.png)\n    \n2.  Random walk\n    ![unknown_filename.3.png](./_resources/[resource]_IMU_common_specifications,_error_models_etc.resources/unknown_filename.3.png)\n    \n3.  Autoregressive process\n\n',title:"Untitled Page"},"/studienarbeit/reversed-kinematics-relations":{content:'---\ntitle: Reversed kinematics relations\ndate: "2021-05-24"\ntags:\n  - -sa/processed\n  - math/kinematics\n---\n\nSource: [Woernle Mehrkörpersysteme](woernle-mehrkörpersysteme.md)\n**See also**: [Kinematics primer](kinematics-primer.md)\n\n|     |     |\n| --- | --- |\n| ![unknown_filename.png](./_resources/Reversed_kinematics_relations.resources/unknown_filename.png)\u003cbr\u003e\u003cbr\u003ePosition\u003cbr\u003e![unknown_filename.1.png](./_resources/Reversed_kinematics_relations.resources/unknown_filename.1.png)\u003cbr\u003e\u003cbr\u003eVelocity\u003cbr\u003e![unknown_filename.2.png](./_resources/Reversed_kinematics_relations.resources/unknown_filename.2.png) (given that omega\\_00 = 0)\u003cbr\u003e![unknown_filename.3.png](./_resources/Reversed_kinematics_relations.resources/unknown_filename.3.png) (given that v\\_00 = 0)\u003cbr\u003e\u003cbr\u003eAcceleration\u003cbr\u003e![unknown_filename.4.png](./_resources/Reversed_kinematics_relations.resources/unknown_filename.4.png)\u003cbr\u003e    given ![unknown_filename.5.png](./_resources/Reversed_kinematics_relations.resources/unknown_filename.5.png), ![unknown_filename.2.png](./_resources/Reversed_kinematics_relations.resources/unknown_filename.2.png), ![unknown_filename.6.png](./_resources/Reversed_kinematics_relations.resources/unknown_filename.6.png)\u003cbr\u003e\u003cbr\u003e![unknown_filename.7.png](./_resources/Reversed_kinematics_relations.resources/unknown_filename.7.png) | ![unknown_filename.8.png](./_resources/Reversed_kinematics_relations.resources/unknown_filename.8.png) |\n\n',title:"Untitled Page"},"/studienarbeit/rigid-cystoscope-dimensions":{content:'---\ntitle: Rigid cystoscope dimensions\ndate: "2021-08-20"\nexternal_url: "http://en.wikipedia.org/wiki/Cystoscopy"\ntags:\n  - to-do/orphan\n  - -sa/processed\n  - medical/surgery/endoscope\n---\n\n**![unknown_filename.2.png](./_resources/Rigid_cystoscope_dimensions.resources/unknown_filename.2.png)**\nca 20cm x 65 cm = 0.02m x 0.065 m\n\n* * *\n\nSource: \u003chttp://en.wikipedia.org/wiki/Cystoscopy\u003e\n\nThe sizes of the sheath of the rigid cystoscope are 17 [French gauge](http://en.wikipedia.org/wiki/French_catheter_scale) (5.7 mm diameter), 19 Fr gauge (6.3 mm diameter), and 22 Fr gauge (7.3 mm diameter).\n\n* * *\n\n**Source**: [http://www.karlstorz.com/cps/rde/xbcr/karlstorz\\_assets/ASSETS/3405020.pdf](http://www.karlstorz.com/cps/rde/xbcr/karlstorz_assets/ASSETS/3405020.pdf)\nCamera\n![unknown_filename.png](./_resources/Rigid_cystoscope_dimensions.resources/unknown_filename.png)![unknown_filename.1.png](./_resources/Rigid_cystoscope_dimensions.resources/unknown_filename.1.png)\n\n',title:"Untitled Page"},"/studienarbeit/rigid-cystoscope-mechanism":{content:'---\ntitle: Rigid cystoscope mechanism\ndate: "2021-06-10"\ntags:\n  - -sa/processed\n  - discussion/2021/2021-06\n  - medical/surgery/endoscope\n---\n\n**Parent**: [Update 2021-06-11](update-2021-06-11.md)\n\n![unknown_filename.png](./_resources/Rigid_cystoscope_mechanism.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/rlabbe-kalman-bayesian-filters-in-python":{content:'---\ntitle: (rlabbe) Kalman/Bayesian filters in Python\ndate: "2020-08-27"\nexternal_url: "http://nbviewer.jupyter.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/00-Preface.ipynb"\ntags:\n  - -resources/-bibliography\n  - -sa/processed\n  - -resources/tutorials\n  - discussion/2020/2020-09\n  - -resources/-bibliography/bib-skimmed\n---\n\nURL: \u003chttp://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\u003e\nnbviewer link: [http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table\\_of\\_contents.ipynb](http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb)\n\nAbstract:\n\n*   Introductory text with Python code\n*   Caveat: most of the code is written for didactic purposes, may not be the most efficient solution (nor numerically stable)\n\nRecommended other works\ns. [Works of possible interest](bibliography/works-of-interest.md)\n\nChapters\n\n1.  Preface\n    *   [Why Kalman filters?](why-kalman-filters_.md)\n    *   [Aim and main principle of Kalman filters](aim and-main-principle-of-kalman-filters.md)\n    *   [Expected value](expected-value.md)\n2.  [g-h filter or α-β filter](g-h-filter-or-α-β-filter.md)\n3.  [Discrete Bayesian filter](discrete-bayesian-filter.md)\n4.  [Gaussian distribution](gaussian-distribution.md)\n5.  [1D Kalman filters](1d-kalman-filters.md)\n6.  [Multivariate Kalman filters](multivariate-kalman-filters.md)\n\n',title:"Untitled Page"},"/studienarbeit/rotation-vector-representation":{content:'---\ntitle: Rotation vector representation\ndate: "2021-08-17"\ntags:\n  - -sa/processed\n  - math/rotations\n---\n\n**Parents**: [rotations-so3-group-index](rotations/rotations-so3-group-index.md), [orientation-parametrisations](orientation-parametrisations.md)\n\n**Source**: [Markley 2014](bibliography/markley-2014.md)\n\nCombine the [Euler axis/angle](rotations/euler-axis-angle-representation.md) into a three component rotation vector\n$$ \\mathbf{\\theta} \\equiv \\theta \\mathbf{e}$$\n\nConvenient for analysis, but not for computation\n\n',title:"Untitled Page"},"/studienarbeit/running-sofa-with-python":{content:'---\ntitle: Running SOFA with Python\ndate: "2020-07-15"\ntags:\n  - software/SOFA\n  - software/python\n  - -sa/processed\n  - software/python/pipenv\n---\n\nParent: [SofaPython Index](sofapython-index.md)\n\nFrom command line\nAdd to path environment and then execute runSofa via command line\n\nWith a python script\nrunSofa -l SofaPython ./script\\_name.py\n- [x] How to make SofaPython loaded by default? In bin/plugin\\_list.conf? Yes\nsofa-launcher might be useful\n\nWith pipenv\npipenv run runsofa\n\n',title:"Untitled Page"},"/studienarbeit/sa-todo":{content:'---\ntitle: SA TODO\ndate: "2020-07-30"\ntags:\n  - to-do\n  - -sounding-board\n  - -master\n  - -sa/processed\n---\n\nStudienarbeit\nCamera-based localisation\n\n*   [x] Find a classification of approaches/techniques\n    *   [x] Briefly describe each\n    *   [x] See if it applies to the project\n*   [x] Look into the most promising approach -- how to implement (DefSLAM)\n*   DefSLAM\n    *   [x] Install DefSLAM library\n    *   [x] Skim through an existing VI-SLAM (rigid) implementation to see how sensor fusion is done (as an overview for the coming sensor fusion task)\n        *   [x] VINS-Mono, [VIORB](http://www.evernote.com/shard/s484/nl/217355218/014a8f28-8f7b-4178-aea8-c65f9ae1dd73) paper\n    *   [x] Prepare dummy data for testing VI-SLAM (eventually VI-DefSLAM) — interpolate data between frames and add noise/bias\n    *   [x] Go through code\n        *   [x] Get the executables working\n        *   [x] VideoCapture OpenCV problem -- reinstall with all FFMMPEG options\n        *   [x] Figure out g2o\n        *   [x] Go through the rest of DefSLAM\n        *   [x] Go through the rest of ORBSLAM3\n            *   IMU term in cost function\n            *   IMU preintegration\n            *   IMU initialisation\n*   - [x] implement imu term in optimisation (either using ekf (s. [imu index](imu-index.md)), or [orbslam3-approach](orbslam3-approach.md)) for doing VI-DefSLAM. scrapped\n*   [x] Use EKF to estimate IMU calibration pose (between IMU and camera)\n    *   [x] model for generating imu data from camera, s. [obtaining imu measurements from camera by forward kinematics](obtaining imu measurements-from-camera-by-forward-kinematics.md)\n    *   [x] Come up with equations correlating SLAM CS to real camera CS: just subtract theta\\_notch\n    *   [x] Implement filter using IMU data + camera measurements\n*   ~~- [x] Finalise interface (Python + DefSLAM)~~\n    *   - [x] DefSLAM + SWIG scrapped for now\n*   [ ] Make pretty plots/animations\n*   [x] Presentation\n*   [ ] [Thesis](private/thesis-sa/thesis.md)\n\n',title:"Untitled Page"},"/studienarbeit/scaradozzi-2018":{content:'---\ntitle: (Scaradozzi 2018) SLAM application in surgery\ndate: "2020-08-06"\ntags:\n  - to-do/to-clarify\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-read\n  - discussion/2020/2020-08\n  - -sa/processed\n---\n\n## Abstract:\n\n*   SLAM\'s potential in image-guided surgery assuming static environment\n*   Review of main techniques in general robotics SLAM\n*   Insight into visual SLAM\n*   SLAM in surgery\n\n## Chapters\n* [what-is-slam](SLAM/what-is-slam.md)\n* [Filter-based vs optimisation-based SLAM](filter-based-vs-optimisation-based-slam.md)\n\n\u003cpre\u003e\u003c/pre\u003e\n* [General Kalman Filter](general-kalman-filter.md)\n* [General EKF](general-ekf.md)\n* [Unscented Kalman Filter](unscented-kalman-filter.md)\n* [Information Filter](information-filter.md)\n\n....\n\n## Takeaway\n\n*   EKF is popular in surgery SLAM techniques\n*   Deformable environment encumbers precise registration [ ]  and data fusion\n\n',title:"Untitled Page"},"/studienarbeit/scene-graph-general":{content:'---\ntitle: Scene graph (general)\ndate: "2020-08-09"\ntags:\n  - software/SOFA/data-types\n  - -sa/processed\n---\n\nSource: [http://en.wikipedia.org/wiki/Scene\\_graph](http://en.wikipedia.org/wiki/Scene_graph)\nSee also: [Scene graph in SOFA](scene-graph-in-sofa.md)\n\n*   A general data structure\n*   Collection of nodes in a graph/tree\n\n',title:"Untitled Page"},"/studienarbeit/scene-graph-in-sofa":{content:'---\ntitle: Scene graph in SOFA\ndate: "2020-07-15"\ntags:\n  - software/SOFA/data-types\n  - -sa/processed\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\nParent: [Data structure in SOFA](data-structure-in-sofa.md)\nBacklinks: [SOFA Introduction](sofa-introduction.md)\nSee also: [Scene graph (general)](scene-graph-(general).md)\n\n*   Pool of simulated objects and algorithms in a hierarchical data structure\n*   Scenes can be built procedurally or read from XML files\n*   Root node represents whole simulation\n*   Graph is processed by using [visitors](visitors.md)\n\nA scene graph node\n\n*   Gathers components associated with the same DOFs/topology\n*   Connections between non-sibling components require explicit references\n\nExample:\n![unknown_filename.png](./_resources/Scene_graph_in_SOFA.resources/unknown_filename.png)\n\n*   The collision spheres of the rigid object are in a child contact node of their own, because\n    *   they are not independent DOFs (separate from independent DOFs in [MechanicalState](http://www.evernote.com/shard/s484/nl/217355218/d4586073-9cf7-40f7-9750-a8736a94457f))\n    *   they are of a different data type\n*   Interactions between the rigid and deformable objects are handled by a shared component (ContactSpring) defined as a sibling node to both (coupling)\n    *   Soft coupling using penalty forces\n        *   Can be modelled by a constant interaction force (assumption) during each time step\n        *   Compatible with all explicit time intergration schemes\n    *   Hard coupling using penalty forces / constraint-based interaction via [Lagrange multipliers](http://www.evernote.com/shard/s484/nl/217355218/d3ac75ba-fe8e-4ef2-bc5f-2fe53d54d4c0)\n        *   Stiff interaction forces\n        *   Implicit integration necessary, for large time steps without any instabilities\n*   Generally more efficient to process independent interaction groups using separate solvers\n\n',title:"Untitled Page"},"/studienarbeit/schneider-2013-how-to-not-make-the-ekf-fail":{content:'---\ntitle: Schneider 2013 How to not make the EKF fail\ndate: "2021-08-18"\nexternal_url: "http://www.researchgate.net/publication/263942618_How_To_NOT_Make_the_Extended_Kalman_Filter_Fail/citations"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-read\n  - filters/EKF\n  - -sa/processed\n---\n\n**Authors**: Schneider, Georgakis\n**URL**: [http://www.researchgate.net/publication/263942618\\_How\\_To\\_NOT\\_Make\\_the\\_Extended\\_Kalman\\_Filter\\_Fail/citations](http://www.researchgate.net/publication/263942618_How_To_NOT_Make_the_Extended_Kalman_Filter_Fail/citations)\n**DOI** [10.1021/ie300415d](http://dx.doi.org/10.1021/ie300415d)\n\n[Measurement noise R, V (landmark)](measurement-noise-r,-v-(landmark).md)\n[Kalman filter initial estimates](kalman-filter-initial-estimates.md)\n[Process noise Q and W (odometry)](process-noise-q-and-w-(odometry).md)\n[Kalman filter performance metric](kalman-filter-performance-metric.md)\n\n',title:"Untitled Page"},"/studienarbeit/scipy.optimize":{content:'---\ntitle: scipy.optimize\ndate: "2021-08-26"\nexternal_url: "http://stackoverflow.com/questions/52438263/scipy-optimize-gets-trapped-in-local-minima-what-can-i-do"\ntags:\n  - -sounding-board\n  - -sa/processing\n---\n\n\u003chttp://stackoverflow.com/questions/52438263/scipy-optimize-gets-trapped-in-local-minima-what-can-i-do\u003e\n\nLocal optims sensitive to initial value.\nWorkarounds:\n\n*   use your optimization in a loop with random starting points inside your boundaries\n*   use an algorithm that can break free of local minima, I can recommend scipy\'s basinhopping()\n    It repeats your minimize procedure multiple times and get multiple local minimums. The minimal one is the global minimum.\n    \n*   use a global optimization algorithm and use it\'s result as initial value for a local algorithm. Recommendations are NLopt\'s DIRECT or the MADS algorithms (e.g. NOMAD). There is also another one in scipy, shgo, that I have no tried yet.\n\n[http://scipy-lectures.org/advanced/mathematical\\_optimization/\n- choosing-a-method](http://scipy-lectures.org/advanced/mathematical_optimization/\n- choosing-a-method)\n\nWithout knowledge of the gradient:\n\n*   In general, prefer BFGS or L-BFGS, even if you have to approximate numerically gradients. These are also the default if you omit the parameter method - depending if the problem has constraints or bounds\n*   On well-conditioned problems, Powell and Nelder-Mead, both gradient-free methods, work well in high dimension, but they collapse for ill-conditioned problems.\n\nWith knowledge of the gradient:\n\n*   BFGS or L-BFGS.\n*   Computational overhead of BFGS is larger than that L-BFGS, itself larger than that of conjugate gradient. On the other side, BFGS usually needs less function evaluations than CG. Thus conjugate gradient method is better than BFGS at optimizing computationally cheap functions.\n\nWith the Hessian:\n\n*   If you can compute the Hessian, prefer the Newton method (Newton-CG or TCG).\n\nIf you have noisy measurements:\n\n*   Use Nelder-Mead or Powell.\n\n',title:"Untitled Page"},"/studienarbeit/segfault-in-deftracking-imu-branch":{content:'---\ntitle: Segfault in DefTracking (imu branch)\ndate: "2021-01-20"\ntags:\n  - -sa/processed\n  - software/cpp\n  - SLAM/algos/DefSLAM\n---\n\n![unknown_filename.png](./_resources/Segfault_in_DefTracking_(imu_branch).resources/unknown_filename.png)\n\n/home/user3/slam/datasets/mandala0/images/stereo\\_im\\_l\\_1560936003993.png i:  30\nPOINTS matched:10\nTrack lost soon after initialisation, reseting...\n/home/user3/slam/datasets/mandala0/images/stereo\\_im\\_l\\_1560936004022.png i:  31\nSystem Reseting\nNORMAL ESTIMATOR IN - NORMALS REESTIMATED : 0 - 0\nNORMAL ESTIMATOR OUTPoints potential : 939  70\nNew template requested\nNumber Of normals 0 0x5555636b1fb0\nNot enough normals\nReseting Local Mapper... done\nReseting Loop Closing... done\nReseting Database... done\n\nThread 1 "DefSLAM" received signal SIGSEGV, Segmentation fault.\n0x00007ffff78d9fae in cv::Mat::Mat (m=..., this=0x7ffffffeaea0) at /usr/local/include/opencv4/opencv2/core/mat.inl.hpp:545\n545             step\\[0\\] = m.step\\[0\\]; step\\[1\\] = m.step\\[1\\];\n(gdb) bt\n#0  0x00007ffff78d9fae in cv::Mat::Mat(cv::Mat const\u0026) (m=..., this=0x7ffffffeaea0) at /usr/local/include/opencv4/opencv2/core/mat.inl.hpp:545\n#1  0x00007ffff78d9fae in defSLAM::DefTracking::UpdateLastFrame() (this=0x7ffff7e50010)\n    at /home/user3/slam/DefSLAM/Modules/Tracking/DefTracking.cc:424\n#2  0x00007ffff78e00de in defSLAM::DefTracking::TrackWithMotionModel() (this=0x7ffff7e50010)\n    at /home/user3/slam/DefSLAM/Modules/Tracking/DefTracking.cc:357\n#3  0x00007ffff78db3fb in defSLAM::DefTracking::Track() (this=0x7ffff7e50010) at /home/user3/slam/DefSLAM/Modules/Tracking/DefTracking.cc:103\n#4  0x00007ffff4555d49 in ORB\\_SLAM2::Tracking::GrabImageMonocular(cv::Mat const\u0026, double const\u0026) (this=0x7ffff7e50010, im=..., timestamp=@0x7fffffffded8: 1560936004022) at /home/user3/slam/DefSLAM/Thirdparty/ORBSLAM\\_2/src/Tracking.cc:278\n#5  0x00007ffff781d25f in defSLAM::System::TrackMonocularIMU(cv::Mat const\u0026, double const\u0026, std::vector\u003cORB\\_SLAM3::IMU::Point, std::allocator\u003cORB\\_SLAM3::IMU::Point\u003e \u003e const\u0026) (this=0x7fffffffe100, im=..., timestamp=@0x7fffffffded8: 1560936004022, vImuMeas=std::vector of length 5, capacity 8 = {...}) at /home/user3/slam/DefSLAM/Modules/Common/System.cc:313\n#6  0x000055555555cd1b in main(int, char\\*\\*) (argc=\u003coptimized out\u003e, argv=\u003coptimized out\u003e) at /home/user3/slam/DefSLAM/Apps/vi.cc:91\n(gdb)\n\n',title:"Untitled Page"},"/studienarbeit/sending-data-using-sockets":{content:'---\ntitle: Sending data using sockets\ndate: "2020-07-17"\ntags:\n  - software/python\n  - software/sockets\n  - -sa/processed\n---\n\n**Parent**: [SofaPython Index](sofapython-index.md)\n\n\u003chttp://github.com/psomers3/PyDataSocket\u003e\n\n\u003chttp://docs.python.org/3/howto/sockets.html\u003e\nSockets: form of IPC (inter-process communication), for cross-platform communication\n(Alternatives, for fast IPC: pipes, shared memory)\n\n*   “client” socket - an endpoint of a conversation\n    *   e.g. browser, other client applications\n*   “server” socket, which is more like a switchboard operator. The client application (your browser, for example) uses\n    *   e.g. web server (uses both server and client sockets)\n\nRoughly, how a socket works (ex: clicking a link on the browser)\nClient socket (browser) / Receive\n\n*   Creation of a streaming socket s\n*   Socket connects to web server on port 80 (normal http port)\n*   Connect completes\n*   Socket s is used to request the text\n*   A reply is sent\n*   The socket reads the reply, is destroyed\n\nServer socket (web server) / Send\n\n*   Create a streaming socket\n*   Socket is bound to a public host and a well-known port\n    *   public host: socket.gethostname()\n    *   So that the socket is visible to the outside world\n    *   e.g. if s.bind((\'localhost\', 80)), this is still a server socket, but only visible within the machine\n*   Socket listens (on the defined port) to x connections before refusing outside connections\n\nMain loop\n\n*   Server socket accepts connections from outside\n*   A server socket doesn\'t actually send any data nor does it receive any; it just produces client sockets\n    *   Each cs created in response to another "c"s doing a connect to the host\n\nPorts\n\n*   low number port are usually reserved (e.g. HTTP, etc)\n*   to use an unreserved port, try a high number (4 digits)\n\n',title:"Untitled Page"},"/studienarbeit/several-unwanted-effects-using-gh-filters":{content:'---\ntitle: Several unwanted effects using gh filters\ndate: "2020-08-27"\ntags:\n  - -sa/processed\n  - filters/gh-filter\n---\n\nParent: [g-h filter or α-β filter](g-h-filter-or-α-β-filter.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nEffect of bad initial conditions: ringing (sinusoidal over- and undershooting before finally settling onto a trajectory)\n![unknown_filename.png](./_resources/Several_unwanted_effects_using_gh_filters.resources/unknown_filename.png)\n\nEffect of very noisy data\n![unknown_filename.1.png](./_resources/Several_unwanted_effects_using_gh_filters.resources/unknown_filename.1.png)\n\nEffect of acceleration (in data)\n![unknown_filename.2.png](./_resources/Several_unwanted_effects_using_gh_filters.resources/unknown_filename.2.png)\nFilter lags behind because it uses a model that assumes constant velocity in each propagation step.\nHence, a filter is only as good as the mathematical model used to describe the phenomenon.\n\n',title:"Untitled Page"},"/studienarbeit/sfm":{content:'---\ntitle: Structure from Motion\ndate: "2020-10-05"\ntags:\n  - -sa/to-be-processed\n  - to-do/missing-tag\n  - -published\n---\n\n**Source**: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Schonberger_Structure-From-Motion_Revisited_CVPR_2016_paper.pdf\n\n**Note**:\n* This paper uses incremental SfM\n* Corresponding paper for COLMAP\n\n# Structure from motion\nReconstruction of 3D structure from a sequence of 2D images of that structure, taken from different viewpoints.\n\n![](_img/sfm-pipeline.png)\n1. Search for correspondence between images --\u003e output: scene graph (nodes: images, edges: verified pairs)\n	1. Feature extraction\n	2. [Feature matching](studienarbeit/feature-matching.md)  \n		Output: set of image pairs and their associated feature correspondences\n	3. Verification: do features map to the same scene point?  \n		Also: filter outliers e.g. using [RANSAC](SLAM/ransac.md)\n2. Scene graph initialises the reconstruction stage --\u003e output: {camera pose estimates, set of scene points}\n\n\n---\n\n**Source**: https://en.wikipedia.org/wiki/Structure_from_motion\n\n# Structure from motion\n\nUses [motion-parallax](definitions/motion-parallax.md).\n\nGeometric information to be estimated:\n* 3D structure\n* Camera motion\n\n## Tasks\n* find [correspondence between images](studienarbeit/feature-matching.md)\n	* use feature detectors to detect features\n	* feature matching to track the features across images  \n		e.g. Lucas-Kanade tracker, [RANSAC](SLAM/ransac.md) to filter outliers\n* reconstruct the 3D object\n* reconstruct the camera motion\n\nFeature trajectories over time are used to reconstruct their 3D positions as well as the camera motion [Dellaert, Thrun 2000].\n\nAlternatively: feature-free methods / direct methods, where geometric information is directly estimated from the images without abstracting to features [LSD-SLAM]\n\n\n## Approaches to SfM\n* Incremental SfM: solve for camera poses one by one [Schönberger, Frahm 2016]\n* Global SfM: solve for all camera poses at once [Tomasi, Kanade 1992]\n* [NRSfM](studienarbeit/nrsfm.md)',title:"Untitled Page"},"/studienarbeit/sft":{content:'---\ntitle: Shape from Motion\ndate: "2020-11-20"\ntags:\n  - to-do/go-through-literature-later\n  - SLAM/deformable-SLAM\n  - to-do/not-good-enough\n  - -sa/to-be-processed\n  - -published\n---\n\n\nInitial paper?: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010934https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010934\n\n## Goal\nreconstruct the surface of an object\n* reference 3D shape (template) of the object is available\n* under a specific deformation constraint\n\n---\n\n**Source**: [lamarca-2020](studienarbeit/lamarca-2020.md)\n\n## SFT (shape from template)\n\n*   uses only a single image — faster than [nrsfm](studienarbeit/nrsfm.md)\n*   lower computational cost\n*   must have a known 3D template (textured model)\n\n### SfT methods\n*   require:\n    *   1 monocular image\n    *   1 textured shape at rest (template) "geometry" as the deformation model\n*   different definitions of the deformation model\n    *   analytic, e.g. isometric deformation-based: assumes preserved geodesic distance between surface points\n        *   isometry for SfT has proven to be well-posed --\u003e led to stable, real-time solutions\n    *   energy-based; jointly minimises {energy shape w.r.t. template \\[shape at rest\\] + reprojection error for image correspondences}\n*   classification according to \\[[http://www.cv-foundation.org/openaccess/content\\_cvpr\\_2015/papers/Gallardo\\_Shape-From-Template\\_in\\_Flatland\\_2015\\_CVPR\\_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Gallardo_Shape-From-Template_in_Flatland_2015_CVPR_paper.pdf)\\]\n    *   statistics-based\n        *   use data to learn the space of deformations\n        *   effective in low-dimensional spaces\n    *   physics-based\n        *   uses mathematical models (based on physical laws) in order to compute the space of deformations\n        *   e.g. isometric model',title:"Untitled Page"},"/studienarbeit/showing-correlation-using-error-ellipses":{content:"---\ntitle: Showing correlation using error ellipses\ndate: \"2020-09-01\"\ntags:\n  - to-do/to-clarify\n  - -sa/processed\n  - math/statistics\n---\n\nParent: [Error ellipse/Confidence ellipse](error-ellipse_confidence-ellipse.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nA slanted ellipse implies correlation\n- [ ] The 'thinner' side isn't necessarily more accurate, it just means that the spread of data is reduced along this dimension (when viewing sensor data, for example) \n\nExample\nFirst epoch\n![unknown_filename.1.png](./_resources/Showing_correlation_using_error_ellipses.resources/unknown_filename.1.png)\n**Yellow**: prior (very uncertain about position)\n**Green**: evidence (more accurate in one of the dimensions than the other; more certainty compared to prior)\n**Blue**: posterior via [multiplication](multiplication.md)\n\nPosterior retains the shape of the evidence (which has more certainty than the prior)\n\nSecond epoch\n![unknown_filename.png](./_resources/Showing_correlation_using_error_ellipses.resources/unknown_filename.png)\n\n*   Old posterior now becomes the next prior (in yellow)\n*   A new measurement (in green) is made\n*   The resulting covariance shape reflects the physical layout of the system (geometry of the problem)\n*   Enables triangulation of the system (via use of two sensors at two different locations) -- optimal triangulation when sensors are orthogonal to each other, as in the example\n\n",title:"Untitled Page"},"/studienarbeit/simplex":{content:'---\ntitle: Simplex\ndate: "2020-08-09"\ntags:\n  - -definitions\n  - -sa/processed\n---\n\nSource: \u003chttps://en.wikipedia.org/wiki/Simplex\u003e\nParent: [Barycentric coordinates](Barycentric coordinates.md)\n\nA triangle in arbitrary dimensions\n![unknown_filename.jpeg](./_resources/Simplex.resources/unknown_filename.jpeg)\n\n|     |     |\n| --- | --- |\n| 0-simplex | point |\n| 1-simplex | line |\n| 2-simplex | triangle |\n| 3-simplex | tetrahedron |\n\n',title:"Untitled Page"},"/studienarbeit/simulation-algorithms-in-sofa":{content:'---\ntitle: Simulation algorithms in SOFA\ndate: "2020-08-22"\ntags:\n  - -sa/processed\n  - software/SOFA/simulation-algos\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\n\n*   ODE integration ([ODE solvers](ode-solvers.md))\n*   Linear equation solution ([linear solvers](linear-solvers.md))\n*   Complex constraints ([constraint solvers](http://www.evernote.com/shard/s484/nl/217355218/d3ac75ba-fe8e-4ef2-bc5f-2fe53d54d4c0))\n*   [Collision detection and response](http://www.evernote.com/shard/s484/nl/217355218/200493ba-d1b9-4d1e-bc0c-8ad07ac3a3fa)\n*   GPU support\n\n',title:"Untitled Page"},"/studienarbeit/slam-resources":{content:'---\ntitle: SLAM resources\ndate: "2020-07-27"\ntags:\n  - -resources\n  - SLAM\n  - -sa/processed\n  - -resources/tutorials\n---\n\nParent: [SLAM Index](SLAM/slam_index.md)\n\nTheory\n\n*   [Wikipedia SLAM](wikipedia-slam.md)\n    *   [http://en.wikipedia.org/wiki/Maximum\\_a\\_posteriori\\_estimation](http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation)\n*   Thrun - Probabilistic Robotics\n*   [SLAM for dummies](http://www.evernote.com/shard/s484/nl/217355218/3bc16d22-4339-40f5-9990-c019864b6e9a)\n*   [Andrew Davison](http://www.doc.ic.ac.uk/~ajd/index.html) research page at the [Department of Computing](http://en.wikipedia.org/wiki/Department_of_Computing%2C_Imperial_College_London), [Imperial College London](http://en.wikipedia.org/wiki/Imperial_College_London) about SLAM using vision.\n    *   Paper 2002 on monocular SLAM\n*   [SLAM lectures on YouTube](http://www.youtube.com/watch?v=B2qzYCeT9oQ\u0026list=PLpUPoM7Rgzi_7YWn14Va2FODh7LzADBSm)\n*   [http://openslam-org.github.io](http://openslam-org.github.io/)/\n\n**Tutorials**\nSLAM summer school SS06: \u003chttp://www.robots.ox.ac.uk/~SSS06/Website/\u003e\n\nProgramming\n[Programmatic implementations of MonoSLAM](programmatic-implementations-of-monoslam.md)\n\n',title:"Untitled Page"},"/studienarbeit/slam-specific-jacobians":{content:'---\ntitle: SLAM-specific jacobians\ndate: "2020-07-29"\ntags:\n  - filters/EKF\n  - -sa/processed\n---\n\nSource: [SLAM for Dummies](slam-for-dummies.md)\nBacklinks:  [EKF matrices/vectors](ekf-matrices_vectors.md)\n\nJxr\n\n*   Jacobian of the prediction of landmarks, which does not include prediction of theta, w.r.t. robot POSE\n*   same as J\\_prediction model, except without rotation term\n    ![unknown_filename.png](./_resources/SLAM-specific_jacobians.resources/unknown_filename.png)\n    \n\nJz\nJacobian of prediction of landmarks, but w.r.t. \\[range, bearing\\]\n![unknown_filename.1.png](./_resources/SLAM-specific_jacobians.resources/unknown_filename.1.png)\n\n',title:"Untitled Page"},"/studienarbeit/smooth-polymial-trajectory-generation":{content:'---\ntitle: Smooth polymial trajectory generation\ndate: "2021-07-19"\ntags:\n  - -sa/processed\n  - math\n---\n\n**Source**: FLS handouts\n\n![unknown_filename.png](./_resources/Smooth_polymial_trajectory_generation.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/smug-filter":{content:'---\ntitle: Smug filter\ndate: "2020-08-31"\ntags:\n  - -definitions\n  - -sa/processed\n  - filters\n---\n\nParent: [1D Kalman filter algorithm](1d-kalman-filter-algorithm.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\nA filter that, once enough measurements are made, becomes very confident in its prediction (P gets smaller with time while the filter becomes more inaccurate!).\nFrom then on it will ignore measurements\n\nTo avoid this: add a bit of error to the prediction step, e.g. using the [process variance](http://www.evernote.com/shard/s484/nl/217355218/dd75d924-7153-4b9f-81a1-ec6237d4ec25)\n\n',title:"Untitled Page"},"/studienarbeit/sockets-errno-10054":{content:'---\ntitle: Sockets Errno 10054\ndate: "2020-07-20"\nexternal_url: "http://docs.microsoft.com/en-gb/windows/win32/winsock/windows-sockets-error-codes-2?redirectedfrom=MSDN"\ntags:\n  - software/sockets\n  - -sa/processed\n---\n\nParent: [SofaPython Index](sofapython-index.md)\n\nWSAECONNRESET\n10054\nConnection reset by peer.\nAn existing connection was forcibly closed by the remote host. This normally results if the peer application on the remote host is suddenly stopped, the host is rebooted, the host or remote network interface is disabled, or the remote host uses a hard close (see setsockopt for more information on the SO\\_LINGER option on the remote socket). This error may also result if a connection was broken due to keep-alive activity detecting a failure while one or more operations are in progress. Operations that were in progress fail with WSAENETRESET. Subsequent operations fail with WSAECONNRESET.\n\n',title:"Untitled Page"},"/studienarbeit/sofa-cataract-surgery":{content:'---\ntitle: SOFA Cataract surgery\ndate: "2020-07-16"\ntags:\n  - -resources/videos\n  - -sa/processed\n---\n\n\u003chttp://www.sofa-framework.org/applications/gallery/eye-surgery-simulator-insimo/\u003e\n\n|     |     |\n| --- | --- |\n| [![unknown_filename.png](./_resources/SOFA_Cataract_surgery.resources/unknown_filename.png)](http://www.sofa-framework.org/applications/gallery/eye-surgery-simulator-insimo/) | [SOFA – Cataract Surgery – InSimo](http://www.sofa-framework.org/applications/gallery/eye-surgery-simulator-insimo/)\u003cbr\u003ewww.sofa-framework.org\u003cbr\u003eThe SOFA technology is at the core of a advanced eye surgery simulator developed in the context of the HelpMeSee project. HelpMeSee is an American foundation with a singular mission:… read more → |     |\n\n',title:"Untitled Page"},"/studienarbeit/sofa-extended-documentation":{content:'---\ntitle: SOFA extended documentation\ndate: "2020-07-15"\ntags:\n  - software/SOFA\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-read\n  - -sa/processed\n---\n\nSource: \u003chttp://hal.inria.fr/hal-00681539\u003e\nAuthors: Faure et al\nBacklinks: [Scope of Studienarbeit](scope-of-studienarbeit.md)\n\nAbstract\n\n*   SOFA: open source C++ library\n*   mainly for interactive physical/medical simulation\n*   modular approach by decomposing simulators into its constituent components (DOF, differential equations, solvers etc), and organising them in a scenegraph data structure\n*   multimodel representation of objects (collision model, visual model etc)\n\nChapters\n\nRead\n\n*   1: [introduction](introduction.md)\n*   2: [multimodel framework](http://www.evernote.com/shard/s484/nl/217355218/61adbad2-ef29-4eb6-8b79-bc7e2065df24?title=Models%20in%20SOFA)\n*   3: [data structures](http://www.evernote.com/shard/s484/nl/217355218/2f8350ca-46bc-4a46-aa5d-d40d64881a65?title=Data%20structure%20in%20SOFA)\n    *   3.1 [scenegraph](http://www.evernote.com/shard/s484/nl/217355218/bf18694a-5cea-4dbe-8ba2-95b6c187a636) and [visitors](http://www.evernote.com/shard/s484/nl/217355218/8e1b1539-2194-4c70-b3b5-c3d4328fa151)\n    *   3.2 data and engines\n    *   3.3 [mesh geometry](mesh-geometry.md) and [mesh-topology](mesh-topology.md)\n*   4: [simulation algorithms in SOFA](simulation-algorithms-in-sofa.md)\n    *   ODE integration ([ODE solvers](ode-solvers.md))\n    *   Linear equation solution ([linear solvers](linear-solvers.md))\n    *   Complex constraints ([constraint solvers](constraint-solvers.md))\n    *   [Collision detection and response](collision-detection-and-response.md)\n    *   GPU support\n\nSkimmed\n\n*   5: user interface\n    *   5.3: [haptic rendering](http://www.evernote.com/shard/s484/nl/217355218/79a1235b-03d4-4fbe-a1a6-ad4bedc46d92)\n\nLater\n\n*   7: conclusion\n*   6: examples\n\n',title:"Untitled Page"},"/studienarbeit/sofa-introduction":{content:'---\ntitle: SOFA Introduction\ndate: "2020-07-15"\ntags:\n  - software/SOFA\n  - -sa/processed\n---\n\nSource:  [SOFA extended documentation](sofa-extended-documentation.md)\n\nGoal of SOFA: To provide a highly modular framework for interactive medical simulation, enabling collaboration across different disciplines\nConcept: [scene-graph](scene-graph.md)\\-based multimodel representatios\n\nHow it works:\n\n*   Simulators are broken down into independent components\n    *   Component: an aspect of the simulation\n    *   e.g. DOF, forces, constraints, ODEs/PDEs, solvers, algorithms\n*   Components are organised in a [scene graph](scene-graph.md) data structure\n*   Simulated objects represented via several [models](models.md)\n\n',title:"Untitled Page"},"/studienarbeit/sofapython-api-documentation-links":{content:'---\ntitle: SofaPython API/Documentation links\ndate: "2020-07-17"\ntags:\n  - software/SOFA\n  - software/python\n  - -resources\n  - -sa/processed\n---\n\n**Parent:** [SofaPython Index](sofapython-index.md)\n\nSP2\n\n*   [SofaPython pdf](http://github.com/sofa-framework/sofa/tree/master/applications/plugins/SofaPython/doc)\n*   \u003chttp://www.sofa-framework.org/api/master/plugins/SofaPython/html/index.html\u003e\n*   \u003chttp://sofacomponents.readthedocs.io/en/latest/index.html\u003e\n\nSP3\n\n*   \u003chttp://sofapython3.readthedocs.io/en/latest/menu/SofaPlugin.html\u003e\n\n',title:"Untitled Page"},"/studienarbeit/sofapython-index":{content:"---\ntitle: SofaPython Index\ndate: \"2020-08-22\"\ntags:\n  - software/SOFA\n  - software/python\n  - -master\n  - -sa/processed\n  - software/SOFA/SofaPython3\n---\n\nBuilding/setup\n[Building SOFA on Windows](building-sofa-on-windows.md)\n[Someone's SP3 setup](someone's-sp3-setup.md)\n\nRunning\n[Running SOFA with Python](running-sofa-with-python.md)\n[Using python with existing scene](using-python-with-existing-scene.md)\n[Basic python script in Sofa](basic-python-script-in-sofa.md)\n[Initialising graph in SP3](initialising-graph-in-sp3.md)\n\nPlugins\n[Possible plugins](possible-plugins.md)\n[Install ROSConnector in SOFA](install-rosconnector-in-sofa.md)\n[STLIB (Sofa Template Library)](stlib-(sofa-template-library).md)\n[Registration](registration.md)\n\nCommunication\n[Sending data using sockets](sending-data-using-sockets.md)\n[Sockets Errno 10054](sockets-errno-10054.md)\n[External data in SOFA](external-data-in-sofa.md)\n\nDocumentation\n[SofaPython API/Documentation links](sofapython-api_documentation-links.md)\n[Cheatsheet](cheatsheet.md)\n\n",title:"Untitled Page"},"/studienarbeit/solà-2014-slam-with-ekf":{content:'---\ntitle: Solà 2014 SLAM with EKF\ndate: "2021-05-13"\ntags:\n  - -resources\n  - filters/EKF\n  - -sa/processed\n---\n\n*   Notes on EKF-SLAM that uses landmarks\n*   MATLAB code\n*   Notes on partial landmark initialisation (convariance matrix)\n*   Notes on the linearity of the observation function in scale\n\n',title:"Untitled Page"},"/studienarbeit/solà-2017-quaternion-kinematics-for-eskf":{content:'---\ntitle: Solà 2017 Quaternion kinematics for ESKF\ndate: "2021-05-14"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-read\n  - -sa/processed\n  - -published\n---\n\n**Link**: \u003chttp://www.iri.upc.edu/people/jsola/JoanSola/objectes/notes/kinematics.pdf\u003e  \n**Author**: Joan Solà  \n\n## Abstract\n*   Primer on quaternion/rotation group math\n*   Math for error state Kalman filters using IMUs\n\n## Contents/Chapters\n\n1.  [Unit quaternions](rotations/unit-quaternions.md), [double cover](rotations/quaternion-double-cover.md)\n2.  Rotations, s. also [SO(3) 3D rotation group](rotations/so3-3d-rotation-group.md)\n3.  [Quaternion conventions](quaternion-conventions.md)\n4.  Perturbations, derivatives, integrals\n5.  [Error-State Kalman Filter](studienarbeit/50.5-error-state-kalman-filter.md) for IMU-driven systems\n    *   [Variables in ESKF](studienarbeit/50.5.1.1-states-of-the-eskf-for-estimating-imu-pose.md)\n    *   [IMU measurement model](studienarbeit/40.1-imu-measurement-model.md)\n    *   [IMU motion model](studienarbeit/50.3-modelling-imu-in-kf.md)\n        * [The initial gravity vector/orientation for the IMU ESKF](studienarbeit/50.5.1.2-the-initial-gravity-vector-orientation-for-the-imu-eskf.md)\n    *   [IMU nominal-state and error-state kinematics](studienarbeit/50.5.1-imu-nominal-state-and-error-state-kinematics.md)\n    *   [IMU ESKF prediction equations](studienarbeit/50.6-eskf-prediction-equations.md)\n6.  [Fusing IMU + other sensors](studienarbeit/50.7-eskf-update-fusing-imu-with-complementary-sensory-data.md)\n7.  ESKF using global angular errors\n\n',title:"Untitled Page"},"/studienarbeit/someone's-sp3-setup":{content:'---\ntitle: Someone\'s SP3 setup\ndate: "2020-09-30"\ntags:\n  - software/SOFA/SofaPython3\n  - -sa/to-be-processed\n---\n\n**Parent**: [SofaPython Index](sofapython-index.md)\n\n\u003chttp://gist.github.com/pedroperrusi/9fdd4257db72465c8fb481381f396c51\u003e\n\n',title:"Untitled Page"},"/studienarbeit/song-2018-mis-slam":{content:'---\ntitle: Song 2018 MIS-SLAM\ndate: "2020-08-24"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-read\n  - SLAM/deformable\n  - -sa/to-be-processed\n---\n\n**Authors**: Song et al\n\n**Note**: referred to in [Lamarca 2020](lamarca-2020.md) as a stereovisual deformable SLAM, uses CPU and GPU, nonlinear optimisation\nVideo: \u003chttp://www.youtube.com/watch?v=2pXokldQBWM\u003e\n\n## Abstract\n\n*   Uses CPU and GPU\n*   CPU for ORBSLAM (initial global position)\n*   GPU for deformable tracking and dense mapping\n\n## Contents/Chapters\n\n*   Poor localisation of scope in MIS, compared with open surgery\n*   Related works mentioned\n    *   don\'t provide a  RT and robust solution for localisation while reconstructing dense deformable surfaces\n    *   focus on the monocular scope, fail to solve the problem of missing scale\n*   Fast movement\n    *   makes visual odometry unstable\n    *   causes blurry images\n    *   worsens registrations\n*   ORB-SLAM proven to be suitable for coupling with dense deformable SLAM\n\n## Initial tracking: ORB-SLAM\n\n*   ORB features and global pose are uploaded to GPU\n*   Upload the matched ORB features every time a new observation is made\n*   The initial global pose makes the system significantly robuster\n\n## Deformable tracking and dense mapping\n\n*   Receives initial global pose from the CPU\n*   Initialises the model with an estimated depth\n*   From model: extract potential visible points\n*   Project these points onto 2D depth images\n*   Registration:\n    *   to estimate optimum global pose\n    *   to estimate non-rigid warping field\n*   Deform the model based on this transformation (registration)\n*   Fuse the deformed model with the new observation\n\nIncludes in-vivo validation with deformation (Hamlyn datasets)\n\n## Challenges\n\n*   Texture blending\n*   Slow optimisation due to large increase in number of nodes\n*   Loop closure still needs to be implemented (or improved upon)\n\n## Takeaway\n\nORB-SLAM seems to be implementable in deformable environment situations\n\n',title:"Untitled Page"},"/studienarbeit/spaces-in-mathematics":{content:'---\ntitle: Spaces in mathematics\ndate: "2020-11-27"\ntags:\n  - -definitions\n  - -sa/processed\n  - math\n---\n\nSource: [http://upload.wikimedia.org/wikiversity/en/c/cd/Spaces\\_in\\_mathematics.pdf](http://upload.wikimedia.org/wikiversity/en/c/cd/Spaces_in_mathematics.pdf)\n\nTypes of spaces in mathematics\n\n*   Euclidian spaces (3D space, 2D space/Euclidian plane)\n*   Linear spaces\n*   Topological spaces\n*   Hilbert spaces\n*   etc.\n\nWhat is a space?\n\n*   No real definition\n*   Made of\n    *   selected mathematical objects which are treated as points\n    *   selected relationships between these points\n*   Points can be\n    *   elements of a set\n    *   functions\n    *   subspaces\n*   Isomorphic spaces are considered identical\n*   Isomorphism between two spaces: one-to-one mapping between the points, that preserves the relationships between the points\n\n',title:"Untitled Page"},"/studienarbeit/sparse-feature-based-vslam":{content:'---\ntitle: Sparse/Feature-based VSLAM\ndate: "2020-07-30"\ntags:\n  - SLAM/sparse-vs-dense/feature-based\n  - -sa/processed\n  - -published\n---\n\nParent: [Visual SLAM Implementation Framework](SLAM/vslam-framework.md), [slam_index](SLAM/slam_index.md)\nSee also: [Feature-based vs direct SLAM workflow](feature-based-vs-direct-slam-workflow.md)\n\nSource: [cometlabs](bibliography/cometlabs.md)\n\n*   Front-end part of the [Visual SLAM Implementation Framework](SLAM/vslam-framework.md)\n*   Use only a small selected subset of the pixels in an image frame\n*   [Feature maps](feature-maps.md) generated are point clouds --\u003e used to track the camera pose\n*   Requires feature extraction and [matching](studienarbeit/feature-matching.md)\n*   To minimise: reprojection error (difference between a point\'s tracked location and where it is expected to be given camera pose estimate)\n*   Pose estimation based on [RANSAC](SLAM/ransac.md)\n*   A frame with most of its features concentrated in a small area: bad as the features are more likely to overlap\n\n![unknown_filename.png](./_resources/Sparse_Feature-based_VSLAM.resources/unknown_filename.png)\nSparse\n\n![unknown_filename.1.png](./_resources/Sparse_Feature-based_VSLAM.resources/unknown_filename.1.png)\nSemi-dense\n\n',title:"Untitled Page"},"/studienarbeit/spherical-wrist":{content:'---\ntitle: Spherical wrist\ndate: "2021-05-25"\ntags:\n  - to-do/orphan\n  - math/kinematics\n  - -sa/to-be-processed\n---\n\n\u003chttp://www1.cs.columbia.edu/~allen/F15/NOTES/forwardspong.pdf\u003e\n\nS. also \u003chttp://www.youtube.com/watch?v=S6TFakW5YcI\u003e\n\n![unknown_filename.1.png](./_resources/Spherical_wrist.resources/unknown_filename.1.png)\n\n![unknown_filename.png](./_resources/Spherical_wrist.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/state-of-the-art-slam":{content:'---\ntitle: State-of-the-art SLAM\ndate: "2020-08-07"\ntags:\n  - SLAM\n  - discussion/2020/2020-08\n  - -sa/processed\n---\n\nParent: [SLAM Index](SLAM/slam_index.md)\nBacklinks: [Qin 2019 General Optimization-based Framework (Multisensor)](qin-2019-general-optimization-based-framework-(multisensor).md)\n\nThings that I\'ve seen mentioned several times so far\n\n*   **ORBSLAM**: monocular\n*   MonoSLAM: monocular (old?) — Andrew Davison\n*   OKVIS: visual inertial, stereovision\n*   PTAM: parallel tracking and mapping\n*   MSCKF: real-time EKF\n*   VINS-mono: visual inertial, monocular\n\n[http://en.wikipedia.org/wiki/List\\_of\\_SLAM\\_Methods](http://en.wikipedia.org/wiki/List_of_SLAM_Methods)\n\n',title:"Untitled Page"},"/studienarbeit/stlib-sofa-template-library":{content:'---\ntitle: STLIB (Sofa Template Library)\ndate: "2020-07-17"\ntags:\n  - software/SOFA/plugins\n  - -sa/processed\n---\n\nParent: [SofaPython Index](sofapython-index.md)\n\n\u003chttp://github.com/SofaDefrost/STLIB\u003e\n\nAPI doc: \u003chttp://stlib.readthedocs.io/en/latest/index.html\u003e\n\n*   contains sofa scene template\n    *   common scene template used regularly\n    *   templates should be compatible with .pyscn and PSL scenes\n\n',title:"Untitled Page"},"/studienarbeit/surface-alignment-in-defslam":{content:'---\ntitle: Surface alignment in DefSLAM\ndate: "2020-11-25"\ntags:\n  - -sa/processed\n  - to-do/missing-link\n  - SLAM/algos/DefSLAM\n---\n\n**Parent**: [Mapping step-by-step in DefSLAM](mapping-step-by-step-in-defslam.md)  \n**Source**: [lamarca-2020](studienarbeit/lamarca-2020.md)\n\n## Goal:\n\n*   to scale the up-to-scale surface (output of [NRSfM](studienarbeit/nrsfm.md)) to the proper dimensions\n*   get an idea of the proper dimensions from the already estimated map\n    *   i.e. resulting surface must match the scale of the template $\\mathcal{T}_{k-1}$\n    *   $\\mathcal{T}_{k-1}$: deformed map generated by the tracker at the instance of KF $=k$ insertion, with shape-at-rest of $\\mathcal{S}_{k-1}$ generated from KF: $(k-1)$\n*   result: scale-corrected shape-at-rest $\\mathcal{S}_k$\n\n## Method:\n\n*   alignment of the map points using Sim(3)\n*   Sim(3): basically a transformation that enlarges/shrinks by a certain scale (as far as I understand it). For more info see [Wiki](http://en.wikipedia.org/wiki/Similarity_%28geometry%29-In_Euclidean_space).\n    ![unknown_filename.png](./_resources/Surface_alignment_in_DefSLAM.resources/unknown_filename.png) scale $\\hat{\\mathcal{S}}$ to $\\mathcal{S}$\n    \n*   Minimise the error between transformation of map points in $\\hat{\\mathcal{S}}$ and the map points in $\\mathcal{S}$\n    ![unknown_filename.1.png](./_resources/Surface_alignment_in_DefSLAM.resources/unknown_filename.1.png)\n    where the arguments are the transformation parameters\n    \n\n## Generating of the new template $\\mathcal{T}_{k}$ from $\\mathcal{S}_k$\n\n*   Use Delaunay triangulation to make a triangular mesh from $\\mathcal{S}_k$\n*   New map point poses are computed\n    *   from the camera observation\n    *   by constraining them to be on the surface $\\mathcal{S}_k$ (embedding of the re-observed map points)\n\n',title:"Untitled Page"},"/studienarbeit/symbolic-container-for-probe":{content:'---\ntitle: Symbolic container for Probe\ndate: "2021-07-17"\ntags:\n  - -sounding-board\n  - -sa/processing\n  - to-do/orphan\n---\n\n![unknown_filename.1.jpeg](./_resources/Symbolic_container_for_Probe.resources/unknown_filename.1.jpeg)![unknown_filename.jpeg](./_resources/Symbolic_container_for_Probe.resources/unknown_filename.jpeg)\n\n',title:"Untitled Page"},"/studienarbeit/system-forcetrajectory":{content:"---\ntitle: System::forceTrajectory\ndate: \"2021-03-04\"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n  - discussion/2021/2021-03\n---\n\nParent: [DefSLAM branch overview](defslam-branch-overview.md)\n\nReference: DefSLAMGT (stereo as ground truth)\nFor testing: DefSLAMVI\n\nDescription\nForce update of DefSLAMVI's current frame pose to that of DefSLAMGT's for the frames 230 to 239\n\nWithout System::Reset\n![unknown_filename.png](./_resources/System__forceTrajectory.resources/unknown_filename.png)\nFrame pose is 'updated' during the interval, but after the interval, the optimisation (which uses frame pose as an estimate and also uses map node positions) makes the system resume it's trajectory before the update\n\n(below: with pure monocular trajectory, without any forced updates)\n![unknown_filename.2.png](./_resources/System__forceTrajectory.resources/unknown_filename.2.png)\n\nWith System::Reset\nThe system is reset after every forced pose update (i.e. every frame)\n![unknown_filename.1.png](./_resources/System__forceTrajectory.resources/unknown_filename.1.png)\nImmediately after the force update interval, everything goes to zero. Segfault at frame 252.\n\n",title:"Untitled Page"},"/studienarbeit/system-in-a-vin-problem-with-imu-preintegration":{content:'---\ntitle: System in a VIN problem with IMU preintegration\ndate: "2020-11-27"\ntags:\n  - SLAM\n  - -sa/processed\n  - sensors/IMU\n---\n\nSource: [Forster 2017 IMU Preintegration](forster-2017-imu-preintegration.md)\n\n![Image.png](./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/Image.png)\n\nState x\\_i of the system at time i\n![unknown_filename.png](./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.png)\n\nwith\n![unknown_filename.1.png](./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.1.png)\n![unknown_filename.2.png](./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.2.png)\n![unknown_filename.3.png](./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.3.png)\n\n|     |     |\n| --- | --- |\n| ![unknown_filename.4.png](./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.4.png) | All keyframes up till time k |\n| ![unknown_filename.5.png](./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.5.png) | State of all keyframes |\n| ![unknown_filename.6.png](./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.6.png) | camera measurements |\n| ![unknown_filename.7.png](./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.7.png) | IMU measurements between KFs i and j (consecutive) |\n| ![unknown_filename.8.png](./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.8.png) | Set of measurements up till time k |\n\nIMU pose: ![unknown_filename.9.png](./_resources/System_in_a_VIN_problem_with_IMU_preintegration.resources/unknown_filename.9.png), maps a point in B to W\n\n',title:"Untitled Page"},"/studienarbeit/template-for-a-bibliography-entry":{content:'---\ntitle: Template for a bibliography entry\ndate: "2020-08-04"\ntags:\n  - -resources/-bibliography/meta\n  - -sa/processed\n---\n\nSource\n**Backlinks**\n\nAuthors\nAbstract\nContents/Chapters\nTakeaway\n\n',title:"Untitled Page"},"/studienarbeit/template-in-defslam":{content:'---\ntitle: Template in DefSLAM\ndate: "2020-11-20"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\n**Source**: [Lamarca 2019 DefSLAM](lamarca-2019-defslam.md)\n\n![unknown_filename.1.png](./_resources/Template_in_DefSLAM.resources/unknown_filename.1.png)\n\nTemplate\n\n*   2D triangular mesh floating in the 3D space\n*   consists of a set of 2D triangular facets F\n*   a facet has 3 nodes (set V) and 3 edges (set E)\n*   map points observed in keyframe k are embedded in the facets\n\nMap point coordinates in [barycentric coordinates](barycentric-coordinates.md)\n![unknown_filename.2.png](./_resources/Template_in_DefSLAM.resources/unknown_filename.2.png)\n![unknown_filename.png](./_resources/Template_in_DefSLAM.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/template-substitution-in-defslam":{content:'---\ntitle: Template substitution in DefSLAM\ndate: "2020-11-25"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n  - -published\n---\n\n**Parent**: [Mapping step-by-step in DefSLAM](mapping-step-by-step-in-defslam.md)  \n\n**Source**: [lamarca-2020](studienarbeit/lamarca-2020.md)\n\n![Image.png](./_resources/Template_substitution_in_DefSLAM.resources/Image.png)\n\n*   Tracking runs at frame-rate, and mapping at keyframe-rate\n*   Tracking processes Nm frames during a whole mapping run\n\n## Process\n\n1.  New keyframe $k$ is made. Now at time $t=k$\n    *   At this point, the template in the tracking is still based on the old shape-at-rest, S\\_(k-1)\n    *   Mapping thread starts\n        *   creates surface S\\_k which is aligned to prev. template T\\_(k-1)\n        *   k is set as the reference keyframe\n        *   from S\\_k, create template T\\_k and from now on use this template instead of the old one T\\_(k-1)\n2.  At time t=k+Nm, use data from the tracking thread\n    *   image points at t=k+Nm\n    *   deform the recently computed template T\\_k based on these images\n        *   use [SfT but neglecting the temporal term](studienarbeit/tracking-optimisation-in-defslam.md) (to allow large deformation, "as a lot might have happened in the time span of Nm")\n        *   so now we get a T\\_k that is deformed (updated) to the most recent image points\n    *   we do this extra step instead of passing T\\_k (from step 1) to the tracker immediately because, due to the new points occurring at t=k+Nm, using the original T\\_k might lead to data association errors\n    *   mapper passes the new template T\\_k (t=k+Nm) to the tracker\n\n',title:"Untitled Page"},"/studienarbeit/the-making-of-endoslam-dataset":{content:'---\ntitle: The making of EndoSLAM dataset\ndate: "2020-11-25"\ntags:\n  - -resources\n  - -sa/processed\n  - discussion/2020/2020-12\n---\n\n[http://www.youtube.com/watch?v=G\\_LCe0aWWdQ](http://www.youtube.com/watch?v=G_LCe0aWWdQ)\n![unknown_filename.png](./_resources/The_making_of_EndoSLAM_dataset.resources/unknown_filename.png)\n![unknown_filename.1.png](./_resources/The_making_of_EndoSLAM_dataset.resources/unknown_filename.1.png)\n\nGithub: \u003chttp://github.com/CapsuleEndoscope/EndoSLAM\u003e\n\n',title:"Untitled Page"},"/studienarbeit/top-down-mapping-master-to-slave":{content:'---\ntitle: Top-down mapping (master to slave)\ndate: "2020-08-09"\ntags:\n  - software/SOFA/mappings\n  - -sa/processed\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\nParent: [Mappings](mappings.md)\n\nMapping of a master states to the slave states\n![unknown_filename.png](./_resources/Top-down_mapping_(master_to_slave).resources/unknown_filename.png)\n![unknown_filename.1.png](./_resources/Top-down_mapping_(master_to_slave).resources/unknown_filename.1.png) with the Jacobian ![unknown_filename.2.png](./_resources/Top-down_mapping_(master_to_slave).resources/unknown_filename.2.png) (kinematic relation)\n![unknown_filename.3.png](./_resources/Top-down_mapping_(master_to_slave).resources/unknown_filename.3.png)\n\nLinear/nonlinear mappings\n\n*   In linear mappings, J and J are the same\n*   In nonlinear mappings, J is nonlinear w.r.t. x\\_m, i.e. not a matrix\n\nSurfaces\n\n*   Surfaces embedded in deformable cells: J contains [barycentric coordinates](http://www.evernote.com/shard/s484/nl/217355218/047632d6-892f-4007-917a-b5d97be87350)\n*   Surfaces attached to rigid bodies: each row of J encodes ![unknown_filename.4.png](./_resources/Top-down_mapping_(master_to_slave).resources/unknown_filename.4.png) for each vertex\n\n',title:"Untitled Page"},"/studienarbeit/topological-changes-during-elastic-registration":{content:'---\ntitle: Topological changes during elastic registration\ndate: "2020-07-23"\ntags:\n  - -resources/videos\n  - -sa/processed\n  - registration\n---\n\n\u003chttp://www.sofa-framework.org/applications/gallery/augmented-reality-in-nephrology/\u003e\n\n\u003chttp://www.youtube.com/watch?v=3rfdL3-wWE0\u003e\n![unknown_filename.png](./_resources/Topological_changes_during_elastic_registration.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/topological-slam":{content:'---\ntitle: Topological SLAM\ndate: "2020-07-30"\ntags:\n  - SLAM\n  - -sa/processed\n---\n\nParent: [Classification of image-based camera localization approaches](classification-of-image-based-camera-localization-approaches.md)\nSource: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)\n\n*   does not need accurate computation of 3D maps\n*   represents the environment by connectivity or topology\n\ne.g. Kuipers \\[130\\] used a hierarchical description of the spatial environment\n\n*   a topological network description mediates between a control and metrical level\n*   distinctive places and paths are defined by their properties at the control level\n    *   serve as nodes and arcs of the topological model\n\nDecreasing in popularity\n\n*   Topological SLAM has been modified into metric SLAM as [loop detection](http://www.evernote.com/shard/s484/nl/217355218/75ada851-3e4e-88e0-9788-ee8cc5e2f104?title=Loop%20closure%20detection) these years\n*   Studies on pure topological SLAM are reducing.\n\n',title:"Untitled Page"},"/studienarbeit/tracking-grabimagemonocular":{content:'---\ntitle: Tracking_GrabImageMonocular\ndate: "2020-10-21"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\nParent: [defSLAM::System::TrackMonocular](defslam-system-trackmonocular.md)\n\ncv::Mat ORB\\_SLAM2::Tracking::GrabImageMonocular\n\n*   colour conversion\n*   make frame using image, timestamp, ORB stuff, calibration data, etc.\n    mCurrentFrame = new [Frame](http://www.evernote.com/shard/s484/nl/217355218/f943b3a2-8f95-47c1-a716-4f8aedd164e3)(mImGray, timestamp, mpORBextractorLeft,\n    \n\n mpORBVocabulary, mK, mDistCoef, mbf, mThDepth, im)\n\n*   perform tracking: [Track](http://www.evernote.com/shard/s484/nl/217355218/f08bea14-abcd-44e3-898e-d138d730160f)();\n*   return camera pose\n    return mCurrentFrame-\u003emTcw.clone();\n\n',title:"Untitled Page"},"/studienarbeit/tracking-in-viorb":{content:'---\ntitle: Tracking in VIORB\ndate: "2020-10-20"\ntags:\n  - to-do/missing-tag\n  - SLAM/algos/VIORB\n  - -sa/to-be-processed\n---\n\n**Source**: [Mur-Artal 2017 VI-ORB](bibliography/mur-artal-2017-vi-orb.md)\n\nTracking in VIORB\n\n*   Visual-inertial tracking at frame rate, instead of using an ad-hoc motion model as in the original ORB-SLAM\n*   Tracked states: \\[sensor pose (R, p), velocities v, biases b\\]\n*   Once the camera pose is predicted, map points are projected, then matches with existing features on the frame\n*   Then optimise the current frame j, depending on whether\n    *   the map has just been updated\n    *   the map is unchanged\n\nHere, the optimisation function for tracking (when map unchanged) is:\n![unknown_filename.1.png](./_resources/Tracking_in_VIORB.resources/unknown_filename.1.png)\n![unknown_filename.png](./_resources/Tracking_in_VIORB.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/tracking-optimisation-in-defslam":{content:'---\ntitle: Tracking optimisation in DefSLAM\ndate: "2020-11-20"\ntags:\n  - to-do/to-clarify\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\n**Source**: [lamarca-2020](studienarbeit/lamarca-2020.md)\n\n## Optimisation function\n*   Minimises\n    *   reprojection error (in the image)\n    *   deformation energy (of the template)\n    *   boundary nodes of the local zone are fixed (i.e. not set as arguments to the optimisation function)\n        *   this makes the absolute camera pose observable [ ] how?\n        *   in order to constrain the gauge freedoms\n*   Initial guess: values from previous optimisation (i.e. previous frame: t-1)\n\n![unknown_filename.1.png](./_resources/Tracking_optimisation_in_DefSLAM.resources/unknown_filename.1.png)\n\n### Reprojection error\n![unknown_filename.png](./_resources/Tracking_optimisation_in_DefSLAM.resources/unknown_filename.png)\nrobust against outliers due to Huber robust kernel\n\n### Deformation energy\nDeformation energy = stretching energy + bending energy + temporal term\n![unknown_filename.2.png](./_resources/Tracking_optimisation_in_DefSLAM.resources/unknown_filename.2.png)\n\n*   "involves two spatial and one temporal constraint"\n*   requires the derivatives of each term/regulariser\n*   stretching energy: change in edge lengths in the local zone (w.r.t. shape at rest)\n    ![unknown_filename.5.png](./_resources/Tracking_optimisation_in_DefSLAM.resources/unknown_filename.5.png)\n    \n*   bending energy: change in node mean curvatures (w.r.t. shape at rest)\n    *   Node mean curvature: estimated using the discrete Laplacian operator\n        ![unknown_filename.4.png](./_resources/Tracking_optimisation_in_DefSLAM.resources/unknown_filename.4.png)\n        \n*   temporal term/reference regulariser: smoothness between estimates\n    *   to prevent abrupt changes between frames\n    *   to keep the template as close as possible to its initial position\n        ![unknown_filename.3.png](./_resources/Tracking_optimisation_in_DefSLAM.resources/unknown_filename.3.png)\n\n',title:"Untitled Page"},"/studienarbeit/transforming-velocities-to-another-frame":{content:'---\ntitle: Transforming velocities to another frame\ndate: "2021-05-03"\ntags:\n  - -sa/processed\n  - math/kinematics\n---\n\n\u003chttp://physics.stackexchange.com/questions/197009/transform-velocities-from-one-frame-to-an-other-within-a-rigid-body\u003e\n\nTransforming velocities to another frame\n![unknown_filename.png](./_resources/Transforming_velocities_to_another_frame.resources/unknown_filename.png)\n\nFurther reading: \u003chttp://core.ac.uk/download/pdf/154240607.pdf\u003e\n\n',title:"Untitled Page"},"/studienarbeit/trocar":{content:'---\ntitle: Trocar\ndate: "2021-05-13"\ntags:\n  - medical/surgery\n  - -definitions\n  - -sa/processed\n---\n\nSource: \u003chttps://en.wikipedia.org/wiki/Trocar\u003e\n\nHere: surgical trocar\n\n*   Used in [laparoscopic surgery](laparoscopic surgery.md)\n*   A medical device which is used to make small incisions and allows insertion of other surgical instruments into the body cavity\n\n![unknown_filename.png](./_resources/Trocar.resources/unknown_filename.png)![unknown_filename.1.png](./_resources/Trocar.resources/unknown_filename.1.png)\n\nSource: [Leiner Digital Endoscope Design](Leiner Digital Endoscope Design.md)\nIn an arthroscopic procedure (knee?):\n\n*   trocar is inserted into the cannula\n*   both are pushed through the skin\n*   trocar is replaced by obturator (blunt rod) to open the area up. The trocar isn\'t used anymore because it is sharp; could cause more damage to the area.\n*   cannula is placed in position\n*   obturator is removed, arthroscope is inserted\n\n',title:"Untitled Page"},"/studienarbeit/univariate-gaussian-distribution":{content:'---\ntitle: Univariate Gaussian distribution\ndate: "2020-08-31"\ntags:\n  - -sa/processed\n  - math/statistics\n---\n\nParent: [Gaussian distribution](gaussian-distribution.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\n![unknown_filename.png](./_resources/Univariate_Gaussian_distribution.resources/unknown_filename.png)\n\n*   If normalised (area under the graph is 1): Gaussian distribution\n*   If not normalised: Gaussian function\n\n![unknown_filename.2.png](./_resources/Univariate_Gaussian_distribution.resources/unknown_filename.2.png)\n\nNotation: ![unknown_filename.1.png](./_resources/Univariate_Gaussian_distribution.resources/unknown_filename.1.png)\nThe random variable X has a Gaussian distribution with mean ... and variance ... .\n\n',title:"Untitled Page"},"/studienarbeit/unscented-kalman-filter":{content:'---\ntitle: Unscented Kalman Filter\ndate: "2020-08-23"\ntags:\n  - -sa/processed\n  - filters/kalman-filter\n---\n\n**Parent**: [General Kalman Filter](general-kalman-filter.md)\n\nSource: [Scaradozzi 2018 SLAM application in surgery](studienarbeit/scaradozzi-2018.md)\n\n*   developed to overcome main problems of the [EKF](http://www.evernote.com/shard/s484/nl/217355218/a3417515-123a-4310-ac2f-937cd4878942)\n*   like EKF, approximates the state distribution with a Gaussian Random Variable\n    *   only the representation is different—using alpha points (a minimal set of sample points)\n    *   capture posterior mean and covariance accurately for any nonlinearity, up to3rd order Taylor\n\n',title:"Untitled Page"},"/studienarbeit/update-2021-06-11":{content:'---\ntitle: Update 2021-06-11\ndate: "2021-06-11"\nexternal_url: "http://www.youtube.com/watch?v=FixJhTX3Fy0\u0026list=PLTQ3A1AyP3Twcr2dcB5I90Rv_n8HlMM7k\u0026index=7"\ntags:\n  - to-do\n  - -sa/processed\n  - discussion/2021/2021-06\n---\n\nGeneral model for probe (from [Forward kinematics IMU to camera](forward-kinematics-imu-to-camera.md))\n\n*   Using the standard DH convention\n\n            ![unknown_filename.png](./_resources/Update_2021-06-11.resources/unknown_filename.png)![unknown_filename.3.png](./_resources/Update_2021-06-11.resources/unknown_filename.3.png)\n            Note: I have since changed the axis configuration at the camera part—above diagram is no longer up to date; to be updated!\n\n*   This has the IMU (B) as the base and the camera (C) as the end effector\n*   Using robotics-toolbox-python: \u003chttp://github.com/petercorke/robotics-toolbox-python\u003e\n\nSimplified model (from [Forward kinematics IMU to camera](forward-kinematics-imu-to-camera.md))\nCurrently using a simplified model with all degrees of freedom set to 0 or constant\n![unknown_filename.1.png](./_resources/Update_2021-06-11.resources/unknown_filename.1.png)![unknown_filename.2.png](./_resources/Update_2021-06-11.resources/unknown_filename.2.png)\n\nModification to (above) existing robot model \nProbably need to modify the rotational joints around the pivot\ns. [Rigid cystoscope mechanism](rigid-cystoscope-mechanism.md)\n\nCode on GitHub\n[http://github.com/feudalism/dvi-ekf/tree/fake\\_imu](http://github.com/feudalism/dvi-ekf/tree/fake_imu)\n\n*   simple\\_robot.py — main implementation\n*   tests.py — unit tests for testing individual functionalities in the Probe, Imu, etc. classes\n\nEquations for obtaining ang\\_vel, acc.\ns. [Equations for obtaining omega (angular velocity) and acceleration of IMU from camera](equations for obtaining omega (angular velocity) and-acceleration-of-imu-from-camera.md)\n\nContainer class for IMU data\n\n*   Imu class\n    *   Imu.omega : OmegaB\n    *   Imu.acc\n*   OmegaB\n    *   \\_\\_init\\_\\_(R\\_WB\\_expr, om\\_BC\\_expr)\n    *   evaluate(cam\\_values)\n        *   To evaluate the equation om\\_B = om\\_C + om\\_CB\n        *   To be extended to take robot DOF into account:\n            evaluate(cam\\_values, robot DOF)\n            \n\nSonstiges\n\n*   Videos (whole playlist: [http://www.youtube.com/playlist?list=PLTQ3A1AyP3Twcr2dcB5I90Rv\\_n8HlMM7k](http://www.youtube.com/playlist?list=PLTQ3A1AyP3Twcr2dcB5I90Rv_n8HlMM7k))\n    For seeing how the camera+tube rotation works\n    [Retrograde En block TURBT](http://youtu.be/FixJhTX3Fy0?t=91) Timestamp 1:32 (I think the vid of the surgeon\'s hands lags behind the cystoscopy video)\n    [5 Cystoscopy How to advance the cystoscope through urethra](http://www.youtube.com/watch?v=gvgdcKJe_Tw) Timestamp 2:15 (cam + tube rotate together)\n    [6 Cystoscopy What scheme to follow to examine bladder](http://youtu.be/Bqllv_9ewCY) Timestamps 3:36, 4:04 (independent rotation)\n    \n*   Evernote\n\n* * *\n\nAfter meeting notes\n[Notch positions due to scope rotation](notch-positions-due-to-scope-rotation.md)\n\nTo do:\n- [ ] come up with equations for constraining SLAM virtual CS to real camera — to be used in KF later on, not for forward kinematics\n\n  the SLAM rotation corresponds to the rotation of the real camera (same angular rate)\n\nNote: relative rotation of sheath to camera is represented by the notch angular displacement — this becomes an additional measurement\n\n',title:"Untitled Page"},"/studienarbeit/update-2021-07-19":{content:'---\ntitle: Update 2021-07-19\ndate: "2021-06-20"\ntags:\n  - -sa/processed\n  - discussion/2021/2021-07\n---\n\nAgenda\n\n*   Comparison prop only, prop + update\n*   Comparison P+U plots (Rp 1000, Rp .01, Rp 1e-6)\n*   Changes to equations (pc, qc, err\\_pc, err\\_theta\\_c) s. [KF kinematics](kf-kinematics.md)\n*   Currently: getting probe output as function of DOFs\n\nOpen tasks\ns. also [dvi-eskf project board](http://github.com/feudalism/dvi-ekf/projects/3)\n- [ ] debug update stage???\n- [x] get probe outputs as symbols/functions of DOFs\n- [x] switch from rigid probe to rotating scope\n    - [x] at which point do I compensate for notch rotation? (~~in Probe class, o~~r in Filter)\n- [ ] visualisations/plots/animations for case with rotating scope\n\nThesis\n\n*   Modifications to working title?\n*   [Thesis restructure](private/thesis-sa/thesis.md)\n\n',title:"Untitled Page"},"/studienarbeit/using-python-with-existing-scene":{content:'---\ntitle: Using python with existing scene\ndate: "2020-07-15"\ntags:\n  - software/SOFA\n  - software/python\n  - -sa/processed\n---\n\nParent: [SofaPython Index](sofapython-index.md)\n\nIn scene graph\n\n1.  Add plugin in the scene using RequiredPlugin\n2.  Define a PythonScriptController in the scene graph\n\n',title:"Untitled Page"},"/studienarbeit/variance,-standard-deviation,-covariances":{content:'---\ntitle: Variance, standard deviation, covariances\ndate: "2020-08-31"\nexternal_url: "http://nbviewer.jupyter.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/03-Gaussians.ipynb"\ntags:\n  - -definitions\n  - -sa/processed\n  - math/statistics\n---\n\nBacklinks: [Multivariate Gaussian distributions](multivariate-gaussian-distributions.md)\n\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\nSee also: [Empirical rule 68/95/99.7](empirical-rule-68_95_99.7.md)\n\n![unknown_filename.1.png](./_resources/Variance,_standard_deviation,_covariances.resources/unknown_filename.1.png)\n![unknown_filename.png](./_resources/Variance,_standard_deviation,_covariances.resources/unknown_filename.png)\n\nHow much do the values vary from the mean?\n\n![unknown_filename.2.png](./_resources/Variance,_standard_deviation,_covariances.resources/unknown_filename.2.png)\n\n*   There are other ways of calculating variance (e.g. by using absolute values of error instead of error squared).\n*   The other methods may be better w.r.t. outliers (outliers get magnified in the square term)\n\nProcess variance: error in the process model\nSensor variance: error in each sensor measurement\n\nIn multivariate Gaussian distributions, we also calculate covariances:\n![unknown_filename.3.png](./_resources/Variance,_standard_deviation,_covariances.resources/unknown_filename.3.png)\n\nand represent them in a covariance matrix (symmetric)\n![unknown_filename.4.png](./_resources/Variance,_standard_deviation,_covariances.resources/unknown_filename.4.png)\n\nCovariance calculation in numpy: uses biased estimator by default for small sample sizes (division by (N-1) instead of by N)\n\n',title:"Untitled Page"},"/studienarbeit/variance-of-the-1d-kalman-filter":{content:'---\ntitle: Variance of the 1D Kalman filter\ndate: "2020-08-31"\ntags:\n  - -sa/processed\n  - filters/kalman-filter\n---\n\nParent: [1D Kalman filters](1d-kalman-filters.md)\nSource: [rlabbe Kalman/Bayesian filters in Python](rlabbe-kalman_bayesian-filters-in-python.md)\n\ni.e., what variance is show by the estimated output/posterior?)\n\n*   Always converges to a fixed value if the sensor and process variances are constant\n*   We can run simulations to determine the value to which the filter variance converges\n*   Then hard code this value into the filter (+ with first sensor measurement as initial value, the filter should have good performance)\n*   Alternative: instead of using the variance value, use the calculated Kalman gain\n\nExample implementation using the Kalman gain\nHowever, using the Kalman gain obscures the Bayesian approach\n![unknown_filename.png](./_resources/Variance_of_the_1D_Kalman_filter.resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/vecid":{content:'---\ntitle: VecId\ndate: "2020-08-22"\ntags:\n  - software/SOFA\n  - -sa/processed\n---\n\nSource: [SOFA extended documentation](SOFA extended documentation.md)\nParent: [ODE solvers](ODE solvers.md)\n\n*   Uniquely identifies state vectors (which are scattered over all [MechanicalStates](MechanicalStates.md))\n*   Mechanical operations (e.g. allocating a state vector, accumulating forces) are implemented using a specialised [visitor](visitor.md) parametrised on VecIds\n\n',title:"Untitled Page"},"/studienarbeit/vi.cc-using-kalman-for-xyz-states":{content:'---\ntitle: vi.cc using kalman for xyz states (what goes on with the map?)\ndate: "2021-03-15"\ntags:\n  - -sa/processed\n  - discussion/2021/2021-03\n---\n\nOffline Kalman\n![unknown_filename.1.png](./_resources/vi.cc_using_kalman_for_xyz_states_(what_goes_on_with_the_map_).resources/unknown_filename.1.png)\n\nKalman and live DefSLAM\n![unknown_filename.png](./_resources/vi.cc_using_kalman_for_xyz_states_(what_goes_on_with_the_map_).resources/unknown_filename.png)\n\n',title:"Untitled Page"},"/studienarbeit/viewer-segfault":{content:'---\ntitle: Viewer segfault\ndate: "2021-02-02"\ntags:\n  - -sa/processed\n  - SLAM/algos/DefSLAM\n---\n\n![unknown_filename.png](./_resources/Viewer_segfault.resources/unknown_filename.png)\n\n**Error**\n\\[New Thread 0x7fff86ffd700 (LWP 1117)\\]\nNORMALS REESTIMATED : 277 - 277\n\\[Thread 0x7fff86ffd700 (LWP 1117) exited\\]\nNORMAL ESTIMATOR OUTPoints potential : 293  70\nNew template requested\nNumber Of normals 277 0x555566923da0\n\\-0.79956  0.655022 -0.594482POINTS matched:167\nPoints\nScale Error Keyframe : 1\nstan dev 0.310974\nchi 0.013115  0.01  201\nSurfaceRegistration not sucessful (Not enough points to align or chi2 too big\n\nThread 6 "DefSLAM" received signal SIGSEGV, Segmentation fault.\n\\[Switching to Thread 0x7fffc1996700 (LWP 275)\\]\n\\_\\_memmove\\_avx\\_unaligned\\_erms () at ../sysdeps/x86\\_64/multiarch/memmove-vec-unaligned-erms.S:379\n379     ../sysdeps/x86\\_64/multiarch/memmove-vec-unaligned-erms.S: No such file or directory.\n\n**Backtrace**\n(gdb) bt\n#0  0x00007ffff4f7eb80 in \\_\\_memmove\\_avx\\_unaligned\\_erms ()\n    at ../sysdeps/x86\\_64/multiarch/memmove-vec-unaligned-erms.S:379\n#1  0x00007fffa6e31853 in  () at /usr/lib/x86\\_64-linux-gnu/dri/swrast\\_dri.so\n#2  0x00007fffa7267cc1 in  () at /usr/lib/x86\\_64-linux-gnu/dri/swrast\\_dri.so\n#3  0x00007fffa68d1012 in  () at /usr/lib/x86\\_64-linux-gnu/dri/swrast\\_dri.so\n#4  0x00007fffa68d2bfa in  () at /usr/lib/x86\\_64-linux-gnu/dri/swrast\\_dri.so\n#5  0x00007fffa6a4dfc7 in  () at /usr/lib/x86\\_64-linux-gnu/dri/swrast\\_dri.so\n#6  0x00007fffa6a4fc50 in  () at /usr/lib/x86\\_64-linux-gnu/dri/swrast\\_dri.so\n#7  0x00007ffff78f6ca7 in defSLAM::MeshDrawer::drawMesh(double, bool) (this=0x5555661fb0f0, alpha=alpha@entry=1, drawedges=drawedges@entry=true) at /home/user3/slam/DefSLAM/Modules/Viewer/MeshDrawer.cc:112\n#8  0x00007ffff78e6141 in defSLAM::DefMapDrawer::DrawTemplate() (this=0x5555557f8e40)\n    at /home/user3/slam/DefSLAM/Modules/Viewer/DefMapDrawer.cc:250\n#9  0x00007ffff78ee739 in defSLAM::DefViewer::Run() (this=0x5555557f70c0)\n    at /home/user3/slam/DefSLAM/Modules/Viewer/DefViewer.cc:169\n\nPossible solution\n\u003chttp://stackoverflow.com/questions/43805701/can-someone-interpret-this-for-me\u003e\n\n',title:"Untitled Page"},"/studienarbeit/visitors":{content:'---\ntitle: Visitors\ndate: "2020-07-15"\ntags:\n  - software/SOFA/data-types\n  - -sa/processed\n---\n\nSource: [SOFA extended documentation](SOFA extended documentation.md)\nParent: [Data structure in SOFA](Data structure in SOFA.md)\nBacklinks: [Scene graph in SOFA](Scene graph in SOFA.md), [Simulation algorithms in SOFA](Simulation algorithms in SOFA.md)\n\n*   For processing of data structure: parent to child\n*   Allows decoupling of physical model from simulation algo\n    e.g. Easy to replace a time integrator, which wouldn\'t be the case in a dataflow graph (coupling of data and algo)\n    \n*   Nodes with multiple parents are traversed only once (pruning of top-down traversals)\n\n',title:"Untitled Page"},"/studienarbeit/visual-inertial-datasets":{content:'---\ntitle: Visual-inertial datasets\ndate: "2020-11-06"\ntags:\n  - -resources\n  - -sa/processed\n---\n\n\u003chttp://sites.google.com/view/awesome-slam-datasets/home\u003e\n\n|     |     |     |\n| --- | --- | --- |\n| \u003chttp://fpv.ifi.uzh.ch/\u003e | Aggressive drone racing |     |\n| \u003chttp://www.lirmm.fr/aqualoc/\u003e | Underwater | Monochromatic |\n| \u003chttp://vision.in.tum.de/data/datasets/visual-inertial-dataset\u003e | TUM indoor/urban, slides | fisheye camera\u003cbr\u003eUsed in ORBSLAM3 |\n| \u003chttp://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets\u003e | EUROC MAV | stereo, monochr\u003cbr\u003eUsed in ORBSLAM3 |\n\n',title:"Untitled Page"},"/studienarbeit/visual-model":{content:'---\ntitle: Visual model\ndate: "2020-08-09"\ntags:\n  - software/SOFA/models\n  - -sa/processed\n---\n\nSource: [SOFA extended documentation](sofa-extended-documentation.md)\nParent: [Models in SOFA](models-in-sofa.md)\n\n*   More detailed geometry than that of the [internal model](internal-model.md), hence uses different meshes\n*   [Mappings](mappings.md) are used to update the visual model with the deformations taking place\n*   Contains rendering parameters\n\n**Libraries for rendering graphics**\n\n*   OGRE (external)\n*   Open Scene Graph (external)\n*   SOFA\'s own library based on openGL\n\n',title:"Untitled Page"},"/studienarbeit/weiss-thesis-vision-based-navigation-for-micro-helicopters":{content:'---\ntitle: (Weiss Thesis) Vision based navigation for micro helicopters\ndate: "2021-04-25"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-skimmed\n  - -sa/to-be-processed\n---\n\nSource\nBacklinks\n\nAuthors Stephan Weiss\nAbstract\n\n*   Issues that arise during state estimation and sensor self-calibration\n*   Application area: large and unknown areas, micro helicopter\n*   Vision based method used uses SfM, is compares mapless and map-based methods\n*   Statistical and modular sensor fusion strategy\n    *   recovery of pose and drifts\n    *   modular: camera as a black box sensor, allows other sensors additionally\n*   Observability analysis\n\nLiterature review\n\n*   Fusing IMU with monocular vision: given [extrinsic parameters](extrinsic parameters.md), an IMU is able to recover metric scale, as well as help transition across short vision failure period\n*   Armesto et al. uses loose coupling: compares EKF with UKF, says UKF is more accurate but EKF is computationally cheaper/faster.\n*   (Mirzaei and Roumeliotis (2008)) and (Kelly and Sukhatme (2011)) KF-based calibration\n\nContents/Chapters\nTakeaway\n\n',title:"Untitled Page"},"/studienarbeit/whampsey-mekf":{content:'---\ntitle: Whampsey MEKF\ndate: "2021-08-14"\nexternal_url: "http://matthewhampsey.github.io/blog/2020/07/18/mekf"\ntags:\n  - -resources/-bibliography\n  - -resources/-bibliography/bib-read\n  - -sa/processed\n  - filters/kalman-filter\n  - math/quaternions\n  - discussion/2021/2021-08\n  - filters/MEKF\n  - -published\n---\n\n**Source**: \u003chttp://matthewhampsey.github.io/blog/2020/07/18/mekf\u003e\n\n## Motivation\n\n*   Working with noisy IMU measurements\n*   IMUs usually provide redundant information that can be used to improve [dead-reckoning](definitions/odometry.md)\n\nUses: Hamilton [quaternion convention](studienarbeit/quaternion-conventions.md).\n\n* [Which orientation parametrisation to choose?](rotations/20.4-which-orientation-parametrisation.md)\n* [50.5-error-state-kalman-filter](studienarbeit/50.5-error-state-kalman-filter.md)\n\n',title:"Untitled Page"},"/studienarbeit/wikipedia-lokalisierung":{content:'---\ntitle: Wikipedia Lokalisierung\ndate: "2020-08-23"\ntags:\n  - -resources/-bibliography\n  - localisation\n  - -resources/-bibliography/bib-read\n  - -sa/processed\n---\n\nSource: [http://de.wikipedia.org/wiki/Lokalisierung\\_(Robotik](http://de.wikipedia.org/wiki/Lokalisierung_(Robotik))\n\n* [Localisation](localisation.md)\n* [Particle filters](particle-filters.md)\n',title:"Untitled Page"},"/studienarbeit/wikipedia-slam":{content:"---\ntitle: Wikipedia SLAM\ndate: \"2020-07-27\"\ntags:\n  - -resources/-bibliography\n  - SLAM\n  - -resources/-bibliography/bib-to-read\n  - -sa/processed\n---\n\nSource: [http://en.wikipedia.org/wiki/Simultaneous\\_localization\\_and\\_mapping](http://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping)\nParents: [SLAM Index](SLAM/slam_index.md), [slam-resources](slam-resources.md)\n\nDifferent types of sensors give rise to different SLAM algorithms whose assumptions are most appropriate to the sensors.\n\n*   At one extreme, visual features provide details of many points within an area --\u003e rendering SLAM unnecessary\n    *   shapes in these point clouds can be easily and unambiguously aligned at each step via [image registration](http://en.wikipedia.org/wiki/Image_registration).\n*   At the opposite extreme, [tactile sensors](http://en.wikipedia.org/wiki/Tactile_sensor) are extremely sparse\n    *   they contain only information about points very close to the agent\n    *   require strong prior models to compensate in purely tactile SLAM.\n*   Most practical SLAM tasks fall somewhere between these visual and tactile extremes.\n\nAlgorithms\n\n*   Statistical techniques\n    *   Kalman\n    *   Particle/Monte Carlo\n*   Set-membership techniques\n    *   Bundle adjustment\n    *   Maximum a posteriori estimation (MAP) -- SLAM for image data\n\nGiven:\n\n*   Controls u\n*   Sensor measurements o\n*   Time steps t\n\nTo estimate:\n\n*   Agent's state x\n*   Map of environment m\n\nAll quantities are probabilistic.\n\nObjective is to compute:\n![c607fe964e04d2d7c46f8420596205eb67737000](http://wikimedia.org/api/rest_v1/media/math/render/svg/c607fe964e04d2d7c46f8420596205eb67737000)\n\nUse Bayes' rule, EM algorithm\n\nSensor models\n\n*   landmark based\n*   raw data based\n    *   make no assumption that landmarks can be identified\n\n",title:"Untitled Page"},"/studienarbeit/wikipedia-visual-odometry":{content:'---\ntitle: Wikipedia Visual odometry\ndate: "2020-07-27"\ntags:\n  - -resources/-bibliography\n  - localisation\n  - -resources/-bibliography/bib-read\n  - -sa/processed\n  - localisation/odometry\n---\n\nSource: [http://en.wikipedia.org/wiki/Visual\\_odometry](http://en.wikipedia.org/wiki/Visual_odometry)\n\n[Odometry](definitions/odometry.md)\n[Visual sensors for localisation](sensors/visual-sensors-for-localisation.md)\n\n',title:"Untitled Page"},"/studienarbeit/woernle-mehrkoerpersysteme":{content:'---\ntitle: (Woernle) Mehrkoerpersysteme\ndate: "2021-05-24"\ntags:\n  - -resources/-bibliography\n  - -sa/processed\n  - math/kinematics\n  - -resources/-bibliography/bib-skimmed\n---\n\nAuthor: Christoph Woernle\nContents:\n\n*   Kinematics, kinetics (Dynamik)\n    *   Some basics\n        *   [Converting velocity from CS1 to CS0](converting-velocity-from-cs1-to-cs0.md)\n        *   [Chaining rotation matrices and angular velocities](chaining-rotation-matrices-and-angular-velocities.md)\n        *   [Poisson equation for skew symmetric matrix of angular velocity](poisson equation for skew-symmetric-matrix-of-angular-velocity.md)\n        *   [Differentiation in different coordinate systems](differentiation-in-different-coordinate-systems.md)\n    *   [Kinematics primer](kinematics-primer.md)\n    *   [Reversed kinematics relations](reversed-kinematics-relations.md)\n    *   [Rotations as xyz Bryan-Tait angles (Kardanwinkel)](rotations-as-xyz-bryan-tait-angles-(kardanwinkel).md)\n*   Holonomic systems, non-holonomic systems\n*   Holonomic constraints\n\n',title:"Untitled Page"},"/studienarbeit/world-to-camera-trafo":{content:"---\ntitle: World to camera trafo\ndate: \"2021-03-03\"\ntags:\n  - sensors/cameras\n  - -sa/processed\n---\n\nParent: [SLAM Index](SLAM/slam_index.md)\nSee also: [Pinhole camera model](pinhole camera model.md), [pinhole camera-projection-function](pinhole-camera-projection-function.md)\nSource: \u003chttp://www.cse.psu.edu/~rtc12/CSE486/lecture12.pdf\u003e\n\n|     |     |     |\n| --- | --- | --- |\n| Camera coordinates (X, Y, Z) | World coordinates (U, V, W) | Image plane (x, y) / Pixel coordinates (u, v) |\n| ![unknown_filename.1.png](./_resources/World_to_camera_trafo.resources/unknown_filename.1.png) | ![unknown_filename.png](./_resources/World_to_camera_trafo.resources/unknown_filename.png) | ![unknown_filename.2.png](./_resources/World_to_camera_trafo.resources/unknown_filename.2.png)\u003cbr\u003e![unknown_filename.3.png](./_resources/World_to_camera_trafo.resources/unknown_filename.3.png) |\n\nForward projection\n\n![unknown_filename.4.png](./_resources/World_to_camera_trafo.resources/unknown_filename.4.png)\n\nRepresenting 2D point as a fictitious 3D point (x', y', z') \\[for matrix calculations\\]\nConvention: Given (x', y', z'), we can recover the 2D point (x, y) as\n![unknown_filename.5.png](./_resources/World_to_camera_trafo.resources/unknown_filename.5.png)\n\nWorld to camera trafo\n\n*   Translation of -C as seen in W-CS\n*   Apply rotation Rcw to get P in camera coordinates\n    pc = Rcw (pw - cw)\n    \n\n![unknown_filename.6.png](./_resources/World_to_camera_trafo.resources/unknown_filename.6.png)\n![unknown_filename.7.png](./_resources/World_to_camera_trafo.resources/unknown_filename.7.png)\n\n",title:"Untitled Page"},"/templates/post":{content:"---\ntitle: {{title}}\ndate: {{date}}\ntags:\n  - \n---\n",title:"Untitled Page"},"/unlisted/minutes/2021-10-15":{content:"---\ntitle: 2021-10-15\ndate: 2021-10-28\ntags:\n  - -ma\n---\n\n# 2021-10-15 Vorstellung\nOverall localisation project\n* Part that deals with localisation and mapping (graph stuff)\n* Part that deals with deformation -- using Pytorch 3D\n\n## Pytorch 3D\n* 3D ground truth models are expensive\n* Idea: generate 3D models (mesh) from 2D data (graph as a texture)\n* \n## Localisation using graphs\n* Deformation problem: end nodes can appear to change position based on light conditions, bifurcation nodes are not affected\n* Use neural network to extract adjacency matrix in real time\n* **One problem**: Extraction of adjacency matrix is not robust to changes in matrix dimension, therefore do zero padding (as a partial solution to the problem)\n\n### Idea\n* First NN to extract node positions, node attributes such as\n	* polynomial degree, coefficients, etc\n	* number of connections\n* Second NN to extract adjacency matrix between two nodes only (prerequisite: node positions already known, number of connections already known)\n* Threading -- a thread can be terminated once the number of adjacencies has been found corresponding to the number of connections",title:"Untitled Page"},"/unlisted/minutes/2021-10-26":{content:"---\ntitle: 2021-10-26\ndate: 2021-10-28\ntags:\n  - -ma\n---\n\n# 2021-10-26 Kick-off meeting\n## Tasks\n* [x] Read Regine's documentation, SA thesis\n* [x] Generate training data with ground truth (labels) using Regine's code\n	* [x] Trim videos to show only relevant sections (where blood vessels are visible) --- (differentiating the situations to be implemented in other work) --- creating function in Python to trim\n	* [x] Framework that runs all the functions in one go\n\n## Notes\n* Meetings Tuesday 10 a.m.\n* Previous work by Regine: not real time --\u003e achieve real time capability by using Unet\n* Existing code does:\n	* Filter colour photo to black and white\n	* Skeletonises the photo\n	* Extracts node position from the photo\n	* Extracts node attributes\n* NN input: 256x256x3 photo\n* NN output: 256x256xn, with n channels\n	* Channel 1: filtered photo\n	* Channel 2: skeletonised photo\n	* Channel 3: node position\n	* Channel 4: polynomial degree\n* Maybe skip\n	* Channel 1 (filtered photo) and go to output 2 (skeletonished photo) directly\n	* Channel 3 (node pos) to go to polynomial degree directly, because once a nonzero number is in the matrix, it indicates that a node exists at that position!",title:"Untitled Page"},"/unlisted/minutes/2021-11-02":{content:"---\ntitle: 2021-11-02\ndate: 2021-11-02\ntags:\n  - -ma\n---\n# Agenda\n\n## To clarify\n* [x] Training data\n* [x] Zugang zum Rechner Tuebingen\n* [x] Hiwi\n\n----\n\n## Training data\n* Did a parameter study for B-COSFIRE filter parameters\n* Automated some functions: naming convention, trimming according to video_data.py\n* Some training data GRK021, GRK008\n* Goal: ca. 3,000 photos, apply transformations to these to get more training data ~10,000\n\n----\n\n## Questions\n* Which videos are already 'done', which are more important, in what order to process? -- doesn't matter which, as long as I know which videos I took them from\n* Do the images have to be squares? -- No (corrected: s. [2021-11-03](unlisted/minutes/2021-11-03.md))\n* Moving circle -- s. GRK008 video (corrected: s. [2021-11-03](unlisted/minutes/2021-11-03.md))\n	* Use a small mask \u0026 larger crop area? -- Informationsverlust eher besser als unnuetzliche Daten\n	* ~~Set custom crop and mask parameters for each video section?~~\n\n![](_img/graph-excentre.png)\n\n---\n\n## To-do\n* [ ] Generate up to 3k training photos\n* [x] Install VNC viewer, try to connect to Tuebingen\n* [x] Read up on tensorflow - workflow how to train CNN -\u003e bis morgen (Besprechung 3.11.2021 um 10 Uhr)\n* [ ] [Hiwi] combine Peter's existing PyQT GUI with PyTorch 3D to enable changing the camera view using mouse -- read Jupyter notebook tutorial, evtl. Visual Code dazu installieren\n\n\n---\n\n## Hiwi\n1000-1920",title:"Untitled Page"},"/unlisted/minutes/2021-11-03":{content:"---\ntitle: 2021-11-03\ndate: 2021-11-03\ntags:\n  - -ma\n---\n## To-do\n### Old\n* [ ] Generate up to 3k training photos\n* [ ] [Hiwi] combine Peter's existing PyQT GUI with PyTorch 3D to enable changing the camera view using mouse\n	* [ ] read Jupyter notebook tutorial\n	* [ ] go through PyQT stuff\n\n### New\n* [x] Resize images to be smaller, squared, a square of two, e.g. 512 x 512 px\n	* To avoid unnecessary padding in the model\n	* Avoids the need for resizing within the model\n	* Note: if do end up using `tf.image.resize()`, then make sure to do graph extraction (apply `numpy` to get node positions matrix)\n* [x] Crop images according to centre of endoscope image (doch nicht so wie gestern [2021-11-02](unlisted/minutes/2021-11-02.md))\n* [x] Activate Tensorboard (to see if network is converging while training)\n	* [x] Tensorboard tutorial\n	* [x] --\u003e not working, ask Johannes --\u003e install TensorBoard locally\n* [x] Look through the stuff Johannes sent me\n	* [x] Folder `03_CNN` in my student folder\n	* [x] https://github.com/zhixuhao/unet (simple implementation)\n	* [x] https://github.com/karolzak/keras-unet\n* [x] In model: adjust the convolution layer dimensions so that it fits the image input size and image output size + channels\n* [ ] For later: adjust filter parameters (so that structures are less thick and that it finds some more of the finer structures) and up the Otsu thresholding\n* [ ] Extract adjacency matrix and save to be used as labels later\n\n## Notes\n* `DataGenerator` for generating batches of images, data augmentation\n* `Conv2D` 3x3 is ok, stays as is\n\n\n## Waiting on\n* [x] Model from Johannes\n* [x] [Hiwi] PyTorch3d on Tuebingen -- ready on 04.11.2021",title:"Untitled Page"},"/unlisted/minutes/2021-11-09":{content:"---\ntitle: 2021-11-09\ndate: 2021-11-09\ntags:\n  - -ma\n---\n# Agenda\n	  \n## To-dos\n### Ongoing\n* [ ] **Training data generation (up to 3k), including adjacency matrix**\n* [x] **Figure out model checkpoints** -- still don't understand\n* [ ] Adjust model -- add more output channels\n* [ ] Start HiWi (Pytorch 3D + PyQT GUI) by the end of the week\n\n\n### New\n* [x] Model summary -- ca. 30M parameters \n* [ ] **Make new training data for 256px images (two folders: 256px, 512px)**\n* [ ] Implement data augmentation\n	* Rotations, translation, flip ok\n	* Schwierig oder nicht gut machbar: blurring, stretching, zoom\n\n\n## U-Net\n* `DataGenerator`, so far without augmentation\n* Based on the class `UNet` in `03_CNN` -- modifications:\n	* Branched outputs (to apply different losses/activation functions)\n		* Filtered output: linear activation, MSE loss\n		* Skeletonised output: sigmoid activation, binary crossentropy loss\n	* ~~Alternative: final conv2D layer with 4 filters?~~\n* Current results using small dataset (100 photos) and low number of epochs (5 epochs)\n	* Training set sample  \n	  ![](_img/21-11-09-training-set.png)\n	* Validation output (Input; Filter GT + Filter predicted; Skel GT + Skel predicted)\n	  ![](_img/21-11-09-first-validation.png)\n	  \n	  \n## To clarify\n* [x] SA\n* [ ] VGGNet\n\n\n## Notes\n* Workflow established using minimal example with two image outputs.\n* Next step: data augmentation, some pre-processing of data.\n* After that: add the remaining two channels.\n* After getting model to work with 256px, proceed to 'edge detection' network (finding the adjacency matrix between two nodes), possibly while training the model to work with 512px in the background.",title:"Untitled Page"},"/unlisted/minutes/2021-11-16":{content:"---\ntitle: 2021-11-16\ndate: 2021-11-13\ntags:\n  - -ma\n---\n# Agenda\n\n\n## Notes\n* Scrapped translations as a data augmentation option\n* Implemented function to sort nodes (left to right, up to down) before graph creation, thus ensuring that the adjacency matrix uses sorted node positions\n* Testing suite in TestGraph (check if node positions are sorted, check if adj matrix matches photo)\n* Testing suite TestDataGeneration\n\n## To do\n* [ ] Preliminaries\n	* [x] Revert to old filter parameters\n	* [x] Check adjacency matrix generation\n		* [x] Make sure node positions are sorted\n		* [x] Make sure the matrix fits the skeletonised picture -- added test\n	* [ ] Implement data augmentation for all input and output layers\n		* Rotations, flip ok\n		* No translations: we want to images to be centred\n		* Schwierig oder nicht gut machbar: blurring, stretching, zoom\n	* [ ] Function to calculate node neighbours\n	* [ ] Function to classify node types (crossing, end, invalid)\n	* [ ] Extract sections GRK008 (low prio)\n* [ ] Training data generation (up to 3k w/o augmentation)\n* [ ] Adjust model -- add more output channels (5: filtered, skeletonised, node positions, neighbours, node types)\n* [ ] HiWi (Pytorch 3D + PyQT GUI)\n* [x] Model summary -- ca. 30M parameters \n* [x] Make new training data for 256px images (two folders: 256px, 512px)\n* [ ] Literature review\n* [ ] ~~Figure out model checkpoints~~ still don't get it\n\n\n## Questions\n* Einlesen von Knotenpositionen als Bilddatei?  \n	![](Pasted%20image%2020211110185607.png)\n* ~~ID of nodes (how are they generated? random?)~~",title:"Untitled Page"},"/unlisted/minutes/ma-minutes":{content:"---\ntitle: ma-minutes\ndate: 2021-10-28\ntags:\n  - -ma\n---\n\n* [2021-10-15](unlisted/minutes/2021-10-15.md)\n* [2021-10-26](unlisted/minutes/2021-10-26.md)\n* [2021-11-02](unlisted/minutes/2021-11-02.md)\n* [2021-11-03](unlisted/minutes/2021-11-03.md)\n* [2021-11-09](unlisted/minutes/2021-11-09.md)\n* [2021-11-16](unlisted/minutes/2021-11-16.md)",title:"Untitled Page"}};for(const[a,b]of Object.entries(scrapedContent))contentIndex.add(a,b.content);const stopwords=['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most','other','some','such','no','nor','not','only','own','same','so','than','too','very','s','t','can','will','just','don','should','now'],highlight=(i,j)=>{const a=15,k=j.split(/\s+/).filter(a=>a!==""),b=i.split(/\s+/).filter(a=>a!==""),f=a=>k.some(b=>a.toLowerCase().includes(b.toLowerCase())),d=b.map(f);let e=0,g=0;for(let b=0;b<Math.max(d.length-a,0);b++){const f=d.slice(b,b+a),c=f.reduce((a,b)=>a+b,0);c>e&&(e=c,g=b)}const c=Math.max(g-a,0),h=Math.min(c+2*a,b.length),l=b.slice(c,h).map(a=>{return f(a)?`<span class="search-highlight">${a}</span>`:a}).join(" ").replaceAll('</span> <span class="search-highlight">'," ");return`${c===0?"":"..."}${l}${h===b.length?"":"..."}`},resultToHTML=({url:b,title:c,content:d,term:a})=>{const e=d.split("---")[2],f=removeMarkdown(e),g=highlight(c,a),h=highlight(f,a);return`<div class="result-card" id="${b}">
        <h3>${g}</h3>
        <p>${h}</p>
    </div>`},source=document.getElementById('search-bar'),results=document.getElementById("results-container");source.addEventListener('input',b=>{const a=b.target.value;contentIndex.search(a,{limit:5,depth:3,suggest:!0}).then(c=>{const d=[...new Set(c)],b=d.map(a=>({url:a,title:scrapedContent[a].title,content:scrapedContent[a].content}));if(b.length===0)results.innerHTML=`<div class="result-card">
            <p>No results.</p>
        </div>`;else{results.innerHTML=b.map(b=>resultToHTML({...b,term:a})).join("\n");const c=document.getElementsByClassName("result-card");[...c].forEach(b=>{b.onclick=()=>{window.location.href=`${b.id}#:~:text=${encodeURIComponent(a)}`}})}})});const searchContainer=document.getElementById("search-container");function openSearch(){searchContainer.style.display==="none"||searchContainer.style.display===""?(source.value="",results.innerHTML="",searchContainer.style.display="block",source.focus()):searchContainer.style.display="none"}function closeSearch(){searchContainer.style.display="none"}document.addEventListener('keydown',a=>{a.key==="/"&&(a.preventDefault(),openSearch()),a.key==="Escape"&&(a.preventDefault(),closeSearch())}),window.addEventListener('DOMContentLoaded',()=>{const a=document.getElementById("search-icon");a.addEventListener('click',a=>{openSearch()}),a.addEventListener('keydown',a=>{openSearch()})})</script><div class=singlePage><header><h1>Kalman gain for EKF</h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><div class=backlinks-container></div><aside class=mainTOC><h3>Table of Contents</h3><nav id=TableOfContents></nav></aside></article><hr><article><p>Source:
<a href=../../slam-for-dummies rel=noopener>SLAM for Dummies</a>
Backlinks: 
<a href=../../ekf-matrices rel=noopener>EKF matrices</a></p><p>How much we will trust the observed landmarks</p><ul><li>compromise between odometry and landmark correction</li><li>uses<ul><li>uncertainty of observed landmarks</li><li>measure of quality of the range measurement device</li><li>odometry performance</li></ul></li></ul><p>Gains for range and brearing (3+2n x 2)</p><p><img src=./_resources/Kalman_gain_for_EKF.resources/unknown_filename.png alt=unknown_filename.png></p></article><div id=contact_buttons><footer><p>Salehah © 2021 |
powered by <a href=https://github.com/jackyzha0/quartz>Quartz</a></p><a href=../../>Home</a>
<a href=https://github.com/salehahr>Github</a></footer></div><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></div></main></body></html>