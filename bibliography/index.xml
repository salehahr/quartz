<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bibliography on Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/bibliography/</link><description>Recent content in Bibliography on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><lastBuildDate>Sat, 14 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://salehahr.github.io/zettelkasten/bibliography/index.xml" rel="self" type="application/rss+xml"/><item><title>bonn-3D-cs</title><link>https://salehahr.github.io/zettelkasten/bibliography/bonn-3D-cs/</link><pubDate>Sat, 14 May 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/bonn-3D-cs/</guid><description>Source: https://www.youtube.com/playlist?list=PLyWhIjKEKFn9pb7tVFmZObpp2KhReb5wP
Author: Wolfgang Förstner
Motivation Integrating measurements from different viewpoints Observation of moving objects Notation Notation Description $\mathcal{R}$ rotation transformation $\mathbf{R}$ rotation matrix $\mathcal{X} = \mathcal{X}(x, y) = \mathcal{X}(\mathbf{x})$ 2D point given by the coordinates ($x, y$) $\mathbf{x}$ coordinate vector $\mathbf{x}_2$ coordinate vector with id 2 ${}^2 \mathbf{x}$ coordinate vector in coordinate system 2 ${}_2 \mathbf{T}^1$ active transformation that transforms point 1 to point 2 ${}^w \mathbf{T}_b$ passive transformation that transforms the point representation in CS $b$ to CS $w$ Selected Contents Motions in the plane Active vs.</description></item><item><title>blitzstein</title><link>https://salehahr.github.io/zettelkasten/bibliography/blitzstein-hwang/</link><pubDate>Thu, 12 May 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/blitzstein-hwang/</guid><description>Link: http://probabilitybook.net/
Authors: Blitzstein, Hwang
(skimmed)
Contents Probability Sample space Event Naive probability Probability space Frequentist vs. Bayesian probablity interpretation Probability function Conditional probability Concept: incorporate observed evidence into previously held belief Bayes' rule</description></item><item><title>unet-paper</title><link>https://salehahr.github.io/zettelkasten/bibliography/unet-paper/</link><pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/unet-paper/</guid><description>Source: U-Net paper
Contributions Architecture + training strategy using data augmentation Best performing network on ISBI challenge for segmentation of structures in microscopy images, cell tracking in microscopy images History Up till now, CNN mainly for image classification: image &amp;ndash;&amp;gt; single label Problems in biomedical image processing localisation also necessary: which class label belongs to which pixel? sparse dataset (not enough annotated data) Ciresan: network in sliding window setup to predict pixel class label provides patch around pixel as input localisation is successful training data in patches &amp;gt; training data as images themselves (1 image &amp;ndash;&amp;gt; many patches) drawbacks: slow, NW run separately for each patch redundancy: overlapping patches trade-off between localisation accuracy and use of context better localisation: small patches, but NW sees little context (bad for classification) better context: large patches, but max pooling layers reduce localisation accuracy Idea Want both: localisation accuracy, and classification accuracy (use of context)</description></item><item><title>ese-101</title><link>https://salehahr.github.io/zettelkasten/bibliography/ese-101/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/ese-101/</guid><description>Embedded Software Engineering 101 by embedded.fm Microcontroller basics</description></item><item><title>samek-embedded</title><link>https://salehahr.github.io/zettelkasten/bibliography/samek-embedded/</link><pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/samek-embedded/</guid><description>Modern Embedded Systems Programming Course by Miro Samek
Math Bit logic Integer overflow Basics Registers and memory RISC and CISC architectures Clock gating Interrupts Programming General considerations for non-linear control flow Preprocessor macros Variables Local vs. non-local variables Pointers Arrays in C Volatile Sample programs Using TM4C123G.</description></item><item><title>Google ML Course</title><link>https://salehahr.github.io/zettelkasten/bibliography/google-ml-course/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/google-ml-course/</guid><description>Link: https://developers.google.com/machine-learning/crash-course/
Main topics What is ML ML terminology Hyperparameters Data in ML Models Linear regression Linear logistic regression Classification Multi-class classification SoftMax Metrics Bias Losses Precision and recall Reducing loss Batching in ML Redundancy Variations of gradient descent Backpropagation Generalisation Overfitting Splitting the dataset Regularisation Neural networks Embeddings Definitions Convex problems Batch</description></item><item><title>(Phil's Lab) Sensor Fusion series</title><link>https://salehahr.github.io/zettelkasten/bibliography/phils-lab-sensor-fusion/</link><pubDate>Wed, 29 Sep 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/phils-lab-sensor-fusion/</guid><description>Source: https://www.youtube.com/watch?v=RZd6XDx5VXo
Sensor fusion Gyroscope Example: UAV attitude estimation Goal:
To estimate roll and pitch angles of aircraft These angles are needed for feedback in autonomous control of an unmanned UAV We have:
3-axis accelerometer [$\text{m/s}^2$] 3-axis gyroscope [$\text{rad/s}$] Note:
Here, angles are measured in body frame and not in fixed/inertial frame!
Measured quantities:
acceleration
$\mathbf{a}_B = \left[\begin{array}{ccc} a_x &amp;amp; a_y &amp;amp; a_z \end{array}\right]^\text{T}$ roll rates from gyrometer (body) $\neq$ derivative of Euler angles (fixed)</description></item><item><title>(Markley 2014) Fundamentals of Spacecraft Attitude Determination and Control</title><link>https://salehahr.github.io/zettelkasten/bibliography/markley-2014/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/markley-2014/</guid><description>Authors: FL Markley, John Crassidis
DOI: 10.1007/978-1-4939-0802-8
Note/Nomenclature: This book interpetes rotations/transformations in the passive/alias sense (I&amp;rsquo;m not a fan) Quaternions in JPL conventions instead of Hamiltonian (not a fan of this either&amp;hellip;) Rotation matrix = attitude matrix Introduction Attitude determination: memoryless approach without using statistics Attitude estimation: approaches with memory, uses statistical info from a series of measurements filter approaches uses a dynamic motion model of the object Quaternions Quaternion conventions Quaternion multiplication Rotations &amp;ldquo;Euler&amp;rsquo;s theorem: any rotation is a rotation about a fixed axis&amp;rdquo;</description></item><item><title>(Science Focus) How can one eye alone provide depth perception</title><link>https://salehahr.github.io/zettelkasten/bibliography/science-focus/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/science-focus/</guid><description>Source: http://www.sciencefocus.com/the-human-body/how-can-one-eye-alone-provide-depth-perception/
Author: Hilary Guite
In humans with normal binocular vision, depth perception is obtained using the parallax in the two overlapping fields of vision (&amp;ldquo;binocular disparity&amp;rdquo;)
Each single field of vision has a slightly different view to the other
If vision in one eye is impaired, depth perception is still obtainable even with only one eye. Some tricks that the brain uses:
We know the real size of things Using perspective, e.</description></item><item><title>(Mur-Artal 2017) VI-ORB</title><link>https://salehahr.github.io/zettelkasten/bibliography/mur-artal-2017-vi-orb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/mur-artal-2017-vi-orb/</guid><description>URL: http://ieeexplore.ieee.org/abstract/document/7817784
Authors: Mur-Artal, Tardós
Code: http://paperswithcode.com/paper/visual-inertial-monocular-slam-with-map-reuse
Results (video): http://www.youtube.com/watch?v=JXRCSovuxbA
Abstract current VI odometry approaches: drift accumulates due to lack of loop closure therefore there is a need for tightly-coupled VI-SLAM with loop closure and map reuse here: focus on monocular case, but applicable to other camera configurations builds on ORB-SLAM (from same author) IMU initialisation method (initialises: scale, gravity direction, velocities, gyroscope bias, accelerometer bias) depends on visual monocular initialisation (coupled initialisation) Other works: recent tightly-coupled VIO (both filtering- and optimisation-based) lack loop closure, so drift accumulates</description></item><item><title>(Liu 2020) Learned Descriptor</title><link>https://salehahr.github.io/zettelkasten/bibliography/liu-2020/</link><pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/liu-2020/</guid><description>Note: I&amp;rsquo;m only reading this paper for the into to SLAM/SfM
Abstract Problem: 3D reconstuction has subpar performance when dealing with endoscopic videos, partly due to local descriptors &amp;hellip; Introduction Correspondence estimation: match between 2D points in image and corresponding 3D location (s. registration ) Correspondence estimation is needed by SfM, SLAM, &amp;hellip; SfM + SLAM combination has been shown to be effective for surgical navigation in endoscopy &amp;ndash; simultaneous estimation of sparse 3D structure of the observed scene camera trajectory Complementarity of SfM + SLAM Good camera tracking requires dense 3D reconstruction</description></item><item><title>(rlabbe) Kalman/Bayesian filters in Python</title><link>https://salehahr.github.io/zettelkasten/bibliography/rlabbe/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/rlabbe/</guid><description>URL: http://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python nbviewer link: http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb Abstract:
Introductory text with Python code Caveat: most of the code is written for didactic purposes, may not be the most efficient solution (nor numerically stable) Recommended other works s. Works of possible interest Chapters
Preface Why Kalman filters? [Aim and main principle of Kalman filters](aim and-main-principle-of-kalman-filters.md) Expected value g-h filter or α-β filter Discrete Bayesian filter Gaussian distribution 1D Kalman filters Multivariate Kalman filters</description></item><item><title>Chen 2018 Review of VI SLAM</title><link>https://salehahr.github.io/zettelkasten/bibliography/chen-2018-review/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/chen-2018-review/</guid><description>Source: http://www.mdpi.com/2218-6581/7/3/45
Authors: Chen et. al
Abstract Survey on visual-inertial SLAM over the last 10 years Aspects: filtering vs optimisation based, camera type, sensor fusion type Explains core theory of SLAM, feature extraction, feature tracking, loop closure Experimental comparison of filtering-based and optimisation-based methods Research trends for VI-SLAM Recommended other works s. Works of possible interest Contents/Chapters SLAM SLAM: build a real-time map of the unknown environment based on sensor data, while the sensor (robot) itself is traversing the environment</description></item><item><title>Works of possible interest</title><link>https://salehahr.github.io/zettelkasten/bibliography/works-of-interest/</link><pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/works-of-interest/</guid><description>General SLAM Cadena 2016 &amp;ndash; Past, Present, and Future of SLAM durrant-whyte 2006 slam tutorial part i Prerequisites g2o paper - graph-based SLAM Existing SLAM algorithms MonoSLAM, works by Andrew Davison focusing on fusion instead of vision-only SLAM
Maplab (filtering-based) not looking at filtering-based algos
mentioned in the Chen 2018 Review of VI SLAM paper:
ORB-SLAM paper — ORB features VIORB implementation ORB-SLAM3 (improves on ORBSLAM, incl.</description></item><item><title>(Wu 2018) Image-based camera localization</title><link>https://salehahr.github.io/zettelkasten/bibliography/wu-2018/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/wu-2018/</guid><description>Authors: Wu, Tang, Li
Abstract/Contents overview (classification) of image-based camera localization classification of image-based camera localization approaches techniques, trends only considers 2D cameras focuses on points as features in images (not lines etc) Chapters Classification of image-based camera localization approaches Multisensor fusion — why use the visual-inertial sensor combination? Loose vs Tight coupling Filter localisation methods Some optimisation-based tightly-coupled multisensor SLAM algorithms Questions What&amp;rsquo;s a metric map &amp;ndash; normal map (with landmarks, normal distances) as opposed to a topological one Takeaway Learning SLAM is gaining in popularity, but geometric SLAM is often the chosen method for most applications, as it is more generalisable and at the same time reasonably accurate For reliability and low cost practical applications, multisensor vision-centred fusion is an effective method.</description></item><item><title>Cometlabs What You Need to Know About SLAM</title><link>https://salehahr.github.io/zettelkasten/bibliography/cometlabs/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/cometlabs/</guid><description>Source: http://blog.cometlabs.io/teaching-robots-presence-what-you-need-to-know-about-slam-9bf0ca037553
SLAM chicken and egg problem Position acquisition Multisensor fusion Sensors (absolute measurements) for measuring distance to landmarks Mapping representations in robotics Visual SLAM Implementation Framework Feature-based vs direct SLAM workflow Sparse/Feature-based VSLAM Dense/direct VSLAM</description></item><item><title>Riisgaard SLAM for dummies</title><link>https://salehahr.github.io/zettelkasten/bibliography/riisgaard-slam-for-dummies/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/riisgaard-slam-for-dummies/</guid><description>Authors: Søren Riisgaard and Morten Rufus Blas Parent: SLAM resources Abstract:
Tutorial introduction to SLAM, with minimal prerequisites for the understanding of SLAM as explained here Mostly explains a single approach to the steps involved in SLAM Complete solution for SLAM using EKF (extended Kalman filter) Only considers 2D motion, not 3D Chapters
What is SLAM? Overview of SLAM using EKF Hardware Robot Range measurement device SLAM process Step 1: Odometry update Step 2: Reobservation Step 3: Add new landmarks Laser data Odometry data Landmarks Landmark extraction 1.</description></item></channel></rss>