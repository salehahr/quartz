<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>filters on Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/tags/filters/</link><description>Recent content in filters on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><lastBuildDate>Wed, 18 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://salehahr.github.io/zettelkasten/tags/filters/index.xml" rel="self" type="application/rss+xml"/><item><title>50.2.3 Kalman filter initial estimates</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.3-kalman-filter-initial-estimates/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.3-kalman-filter-initial-estimates/</guid><description>Source: [Schneider 2013 How to not make the EKF fail](schneider 2013 how to-not-make-the-ekf-fail.md)
Initial state estimate x0, P0
Filter generally not badly affected by wrong initial state x0, but convergence will be slow if we are way off
If P0 too small whereas x0 is way off
the gain K becomes small filter relies on the model more than on the measurements Thus: important to have a consistent pair x0, P0</description></item><item><title>50.2.40 Kalman filter performance metric</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.40-kalman-filter-performance-metric/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.40-kalman-filter-performance-metric/</guid><description>Source: [Schneider 2013 How to not make the EKF fail](schneider 2013 how to-not-make-the-ekf-fail.md)
k: time / step j: how many EKF runs? in tutorial: EKF was ran 1000 times (non-deterministic system due to noise)</description></item><item><title>Schneider 2013 How to not make the EKF fail</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/schneider-2013-how-to-not-make-the-ekf-fail/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/schneider-2013-how-to-not-make-the-ekf-fail/</guid><description>Authors: Schneider, Georgakis URL: http://www.researchgate.net/publication/263942618_How_To_NOT_Make_the_Extended_Kalman_Filter_Fail/citations DOI 10.1021/ie300415d Measurement noise R, V (landmark) Kalman filter initial estimates Process noise Q and W (odometry) Kalman filter performance metric</description></item><item><title>50.4.1 Additive quaternion filtering</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.4.1-additive-quaternion-filtering/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.4.1-additive-quaternion-filtering/</guid><description>Parents: [Quaternion index](quaternion index.md), which orientation parametrisation to-choose? Source: Markley 2014 Additive quaternion filtering Additive quaternion error Methods of enforcing the normalisation
Renormalise the estimate by brute force Modify KF update equations to enforce a norm constraint using a Lagrange multiplier
[1] and [2] yield biased estimates of the quaternion
Methods that don&amp;rsquo;t enforce normalisation
Define the rotation matrix to be guarantees orthogonality introduces unobservable DOF: the quaternion norm Use the above equation without the ||q||-2 factor &amp;ndash;&amp;gt; no orthogonality</description></item><item><title>50.4.2 Multiplicative quaternion filtering (MEKF)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.4.2-multiplicative-quaternion-filtering-mekf/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.4.2-multiplicative-quaternion-filtering-mekf/</guid><description>See also: Which orientation parametrisation to choose? Source: Markley 2014 Main idea is to use
the quaternion as a global rotation representation
a three component state vector as the local representation of rotation errors each term (q_true, delta_q, q_est) is a normalised unit quaternion
Any of the rotation error representations can be used to calculate delta_theta, which is part of the error state of the MEKF.</description></item><item><title>50.7.2 Calculation of K and P in ESKF update</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.2-calculation-of-k-and-p-in-eskf-update/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.2-calculation-of-k-and-p-in-eskf-update/</guid><description>Parent: 50.3 Error-State Kalman Filter , eskf-update See also: Evaluation of the H Jacobian Source: Solà 2017 Quaternion kinematics for ESKF The filter correction equations are (yields a posteriori estimates)
Notes:
Here, the simplest form of the covariance update is used. This has poor numerical stability, however (no guarantee of symmetricity or positive definiteness) More stable forms are e.g. Joseph form (symmetric and positive) Error correction?</description></item><item><title>Maley 2013 MEKF for Nonspinning Guided Projectiles</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/maley-2013-mekf-for-nonspinning-guided-projectiles/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/maley-2013-mekf-for-nonspinning-guided-projectiles/</guid><description>Source: http://apps.dtic.mil/sti/citations/ADA588831</description></item><item><title>Whampsey MEKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/whampsey-mekf/</link><pubDate>Sat, 14 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/whampsey-mekf/</guid><description>http://matthewhampsey.github.io/blog/2020/07/18/mekf
Motivation:
Working with noisy IMU measurements IMUs usually provide redundant information that can be used to improve dead-reckoning Uses: Hamilton quaternion convention Which orientation parametrisation to choose? Error-State Kalman Filter</description></item><item><title>Whampsey MEKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/whampsey-mekf/</link><pubDate>Sat, 14 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/whampsey-mekf/</guid><description>http://matthewhampsey.github.io/blog/2020/07/18/mekf
Motivation:
Working with noisy IMU measurements IMUs usually provide redundant information that can be used to improve dead-reckoning Uses: Hamilton quaternion convention Which orientation parametrisation to choose? Error-State Kalman Filter</description></item><item><title>KF kinematics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kf-kinematics/</link><pubDate>Fri, 09 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kf-kinematics/</guid><description>Overview of KF states (true, nominal, error) Nominal state kinematics Error state kinematics Old stuff:</description></item><item><title>50.5.1.1 States of the ESKF for estimating IMU pose</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.5.1.1-states-of-the-eskf-for-estimating-imu-pose/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.5.1.1-states-of-the-eskf-for-estimating-imu-pose/</guid><description>Parent: IMU index Backlinks: Fusing IMU with complementary sensory data Source: Solà 2017 Quaternion kinematics for ESKF Full state Vector with 19 elements The corresponding kinematics equations/motion model is given in IMU kinematic equations/motion model .
Notes:
The angular error in 3D space is given by the notation. In quaternion space, this angle is halved, with  (&amp;lsquo;double cover&amp;rsquo;, s. [Unit quaternions](unit quaternions.md), rotation-error-representation )
The angular error is defined locally w.</description></item><item><title>50.6 ESKF prediction equations</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.6-eskf-prediction-equations/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.6-eskf-prediction-equations/</guid><description>Parents: [IMU index](imu index.md), 50.3 error-state-kalman-filter Source: Solà 2017 Quaternion kinematics for ESKF Error state system equation becomes: where (s. IMU nominal-state and error-state kinematics for an overview of the nonlinear kinematics equations)
State propagation (without considering noise) — produces a state estimate (a priori) Note: this always returns zero as the mean of the error initialises to zero!
Covariance propagation (considers noise); a priori estimate with the Jacobians (transition matrix approximated using first order Euler, more precise methods are available)</description></item><item><title>50.7.1 Observation of the error state (filter correction)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.1-observation-of-the-error-state-filter-correction/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.1-observation-of-the-error-state-filter-correction/</guid><description>Parents: [50.3 Error-State Kalman Filter](50.3 error-state kalman filter.md), [50.5 eskf update / fusing imu with complementary sensory data](50.5 eskf update _ fusing-imu-with-complementary-sensory-data.md) Source: Solà 2017 Quaternion kinematics for ESKF Given is a non-IMU sensor with the measurement function [ Solà , markley ] where x_t is the true state and v is a white Gaussian noise Source: Markley If the measurements are given in quaternion form:
we can directly calculate the orientation error between measured orientation and estimated orientation this becomes our &amp;lsquo;measured&amp;rsquo; angular error which can be used to calculate the residual term [JPL convention] q_meas x (q_est).</description></item><item><title>50.7.1.1 H Jacobian matrix in the ESKF filter correction</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.1.1-h-jacobian-matrix-in-the-eskf-filter-correction/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.1.1-h-jacobian-matrix-in-the-eskf-filter-correction/</guid><description>Parent: Filter correction , eskf-update Source: Solà 2017 Quaternion kinematics for ESKF Evaluation of the H Jacobian
In the prediction stage, the filter estimates the error state. Therefore, the Jacobian H needs to be defined w.r.t. the error state , and evaluated at the true state estimate  However, as the error state mean is zero (not yet observed), the true state is approximated to the nominal state  Thus we can use the nominal state as the evaluation point The first Jacobian Depends on the sensor&amp;rsquo;s particular measurement function The second Jacobian with Source: Markley 2014 Measurement sensitivity matrix (Jacobian w.</description></item><item><title>50.7.3 ESKF reset</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.3-eskf-reset/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.7.3-eskf-reset/</guid><description>Parent: Fusing IMU with complementary sensory data Backlinks: 50.3 Error-State Kalman Filter Source: Markley moves the rotation error to the global rotation this keeps the rotation error small and far from any singularities To update the global state, the reset has to obey The reset has to preserve the quaternion norm, therefore an exact unit norm expression must be used, instead of an approximation. Using the Rodrigues parameter , the reset becomes which leads to a two step update (1.</description></item><item><title>50.5 Error-State Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.5-error-state-kalman-filter/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.5-error-state-kalman-filter/</guid><description>Source: Markley An EKF propagates the expectation and covariance of the state The MEKF propagates the expectation and the covariance of the error state Source: Whampsey MEKF Previously: orientation is represented by one state Now: orientation is split up into  a large signal q_nom (nominal orientation) and a small signal (perturbation angle alpha) &amp;ndash; parametrises an error quaternion  This reformulates the error in terms of the group operation and so maintains the rotation invariance (rotation preserves the origin, length, angle between two vectors, orientation, etc.</description></item><item><title>Cyril Stachniss EKF-SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cyril-stachniss-ekf-slam/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cyril-stachniss-ekf-slam/</guid><description>Links:
Course material: http://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/ Lectures: http://www.youtube.com/playlist?list=PLgnQpQtFTOGQrZ4O5QzbIHgl3b1JHimN_&amp;amp;feature=g-list</description></item><item><title>Solà 2014 SLAM with EKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sol%C3%A0-2014-slam-with-ekf/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sol%C3%A0-2014-slam-with-ekf/</guid><description> Notes on EKF-SLAM that uses landmarks MATLAB code Notes on partial landmark initialisation (convariance matrix) Notes on the linearity of the observation function in scale</description></item><item><title>Discussion 2021-05-10</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-05-10/</link><pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/discussion-2021-05-10/</guid><description>Agenda
Change/Reduction of scope of SA (from fusing IMU with camera) to using sensor fusion to determine transformation parameters between IMU and camera Camera and IMU setup involves kinematic modelling (not fixed transformation as previously assumed!) Offline implementation in Python/MATLAB (scripting language) HiWi tasks can include DefSLAM bindings / interface C++ bindings of skrogh EKF implementation? HiWi prioritises Versuchsstand for now Tasks
Find an EKF implementation that works well and can be used with DefSLAM + IMU data implement kinematic model equations in the prediction-step, s.</description></item><item><title>note KF with missing measurements</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/note-kf-with-missing-measurements/</link><pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/note-kf-with-missing-measurements/</guid><description>Sources http://math.stackexchange.com/questions/982982/kalman-filter-with-missing-measurement-inputs http://opencv-users.1802565.n2.nabble.com/Kalman-filters-and-missing-measurements-td2886593.html
For a missing measurement:
use the last state estimate as a measurement set the covariance matrix of the measurement to essentially infinity. This would cause a Kalman filter to essentially ignore the new measurement since the ratio of the variance of the prediction to the measurement is zero. The result will be a new prediction that maintains velocity/acceleration but whose variance will grow according to the process noise.</description></item><item><title>note KF with different sampling rate</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/note-kf-with-different-sampling-rate/</link><pubDate>Wed, 24 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/note-kf-with-different-sampling-rate/</guid><description>Source: http://stackoverflow.com/questions/59566384/kalman-filter-with-different-sampling-rate
Approach 1: KF with variable dt Approach 2: KF with static dt
&amp;lsquo;Sub&amp;rsquo; updates? e.g.
predict() update() with sensor A skip update() for sensor B since no measurement arrived update() with sensor c repeat Generally discouraged:
If not predicting before each update, there is the risk of the filter lagging behind real world dynamics. The update step at t=k compares a measurement zk to the projected (predicted) state xk.</description></item><item><title>50.2.30 Multivariate Kalman filter algorithm</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.30-multivariate-kalman-filter-algorithm/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.30-multivariate-kalman-filter-algorithm/</guid><description>Parent: Multivariate Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Initialisation
Initialise filter state Initialise belief in the state Predict
Propagate state to the next time step using the system model [prediction] Adjust belief to take into account the prediction uncertainty [prior] Update
Obtain measurement and associated belief about its accuracy Calculate residual (prior - measurement) Calculate scaling factor/Kalman gain Set estimated state to be on the residual line based on the scaling factor Update the belief in the state based on measurement certainty Designing the measurement function</description></item><item><title>Hidden variables in a multivariate Kalman filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/hidden-variables-in-a-multivariate-kalman-filter/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/hidden-variables-in-a-multivariate-kalman-filter/</guid><description>Parent: Multivariate Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Example: Blue error ellipse:
Certainty in position x=0 No idea about the velocity (long in y-axis) We know that position and velocity are correlated, i.e. the next position depends on the current velocity value (red error ellipse — likelihood/prediction for the next step) e.g. if v=5m/s, the next position is 5m +- position uncertainties
We get a position update (new blue error ellipse) The new covariance (posterior) is obtained by multiplying the previous two covariances —&amp;gt; intersection The posterior&amp;rsquo;s tilt implies that there is some correlation between position and velocity Not only are we now more certain about the velocity, but our position certainty also increases (compared to not considering the velocity at all)!</description></item><item><title>Multivariate Kalman filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/multivariate-kalman-filters/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/multivariate-kalman-filters/</guid><description>Parent: rlabbe Kalman/Bayesian filters in Python [Hidden variables in a multivariate Kalman filter](hidden variables-in-a-multivariate-kalman-filter.md)
Here:
Focus is on a subset of problems describable using Newton&amp;rsquo;s equations of motion Discretised continuous-time kinematic filters Multivariate Kalman filter algorithm Designing the filter
State (x, P) Process (F, Q) Measurement (z, R) Measurement function H Control inputs (B, u) Assumptions of the Kalman filter
The sensors and motion model have Gaussian noise Everything is linear If the assumptions are true, then the Kalman filter is optimal in a least squares sense</description></item><item><title>50.2.10 Discrete Bayesian filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10-discrete-bayesian-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10-discrete-bayesian-filter/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python The Kalman filter is a subset of Bayesian filters
Predict and update steps like in the g-h filter Here: error percentages are used to implicitly compute the g and h parameters Steps
[Initialise our belief in the state] The predict step always degrades our knowledge (belief/prior) However, in the update step , we add another measurement. This, will always improve our knowledge regardless of noise, enabling convergence Limitations of the discrete Bayes filter</description></item><item><title>50.2.10.1 Discrete Bayesian Filter Predict Step</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10.1-discrete-bayesian-filter-predict-step/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10.1-discrete-bayesian-filter-predict-step/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python The predict step uses the total probability theorem.
Computes total probability of multiple possible events Uses the system model (propagates the states from prev. time step [posterior] to the next one); prediction Accounts for the uncertainty (kernel) in the prediction: produces a prior Generalise the uncertainty using a kernel (distributes the uncertainty over a range around the prediction) Integrate the kernel into the calculations by using convolution * Convolving the &amp;ldquo;current probabilistic estimate&amp;rdquo; with the &amp;ldquo;probabilistic estimate of how much we think the position has changed&amp;rdquo; (from system model) The prior is a &amp;lsquo;degraded&amp;rsquo; version of the belief i.</description></item><item><title>50.2.10.2 Bayesian Filter Update Step</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10.2-bayesian-filter-update-step/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.10.2-bayesian-filter-update-step/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python The update step uses Bayes' Theorem Produces the posterior by using the likelihood and the prior Also incorporates sensor data (measurements), as the measurements go into the likelihood calculation Update algorithm
Get a measurement, and associated belief about its accuracy Compute likelihood from the measurement and the measurement accuracy assumption Update the posterior using the likelihood and the prior</description></item><item><title>50.2.20 1D Kalman filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.20-1d-kalman-filters/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.20-1d-kalman-filters/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python [Deriving Kalman filter from Discrete Bayes using Gaussians](deriving kalman filter-from-discrete-bayes-using-gaussians.md) 1D Kalman filter algorithm Kalman gain using Gaussians Variance of the 1D Kalman filter Factors affecting Kalman filter performance</description></item><item><title>50.2.20.1 1D Kalman filter algorithm</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.20.1-1d-kalman-filter-algorithm/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.20.1-1d-kalman-filter-algorithm/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Initialisation:
Initialise the state of the filter Initialise the belief in the state Predict step:
Gaussian addition prior = predict(x, process_model) Incorporate process variance in order to prevent smug filtering Update step:
Gaussian multiplication likelihood = gaussian(z, sensor_var) x = update(prior, likelihood) The output of both steps is a Gaussian probability distribution N(mean, var)</description></item><item><title>Deriving Kalman filter from Discrete Bayes using Gaussians</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/deriving-kalman-filter-from-discrete-bayes-using-gaussians/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/deriving-kalman-filter-from-discrete-bayes-using-gaussians/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Replacing discrete Bayes with Gaussian distributions where the operators in the circles are as of yet undetermined</description></item><item><title>Deriving Kalman filter from Discrete Bayes using Gaussians</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/deriving-kalman-filter-from-discrete-bayes-using-gaussians/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/deriving-kalman-filter-from-discrete-bayes-using-gaussians/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Replacing discrete Bayes with Gaussian distributions where the operators in the circles are as of yet undetermined</description></item><item><title>Factors affecting Kalman filter performance</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/factors-affecting-kalman-filter-performance/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/factors-affecting-kalman-filter-performance/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Difficulties of creating a well-performing Kalman filter: Includes modeling the sensor performance (what variance most accurately represents the reality? Which probability distribution?)
Factors affecting the performance of the Kalman filter
On modelling the process noise/variance Bad initial estimate Filter can recover from this, because we have a certain belief in the sensor measurements Typically the initial value is set to the first sensor measurement Nonlinearity of the system</description></item><item><title>Kalman gain using Gaussians</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-gain-using-gaussians/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-gain-using-gaussians/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Kalman gain in the update step Basically a scaling term that chooses a value between the sensor distr. mean and the posterior distr. mean Gives greater weight to the term with lower variance (we trust this data more!) Mean and variance in terms of the Kalman gain Variance of the filter (i.e., what variance is show by the estimated output/posterior?</description></item><item><title>Kalman vs. nonlinear systems</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-vs.-nonlinear-systems/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-vs.-nonlinear-systems/</guid><description>Parent: Factors affecting Kalman filter performance Source: rlabbe Kalman/Bayesian filters in Python Kalman filter equations are linear Example: approximating a sine-wave signal Explanation:
Back to the basic g-h filter structure: the filter output chooses a value on the residual line The process model in the underlying filter assumes constant velocity (0 acceleration), whereas in the sine example above, the signal is always accelerating</description></item><item><title>Limitations of the discrete Bayes filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/limitations-of-the-discrete-bayes-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/limitations-of-the-discrete-bayes-filter/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python Limitations of the discrete Bayes filter
Scaling Dog tracking example is one-dimensional, but in real life we often want to track more things (e.g. 2D coordinates, velocities) Multidimensional case: store probabilities in a grid 4 tracked variables: O(n^4) per time step High computational cost with high dimensionality Filter is discrete and therefore gives discrete output But a lot of applications require continuous output Discretising a solution space can lead to lots of data (depending on accuracy required) &amp;ndash;&amp;gt; calculations for lots of different probabilities!</description></item><item><title>Smug filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/smug-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/smug-filter/</guid><description>Parent: 1D Kalman filter algorithm Source: rlabbe Kalman/Bayesian filters in Python A filter that, once enough measurements are made, becomes very confident in its prediction (P gets smaller with time while the filter becomes more inaccurate!). From then on it will ignore measurements
To avoid this: add a bit of error to the prediction step, e.g. using the process variance</description></item><item><title>Variance of the 1D Kalman filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/variance-of-the-1d-kalman-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/variance-of-the-1d-kalman-filter/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python i.e., what variance is show by the estimated output/posterior?)
Always converges to a fixed value if the sensor and process variances are constant We can run simulations to determine the value to which the filter variance converges Then hard code this value into the filter (+ with first sensor measurement as initial value, the filter should have good performance) Alternative: instead of using the variance value, use the calculated Kalman gain Example implementation using the Kalman gain However, using the Kalman gain obscures the Bayesian approach</description></item><item><title>50.1 Why Kalman filters?</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.1-why-kalman-filters-/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.1-why-kalman-filters-/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python We work with 2 sources of data:
Sensor measurements Our own predictions (based on knowledge of system behaviour) Sensors are noisy, don&amp;rsquo;t give perfect information
Simple solution: to average readings However, this doesn&amp;rsquo;t work when the sensor is too noisy data collection not possible The prediction, however, is also susceptible to noise (the world is noisy, outside/unaccounted for influences)</description></item><item><title>50.1.1 Aim and main principle of Kalman filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.1.1-aim-and-main-principle-of-kalman-filters/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.1.1-aim-and-main-principle-of-kalman-filters/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python Aim of the Kalman/Bayesian filters: to accumulate (or to somehow blend)
our noisy and limited knowledge (of system behaviour) noisy and limited sensor readings and with these, make the best possible prediction (estimate) of the system state.
Main principles:
use past information to make predictions for the future never throw away information predict/propagation step: calculate prediction based on process model and using previous state data (previous estimate) update step: calculate the estimates based on prediction and measurement Prediction step a.</description></item><item><title>Calculating the estimated state in the GH-filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/calculating-the-estimated-state-in-the-gh-filter/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/calculating-the-estimated-state-in-the-gh-filter/</guid><description>Parent: g-h filter or α-β filter Source: rlabbe Kalman/Bayesian filters in Python Using a gain , the estimate therefore always falls between the measurements (circles) and the predictions (in red). The prediction is dependent on the previous filter output (i.e. last estimate). Here it is modelled to increase by 1 from the previous estimate.
The estimates are not a straight line, but definitely closer in shape to the ground truth than the measurements alone.</description></item><item><title>Effects of varying g</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/effects-of-varying-g/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/effects-of-varying-g/</guid><description>Parent: [Calculating the estimated state in the GH-filter](calculating the-estimated-state-in-the-gh-filter.md) Source: rlabbe Kalman/Bayesian filters in Python The greater the g value, the more we follow the measurements rather than rely on our [model-based] predictions.</description></item><item><title>Effects of varying h</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/effects-of-varying-h/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/effects-of-varying-h/</guid><description>Parent Improving the gh-filter by using h Source: rlabbe Kalman/Bayesian filters in Python The greater the h value, the more we trust the rate of change that we can derive from the measurement data.
a larger h enables us to react to transient (initial condition dependent) changes more rapidly. Because if we have a large difference between our chosen IC and the measurement, this results in a huge residual velocity</description></item><item><title>g-h filter or α-β filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/g-h-filter-or-%CE%B1-%CE%B2-filter/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/g-h-filter-or-%CE%B1-%CE%B2-filter/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python A filter that uses two scaling factors:
g or \alpha for the measurement h or \beta for the rate of change of measurement GH filter algorithm [Calculating the estimated state in the GH-filter](calculating the-estimated-state-in-the-gh-filter.md) Improving the gh-filter by using h Several unwanted effects using gh filters Basis for many other filters, e.g.
Kalman filter Least squares filter Benedict-Bordner filter etc.</description></item><item><title>Gain g of the gh-filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/gain-g-of-the-gh-filter/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/gain-g-of-the-gh-filter/</guid><description>Parent: [Calculating the estimated state based on measurements and predictions](calculating the estimated state-based-on-measurements-and-predictions.md) Backlinks: GH filter algorithm Source: rlabbe Kalman/Bayesian filters in Python Which one do we trust more, the meaasurement z or the prediction x? Applying corresponding weights to both, we obtain the estimate x_est
The prediction is nothing other than a propagated state estimate.
[Me] The prediction is basesd on the model (a priori knowledge) If the model also depends on previous states (which are themselves an output of the filter, i.</description></item><item><title>GH filter algorithm</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/gh-filter-algorithm/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/gh-filter-algorithm/</guid><description>Parent: g-h filter or α-β filter Source: rlabbe Kalman/Bayesian filters in Python Initialisation
Initialise the state of the filter Initialise our belief in the state Prediction
Use system model to propagate the state to the next time step Adjust our belief to account for uncertainty in the prediction Update
Get a measurement and an associated belief about its accuracy Calculate residual = measurement - estimated state Using a certain gain , our updated state estimate is somewhere on the residual line</description></item><item><title>Improving the gh-filter by using h</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/improving-the-gh-filter-by-using-h/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/improving-the-gh-filter-by-using-h/</guid><description>Parent: g-h filter or α-β filter Source: rlabbe Kalman/Bayesian filters in Python Implementing the g value without h We improve the estimation, previously by only predicting the state, by now predicting the rate of change of state. i.e. Also predict the weight gain per day instead of setting it at a constant value. We use the sensor information for this! Even if it&amp;rsquo;s noisy, there&amp;rsquo;s information in there somewhere, and data is always better than a guess.</description></item><item><title>Several unwanted effects using gh filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/several-unwanted-effects-using-gh-filters/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/several-unwanted-effects-using-gh-filters/</guid><description>Parent: g-h filter or α-β filter Source: rlabbe Kalman/Bayesian filters in Python Effect of bad initial conditions: ringing (sinusoidal over- and undershooting before finally settling onto a trajectory) Effect of very noisy data Effect of acceleration (in data) Filter lags behind because it uses a model that assumes constant velocity in each propagation step. Hence, a filter is only as good as the mathematical model used to describe the phenomenon.</description></item><item><title>General Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/general-kalman-filter/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/general-kalman-filter/</guid><description>Backlinks: Main paradigms of SLAM Source: Scaradozzi 2018 SLAM application in surgery KF original algorithm assumes linearity (rarely ever the case)
Variations of the Kalman filter: Extended Kalman Filter (EKF) Unscented Kalman Filter (UKF) Information filtering (IF) — dual to KF Combination of EKF and IF: CF-SLAM, with the goal to be more efficient w.r.t. computational complexity</description></item><item><title>Information Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/information-filter/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/information-filter/</guid><description>Parent: General Kalman Filter Source: Scaradozzi 2018 SLAM application in surgery also same assumptions as the EKF main difference: how the Gaussian belief is represented est. cov. — replaced by information matrix (IM) est. state — replaced by information vector (IV) superior to KF in the following ways data is filtered by summing up the IMs and IVs often numerically more stable Dual character of KF and IF</description></item><item><title>Unscented Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/unscented-kalman-filter/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/unscented-kalman-filter/</guid><description>Parent: General Kalman Filter Source: Scaradozzi 2018 SLAM application in surgery developed to overcome main problems of the EKF like EKF, approximates the state distribution with a Gaussian Random Variable only the representation is different—using alpha points (a minimal set of sample points) capture posterior mean and covariance accurately for any nonlinearity, up to3rd order Taylor</description></item><item><title>General EKF implementation (non-SLAM)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/general-ekf-implementation-non-slam/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/general-ekf-implementation-non-slam/</guid><description>Parent: Extended Kalman Filter Source: SLAM for Dummies General (non-SLAM) implementation of EKF:
only state estimation robot is given a perfect map no map update necessary SLAM implementations of EKF requires map update and therefore the matrices are changed.
Source: Scaradozzi 2018 SLAM application in surgery EKF vs KF circumvents linearity assumption uses nonlinear functions to describe the next state probability measurement probability approximates the state distribution with a Gaussian Random Variable</description></item><item><title>EKF System State</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-system-state/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-system-state/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices , step-2:-re-observation Contains robot POSE and landmark position POSE: (x y theta)_r LM: (x, y)_l1 &amp;hellip; (x,y)_ln; n = num. of landmarks Size: 3+2n rows</description></item><item><title>Kalman gain for EKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-gain-for-ekf/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-gain-for-ekf/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices How much we will trust the observed landmarks
compromise between odometry and landmark correction uses uncertainty of observed landmarks measure of quality of the range measurement device odometry performance Gains for range and brearing (3+2n x 2)</description></item><item><title>50.2.1 Process noise Q and W (odometry)</title><link>https://salehahr.github.io/zettelkasten/SLAM/50.2.1-process-noise-q-and-w-odometry/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/50.2.1-process-noise-q-and-w-odometry/</guid><description>See also: Factors affecting Kalman filter performance Source: Tereshkov 2015 Process noise covariance matrix has no clear physical meaning, cannot be deduced from sensor characteristics Leads to non-intuitive, iterative procedures to tune KFs Which means that KF optimality is rarely achieved in practice Alternative to KF tuning: the use of geometric observers
estimates are expresssed only in terms of quantities with clear geometrical meaning Source: Schneider 2013 If perfect model: $Q$ only describes the covariance of the random process noise Not perfect model, has: parametric errors (-&amp;gt; parameter identification) structural erors (error in model structure) workaround: e.</description></item><item><title>50.2.2 Measurement noise R, V (landmark)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.2-measurement-noise-r-v-landmark/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.2-measurement-noise-r-v-landmark/</guid><description>Parent: Multivariate Kalman filter algorithm Source: rlabbe Kalman/Bayesian filters in Python R models the noise in the sensors as a covariance matrix dim(R) = m x m (m: number of sensors) Possible complications in multisensor systems, the correlation between the sensors might not be clear sensor noise might not be pure Gaussian Source: http://www.linkedin.com/pulse/tuning-extended-kalman-filter-process-noise-training-alex-thompson Ways to obtain R
Using the variances given in the sensor specifications Comopare the measurements against a strong ground truth and derive the variance variable by variable Record the steady state measurements over a long period of time and measure the variance (look at the histogram) Source: [Schneider 2013 How to not make the EKF fail](schneider 2013 how to-not-make-the-ekf-fail.</description></item><item><title>Covariance matrix P</title><link>https://salehahr.github.io/zettelkasten/SLAM/covariance-matrix-p/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/covariance-matrix-p/</guid><description>Source: SLAM for Dummies s. also EKF matrices Covariance matrix P
Covariance: measure of correlation of two variables Correlation: measure of degree of linear dependence A covariance of the robote POSEupdated in Step 1: Odometry update 3x3 B .. C covariance on the first .. nth landmarkStep 3: New landmarks 2x2 D covariance between POSE and first LMupdated in Step 1: Odometry update 2x3 E, etc E = D^T, etcupdated in Step 1: Odometry update 3x2 F=G^T Step 3: New landmarks Initially $P = A$ (robot has not seen any LMs)</description></item><item><title>Measurement model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/measurement-model/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/measurement-model/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices/vectors Estimate of the range and bearing (from landmark) in Step 2: Re-observation x, y, theta - current position estimate lambdax, y - landmark position
Jacobian H w.r.t. x, y, theta (here for regular EKF, not for extended) In SLAM we need additional values for the landmarks here for landmark number two in extended EKF Upper row is for information, not part of matrix First three columns are regular H Landmarks don&amp;rsquo;t have any rotation</description></item><item><title>Prediction model</title><link>https://salehahr.github.io/zettelkasten/SLAM/prediction-model/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/prediction-model/</guid><description>Source: SLAM for Dummies Used in the prediction step .
How to compute an expected position of the robot given the old position and the control input (so basically based on odometry .
Control terms are $\Delta x, \Delta y, \Delta \theta$)
$$ f = \left[ \begin{array}{c} x + \Delta t \cos \theta + q \Delta t \cos \theta \
y + \Delta t \sin \theta + q \Delta t \sin \theta \</description></item><item><title>SLAM-specific jacobians</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/slam-specific-jacobians/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/slam-specific-jacobians/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices/vectors Jxr
Jacobian of the prediction of landmarks, which does not include prediction of theta, w.r.t. robot POSE same as J_prediction model, except without rotation term Jz Jacobian of prediction of landmarks, but w.r.t. [range, bearing]</description></item><item><title>Step 1 Odometry update (Prediction step)</title><link>https://salehahr.github.io/zettelkasten/SLAM/step-1-odometry-update-prediction-step/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/step-1-odometry-update-prediction-step/</guid><description>Source: SLAM for Dummies First step in the three-step EKF Update current state using odometry data Based on the controls given to the robot Calculate estimate of new POSE Update equation: prediction model ($x = x + \Delta x \cdot q$)
Or in a simple model, neglect the error term $q$
State vector gets updated via the prediction model
Jacobian of the prediction model also needs to be updated every iteration (with the controls deltax, &amp;hellip;)</description></item><item><title>Step 2 Re-observation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/step-2-re-observation/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/step-2-re-observation/</guid><description>&amp;laquo;&amp;laquo;&amp;laquo;&amp;lt; HEAD Source: SLAM for Dummies Backlinks: Basic EKF for SLAM Source: SLAM for Dummies content
Second step in the three-step EKF — overview
In this step we update the robot position that we got in [step 1]](studienarbeit/step-1-odometry-update-prediction-step.md) Compensate for errors due to odometry pos_est (odometry-based) - pos_actual (LM-based) = Innovation, (based on the LM that the robot can see)</description></item><item><title>Step 3 New landmarks</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/step-3-new-landmarks/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/step-3-new-landmarks/</guid><description>Source: SLAM for Dummies Backlinks: Basic EKF for SLAM Overview
Landmarks that are new are not dealt with until step 3. Delaying the incorporation of new landmarks until the will decrease the computation cost needed for this step the covariance matrix, P, and the system state, X, are smaller by then. Update state vector x and covariance matrix P with new landmarks Add new landmark to state vector X Add new row and column to covariance matrix Covariance for new landmark Robot-landmark covariance</description></item><item><title>Basic EKF for SLAM</title><link>https://salehahr.github.io/zettelkasten/SLAM/basic-ekf-for-slam/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/basic-ekf-for-slam/</guid><description>Source: SLAM for Dummies A basic EKF implementation of SLAM consists of multiple parts:
Landmark extraction Data association After odometry change (due to robot moving), state estimation from odometry Update of the estimated state using re-observed landmark data Update landmark database with new landmarks Note: at any point in the three steps on the left, the EKF will have an estimate of the robots current position</description></item><item><title>Basic EKF for SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/basic-ekf-for-slam/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/basic-ekf-for-slam/</guid><description>Parent: Extended Kalman Filter , slam_index Backlinks: RANSAC See also: What is SLAM? Source: SLAM for Dummies A basic EKF implementation of SLAM consists of multiple parts:
Landmark extraction Data association After odometry change (due to robot moving), state estimation from odometry Update of the estimated state using re-observed landmark data Update landmark database with new landmarks Note: at any point in the three steps on the left, the EKF will have an estimate of the robots current position</description></item><item><title>EKF matrices/vectors</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-matrices-vectors/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/ekf-matrices-vectors/</guid><description>Source: SLAM for Dummies System state X Estimate of POSE Jacobian of prediction model Landmark range and bearing Jacobian of measurement model Covariance matrix P Kalman gain K SLAM-specific jacobians</description></item><item><title>Extended Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/SLAM/extended-kalman-filter/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/extended-kalman-filter/</guid><description>Parent: SLAM Index Backlinks: RANSAC , nearest neighbour , Filtering in localisation Source: SLAM for Dummies keeps track of an estimate of the position uncertainty keeps track of the uncertainty in the features/landmarks seen General EKF implementation (non-SLAM) Basic EKF for SLAM Diagram: Triangle Robot Stars Landmarks Dashed triangle Robot&amp;rsquo;s position based on odometry alone (where it thinks it is) Dotted triangle Robot&amp;rsquo;s position estimate based on EKF Solid line triangle Robot&amp;rsquo;s actual position in real life!</description></item><item><title>Extended Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/extended-kalman-filter/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/extended-kalman-filter/</guid><description>Parent: SLAM Index Backlinks: RANSAC , [nearest neighbour](nearest neighbour.md), filter-localisation-methods Source: SLAM for Dummies keeps track of an estimate of the position uncertainty keeps track of the uncertainty in the features/landmarks seen General EKF implementation (non-SLAM) Basic EKF for SLAM Diagram: Triangle Robot Stars Landmarks Dashed triangle Robot&amp;rsquo;s position based on odometry alone (where it thinks it is) Dotted triangle Robot&amp;rsquo;s position estimate based on EKF Solid line triangle Robot&amp;rsquo;s actual position in real life!</description></item><item><title>Introduction to the Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/introduction-to-the-kalman-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/introduction-to-the-kalman-filter/</guid><description> http://resourcium.org/journey/introduction-kalman-filter</description></item></channel></rss>