<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>filters/kalman-filter on Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/tags/filters/kalman-filter/</link><description>Recent content in filters/kalman-filter on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><atom:link href="https://salehahr.github.io/zettelkasten/tags/filters/kalman-filter/index.xml" rel="self" type="application/rss+xml"/><item><title>50.2.3 Kalman filter initial estimates</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.3-kalman-filter-initial-estimates/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.3-kalman-filter-initial-estimates/</guid><description>Source: [Schneider 2013 How to not make the EKF fail](schneider 2013 how to-not-make-the-ekf-fail.md)
Initial state estimate x0, P0
Filter generally not badly affected by wrong initial state x0, but convergence will be slow if we are way off
If P0 too small whereas x0 is way off
the gain K becomes small filter relies on the model more than on the measurements Thus: important to have a consistent pair x0, P0</description></item><item><title>50.2.40 Kalman filter performance metric</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.40-kalman-filter-performance-metric/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.40-kalman-filter-performance-metric/</guid><description>Source: [Schneider 2013 How to not make the EKF fail](schneider 2013 how to-not-make-the-ekf-fail.md)
k: time / step j: how many EKF runs? in tutorial: EKF was ran 1000 times (non-deterministic system due to noise)</description></item><item><title>50.4.1 Additive quaternion filtering</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.4.1-additive-quaternion-filtering/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.4.1-additive-quaternion-filtering/</guid><description>Parents: Quaternion index , which orientation parametrisation to-choose? Source: Markley 2014 Additive quaternion filtering Additive quaternion error Methods of enforcing the normalisation Renormalise the estimate by brute force Modify KF update equations to enforce a norm constraint using a Lagrange multiplier
[1] and [2] yield biased estimates of the quaternion
Methods that don&amp;rsquo;t enforce normalisation Define the rotation matrix to be guarantees orthogonality introduces unobservable DOF: the quaternion norm Use the above equation without the ||q||-2 factor &amp;ndash;&amp;gt; no orthogonality</description></item><item><title>Whampsey MEKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/whampsey-mekf/</link><pubDate>Sat, 14 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/whampsey-mekf/</guid><description>Source: http://matthewhampsey.github.io/blog/2020/07/18/mekf
Motivation Working with noisy IMU measurements IMUs usually provide redundant information that can be used to improve dead-reckoning Uses: Hamilton quaternion convention .
Which orientation parametrisation to choose? 50.5-error-state-kalman-filter</description></item><item><title>note KF with missing measurements</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/note-kf-with-missing-measurements/</link><pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/note-kf-with-missing-measurements/</guid><description>Sources http://math.stackexchange.com/questions/982982/kalman-filter-with-missing-measurement-inputs http://opencv-users.1802565.n2.nabble.com/Kalman-filters-and-missing-measurements-td2886593.html
For a missing measurement:
use the last state estimate as a measurement set the covariance matrix of the measurement to essentially infinity. This would cause a Kalman filter to essentially ignore the new measurement since the ratio of the variance of the prediction to the measurement is zero. The result will be a new prediction that maintains velocity/acceleration but whose variance will grow according to the process noise.</description></item><item><title>note KF with different sampling rate</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/note-kf-with-different-sampling-rate/</link><pubDate>Wed, 24 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/note-kf-with-different-sampling-rate/</guid><description>Source: http://stackoverflow.com/questions/59566384/kalman-filter-with-different-sampling-rate
Approach 1: KF with variable dt Approach 2: KF with static dt
&amp;lsquo;Sub&amp;rsquo; updates? e.g.
predict() update() with sensor A skip update() for sensor B since no measurement arrived update() with sensor c repeat Generally discouraged:
If not predicting before each update, there is the risk of the filter lagging behind real world dynamics. The update step at t=k compares a measurement zk to the projected (predicted) state xk.</description></item><item><title>50.2.30 Multivariate Kalman filter algorithm</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.30-multivariate-kalman-filter-algorithm/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.30-multivariate-kalman-filter-algorithm/</guid><description>Parent: Multivariate Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Initialisation
Initialise filter state Initialise belief in the state Predict
Propagate state to the next time step using the system model [prediction] Adjust belief to take into account the prediction uncertainty [prior] Update
Obtain measurement and associated belief about its accuracy Calculate residual (prior - measurement) Calculate scaling factor/Kalman gain Set estimated state to be on the residual line based on the scaling factor Update the belief in the state based on measurement certainty Designing the measurement function</description></item><item><title>Hidden variables in a multivariate Kalman filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/hidden-variables-in-a-multivariate-kalman-filter/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/hidden-variables-in-a-multivariate-kalman-filter/</guid><description>Parent: Multivariate Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Example: Blue error ellipse:
Certainty in position x=0 No idea about the velocity (long in y-axis) We know that position and velocity are correlated, i.e. the next position depends on the current velocity value (red error ellipse — likelihood/prediction for the next step) e.g. if v=5m/s, the next position is 5m +- position uncertainties
We get a position update (new blue error ellipse) The new covariance (posterior) is obtained by multiplying the previous two covariances —&amp;gt; intersection The posterior&amp;rsquo;s tilt implies that there is some correlation between position and velocity Not only are we now more certain about the velocity, but our position certainty also increases (compared to not considering the velocity at all)!</description></item><item><title>Multivariate Kalman filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/multivariate-kalman-filters/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/multivariate-kalman-filters/</guid><description>Parent: rlabbe Kalman/Bayesian filters in Python [Hidden variables in a multivariate Kalman filter](hidden variables-in-a-multivariate-kalman-filter.md)
Here:
Focus is on a subset of problems describable using Newton&amp;rsquo;s equations of motion Discretised continuous-time kinematic filters Multivariate Kalman filter algorithm Designing the filter
State (x, P) Process (F, Q) Measurement (z, R) Measurement function H Control inputs (B, u) Assumptions of the Kalman filter
The sensors and motion model have Gaussian noise Everything is linear If the assumptions are true, then the Kalman filter is optimal in a least squares sense</description></item><item><title>50.2.20 1D Kalman filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.20-1d-kalman-filters/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.20-1d-kalman-filters/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python [Deriving Kalman filter from Discrete Bayes using Gaussians](deriving kalman filter-from-discrete-bayes-using-gaussians.md) 1D Kalman filter algorithm Kalman gain using Gaussians Variance of the 1D Kalman filter Factors affecting Kalman filter performance</description></item><item><title>50.2.20.1 1D Kalman filter algorithm</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.20.1-1d-kalman-filter-algorithm/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.20.1-1d-kalman-filter-algorithm/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Initialisation:
Initialise the state of the filter Initialise the belief in the state Predict step:
Gaussian addition prior = predict(x, process_model) Incorporate process variance in order to prevent smug filtering Update step:
Gaussian multiplication likelihood = gaussian(z, sensor_var) x = update(prior, likelihood) The output of both steps is a Gaussian probability distribution N(mean, var)</description></item><item><title>Deriving Kalman filter from Discrete Bayes using Gaussians</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/deriving-kalman-filter-from-discrete-bayes-using-gaussians/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/deriving-kalman-filter-from-discrete-bayes-using-gaussians/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Replacing discrete Bayes with Gaussian distributions where the operators in the circles are as of yet undetermined</description></item><item><title>Factors affecting Kalman filter performance</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/factors-affecting-kalman-filter-performance/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/factors-affecting-kalman-filter-performance/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Difficulties of creating a well-performing Kalman filter: Includes modeling the sensor performance (what variance most accurately represents the reality? Which probability distribution?)
Factors affecting the performance of the Kalman filter
On modelling the process noise/variance Bad initial estimate Filter can recover from this, because we have a certain belief in the sensor measurements Typically the initial value is set to the first sensor measurement Nonlinearity of the system</description></item><item><title>Kalman gain using Gaussians</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-gain-using-gaussians/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-gain-using-gaussians/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Kalman gain in the update step Basically a scaling term that chooses a value between the sensor distr. mean and the posterior distr. mean Gives greater weight to the term with lower variance (we trust this data more!) Mean and variance in terms of the Kalman gain Variance of the filter (i.e., what variance is show by the estimated output/posterior?</description></item><item><title>Kalman vs. nonlinear systems</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-vs.-nonlinear-systems/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kalman-vs.-nonlinear-systems/</guid><description>Parent: Factors affecting Kalman filter performance Source: rlabbe Kalman/Bayesian filters in Python Kalman filter equations are linear Example: approximating a sine-wave signal Explanation:
Back to the basic g-h filter structure: the filter output chooses a value on the residual line The process model in the underlying filter assumes constant velocity (0 acceleration), whereas in the sine example above, the signal is always accelerating</description></item><item><title>Variance of the 1D Kalman filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/variance-of-the-1d-kalman-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/variance-of-the-1d-kalman-filter/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python i.e., what variance is show by the estimated output/posterior?)
Always converges to a fixed value if the sensor and process variances are constant We can run simulations to determine the value to which the filter variance converges Then hard code this value into the filter (+ with first sensor measurement as initial value, the filter should have good performance) Alternative: instead of using the variance value, use the calculated Kalman gain Example implementation using the Kalman gain However, using the Kalman gain obscures the Bayesian approach</description></item><item><title>50.1 Why Kalman filters?</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.1-why-kalman-filters-/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.1-why-kalman-filters-/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python We work with 2 sources of data:
Sensor measurements Our own predictions (based on knowledge of system behaviour) Sensors are noisy, don&amp;rsquo;t give perfect information
Simple solution: to average readings However, this doesn&amp;rsquo;t work when the sensor is too noisy data collection not possible The prediction, however, is also susceptible to noise (the world is noisy, outside/unaccounted for influences)</description></item><item><title>50.1.1 Aim and main principle of Kalman filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.1.1-aim-and-main-principle-of-kalman-filters/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.1.1-aim-and-main-principle-of-kalman-filters/</guid><description>Source: rlabbe-kalman-bayesian-filters-in-python Aim Aim of the Kalman/Bayesian filters: to accumulate (or to somehow blend)
our noisy and limited knowledge (of system behaviour) noisy and limited sensor readings and with these, make the best possible prediction (estimate) of the system state.
Main principles: use past information to make predictions for the future never throw away information predict/propagation step: calculate prediction based on process model and using previous state data (previous estimate) update step: calculate the estimates based on prediction and measurement Prediction step a.</description></item><item><title>General Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/general-kalman-filter/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/general-kalman-filter/</guid><description>Backlinks: Main paradigms of SLAM Source: Scaradozzi 2018 SLAM application in surgery KF original algorithm assumes linearity (rarely ever the case)
Variations of the Kalman filter: Extended Kalman Filter (EKF) Unscented Kalman Filter (UKF) Information filtering (IF) — dual to KF Combination of EKF and IF: CF-SLAM, with the goal to be more efficient w.r.t. computational complexity</description></item><item><title>Information Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/information-filter/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/information-filter/</guid><description>Parent: General Kalman Filter Source: Scaradozzi 2018 SLAM application in surgery also same assumptions as the EKF main difference: how the Gaussian belief is represented est. cov. — replaced by information matrix (IM) est. state — replaced by information vector (IV) superior to KF in the following ways data is filtered by summing up the IMs and IVs often numerically more stable Dual character of KF and IF</description></item><item><title>Unscented Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/unscented-kalman-filter/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/unscented-kalman-filter/</guid><description>Parent: General Kalman Filter Source: Scaradozzi 2018 SLAM application in surgery developed to overcome main problems of the EKF like EKF, approximates the state distribution with a Gaussian Random Variable only the representation is different—using alpha points (a minimal set of sample points) capture posterior mean and covariance accurately for any nonlinearity, up to3rd order Taylor</description></item><item><title>Introduction to the Kalman Filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/introduction-to-the-kalman-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/introduction-to-the-kalman-filter/</guid><description> http://resourcium.org/journey/introduction-kalman-filter</description></item></channel></rss>