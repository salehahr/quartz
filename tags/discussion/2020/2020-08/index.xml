<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>discussion/2020/2020-08 on</title><link>https://salehahr.github.io/zettelkasten/tags/discussion/2020/2020-08/</link><description>Recent content in discussion/2020/2020-08 on</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><atom:link href="https://salehahr.github.io/zettelkasten/tags/discussion/2020/2020-08/index.xml" rel="self" type="application/rss+xml"/><item><title>Qin 2019 General Optimization-based Framework (Multisensor)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/qin-2019-general-optimization-based-framework-multisensor/</link><pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/qin-2019-general-optimization-based-framework-multisensor/</guid><description>Authors: Qin et al Code: http://github.com/HKUST-Aerial-Robotics/VINS-Fusion (uses ROS)
Abstract:
odometry estimation with multiple sensors, general framework which is optimisation-based demonstrated combinations: stereo cameras monocular cam + IMU stereo cams + IMU sensor = factor in the framework comparison with other state-of-the-art algos Aim:
to create a general algo which supports different multisensor suites also for redundancy: in case of sensor failure, it can be switched out easily Related work:</description></item><item><title>State-of-the-art SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/state-of-the-art-slam/</link><pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/state-of-the-art-slam/</guid><description>Parent: SLAM Index Backlinks: Qin 2019 General Optimization-based Framework (Multisensor) Things that I&amp;rsquo;ve seen mentioned several times so far
ORBSLAM: monocular MonoSLAM: monocular (old?) — Andrew Davison OKVIS: visual inertial, stereovision PTAM: parallel tracking and mapping MSCKF: real-time EKF VINS-mono: visual inertial, monocular http://en.wikipedia.org/wiki/List_of_SLAM_Methods</description></item><item><title>Works of possible interest</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/works-of-possible-interest/</link><pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/works-of-possible-interest/</guid><description>General SLAM
cadena 2016 past, present, and future of slam durrant-whyte 2006 slam tutorial part i Prerequisites
g2o paper - graph-based SLAM Existing SLAM algorithms
MonoSLAM, works by Andrew Davison focusing on fusion instead of vision-only SLAM Maplab (filtering-based) not looking at filtering-based algos mentioned in the Chen 2018 Review of VI SLAM paper ORB-SLAM paper — ORB features VIORB implementation ORB-SLAM3 (improves on ORBSLAM, incl.</description></item><item><title>(Scaradozzi 2018) SLAM application in surgery</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/scaradozzi-2018-slam-application-in-surgery/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/scaradozzi-2018-slam-application-in-surgery/</guid><description>Abstract:
SLAM&amp;rsquo;s potential in image-guided surgery assuming static environment Review of main techniques in general robotics SLAM Insight into visual SLAM SLAM in surgery Chapters What is SLAM? Filter-based vs optimisation-based SLAM General Kalman Filter General EKF Unscented Kalman Filter Information Filter &amp;hellip;.
Takeaway
EKF is popular in surgery SLAM techniques Deformable environment encumbers precise registration and data fusion</description></item><item><title>Lamarca 2020 DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/lamarca-2020-defslam/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/lamarca-2020-defslam/</guid><description>URL: http://arxiv.org/abs/1908.08918 Authors: Lamarca et al Code: http://github.com/UZ-SLAMLab/DefSLAM Results (video): http://www.youtube.com/watch?v=6mmhD2_t6Gs Summary
First monocular SLAM for deformable environments in real-time Most other SLAM implementations assume rigidity Main techniques used (techniques for monocular non-rigid scenes): isometric shape from template (SfT) non-rigid structure from motion (NRSfM) Main principle: computation in two parallel threads (s. DefSLAM framework) Deformation tracking [front end] Deformation mapping [back end] The map from the mapping thread defines the shape-at-rest template used by deformation tracking Validation: compare with ORBSLAM (rigid) Assumes isometric deformation Future work: relocalisation (s.</description></item><item><title>Visual SLAM Implementation Framework</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/visual-slam-implementation-framework/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/visual-slam-implementation-framework/</guid><description>Parent: SLAM Index Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Basic principle:
tracking a set of points through successive frames these tracks are used to triangulate the 3D positions of the points to create the map at the same time, using the the est point locations to calculate the pose of the camera, which could have observed them (i.e. calculate real time 3D structure of a scene from the estimated motion of the camera) Two main architecture components:</description></item></channel></rss>