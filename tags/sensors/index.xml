<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>sensors on</title><link>https://salehahr.github.io/tags/sensors/</link><description>Recent content in sensors on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 23 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://salehahr.github.io/tags/sensors/index.xml" rel="self" type="application/rss+xml"/><item><title>Converting IMU data to inertial frame</title><link>https://salehahr.github.io/studienarbeit/converting-imu-data-to-inertial-frame/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/converting-imu-data-to-inertial-frame/</guid><description>Parent: IMU index Source: http://redshiftlabs.com.au/wp-content/uploads/2018/02/an-1005_-_understanding_euler_angles.pdf IMU outputs are in the body frame of the sensor.
Convention used in the article: yaw/psi (z) - pitch/theta (y) - roll/phi (x) around momentary axes Momentary coordinate systems: W -&amp;gt; W' -&amp;gt; W'' -&amp;gt; B Body acceleration to inertial acceleration W_a = R_WB @ B_a
Body angular rate to inertial angular rate
Each angular rate must be converted to the corresponding frame p: gyro_z -&amp;gt; rotated into W: R_w_w' @ R_w'_w'' @ R_w''_B @ q q: gyro_y -&amp;gt; rotated into W': R_w'_w'' @ R_w''_B @ q r: gyro_x -&amp;gt; rotated into W'': R_w''_B @ r with Gimbal lock: pitch approaches +-90, terms divided by cos90</description></item><item><title>40.1 IMU measurement model</title><link>https://salehahr.github.io/studienarbeit/40.1-imu-measurement-model/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/40.1-imu-measurement-model/</guid><description>Parent: [IMU index](imu index.md), probabilistic models-for-imu Backlinks: [IMU motion model](imu motion model.md), imu kinematic model using euler integration An IMU measures, relative to an inertial frame, acceleration and rotation rate.
The measurements are corrupted by bias and noise (often assumed to be white Gaussian noise ). mkok-2017 Additionally, the acceleration measured is affected by gravity. Note the [assumptions in modelling the true angular velocity in IMUs](assumptions in modelling the-true-angular-velocity-in-imus.</description></item><item><title>40.1.1 Assumptions in modelling the true angular velocity in IMUs</title><link>https://salehahr.github.io/studienarbeit/40.1.1-assumptions-in-modelling-the-true-angular-velocity-in-imus/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/40.1.1-assumptions-in-modelling-the-true-angular-velocity-in-imus/</guid><description>Parent: [IMU index](imu index.md), imu-measurement-model Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
For angular velocity, the termshould really be with
negligible Earth rotation = 0 stationary navigation frame, = 0</description></item><item><title>50.3 IMU motion model in a Kalman filter</title><link>https://salehahr.github.io/studienarbeit/50.3-imu-motion-model-in-a-kalman-filter/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.3-imu-motion-model-in-a-kalman-filter/</guid><description>Parent: IMU index Source: Solà 2017 Quaternion kinematics for ESKF Which states do we use for the motion model? [Choice of states for the IMU motion/kinematics model](choice of states-for-the-imu-motion_kinematics-model.md)
How do we model the IMU motion? [Choice of model for the IMU motion model](choice of model-for-the-imu-motion-model.md)
The kinematics (true state) can be partitioned into a nominal part and an error part, s. variables in ESKF using IMUs . The corresponding [nominal state dynamics and error state dynamics](nominal state-dynamics-and-error-state-dynamics.</description></item><item><title>50.3.1 Choice of states for the IMU motion/kinematics model</title><link>https://salehahr.github.io/studienarbeit/50.3.1-choice-of-states-for-the-imu-motion-kinematics-model/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.3.1-choice-of-states-for-the-imu-motion-kinematics-model/</guid><description>Parent: IMU index See also: [Choice of model for the IMU motion model](choice of model-for-the-imu-motion-model.md)
According to MKok 2017 , we can either
Use the full state vector [+] knowledge about sensor motion is included in model [-] large state vector Or the partial state vector, where the inputs are the inertial measurements from the IMU [+] process noise intuitively represents IMU noise. This is useful when we have no knowledge about the motion model.</description></item><item><title>50.3.2 Choice of model for the KF using IMU readings</title><link>https://salehahr.github.io/studienarbeit/50.3.2-choice-of-model-for-the-kf-using-imu-readings/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.3.2-choice-of-model-for-the-kf-using-imu-readings/</guid><description>Parent: [IMU index](imu index.md), imu kinematic-equations/motion-model According to MKok 2017 , here are some models that assume either a constant acceleration or a constant angular velocity:
Constant acceleration model Constant angular velocity model (Notation: angular velocity of the body with respect to world (n), expressed in body CS)
If motion is unknown, there is also the option of modelling the states using random walk equations.</description></item><item><title>50.5.1 IMU nominal-state and error-state kinematics</title><link>https://salehahr.github.io/studienarbeit/50.5.1-imu-nominal-state-and-error-state-kinematics/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.5.1-imu-nominal-state-and-error-state-kinematics/</guid><description>Parents: [IMU index](imu index.md), 50.3 error-state-kalman-filter Note on discretisation Solà 2017 :
Convert the differential equations to difference equations (use integration) Integration methods may vary Closed form solutions Numerical integration Integration is done for: The nominal state The error state (deterministic part): error state dynamics and control The error state (stochastic part): noise and perturbations Nominal state Error state Model without noise and perturbations Continuous Discrete summary:with the jacobians defined in imu eskf-prediction-equations</description></item><item><title>50.5.1.1 States of the ESKF for estimating IMU pose</title><link>https://salehahr.github.io/studienarbeit/50.5.1.1-states-of-the-eskf-for-estimating-imu-pose/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.5.1.1-states-of-the-eskf-for-estimating-imu-pose/</guid><description>Parent: IMU index Backlinks: Fusing IMU with complementary sensory data Source: Solà 2017 Quaternion kinematics for ESKF Full state Vector with 19 elements The corresponding kinematics equations/motion model is given in IMU kinematic equations/motion model .
Notes:
The angular error in 3D space is given by the notation. In quaternion space, this angle is halved, with  (&amp;lsquo;double cover&amp;rsquo;, s. [Unit quaternions](unit quaternions.md), rotation-error-representation )
The angular error is defined locally w.</description></item><item><title>50.5.1.2 The initial gravity vector/orientation for the IMU ESKF</title><link>https://salehahr.github.io/studienarbeit/50.5.1.2-the-initial-gravity-vector-orientation-for-the-imu-eskf/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.5.1.2-the-initial-gravity-vector-orientation-for-the-imu-eskf/</guid><description>Parent: [IMU index](imu index.md), [choice of model for the imu motion model](choice of model-for-the-imu-motion-model.md)
Notes on the initial gravity vector/orientation for the IMU ESKF Solà 2017 For simplicity, it is assumed that  The gravity vector g is estimated in terms of frame q0
This puts the initial uncertainty on the gravity direction, rather than on the initial orientation.
Doing this improves linearity, because now the equation is linear in g and the initiial rotation R0 has no uncertainty</description></item><item><title>50.6 ESKF prediction equations</title><link>https://salehahr.github.io/studienarbeit/50.6-eskf-prediction-equations/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.6-eskf-prediction-equations/</guid><description>Parents: [IMU index](imu index.md), 50.3 error-state-kalman-filter Source: Solà 2017 Quaternion kinematics for ESKF Error state system equation becomes: where (s. IMU nominal-state and error-state kinematics for an overview of the nonlinear kinematics equations)
State propagation (without considering noise) — produces a state estimate (a priori) Note: this always returns zero as the mean of the error initialises to zero!
Covariance propagation (considers noise); a priori estimate with the Jacobians (transition matrix approximated using first order Euler, more precise methods are available)</description></item><item><title>50.7 ESKF update / Fusing IMU with complementary sensory data</title><link>https://salehahr.github.io/studienarbeit/50.7-eskf-update-fusing-imu-with-complementary-sensory-data/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.7-eskf-update-fusing-imu-with-complementary-sensory-data/</guid><description>Parent: [IMU index](imu index.md), 50.3 error-state-kalman-filter Source: Solà 2017 Quaternion kinematics for ESKF In the ESKF, the arrival of non-IMU sensor data triggers a correction stage. This correction makes the IMU biases observable , allows correct estimation of the biases The correction stage is three-fold:
observe the error state by way of filter correction &amp;lsquo;add&amp;rsquo; the observed errors to the nominal state to get the supposed &amp;lsquo;true&amp;rsquo; state according to the composition rules in variables in ESKF using IMUs reset the error state Source: Markley Fundamentals of Spacecraft Attitude Determination What if several measurements come in without IMU / propagation in between (i.</description></item><item><title>IMU data generation from camera/visual data</title><link>https://salehahr.github.io/studienarbeit/imu-data-generation-from-camera-visual-data/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/imu-data-generation-from-camera-visual-data/</guid><description>Parent: IMU index Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)</description></item><item><title>IMU to camera coordinate transformations</title><link>https://salehahr.github.io/studienarbeit/imu-to-camera-coordinate-transformations/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/imu-to-camera-coordinate-transformations/</guid><description>Parent: IMU index Source: Weiss 2011</description></item><item><title>IMU states, dynamics equations</title><link>https://salehahr.github.io/studienarbeit/imu-states-dynamics-equations/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/imu-states-dynamics-equations/</guid><description>Parent: IMU index Source: Mur-Artal 2017 VI-ORB Evolution of IMU states (world frame to IMU: orientation R, position p, velocity v) between consecutive keyframes
Evolution of IMU states (world frame to IMU: orientation R, position p, velocity v) between consecutive frames
Using the preintegration terms Preintegration (delta) terms and the Jacobians can be computed iteratively as IMU measurements arrive (s. Forster&amp;rsquo;s paper on preintegration)</description></item><item><title>Modelling noise and bias for IMU</title><link>https://salehahr.github.io/studienarbeit/modelling-noise-and-bias-for-imu/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/modelling-noise-and-bias-for-imu/</guid><description>Parent: [IMU index](imu index.md), imu-measurement-model Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
Modelling the noise The noise not only represents measurement noise, but also model uncertainty.
With proper calibration, the three gyroscope axes are independent: Same for accelerometer — assume diagonal for a properly calibrated sensor
Modelling the biases — two approaches
treat bias as constant (due to short experiment times) pre-calibrate in a separate experiment, or make part of the parameters vector treat as slowly time-varying (due to long experiment times or shorter bias stability) make the bias part of the state vector model the bias as a random walk</description></item><item><title>resource IMU common specifications, error models etc</title><link>https://salehahr.github.io/studienarbeit/resource-imu-common-specifications-error-models-etc/</link><pubDate>Sat, 27 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/resource-imu-common-specifications-error-models-etc/</guid><description>Parent: IMU Source: http://www.vectornav.com/resources/imu-specifications
IMU common specifications, bias, scale factor, orthogonality errors, and acceleration sensitivity for gyroscopes.
Source: Woodman - An introduction to inertial navigation Source: Quinchia - A Comparison between Different Error Modeling of MEMS Applied to GPS/INS Integrated Systems
3.2. State-Space Representation for Different Bias Models
First order Gauss-Markov (GM) Random walk Autoregressive process</description></item><item><title>IMU motion model (discrete)</title><link>https://salehahr.github.io/studienarbeit/imu-motion-model-discrete/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/imu-motion-model-discrete/</guid><description>Parent: [IMU index](imu index.md), probabilistic models-for-imu Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
Position dynamics Orientation dynamics (either quaternion or rotation matrix representation) with</description></item><item><title>MKok 2017 Using inertial sensors for position and orientation estimation</title><link>https://salehahr.github.io/studienarbeit/mkok-2017-using-inertial-sensors-for-position-and-orientation-estimation/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/mkok-2017-using-inertial-sensors-for-position-and-orientation-estimation/</guid><description>Source: http://arxiv.org/abs/1704.06053 Authors: M Kok, JD Hol, TB Schön
Abstract
Contents/Chapters Quaternions Probabilistic models for IMU Orientation parametrisations Which orientation parametrisation to choose? Linearisation of an orientation in SO(3) IMU measurement model Modelling noise and bias for IMU IMU motion models IMU prior models</description></item><item><title>Probabilistic models for IMU</title><link>https://salehahr.github.io/studienarbeit/probabilistic-models-for-imu/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/probabilistic-models-for-imu/</guid><description>Parent: IMU index Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
Three main components to the probabilistic models
IMU measurement model (infer knowledge about pose from measurements)  Prediction model (how sensor pose changes over time) Models of the initial pose (prior) Knowledge we are interested in: pose of the sensor
time-varying variables: states  constants: parameters  Knowledge available to us: sensor dynamics, available sensor measurements Conditional probability distribution</description></item><item><title>World to camera trafo</title><link>https://salehahr.github.io/studienarbeit/world-to-camera-trafo/</link><pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/world-to-camera-trafo/</guid><description>Parent: SLAM Index See also: [Pinhole camera model](pinhole camera model.md), pinhole camera-projection-function Source: http://www.cse.psu.edu/~rtc12/CSE486/lecture12.pdf
Camera coordinates (X, Y, Z) World coordinates (U, V, W) Image plane (x, y) / Pixel coordinates (u, v) Forward projection
Representing 2D point as a fictitious 3D point (x', y', z') [for matrix calculations] Convention: Given (x', y', z'), we can recover the 2D point (x, y) as World to camera trafo</description></item><item><title>Gauss-Newton Method on Manifold</title><link>https://salehahr.github.io/studienarbeit/gauss-newton-method-on-manifold/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/gauss-newton-method-on-manifold/</guid><description>Source: Forster 2017 IMU Preintegration Standard approach for optimization on manifold
define a retraction to reparametrise the problem (lifting) retraction bijective map map between an element of the tangent space at x and a neighbourhood of x on the manifold i.e. we work in the tangent space (locally like a Euclidian space) and apply standard optimisation techniques for Gauss-Newton specifically: [ts] squared cost around current estimate [ts] solve the quadratic approximation &amp;ndash;&amp;gt; we get vector in tangent space [m] update the current guess on the manifold  Consider: Reparametrised: Retraction for SE(3) The exponential map of SE(3) as a retraction is possible, but may not be convenient (computationally)</description></item><item><title>IMU kinematic model using Euler integration</title><link>https://salehahr.github.io/studienarbeit/imu-kinematic-model-using-euler-integration/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/imu-kinematic-model-using-euler-integration/</guid><description>Parent: IMU index Source: Forster 2017 IMU Preintegration Backlinks: [IMU preintegration on manifold](imu preintegration on-manifold.md), imu-measurement-model Kinematic model Using Euler integration assuming acc and angVel are constant in the time interval: Using the measurement equations:</description></item><item><title>IMU kinematic model using Euler integration</title><link>https://salehahr.github.io/studienarbeit/imu-kinematic-model-using-euler-integration/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/imu-kinematic-model-using-euler-integration/</guid><description>Parent: IMU index Source: Forster 2017 IMU Preintegration Backlinks: [IMU preintegration on manifold](imu preintegration on-manifold.md), imu-measurement-model Kinematic model Using Euler integration assuming acc and angVel are constant in the time interval: Using the measurement equations:</description></item><item><title>IMU preintegration on manifold</title><link>https://salehahr.github.io/studienarbeit/imu-preintegration-on-manifold/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/imu-preintegration-on-manifold/</guid><description>Parent: IMU index Source: Forster 2017 IMU Preintegration Backlinks: IMU model Preintegration on manifold
Summarising all measurements between the keyframes i and j into a single measurement This preintegrated IMU measurement constrains the motion between two consecutive keyframes Assume IMU is synchronised with the camera The above equations already provide the summarised IMU measurements, however, the integration has to be repeated whenever the linearisation point at t=t_i changes i.</description></item><item><title>IMU preintegration on manifold</title><link>https://salehahr.github.io/studienarbeit/imu-preintegration-on-manifold/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/imu-preintegration-on-manifold/</guid><description>Parent: IMU index Source: Forster 2017 IMU Preintegration Backlinks: IMU model Preintegration on manifold
Summarising all measurements between the keyframes i and j into a single measurement This preintegrated IMU measurement constrains the motion between two consecutive keyframes Assume IMU is synchronised with the camera The above equations already provide the summarised IMU measurements, however, the integration has to be repeated whenever the linearisation point at t=t_i changes i.</description></item><item><title>MAP estimation</title><link>https://salehahr.github.io/studienarbeit/map-estimation/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/map-estimation/</guid><description>Source: Forster 2017 IMU Preintegration Factor graph: way of representing the posterior probability of the states given the available measurements and priors The terms in the equation above are called &amp;lsquo;factors&amp;rsquo;
MAP: maximum a posteriori We want to maximise the probability derived above &amp;ndash;&amp;gt; MAP estimate (aka minimum of negative log posterior) The negative log posterior can be written as a sum of squared residuals, assuming zero-mean Gaussian noise residual errors (prior, IMU, camera) covariance matrices How do we define these residuals?</description></item><item><title>System in a VIN problem with IMU preintegration</title><link>https://salehahr.github.io/studienarbeit/system-in-a-vin-problem-with-imu-preintegration/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/system-in-a-vin-problem-with-imu-preintegration/</guid><description>Source: Forster 2017 IMU Preintegration State x_i of the system at time i with All keyframes up till time k State of all keyframes camera measurements IMU measurements between KFs i and j (consecutive) Set of measurements up till time k IMU pose: , maps a point in B to W</description></item><item><title>Camera calibration</title><link>https://salehahr.github.io/studienarbeit/camera-calibration/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/camera-calibration/</guid><description>Parent: SLAM Index Source: http://de.mathworks.com/help/vision/ug/camera-calibration.html
estimates lens/sensor parameters e.g. to correct lens distortion, determine position, measurement etc there are several camera models, e.g. fisheye, pinhole Camera parameters
intrinsic extrinsic distortion coefficients How to solve for camera parameters?
Need to have 3D world points and the corresponding 2D image points Take multiple images of a calibration pattern to obtain these correspondences With the mapping 3Dp -&amp;gt; 2Dp, solve for camera parameters Evaluate accuracy of estimated camera parameters:</description></item><item><title>Pinhole camera projection function</title><link>https://salehahr.github.io/studienarbeit/pinhole-camera-projection-function/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/pinhole-camera-projection-function/</guid><description>Backlinks: Pinhole camera model See also: World to camera trafo Source: Mur-Artal 2017 VI-ORB 3D points Projection function  Transforms 3D points into 2D points on image plane Focal length:  Principal point:  The projection does not consider the distortion due to the lens
therefore when extracting image features, first undistort their coordinates only then match to projected points (existing features which have undergone projection from 3D to 2D) Source: Lamarca 2019 DefSLAM 3D point: Projection function maps</description></item><item><title>Forster 2017 IMU Preintegration</title><link>https://salehahr.github.io/studienarbeit/forster-2017-imu-preintegration/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/forster-2017-imu-preintegration/</guid><description>Authors: Forster et al
Abstract:
First contribution: preintegration theory (building up on Lupton&amp;rsquo;s work) what&amp;rsquo;s different from Lupton&amp;rsquo;s: addresses manifold structure of the rotation group, analytic derivation of all Jacobians Lupton&amp;rsquo;s work uses Euler angles Using Euler angles and techniques of Euclidian spaces for state propagation/covariance estimation is not properly invariant under rigid transformations uncertainty propagation, a-posteriori bias correction same as Lupton: integration performed in local frame, eliminating need for reintegrating when linearisation point changes Second contribution: integration of the preintegrated IMU model into a visual-inertial pipeline The system presented uses incremental smoothing for fast computation of the optimal MAP estimate Uses structureless model (3D landmarks are not part of the variables to be estimated) for visual measurements &amp;ndash;&amp;gt; allows eliminating large numbers of variables Motivation:</description></item><item><title>IMU index</title><link>https://salehahr.github.io/studienarbeit/imu-index/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/imu-index/</guid><description>Parent: SLAM Index Backlinks: SA TODO General IMU Odometry Why use the visual-inertial sensor combination? IMU to camera coordinate transformations Practical Converting IMU data to inertial frame Modelling Probabilistic models for IMU [Choice of model for the IMU motion model](choice of model-for-the-imu-motion-model.md) [Choice of states for the IMU motion/kinematics model](choice of states-for-the-imu-motion_kinematics-model.md) Variables in ESKF using IMUs [IMU states, dynamics equations](imu states, dynamics equations.md) (kfs and preintegration) / [(kok) imu motion model (discrete)]((kok) imu motion model (discrete).</description></item><item><title>Pinhole camera model</title><link>https://salehahr.github.io/studienarbeit/pinhole-camera-model/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/pinhole-camera-model/</guid><description>Parent: SLAM Index Backlinks: [Camera calibration](camera calibration.md), [pinhole camera projection function](pinhole camera projection function.md), [weiss thesis vision based navigation for micro helicopters](weiss thesis vision-based-navigation-for-micro-helicopters.md) See also: World to camera trafo Source: http://de.mathworks.com/help/vision/ug/camera-calibration.html Does not account for lens distortion (ideal pinhole camera doesn&amp;rsquo;t have a lens) To represent a real camera, the full camera model to be used should include (radial and tangential) lens distortion, (such as the one used in the MATLAB computer vision toolbox)</description></item><item><title>Preintegration of IMU</title><link>https://salehahr.github.io/studienarbeit/preintegration-of-imu/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/preintegration-of-imu/</guid><description>Parent: IMU Backlinks: IMU states, dynamics equations IMU measurements arrive at a higher frequency (frame rate) compared to camera captures (keyframe rate) IMU measurements constrain consecutive states We want to summarise these &amp;lsquo;in-between&amp;rsquo; IMU measurements into one single relative motion constraint between keyframes</description></item><item><title>Preintegration of IMU</title><link>https://salehahr.github.io/studienarbeit/preintegration-of-imu/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/preintegration-of-imu/</guid><description>Parent: IMU Backlinks: IMU states, dynamics equations IMU measurements arrive at a higher frequency (frame rate) compared to camera captures (keyframe rate) IMU measurements constrain consecutive states We want to summarise these &amp;lsquo;in-between&amp;rsquo; IMU measurements into one single relative motion constraint between keyframes</description></item><item><title>Sensors (absolute measurements) for measuring absolute POSE</title><link>https://salehahr.github.io/studienarbeit/sensors-absolute-measurements-for-measuring-absolute-pose/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/sensors-absolute-measurements-for-measuring-absolute-pose/</guid><description>Parent: SLAM hardware Source: Wikipedia Lokalisierung GPS (only for outdoors) Innenraumsensorik Lidar, Ultra Wide Band (UWB), Wireless Fidelity, etc [ Wu ] Compared to these, cameras are flexible and low-cost [ Wu ] (are also passive sensors) [ comet ] Radiobaken</description></item><item><title>Visual sensors for localisation</title><link>https://salehahr.github.io/studienarbeit/visual-sensors-for-localisation/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/visual-sensors-for-localisation/</guid><description>Parents: [SLAM Index](slam index.md), [sensors (absolute measurements) for measuring distance to landmarks](sensors (absolute measurements)-for-measuring-distance-to-landmarks.md)
Source: Wikipedia Visual odometry Process of determining robot POSE by analysing the associated camera images Use sequential camera image to estimate the distance travelled Applications: robotics, computer vision
Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md) Types
Monocular cameras Stereo cameras RGB-D cameras Provide rich visual information, but for that, higher computational cost</description></item><item><title>Laser scanners</title><link>https://salehahr.github.io/studienarbeit/laser-scanners/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/laser-scanners/</guid><description>Source: SLAM for Dummies Backlinks: [Sensors (absolute measurements) for measuring distance to landmarks](sensors (absolute measurements)-for-measuring-distance-to-landmarks.md)
Commonly used [+] Precise, efficient, not much processing work necessary [-] Expensive, bad readings with certain surfaces, bad for underwater applications</description></item><item><title>Monocular cameras</title><link>https://salehahr.github.io/studienarbeit/monocular-cameras/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/monocular-cameras/</guid><description>Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md) Backlinks: Visual sensors for localisation + Simpler hardware implementation + Smaller and cheapter systems - need complexer algos and software because of lack of direct depth information from a 2D image How is the shape of the map generated?
Integrating measurements in the chain of frames over time Use triangulation method As well as camera motion, if camera isn&amp;rsquo;t stationary Depths of points are not observed directly (s.</description></item><item><title>Multisensor fusion</title><link>https://salehahr.github.io/studienarbeit/multisensor-fusion/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/multisensor-fusion/</guid><description>Parent: SLAM Index , geometric-metric-slam Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Avoid limitations of using only one sensor Relative measurements: provide precise positioning information constantly At certain times absolute measurements are made to correct potential errors (correct drift) several approaches (for localisation), e.g. merge sensor feeds at the lowest level before being processed homogeneously hierarchical approaches (fuse state estimates derived independently from multiple sensors) s.</description></item><item><title>Position acquisition (relative vs. absolute)</title><link>https://salehahr.github.io/studienarbeit/position-acquisition-relative-vs.-absolute/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/position-acquisition-relative-vs.-absolute/</guid><description>Parent: SLAM Index See also: SLAM hardware Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md) relative (interoceptive sensors) odometry absolute (exteroceptive sensors) can be used alongside relative measurement sensors in order to correct odometry drift s. [major sensor types in SLAM (absolute measurements)](major sensor types in-slam-(absolute-measurements).md) incl. visual-sensors Beacons direct measurement instead of integrating, therefore error in position does not grow unbounded e.</description></item><item><title>RGB-D cameras</title><link>https://salehahr.github.io/studienarbeit/rgb-d-cameras/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/rgb-d-cameras/</guid><description>Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md) Backlinks: [Visual sensors for localisation](visual sensors-for-localisation.md), stereo-cameras Provide depth information directly Employed by most of the SLAM systems Generate 3D images through structured light or time of flight technology Structured light camera projects a known pattern onto objects Perceives deformation of pattern by an infrared camera This lets depth and surface information of the objects be calculated Time of flight ToF of a light signal between camera and objects is measured &amp;ndash;&amp;gt; from this, depth is obtained Structured light sensors are sensitive to illumination &amp;ndash; not applicable in direct sunlight Limitations of RGB-D cameras</description></item><item><title>Sensors (absolute measurements) for measuring distance to landmarks</title><link>https://salehahr.github.io/studienarbeit/sensors-absolute-measurements-for-measuring-distance-to-landmarks/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/sensors-absolute-measurements-for-measuring-distance-to-landmarks/</guid><description>Parents: SLAM Index , slam-hardware Backlinks: Position acquisition Source:: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Acoustic (Time of Flight) ToF technique Surfaces need to have good acoustic reflection Lack the ability to use surface properties for localisation examples Sonar Ultrasonic, ultrasound Laser rangerfinders ToF and phase-shift techniques Lack the ability to use surface properties for localisation e.g. Lidar Visual sensors for localisation Source: SLAM for Dummies Laser scanners Sonar, usually polaroid sonar [+] cheaper, good for underwater (s.</description></item><item><title>Stereo cameras</title><link>https://salehahr.github.io/studienarbeit/stereo-cameras/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/stereo-cameras/</guid><description>Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md) Backlinks: Visual sensors for localisation two cameras separated by a fixed distance (baseline) observations of the position of the same 3D point in both cameras allows depth to be calculated through triangulation (like humans do) depth measurement limited by baseline and resolution generally, wider baseline &amp;ndash;&amp;gt; better depth estimate (but occupies more physical space) s.</description></item><item><title>SLAM hardware</title><link>https://salehahr.github.io/studienarbeit/slam-hardware/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/slam-hardware/</guid><description>Parent: SLAM Index See also: Position acquisition (relative vs. absolute) Source: SLAM for Dummies Robot parameters to consider Ease of use Odometry performance: how well the robot can estimate its own position, just from the rotation of the wheels Max errors: 2cm per meter moved, 2deg per 45deg turned Bad odometry &amp;ndash;&amp;gt; bad estimation of current position &amp;ndash;&amp;gt; hard to implement SLAM Range measurement device options Source: Wikipedia Lokalisierung Categories of sensors for localisation</description></item></channel></rss>