<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>sensors/cameras on</title><link>https://salehahr.github.io/zettelkasten/tags/sensors/cameras/</link><description>Recent content in sensors/cameras on</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><atom:link href="https://salehahr.github.io/zettelkasten/tags/sensors/cameras/index.xml" rel="self" type="application/rss+xml"/><item><title>World to camera trafo</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/world-to-camera-trafo/</link><pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/world-to-camera-trafo/</guid><description>Parent: SLAM Index See also: [Pinhole camera model](pinhole camera model.md), pinhole camera-projection-function Source: http://www.cse.psu.edu/~rtc12/CSE486/lecture12.pdf
Camera coordinates (X, Y, Z) World coordinates (U, V, W) Image plane (x, y) / Pixel coordinates (u, v) Forward projection
Representing 2D point as a fictitious 3D point (x', y', z') [for matrix calculations] Convention: Given (x', y', z'), we can recover the 2D point (x, y) as World to camera trafo</description></item><item><title>Camera calibration</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/camera-calibration/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/camera-calibration/</guid><description>Parent: SLAM Index Source: http://de.mathworks.com/help/vision/ug/camera-calibration.html
estimates lens/sensor parameters e.g. to correct lens distortion, determine position, measurement etc there are several camera models, e.g. fisheye, pinhole Camera parameters
intrinsic extrinsic distortion coefficients How to solve for camera parameters?
Need to have 3D world points and the corresponding 2D image points Take multiple images of a calibration pattern to obtain these correspondences With the mapping 3Dp -&amp;gt; 2Dp, solve for camera parameters Evaluate accuracy of estimated camera parameters:</description></item><item><title>Pinhole camera projection function</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/pinhole-camera-projection-function/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/pinhole-camera-projection-function/</guid><description>Backlinks: Pinhole camera model See also: World to camera trafo Source: Mur-Artal 2017 VI-ORB 3D points Projection function  Transforms 3D points into 2D points on image plane Focal length:  Principal point:  The projection does not consider the distortion due to the lens
therefore when extracting image features, first undistort their coordinates only then match to projected points (existing features which have undergone projection from 3D to 2D) Source: Lamarca 2019 DefSLAM 3D point: Projection function maps</description></item><item><title>Pinhole camera model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/pinhole-camera-model/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/pinhole-camera-model/</guid><description>Parent: SLAM Index Backlinks: [Camera calibration](camera calibration.md), [pinhole camera projection function](pinhole camera projection function.md), [weiss thesis vision based navigation for micro helicopters](weiss thesis vision-based-navigation-for-micro-helicopters.md) See also: World to camera trafo Source: http://de.mathworks.com/help/vision/ug/camera-calibration.html Does not account for lens distortion (ideal pinhole camera doesn&amp;rsquo;t have a lens) To represent a real camera, the full camera model to be used should include (radial and tangential) lens distortion, (such as the one used in the MATLAB computer vision toolbox)</description></item><item><title>Monocular cameras</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/monocular-cameras/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/monocular-cameras/</guid><description>Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md) Backlinks: Visual sensors for localisation + Simpler hardware implementation + Smaller and cheapter systems - need complexer algos and software because of lack of direct depth information from a 2D image How is the shape of the map generated?
Integrating measurements in the chain of frames over time Use triangulation method As well as camera motion, if camera isn&amp;rsquo;t stationary Depths of points are not ob monocular depth perception depth-perception.</description></item><item><title>RGB-D cameras</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/rgb-d-cameras/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/rgb-d-cameras/</guid><description>Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md) Backlinks: [Visual sensors for localisation](visual sensors-for-localisation.md), stereo-cameras Provide depth information directly Employed by most of the SLAM systems Generate 3D images through structured light or time of flight technology Structured light camera projects a known pattern onto objects Perceives deformation of pattern by an infrared camera This lets depth and surface information of the objects be calculated Time of flight ToF of a light signal between camera and objects is measured &amp;ndash;&amp;gt; from this, depth is obtained Structured light sensors are sensitive to illumination &amp;ndash; not applicable in direct sunlight Limitations of RGB-D cameras</description></item><item><title>Stereo cameras</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/stereo-cameras/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/stereo-cameras/</guid><description>Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md) Backlinks: Visual sensors for localisation two cameras separated by a fixed distance (baseline) observations of the position of the same 3D point in both cameras allows depth to be calculated through triangulation (like humans do) depth measurement limited by baseline and resolution generally, wider baseline &amp;ndash;&amp;gt; better depth estimate (but occupies more physical space) s.</description></item></channel></rss>