<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>-published on Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/tags/published/</link><description>Recent content in -published on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><lastBuildDate>Tue, 05 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://salehahr.github.io/zettelkasten/tags/published/index.xml" rel="self" type="application/rss+xml"/><item><title>Bladder cancer surgery procedure</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/bladder-cancer-surgery/</link><pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/bladder-cancer-surgery/</guid><description>See also: related-types-of-surgery BCS procedure (according to my understanding)
Cystocopy to inspect the bladder If tumour is found, do resection &amp;ndash;&amp;gt; cryosection Cancer detected &amp;ndash;&amp;gt; tumour removal</description></item><item><title>Quaternion double cover</title><link>https://salehahr.github.io/zettelkasten/math/rotations/quaternion-double-cover/</link><pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/rotations/quaternion-double-cover/</guid><description>Parent: Quaternion index Source: Solà 2017 $$ \mathbf{q} = \left[ \begin{array}{c} q_w\ \mathbf{q}_v \end{array} \right] = \left[ \begin{array}{c} \cos\frac{\phi}{2} \ \mathbf{u} \sin\frac{\phi}{2} \end{array} \right] $$ where $\phi$ is the angle rotated by $\mathbf{q}$ on objects in the 3D space $\mathbb{R}^3$.
Recap $\theta$ is the angle in quaternion space (s. unit quaternions ) $\phi$ as the angle in 3D space $\mathbb{R}^3$ Therefore, the angle is halved in quaternion space. $$\theta = \phi / 2$$</description></item><item><title>(Markley 2014) Fundamentals of Spacecraft Attitude Determination and Control</title><link>https://salehahr.github.io/zettelkasten/bibliography/markley-2014/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/markley-2014/</guid><description>Authors: FL Markley, John Crassidis
DOI: 10.1007/978-1-4939-0802-8
Note/Nomenclature: This book interpetes rotations/transformations in the passive/alias sense (I&amp;rsquo;m not a fan) Quaternions in JPL conventions instead of Hamiltonian (not a fan of this either&amp;hellip;) Rotation matrix = attitude matrix Introduction Attitude determination: memoryless approach without using statistics Attitude estimation: approaches with memory, uses statistical info from a series of measurements filter approaches uses a dynamic motion model of the object Quaternions Quaternion conventions Quaternion multiplication Rotations &amp;ldquo;Euler&amp;rsquo;s theorem: any rotation is a rotation about a fixed axis&amp;rdquo;</description></item><item><title>50.4.2 Multiplicative quaternion filtering (MEKF)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.4.2-multiplicative-quaternion-filtering-mekf/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.4.2-multiplicative-quaternion-filtering-mekf/</guid><description>See also: Which orientation parametrisation to choose? Source: Markley 2014 Main idea is to use
the quaternion as a global rotation representation
a three component state vector as the local representation of rotation errors $$ \begin{aligned} \mathbf{q}\text{tr} &amp;amp;= \delta\mathbf{q} (\delta\mathbf{\theta}) \otimes \mathbf{\hat{q}}\
\mathbf{R}(\mathbf{q}\text{tr}) &amp;amp;= \mathbf{R} (\delta\mathbf{\theta}) \mathbf{R} (\mathbf{\hat{q}}) \end{aligned}$$
each term $(\mathbf{q}_\text{tr},~\delta\mathbf{q},~ \mathbf{\hat{q}})$ is a normalised unit quaternion
Any of the rotation error representations can be used to calculate delta_theta, which is part of the error state of the MEKF.</description></item><item><title>Intrinsic vs extrinsic rotations</title><link>https://salehahr.github.io/zettelkasten/math/rotations/intrinsic-vs-extrinsic-rotations/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/rotations/intrinsic-vs-extrinsic-rotations/</guid><description>Parent: Rotations/SO(3) Group Index See also: Active/passive or Alibi/alias rotation transformations Source: http://rock-learning.github.io/pytransform3d/transformation_ambiguities.html We want to rotate first by $R_1$, then by $R_2$.
Extrinsic (global) rotation In global coordinates, extrinsic rotation: $R_2 \cdot R_1$
Intrinsic (local) rotation In local coordinates, intrinsic rotation: $R_1 \cdot R_2$
($R_1$ defines new coordinates in which $R_2$ is applied)
Specifying the convention is relevant when dealing with Euler angles!!!
Illustration Source: bonn-3D-cs Active translation &amp;ndash;&amp;gt; rotation</description></item><item><title>Quaternion to rotation matrix</title><link>https://salehahr.github.io/zettelkasten/math/rotations/quaternion-to-rotation-matrix/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/rotations/quaternion-to-rotation-matrix/</guid><description>Parents: quaternion-index , rotations-so3-group-index See also: orientation-parametrisations Source: Markley 2014 Unit quaternion to rotation matrix</description></item><item><title>Rotation error representation</title><link>https://salehahr.github.io/zettelkasten/math/rotations/rotation-error-representation/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/rotations/rotation-error-representation/</guid><description>Parents: rotations-so3-group-index , quaternion-index See also: Orientation parametrisations , which orientation parametrisation to-choose? Source: Markley 2014 Note:
Only for small angle approximations! all these representations are equivalent through second order as In terms of&amp;hellip;
Rotation vector Quaternions Euler angles Use upper sign if {i,j,k} even permutation of {1,2,3}, lower sign otherwise
gibbs-rodrigues-parameter . MRPs. etc.</description></item><item><title>Rotations / SO(3) group index</title><link>https://salehahr.github.io/zettelkasten/math/rotations/rotations-so3-group-index/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/rotations/rotations-so3-group-index/</guid><description>Group theory SE(3) Special Euclidian Group SO(3) 3D rotation group Lie group, Lie algebra Exponential map Logarithm map Ambiguities in rotation representations Active/passive or Alibi/alias rotation transformations Intrinsic vs extrinsic rotations Summary
  Extrinsic/Global Intrinsic/Local Active $\mathbf{x}_2 = T_2 T_1 \mathbf{x}_0$ $\mathbf{x}_2 = T_1 T_2 \mathbf{x}_0$ Passive $\mathbf{x}^{\prime\prime} = T_1^{-1}, T_2^{-1} \mathbf{x}$ $\mathbf{x}^{\prime\prime} = T_2^{-1}, T_1^{-1} \mathbf{x}$ Active $\longleftrightarrow$ passive: invert transformation Global $\longleftrightarrow$ local: reverse transformation order Rotation representations Orientation parametrisations Rotation error representation Which orientation parametrisation to choose?</description></item><item><title>Active/passive or Alibi/alias transformations</title><link>https://salehahr.github.io/zettelkasten/math/rotations/active-passive-or-alibi-alias-transformations/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/rotations/active-passive-or-alibi-alias-transformations/</guid><description>Parent: Rotations / SO(3) group index See also: Intrinsic vs extrinsic rotations Sources: Wiki , bonn-3D-cs Alibi / Active Alias / Passive Point $\mathcal{x}$ doesn&amp;rsquo;t change, but the coord vector representation changes to the &amp;lsquo;prime&amp;rsquo; system CS $S(x,, y)$ is fixed CS $S(x,, y)$ is rotated Point $\mathcal{x}$ rotates within fixed CS Point $\mathcal{x}$ remains stationary but is represented within a new CS $\mathcal{x}_1 \rightarrow \mathcal{x}_2$ $S_1(x^\prime,y^\prime) \rightarrow S_2(x^{\prime\prime}, y^{\prime\prime})$ $\left( \begin{array}\cos\theta &amp;amp; -\sin\theta &amp;amp; 0\ \sin\theta &amp;amp; \cos\theta &amp;amp; 0\0 &amp;amp; 0 &amp;amp; 1\end{array} \right)$Counterclockwise rotation by theta $\left( \begin{array}\cos\theta &amp;amp; \sin\theta &amp;amp; 0\ -\sin\theta &amp;amp; \cos\theta &amp;amp; 0\0 &amp;amp; 0 &amp;amp; 1\end{array} \right)$ mathematics physics, robotics Source: http://rock-learning.</description></item><item><title>Euler angles</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/euler-angles/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/euler-angles/</guid><description>Parents: rotations-so3-group-index , orientation-parametrisations Source: Phil&amp;rsquo;s Lab Three angles that describe the orientation of an object w.r.t. a fixed coordinate system Roll $\phi$, Pitch $\theta$, Yaw $\psi$ Source: http://en.wikipedia.org/wiki/Euler_angles Possible representations Proper Euler angles (e.g. $zxz$) vs Tait-Bryan (e.g. $xyz$, $zyx$) Intrinsic vs. extrinsic rotations Extrinsic rotations (around fixed CS $xyz$) Intrinsic rotations (around body CS $XYZ = x''' y''' z'''$) As a rotation matrix $$R = X(\alpha) Y(\beta) Z(\gamma)$$</description></item><item><title>Which orientation parametrisation to choose?</title><link>https://salehahr.github.io/zettelkasten/math/rotations/20.4-which-orientation-parametrisation/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/rotations/20.4-which-orientation-parametrisation/</guid><description>Source: MKok 2017 Estimation algorithms (filtering, smoothing) usually assume that the unknown states and parameters are represented in Euclidean space
Due to wrapping and gimbal lock, Euclidian addition and subtraction don&amp;rsquo;t work Also generally don&amp;rsquo;t work for rotation matrices and unit quaternions Constraints (unit quaternion norm, rotation matrix orthogonality) are usually hard to implement in estimation algorithms These concerns led to the development of the MEKF .
To deal with this: Linearisation of an orientation in SO(3) Alternative method to estimate orientation:</description></item><item><title>Whampsey MEKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/whampsey-mekf/</link><pubDate>Sat, 14 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/whampsey-mekf/</guid><description>Source: http://matthewhampsey.github.io/blog/2020/07/18/mekf
Motivation Working with noisy IMU measurements IMUs usually provide redundant information that can be used to improve dead-reckoning Uses: Hamilton quaternion convention .
Which orientation parametrisation to choose? 50.5-error-state-kalman-filter</description></item><item><title>Exponential map</title><link>https://salehahr.github.io/zettelkasten/math/rotations/exponential-map/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/rotations/exponential-map/</guid><description>Parents: Quaternion index , Rotations / SO(3) group index Notation Variables $$\begin{alignedat}{3} &amp;amp;\phi &amp;amp;&amp;amp;\in \mathbb{R}^3\
&amp;amp;\phi^\wedge &amp;amp;&amp;amp;\in \mathfrak{so}(3)\
&amp;amp;\mathbf{R} &amp;amp;&amp;amp;\in \text{SO}(3)\
\end{alignedat}$$
Functions $$\begin{alignedat}{3} \text{(skew) } \wedge &amp;amp;:&amp;amp; \mathbb{R}^3 &amp;amp;&amp;amp;\rightarrow \mathfrak{so}(3) &amp;amp;\
\exp &amp;amp;:&amp;amp; ~&amp;amp;&amp;amp;\mathfrak{so}(3) &amp;amp;\rightarrow \text{SO}(3)\
\text{Exp} &amp;amp;:&amp;amp; ~\mathbb{R}^3 &amp;amp;&amp;amp; &amp;amp;\rightarrow \text{SO}(3) \end{alignedat}$$
Thus, $$\begin{alignedat}{3} &amp;amp;\exp(\phi^\wedge) = \text{Exp}(\phi) = \mathbf{R} \end{alignedat}$$
Source: Forster 2017 IMU Preintegration At the identity Maps an element of the Lie algebra ($\phi^\wedge \in \mathfrak{so}(3)$, a skew symmetric matrix) to a rotation First order approximation $$ \exp(\phi^\wedge) \approx \mathbf{I} + \phi^\wedge$$</description></item><item><title>Lie group, Lie algebra</title><link>https://salehahr.github.io/zettelkasten/math/rotations/lie-group-lie-algebra/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/rotations/lie-group-lie-algebra/</guid><description>Lie group Parent: Rotations / SO(3) group index Source: http://www.seas.upenn.edu/~meam620/slides/kinematicsI.pdf
A group that is a differentiable (smooth) manifold is called a Lie group.
Lie algebra Source: http://en.wikipedia.org/wiki/3D_rotation_group Lie algebra $\mathfrak{so}(3)$
Every Lie group has an associated Lie algebra Lie algebra: linear space with same dimension as the Lie group Consists of all skew-symmetric 3x3 matrices Elements of the Lie algebra $\mathfrak{so}(3)$ are elements of the tangent space of the manifold SO(3)/Lie group at the identity element .</description></item><item><title>30.1.2.2 Endoscope system components</title><link>https://salehahr.github.io/zettelkasten/permanent/30.1.2.2-endoscope-system-components/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/permanent/30.1.2.2-endoscope-system-components/</guid><description>Source: Leiner Imaging system (rod lens array for rigid endoscopes) within a tube/shaft
Illumination/Light source (it&amp;rsquo;s dark inside the body)
separate from the imaging system in order to reduce glare [low contrast of received image] surrounds the imaging system like a ring light Video camera
Coupling device (camera to imaging system)
Sheath and bridge which contain the telescope, fibre optics, and provides a channel for other stuff like irrigation, forceps, other surgical instruments</description></item><item><title>30.1.2.3 Endoscope specification</title><link>https://salehahr.github.io/zettelkasten/permanent/30.1.2.3-endoscope-specification/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/permanent/30.1.2.3-endoscope-specification/</guid><description>Source: Leiner Field of view: maximum angle that can be viewed Leiner Generally constructed so that the FOV is wide enough so that the &amp;lsquo;head on&amp;rsquo; view is visible even when the DOV is not zero. Leiner This is to reduce the chances of bumping the instrument into anatomy right in front of the shaft. Leiner Direction of view: angular offset of the optical axis from the longitudinal axis of the endoscope shaft</description></item><item><title>Distal and proximal ends</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/distal-and-proximal-ends/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/distal-and-proximal-ends/</guid><description>Source: Leiner distal: far from the surgeon
proximal: near the surgeon</description></item><item><title>Endoscope</title><link>https://salehahr.github.io/zettelkasten/permanent/30.1.2-endoscope/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/permanent/30.1.2-endoscope/</guid><description>Source: NHS The general term for the medical instrument used to perform endoscopy is, correspondingly, the endoscope. It is a device that makes use of optical technology to relay images from one end of the scope to another.
Functions not only for looking inside, but also have additional functionalities such as removing small tissue samples (biopsy) Insertion either through
natural body orifices (e.g. mouth, urethra) small incision in case a keyhole surgery is being performed Subtopics Types of endoscopes Components Specification</description></item><item><title>Endoscopes</title><link>https://salehahr.github.io/zettelkasten/permanent/30-endoscopes-index/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/permanent/30-endoscopes-index/</guid><description> Endoscopy Endoscopes (general) Types of endoscopes Endoscope system components Endoscope specification</description></item><item><title>Endoscopy</title><link>https://salehahr.github.io/zettelkasten/permanent/30.1.1-endoscopy/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/permanent/30.1.1-endoscopy/</guid><description>Source: NHS Backlinks: Endoscopes Endoscopy is a procedure which enables inspection of organs inside the body. The prefix &amp;lsquo;endo&amp;rsquo; comes from the Greek language and means &amp;lsquo;within&amp;rsquo; or &amp;lsquo;inside&amp;rsquo;, cf. the prefix &amp;lsquo;exo&amp;rsquo;/&amp;lsquo;ecto&amp;rsquo; meaning &amp;lsquo;outside&amp;rsquo;. Endo- from Greek ἔνδον (within, inside), cf. exo-/ecto- from έκτός (outside) The instrument: endoscope</description></item><item><title>Types of endoscopes</title><link>https://salehahr.github.io/zettelkasten/permanent/30.1.2.1-types-of-endoscopes/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/permanent/30.1.2.1-types-of-endoscopes/</guid><description>Source: Leiner Endoscopes can be classified according to their flexibility, thus resulting in &amp;lsquo;rigid&amp;rsquo;, &amp;lsquo;flexible&amp;rsquo; and &amp;lsquo;semi-rigid&amp;rsquo; variants. Video sensor at distal (&amp;ldquo;away from the surgeon&amp;rdquo;, opposite of proximal) end allows rigid endoscope to be converted to a flexible one solely by mechanical design Note: the term endoscope in hospital environments typically refers to the flexible variant More specialised endoscopes may be referred to by specific names, such as the cystoscope (for scoping bladders).</description></item><item><title>50.3 IMU motion model in a Kalman filter</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.3-modelling-imu-in-kf/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.3-modelling-imu-in-kf/</guid><description>Parent: IMU index Source: Solà 2017 Quaternion kinematics for ESKF Which states do we use for the motion model?
Choice of states for the IMU motion/kinematics model How do we model the IMU motion?
Choice of model for the IMU motion model The kinematics (true state) can be partitioned into a nominal part and an error part, s. variables in ESKF . The corresponding [nominal state dynamics and error state dynamics](nominal state-dynamics-and-error-state-dynamics.</description></item><item><title>50.3.1 Choice of states for the IMU motion/kinematics model</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.3.1-states-for-imu-motion-model/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.3.1-states-for-imu-motion-model/</guid><description>Parent: IMU index See also: Choice of model for the IMU motion model According to MKok 2017 , we can either
Use the full state vector [+] knowledge about sensor motion is included in model [-] large state vector Or the partial state vector, where the inputs are the inertial measurements from the IMU [+] process noise intuitively represents IMU noise. This is useful when we have no knowledge about the motion model.</description></item><item><title>50.3.2 Choice of model for the KF using IMU readings</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.3.2-imu-model-for-kf/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.3.2-imu-model-for-kf/</guid><description>Parent: IMU index , 50.3-modelling-imu-in-kf According to MKok 2017 , here are some models that assume either a constant acceleration or a constant angular velocity:
Constant acceleration model Constant angular velocity model (Notation: angular velocity of the body with respect to world (n), expressed in body CS)
If motion is unknown, there is also the option of modelling the states using random walk equations.</description></item><item><title>50.5.1.1 States of the ESKF for estimating IMU pose</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.5.1.1-states-of-the-eskf-for-estimating-imu-pose/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.5.1.1-states-of-the-eskf-for-estimating-imu-pose/</guid><description>Parent: IMU index Source: Solà 2017 Quaternion kinematics for ESKF Full state Vector with 19 elements The corresponding kinematics equations/motion model is given in IMU kinematic equations/motion model .
Notes The angular error in 3D space is given by the notation $\delta\mathbf{\theta}$.
(s. rotation-error-representation )
The angular error $\delta\mathbf{\theta}$ is defined locally w.r.t. the nominal orientation (classical approach used in most IMU-integration works).
A more optimal approach may be to use a globally-defined angular error.</description></item><item><title>(Science Focus) How can one eye alone provide depth perception</title><link>https://salehahr.github.io/zettelkasten/bibliography/science-focus/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/science-focus/</guid><description>Source: http://www.sciencefocus.com/the-human-body/how-can-one-eye-alone-provide-depth-perception/
Author: Hilary Guite
In humans with normal binocular vision, depth perception is obtained using the parallax in the two overlapping fields of vision (&amp;ldquo;binocular disparity&amp;rdquo;)
Each single field of vision has a slightly different view to the other
If vision in one eye is impaired, depth perception is still obtainable even with only one eye. Some tricks that the brain uses:
We know the real size of things Using perspective, e.</description></item><item><title>Quaternion conventions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-conventions/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/quaternion-conventions/</guid><description>Parent: Quaternion index Source: Solà 2017 Quaternion kinematics for ESKF Source: [ Wikipedia ], markley-2014 For quaternion multiplication : change the order to transform between conventions Hamilton Shuster Transpose of the Hamiltonian version</description></item><item><title>Quaternion index</title><link>https://salehahr.github.io/zettelkasten/math/rotations/quaternion-index/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/rotations/quaternion-index/</guid><description>Notation Quaternion conventions Basic math/properties Quaternion multiplication Identity quaternion Quaternion conjugate Quaternion norm Inverse quaternion Unit quaternions Double cover Calculus Quaternion differentiation As rotation rotations-so3-group-index exponential-map logarithm-map orientation-parametrisations Which-orientation-parametrisation linearisation-of-an-orientation-in-so-3 Quaternion to rotation matrix Rotation error representation For filtering Additive-quaternion-filtering Multiplicative-quaternion-filtering-mekf Literature Solà 2017 Quaternion kinematics for ESKF MKok 2017 Using inertial sensors for position and orientation estimation</description></item><item><title>Quaternion multiplication</title><link>https://salehahr.github.io/zettelkasten/math/rotations/quaternion-multiplication/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/rotations/quaternion-multiplication/</guid><description>Parent: Quaternion index Source: Solà 2017 Quaternion kinematics for ESKF Here: Hamiltonian convention, s. Quaternion conventions Non-commutative  Associative  Distributive Multiplication as a matrix product With
the matrices the skew operator (skew symmetric matrix) s. also cross product Source: Markley 2014 with the matrices</description></item><item><title>Solà 2017 Quaternion kinematics for ESKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sol%C3%A0-2017-quaternion-kinematics-for-eskf/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sol%C3%A0-2017-quaternion-kinematics-for-eskf/</guid><description>Link: http://www.iri.upc.edu/people/jsola/JoanSola/objectes/notes/kinematics.pdf
Author: Joan Solà
Abstract Primer on quaternion/rotation group math Math for error state Kalman filters using IMUs Contents/Chapters Unit quaternions , double cover Rotations, s. also SO(3) 3D rotation group Quaternion conventions Perturbations, derivatives, integrals Error-State Kalman Filter for IMU-driven systems Variables in ESKF IMU measurement model IMU motion model The initial gravity vector/orientation for the IMU ESKF IMU nominal-state and error-state kinematics IMU ESKF prediction equations Fusing IMU + other sensors ESKF using global angular errors</description></item><item><title>Unit quaternions</title><link>https://salehahr.github.io/zettelkasten/math/rotations/unit-quaternions/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/rotations/unit-quaternions/</guid><description>Parent: Quaternion index , orientation-parametrisations See also: quaternion-conventions , quaternion double cover Source: Solà 2017 Properties $$ \begin{aligned} \left\lVert \mathbf{q} \right\rVert &amp;amp;= 1\
\mathbf{q}^{-1} &amp;amp;= \mathbf{q}^* \end{aligned} $$
Can be written in the form $$ \mathbf{q} = \left[ \begin{array}{c} \cos\theta \ \mathbf{u} \sin\theta \end{array} \right] $$
with
$\mathbf{u}$ as a unit vector $\theta$ is the angle between $\mathbf{q}$ and the identity quaternion $\mathbf{q}_I = \left[\begin{array}{cccc}1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{array}\right]^\text{T}$</description></item><item><title>IMU</title><link>https://salehahr.github.io/zettelkasten/sensors/imu/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/imu/</guid><description>Source: Mur-Artal 2017 measures acceleration (from accelerometer) and angular velocity (from gyrometer) of sensor at regular intervals measurements are affected by sensor noise accelerometer bias gyrometer bias accelerometer is further affected by gravity &amp;ndash;&amp;gt; need to subtract effect of gravity</description></item><item><title>Linearisation of an orientation in SO(3)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/linearisation-of-an-orientation-in-so-3/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/linearisation-of-an-orientation-in-so-3/</guid><description>Parents: Rotations / SO(3) group index , Quaternion index , orientation-parametrisations Source: MKok 2017 Rotation of a vector in SO(3)
The SO(3) group is a Lie group , so there exists an exponential map from a corresponding Lie algebra to the SO(3) group a reverse logarithm map Possible to represent orientations using unit quaternions or rotation matrices in SO(3) — linearisation point orientation deviations $\eta_t$ I think this is a global representation</description></item><item><title>Orientation parametrisations</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/orientation-parametrisations/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/orientation-parametrisations/</guid><description>Parents: rotations-so3-group-index , quaternion index , probabilistic models-for-imu See also: Rotation error representation Source: MKok 2017 , markley-2014 Orientation parametrisations
Note: CCW rotation of a vector $x_v$ to $x_u$ corresponds to a CW rotation of the CS $v$ to CS $u$ (see also active/passive transformations ). Rotations are a member of SO(3) rotation matrix unique description of orientation Euler axis/angle Rotation vector not unique, due to wrapping Euler angles not unique, due to wrapping and gimbal lock Unit quaternions not unique, -q and q depict the same orientationProof: http://math.</description></item><item><title>SO(3) 3D rotation group</title><link>https://salehahr.github.io/zettelkasten/math/rotations/so3-3d-rotation-group/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/rotations/so3-3d-rotation-group/</guid><description>Parent: Rotations / SO(3) group index See also: Orientation parametrisations , Linearisation of an orientation , Solà 2017 quaternion kinematics for eskf Source: MKok 2017 All orthogonal matrices with dim 3x3 have the property
$$RR^\text{T} = R^\text{T}R = I_3$$ They are part of the orthogonal group O(3) If, additionally, $\det R = 1$, then the matrix belongs to SO(3) and is a rotation matrix Source: http://en.wikipedia.org/wiki/3D_rotation_group The SO(3) group</description></item><item><title>Template substitution in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/template-substitution-in-defslam/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/template-substitution-in-defslam/</guid><description>Parent: Mapping step-by-step in DefSLAM Source: lamarca-2020 Tracking runs at frame-rate, and mapping at keyframe-rate Tracking processes Nm frames during a whole mapping run Process New keyframe $k$ is made. Now at time $t=k$ At this point, the template in the tracking is still based on the old shape-at-rest, S_(k-1) Mapping thread starts creates surface S_k which is aligned to prev. template T_(k-1) k is set as the reference keyframe from S_k, create template T_k and from now on use this template instead of the old one T_(k-1) At time t=k+Nm, use data from the tracking thread image points at t=k+Nm deform the recently computed template T_k based on these images use SfT but neglecting the temporal term (to allow large deformation, &amp;ldquo;as a lot might have happened in the time span of Nm&amp;rdquo;) so now we get a T_k that is deformed (updated) to the most recent image points we do this extra step instead of passing T_k (from step 1) to the tracker immediately because, due to the new points occurring at t=k+Nm, using the original T_k might lead to data association errors mapper passes the new template T_k (t=k+Nm) to the tracker</description></item><item><title>DefSLAM framework</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-framework/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-framework/</guid><description>Source: Lamarca 2020 DefSLAM See also: template-substitution-in-defslam &amp;ldquo;Fusion of the methods available for processing non-rigid monocular scenes&amp;rdquo;
Deformation tracking [front end]
estimates/recovers/optimises: camera pose scene deformation / deformation of map points (observations) the map points are then embedded into the template Tk (to compute their position on the surface) operates at frame rate SFT-based ( shape from template ), requires prior geometry (template of scene at rest) for the currently being viewed map Map points are deformed (updated) by solving an optimisation problem min { reprojection error + deformation energy } per frame Deformation mapping [back end]</description></item><item><title>Non-rigid Surface from Motion</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm/</guid><description>Notes:
Original NRSFM paper?
https://www.cs.dartmouth.edu/~lorenzo/Papers/TorrHertzBreg-pami08.pdf A Phd thesis [Kumar] https://openresearch-repository.anu.edu.au/handle/1885/164278?mode=full Source: Kumar
The problem with dynamic or non-rigid scenes:
if we project a scene point into a camera image plane, there will be several possible 3D configurations! Allowing arbitrary deformations makes the 3D reconstruction an ill posed problem (underconstrained) &amp;ndash;&amp;gt; need to make additional assumptions about the object or scene (make more constraints)! Source: lamarca-2020 See also: nrsfm-in-defslam , sfm NRSfM (non-rigid structure from motion) batch processing of images to recover deformation computationally demanding — slower than SfT Orthographic NRSfM usually fails with very large deformations uses an orthographic camera projection/model (weak approximation to the perspective camera) — a limitation, as many vision-related applications have a significant perspective effect exploits spatial constraints temporal constraints spatiotemporal constraints usually ok for small deformations, but not for very large deformations Perspective NRSfM the perspective camera model is more accurate than the orthographic one also uses the isometry assumption (as in SfT methods), which has produced good results in NRSfM Parashar 2018 &amp;ldquo;Isometric NRSfM&amp;rdquo; [6] local method that handles occlusions and missing data well</description></item><item><title>NRSfM in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm-in-defslam/</guid><description>Parent: Mapping step-by-step in DefSLAM Source: lamarca-2020 See also: NRSfM Assumptions Isometric deformation Infinitesimal planarity [DEF]: any surface can be approximated as a plane at infinitesimal level, all the while maintaining its curvature at a global level Locality The method used here is a local method &amp;ndash;&amp;gt; implies that it handles missing data and occlusions inherently
surface deformation is modelled locally for each point, under the above assumptions Embedding, $\phi_k$ of the scene surface is a parametrisation — transforms an image point to a point on a 3D surface uses the normalised coordinates of the image Ik (xhat, yhat) Procedure A point is matched in more than two keyframes (warps are used in the matching process )</description></item><item><title>Shape from Motion</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sft/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sft/</guid><description>Initial paper?: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010934https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010934
Goal reconstruct the surface of an object
reference 3D shape (template) of the object is available under a specific deformation constraint Source: lamarca-2020 SFT (shape from template) uses only a single image — faster than nrsfm lower computational cost must have a known 3D template (textured model) SfT methods require: 1 monocular image 1 textured shape at rest (template) &amp;ldquo;geometry&amp;rdquo; as the deformation model different definitions of the deformation model analytic, e.</description></item><item><title>(Mur-Artal 2017) VI-ORB</title><link>https://salehahr.github.io/zettelkasten/bibliography/mur-artal-2017-vi-orb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/mur-artal-2017-vi-orb/</guid><description>URL: http://ieeexplore.ieee.org/abstract/document/7817784
Authors: Mur-Artal, Tardós
Code: http://paperswithcode.com/paper/visual-inertial-monocular-slam-with-map-reuse
Results (video): http://www.youtube.com/watch?v=JXRCSovuxbA
Abstract current VI odometry approaches: drift accumulates due to lack of loop closure therefore there is a need for tightly-coupled VI-SLAM with loop closure and map reuse here: focus on monocular case, but applicable to other camera configurations builds on ORB-SLAM (from same author) IMU initialisation method (initialises: scale, gravity direction, velocities, gyroscope bias, accelerometer bias) depends on visual monocular initialisation (coupled initialisation) Other works: recent tightly-coupled VIO (both filtering- and optimisation-based) lack loop closure, so drift accumulates</description></item><item><title>Why use the visual-inertial sensor combination?</title><link>https://salehahr.github.io/zettelkasten/SLAM/why-use-the-visual-inertial-sensor-combination/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/why-use-the-visual-inertial-sensor-combination/</guid><description>See also: Multisensor fusion Source: Mur-Artal 2017 VI-ORB Cheap but also with good potential Cameras provide rich information but are relatively cheap IMU provides self-motion info, helps recover scale in monocular applications enables estimation of the direction of gravity &amp;ndash;&amp;gt; renders pitch and roll observable Source: Forster 2017 IMU Preintegration Visual-inertial fusion for 3D structure and motion estimation Both cameras and IMUs are cheap, easy to find and complement each other well Camera exteroceptive sensor measures, up to a to-be-determined metric scale, appearance and geometrical structure of a 3D scene IMU interoceptive sensor makes metric scale of monocular cameras, as well as the direction of gravity, observable Source: (Wu 2018) Image-based camera localization Cameras provide rich information of a scene IMU provide odometry self-motion information and accurate short-term motion estimates at high frequency Source: Mirzaei 2008 A Kalman Filter-Based Algorithm for IMU-Camera Calibration: Observability Analysis and Performance Evaluation</description></item><item><title>Structure from Motion</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sfm/</link><pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sfm/</guid><description>Source: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Schonberger_Structure-From-Motion_Revisited_CVPR_2016_paper.pdf
Note:
This paper uses incremental SfM Corresponding paper for COLMAP Structure from motion Reconstruction of 3D structure from a sequence of 2D images of that structure, taken from different viewpoints.
Search for correspondence between images &amp;ndash;&amp;gt; output: scene graph (nodes: images, edges: verified pairs) Feature extraction Feature matching Output: set of image pairs and their associated feature correspondences Verification: do features map to the same scene point?</description></item><item><title>Chen 2018 Review of VI SLAM</title><link>https://salehahr.github.io/zettelkasten/bibliography/chen-2018-review/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/chen-2018-review/</guid><description>Source: http://www.mdpi.com/2218-6581/7/3/45
Authors: Chen et. al
Abstract Survey on visual-inertial SLAM over the last 10 years Aspects: filtering vs optimisation based, camera type, sensor fusion type Explains core theory of SLAM, feature extraction, feature tracking, loop closure Experimental comparison of filtering-based and optimisation-based methods Research trends for VI-SLAM Recommended other works s. Works of possible interest Contents/Chapters SLAM SLAM: build a real-time map of the unknown environment based on sensor data, while the sensor (robot) itself is traversing the environment</description></item><item><title>Visual sensors for localisation</title><link>https://salehahr.github.io/zettelkasten/sensors/visual-sensors-for-localisation/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/visual-sensors-for-localisation/</guid><description>Source: Wikipedia Visual odometry Process of determining robot POSE by analysing the associated camera images Use sequential camera image to estimate the distance travelled Applications: robotics, computer vision
Source: Cometlabs Types
Monocular cameras Stereo cameras RGB-D cameras Provide rich visual information, but for that, higher computational cost
Source: SLAM for Dummies stereo or triclops sytem to measure distance [+] possibly more intuitive (bc humans use vision), more info.</description></item><item><title>Odometry</title><link>https://salehahr.github.io/zettelkasten/definitions/odometry/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/definitions/odometry/</guid><description>See also: Egomotion (vs odometry) Source: https://en.wikipedia.org/wiki/Dead_reckoning In navigation, dead reckoning is the process of calculating one&amp;rsquo;s current position by using a previously determined position, by using estimations of speed and course over elapsed time
s. Brian Douglas video on sensor fusion Source: Wikipedia Visual odometry Data can be generated from actuator movements, e.g. rotary encoders that measure motor shaft rotations This data can be used to estimate changes in position over time Usually has precision problems, e.</description></item><item><title>Cryosection</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cryosection/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cryosection/</guid><description>Source: https://en.wikipedia.org/wiki/Frozen_section_procedure aka frozen section procedure allows rapid analysis of a dissected/resected specimen during the course of surgery the specimen is frozen rapidly and brought to a lab for analysis the results are relayed to the surgeon by intercom benign or malignant when operating on a previously confirmed malignant tissue, information on whether residual cancer was found on the [resection margin](resection margin.md) of the tissue the surgeon makes his decision on how to continue the operation based on the results</description></item><item><title>Cystocopy</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cystocopy/</link><pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cystocopy/</guid><description>Source: https://en.wikipedia.org/wiki/Cystoscopy
Backlinks: [Some questions](Some questions.md)
Endoscopy of the bladder via the urethra Tool involved: cystoscope https://www.youtube.com/watch?v=ybhzlW7ivro
Video of cystocopy and bladder biopsy Modern Latin for bladder: cystis</description></item><item><title>Works of possible interest</title><link>https://salehahr.github.io/zettelkasten/bibliography/works-of-interest/</link><pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/works-of-interest/</guid><description>General SLAM Cadena 2016 &amp;ndash; Past, Present, and Future of SLAM durrant-whyte 2006 slam tutorial part i Prerequisites g2o paper - graph-based SLAM Existing SLAM algorithms MonoSLAM, works by Andrew Davison focusing on fusion instead of vision-only SLAM
Maplab (filtering-based) not looking at filtering-based algos
mentioned in the Chen 2018 Review of VI SLAM paper:
ORB-SLAM paper — ORB features VIORB implementation ORB-SLAM3 (improves on ORBSLAM, incl.</description></item><item><title>Distance between landmarks</title><link>https://salehahr.github.io/zettelkasten/SLAM/distance-between-landmarks/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/distance-between-landmarks/</guid><description>Source: SLAM for Dummies Methods for calculating distance between landmarks :
Euclidean distance (suitable for far distances) Mahalanobis distance (better, but more complex)</description></item><item><title>SLAM Index</title><link>https://salehahr.github.io/zettelkasten/SLAM/slam_index/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/slam_index/</guid><description>Definition Localisation What is SLAM? Sensors for SLAM Position acquisition (relative vs. absolute) SLAM hardware Relative Odometry IMU Absolute Sensors (absolute measurements) for measuring distance to landmarks Visual sensors for localisation Monocular depth perception Pinhole camera model Camera calibration World to camera trafo Fusion Multisensor fusion Loose vs Tight coupling Why use the visual-inertial sensor combination?</description></item><item><title>Feature-based vs direct SLAM workflow</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/feature-based-vs-direct-slam-workflow/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/feature-based-vs-direct-slam-workflow/</guid><description>Parent: SLAM Index Source: cometlabs Feature-based (aka sparse ) direct (aka dense ) Extraction of features required No abstraction necessary Aims to minimise error between point location estimate (from odometry) and location based on camera Tracks objects by minimising photometric error (intensity differences)</description></item><item><title>Loop closure detection</title><link>https://salehahr.github.io/zettelkasten/SLAM/loop-closure-detection/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/loop-closure-detection/</guid><description>Source: cometlabs See also: loop-closing-in-viorb Loop closure Process of observing the same scene by non-adjacent frames and adding a constraint (relationship? association?) between them A long-term data association in the VSLAM Framework (part of front end) Sort of incorporates topological SLAM into metric SLAM Importance Final refinement step (in data association) Important for obtaining a globally consistent SLAM solution, especially when optimising over a long period of time Basic loop closure detection Match the current frame to all previous frames using feature matching</description></item><item><title>(Wu 2018) Image-based camera localization</title><link>https://salehahr.github.io/zettelkasten/bibliography/wu-2018/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/wu-2018/</guid><description>Authors: Wu, Tang, Li
Abstract/Contents overview (classification) of image-based camera localization classification of image-based camera localization approaches techniques, trends only considers 2D cameras focuses on points as features in images (not lines etc) Chapters Classification of image-based camera localization approaches Multisensor fusion — why use the visual-inertial sensor combination? Loose vs Tight coupling Filter localisation methods Some optimisation-based tightly-coupled multisensor SLAM algorithms Questions What&amp;rsquo;s a metric map &amp;ndash; normal map (with landmarks, normal distances) as opposed to a topological one Takeaway Learning SLAM is gaining in popularity, but geometric SLAM is often the chosen method for most applications, as it is more generalisable and at the same time reasonably accurate For reliability and low cost practical applications, multisensor vision-centred fusion is an effective method.</description></item><item><title>Cometlabs What You Need to Know About SLAM</title><link>https://salehahr.github.io/zettelkasten/bibliography/cometlabs/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/cometlabs/</guid><description>Source: http://blog.cometlabs.io/teaching-robots-presence-what-you-need-to-know-about-slam-9bf0ca037553
SLAM chicken and egg problem Position acquisition Multisensor fusion Sensors (absolute measurements) for measuring distance to landmarks Mapping representations in robotics Visual SLAM Implementation Framework Feature-based vs direct SLAM workflow Sparse/Feature-based VSLAM Dense/direct VSLAM</description></item><item><title>Dense/direct VSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/dense-direct-vslam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/dense-direct-vslam/</guid><description>Parent: Visual SLAM Implementation Framework , slam_index See also: Feature-based vs direct SLAM workflow Source: cometlabs Front-end part of the Visual SLAM Implementation Framework Use most or all of the pixels in each received frame Provide more information about the environment Many more pixels, require GPUs Feature-based vs direct SLAM workflow Disadvantages: Don&amp;rsquo;t handle outliers very well (outliers will be processed and implemented into the final map) Slower than feature-based variants Aims to minimise photometric error (intensity differences) Semi-dense</description></item><item><title>Monocular cameras</title><link>https://salehahr.github.io/zettelkasten/sensors/monocular-cameras/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/monocular-cameras/</guid><description>Source: Cometlabs + Simpler hardware implementation + Smaller and cheapter systems - need complexer algos and software because of lack of direct depth information from a 2D image How is the shape of the map generated? Integrating measurements in the chain of frames over time Use triangulation method As well as camera motion, if camera isn&amp;rsquo;t stationary Depths of points are not oberved directly (s.</description></item><item><title>Position acquisition (relative vs. absolute)</title><link>https://salehahr.github.io/zettelkasten/sensors/position-acquisition/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/position-acquisition/</guid><description>See also: SLAM hardware Source: Cometlabs relative (interoceptive sensors) odometry absolute (exteroceptive sensors) can be used alongside relative measurement sensors in order to correct odometry drift beacons direct measurement instead of integrating, therefore error in position does not grow unbounded e.g. laser ranger finders, wifi (collect signal strength across field), GPS (bad for indoors) Lidar, Ultra Wide Band (UWB), Wireless Fidelity, etc [ Wu ] Compared to these, cameras are flexible and low-cost [ Wu ] (are also passive sensors) [ Cometlabs ]</description></item><item><title>Sensors (absolute measurements) for measuring distance to landmarks</title><link>https://salehahr.github.io/zettelkasten/sensors/sensors-absolute/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/sensors-absolute/</guid><description>Parents: slam-hardware Source: Cometlabs Acoustic (Time of Flight) ToF technique Surfaces need to have good acoustic reflection Lack the ability to use surface properties for localisation examples Sonar Ultrasonic, ultrasound Laser rangerfinders ToF and phase-shift techniques Lack the ability to use surface properties for localisation e.g. Lidar Visual sensors for localisation Source: SLAM for Dummies Laser scanners Sonar, usually polaroid sonar [+] cheaper, good for underwater (s.</description></item><item><title>Some optimisation-based tightly-coupled multisensor SLAM algorithms</title><link>https://salehahr.github.io/zettelkasten/SLAM/algos-optimisation-based/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/algos-optimisation-based/</guid><description>Source: Wu 2018 Uses nonlinear optimization may potentially achieve higher accuracy due to the capability to limit linearization errors through repeated linearization of the inherently nonlinear problem [117] Forster 2017 : preintegration theory [118] OKVIS: a novel approach to tightly integrate visual measurements with IMU optimise a joint nonlinear cost function that integrates an IMU error term with the landmark reprojection error in a fully probabilistic manner real-time operation: old states are marginalized to maintain a bounded-sized optimization window Li et al.</description></item><item><title>Sparse/Feature-based VSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sparse-feature-based-vslam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sparse-feature-based-vslam/</guid><description>Parent: Visual SLAM Implementation Framework , slam_index See also: Feature-based vs direct SLAM workflow Source: cometlabs Front-end part of the Visual SLAM Implementation Framework Use only a small selected subset of the pixels in an image frame Feature maps generated are point clouds &amp;ndash;&amp;gt; used to track the camera pose Requires feature extraction and matching To minimise: reprojection error (difference between a point&amp;rsquo;s tracked location and where it is expected to be given camera pose estimate) Pose estimation based on RANSAC A frame with most of its features concentrated in a small area: bad as the features are more likely to overlap Sparse</description></item><item><title>Visual SLAM Implementation Framework</title><link>https://salehahr.github.io/zettelkasten/SLAM/vslam-framework/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/vslam-framework/</guid><description>Source: Cometlabs Basic principle: tracking a set of points through successive frames these tracks are used to triangulate the 3D positions of the points to create the map at the same time, using the the est point locations to calculate the pose of the camera, which could have observed them (i.e. calculate real time 3D structure of a scene from the estimated motion of the camera) Architecture Front-end Abstracts sensor data into models (which are good for estimation) / Processing Data association Short term (feature tracking); features in consecutive sensor measurements Either from sparse maps or dense-maps Long term ( loop closure ; associating new measurements to older landmarks Back-end Performs inference on the abstracted data produced by the front end</description></item><item><title>What is SLAM?</title><link>https://salehahr.github.io/zettelkasten/SLAM/what-is-slam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/what-is-slam/</guid><description>Parent: SLAM index See also: slam-hardware Source: Scaradozzi 2018 Process which allows a mobile robot to
construct a map of its environment (assumed to be unknown) compute its location using the map simultaneously Source: Lamarca 2020 Goal is to locate a sensor in an unknown map/environment, which is simultaneously being reconstructed. Typically used in exploratory trajectories (new or changing environments) Source: Wikipedia SLAM Simultaneous localization and mapping (SLAM)</description></item><item><title>50.2.1 Process noise Q and W (odometry)</title><link>https://salehahr.github.io/zettelkasten/SLAM/50.2.1-process-noise-q-and-w-odometry/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/50.2.1-process-noise-q-and-w-odometry/</guid><description>See also: Factors affecting Kalman filter performance Source: Tereshkov 2015 Process noise covariance matrix has no clear physical meaning, cannot be deduced from sensor characteristics Leads to non-intuitive, iterative procedures to tune KFs Which means that KF optimality is rarely achieved in practice Alternative to KF tuning: the use of geometric observers
estimates are expresssed only in terms of quantities with clear geometrical meaning Source: Schneider 2013 If perfect model: $Q$ only describes the covariance of the random process noise Not perfect model, has: parametric errors (-&amp;gt; parameter identification) structural erors (error in model structure) workaround: e.</description></item><item><title>Covariance matrix P</title><link>https://salehahr.github.io/zettelkasten/SLAM/covariance-matrix-p/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/covariance-matrix-p/</guid><description>Source: SLAM for Dummies s. also EKF matrices Covariance matrix P
Covariance: measure of correlation of two variables Correlation: measure of degree of linear dependence A covariance of the robote POSEupdated in Step 1: Odometry update 3x3 B .. C covariance on the first .. nth landmarkStep 3: New landmarks 2x2 D covariance between POSE and first LMupdated in Step 1: Odometry update 2x3 E, etc E = D^T, etcupdated in Step 1: Odometry update 3x2 F=G^T Step 3: New landmarks Initially $P = A$ (robot has not seen any LMs)</description></item><item><title>Step 1 Odometry update (Prediction step)</title><link>https://salehahr.github.io/zettelkasten/SLAM/ekf-1-prediction/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/ekf-1-prediction/</guid><description>Parent: Basic EKF for SLAM Source: SLAM for Dummies First step in the three-step EKF Update current state using odometry data Based on the controls given to the robot Calculate estimate of new POSE Update equation: prediction model ($x = x + \Delta x \cdot q$)
Or in a simple model, neglect the error term $q$
State vector gets updated via the prediction model
Jacobian of the prediction model also needs to be updated every iteration (with the controls deltax, &amp;hellip;)</description></item><item><title>Basic EKF for SLAM</title><link>https://salehahr.github.io/zettelkasten/SLAM/basic-ekf-for-slam/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/basic-ekf-for-slam/</guid><description>Source: SLAM for Dummies A basic EKF implementation of SLAM consists of multiple parts:
Landmark extraction Data association After odometry change (due to robot moving), state estimation from odometry Update of the estimated state using re-observed landmark data Update landmark database with new landmarks Note: at any point in the three steps on the left, the EKF will have an estimate of the robots current position</description></item><item><title>Data association</title><link>https://salehahr.github.io/zettelkasten/SLAM/data-association/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/data-association/</guid><description>Source: SLAM for Dummies Matching observed landmarks from different scans (different time steps) with each other. Also called &amp;rsquo;re-observing' landmarks.
Problems that can arise The landmark(s) might not be observed every time step (bad landmark) Something might be observed as a landmark, but it never appears again (bad landmark) Wrong association of a landmark to a previously seen landmark Goal Define a suitable data-association policy to minimise the first two problems</description></item><item><title>Landmarks</title><link>https://salehahr.github.io/zettelkasten/SLAM/landmarks/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/landmarks/</guid><description>Source: SLAM for Dummies Features which can easily be re-observed and distinguished from the environment
Characteristics:
Re-observable from different positions and angles Unique (i.e. no mix-up with other landmarks) Plentiful &amp;ndash; should not be so few that robot gets lost (robot spends extended time w/o enough visible landmarks) Stationary Basic landmark extraction</description></item><item><title>Riisgaard SLAM for dummies</title><link>https://salehahr.github.io/zettelkasten/bibliography/riisgaard-slam-for-dummies/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/riisgaard-slam-for-dummies/</guid><description>Authors: Søren Riisgaard and Morten Rufus Blas Parent: SLAM resources Abstract:
Tutorial introduction to SLAM, with minimal prerequisites for the understanding of SLAM as explained here Mostly explains a single approach to the steps involved in SLAM Complete solution for SLAM using EKF (extended Kalman filter) Only considers 2D motion, not 3D Chapters
What is SLAM? Overview of SLAM using EKF Hardware Robot Range measurement device SLAM process Step 1: Odometry update Step 2: Reobservation Step 3: Add new landmarks Laser data Odometry data Landmarks Landmark extraction 1.</description></item><item><title>SLAM hardware</title><link>https://salehahr.github.io/zettelkasten/sensors/slam-hardware/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/sensors/slam-hardware/</guid><description>Parent: SLAM index See also: Position acquisition (relative vs. absolute) Source: SLAM for Dummies Robot parameters to consider Ease of use Odometry performance: how well the robot can estimate its own position, just from the rotation of the wheels Max errors: 2cm per meter moved, 2deg per 45deg turned Bad odometry &amp;ndash;&amp;gt; bad estimation of current position &amp;ndash;&amp;gt; hard to implement SLAM Range measurement device options</description></item><item><title>Registration</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/registration/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/registration/</guid><description> Aligning two points, each in different spaces respectively, together Examples aligning an endoscope coordinate frame to CT data (based on a similarity metric between endoscopic image and CT image) &amp;ndash; from https://pubmed.ncbi.nlm.nih.gov/25991876/ match virtual surface to corresponding endoscopic video &amp;ndash; https://ieeexplore.ieee.org/document/958638 Parent: SofaPython Index allows a matching between deformable surfaces finds spatial transformations to align two point sets or two meshes done based on: either target surfaces (ClosestPointRegistrationForceField , RegistrationContactForceField) or target images (IntensityProfileRegistrationForceField), which requires the use of the image plugin</description></item></channel></rss>