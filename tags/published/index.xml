<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>-published on Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/tags/published/</link><description>Recent content in -published on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><lastBuildDate>Mon, 17 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://salehahr.github.io/zettelkasten/tags/published/index.xml" rel="self" type="application/rss+xml"/><item><title>(Science Focus) How can one eye alone provide depth perception</title><link>https://salehahr.github.io/zettelkasten/bibliography/science-focus/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/science-focus/</guid><description>Source: http://www.sciencefocus.com/the-human-body/how-can-one-eye-alone-provide-depth-perception/
Author: Hilary Guite
In humans with normal binocular vision, depth perception is obtained using the parallax in the two overlapping fields of vision (&amp;ldquo;binocular disparity&amp;rdquo;)
Each single field of vision has a slightly different view to the other If vision in one eye is impaired, depth perception is still obtainable even with only one eye. Some tricks that the brain uses:
We know the real size of things Using perspective, e.</description></item><item><title>Monocular depth perception</title><link>https://salehahr.github.io/zettelkasten/permanent/10-monocular-depth-perception/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/permanent/10-monocular-depth-perception/</guid><description>Depth perception in real life In nature, prey animals typically have eyes on either side of their head to maximise field of view, while most predators have forward-facing eyes with overlapping fields of vision (binocular vision) for maximum depth perception. Humans also have binocular vision. (Some exceptions: fruit bats, killer whales)
We perceive depth, or distance to the objects that we see, based on several visual cues.</description></item><item><title>Parallax</title><link>https://salehahr.github.io/zettelkasten/definitions/parallax/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/definitions/parallax/</guid><description>Source: https://en.wikipedia.org/wiki/Parallax
Definition: The difference in the apparent position of an object viewed from two different positions
This difference is given by the angle between the two lines of sight Binocular vision uses parallax in the overlapping fields of vision in order to gain depth perception Distance measurement (i.e. depth from the viewer) via parallax is based on the principle of triangulation (uses trigonometry, s. Monocular depth perception in humans )</description></item><item><title>(Mur-Artal 2017) VI-ORB</title><link>https://salehahr.github.io/zettelkasten/bibliography/mur-artal-2017-vi-orb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/mur-artal-2017-vi-orb/</guid><description>URL: http://ieeexplore.ieee.org/abstract/document/7817784
Authors: Mur-Artal, Tardós
Code: http://paperswithcode.com/paper/visual-inertial-monocular-slam-with-map-reuse
Results (video): http://www.youtube.com/watch?v=JXRCSovuxbA
Abstract current VI odometry approaches: drift accumulates due to lack of loop closure therefore there is a need for tightly-coupled VI-SLAM with loop closure and map reuse here: focus on monocular case, but applicable to other camera configurations builds on ORB-SLAM (from same author) IMU initialisation method (initialises: scale, gravity direction, velocities, gyroscope bias, accelerometer bias) depends on visual monocular initialisation (coupled initialisation) Other works: recent tightly-coupled VIO (both filtering- and optimisation-based) lack loop closure, so drift accumulates</description></item><item><title>Loop closing in VIORB</title><link>https://salehahr.github.io/zettelkasten/SLAM/loop-closing-in-viorb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/loop-closing-in-viorb/</guid><description>Source: Mur-Artal 2017 VI-ORB See also: Loop closure detection (general) Overview To reduce drift accumulated during exploration (when returning to an already mapped location) Loop detection: of large loops using place recognition Loop correction: first do lightweight pose-graph optimisation (PGO), then do full BA in a separate thread (in order not to interfere with real-time operations) Implementation After loop detection: do match validation (alignment of points between keyframes) Then pose-graph optimisation to reduce the accumulated error in trajectory (PGO: pose-only, ignores IMU info) IMU info ignored, but velocities are corrected by rotating them according to keyframe orientation &amp;ndash;&amp;gt; suboptimal, but should be accurate enough to allow IMU data to be used right after the PGO in ORBSLAM: PGO is 7-DoF optimisation (due to scale + 3 rot + 3 xyz) in VIORB, 6 DoF (scale is known from initialisation bzw.</description></item><item><title>Why use the visual-inertial sensor combination?</title><link>https://salehahr.github.io/zettelkasten/SLAM/why-use-the-visual-inertial-sensor-combination/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/why-use-the-visual-inertial-sensor-combination/</guid><description>See also: Multisensor fusion Source: Mur-Artal 2017 VI-ORB Cheap but also with good potential Cameras provide rich information but are relatively cheap IMU provides self-motion info, helps recover scale in monocular applications enables estimation of the direction of gravity &amp;ndash;&amp;gt; renders pitch and roll observable Source: Forster 2017 IMU Preintegration Visual-inertial fusion for 3D structure and motion estimation Both cameras and IMUs are cheap, easy to find and complement each other well Camera exteroceptive sensor measures, up to a to-be-determined metric scale, appearance and geometrical structure of a 3D scene IMU interoceptive sensor makes metric scale of monocular cameras, as well as the direction of gravity, observable Source: (Wu 2018) Image-based camera localization Cameras provide rich information of a scene IMU provide odometry self-motion information and accurate short-term motion estimates at high frequency Source: Mirzaei 2008 A Kalman Filter-Based Algorithm for IMU-Camera Calibration: Observability Analysis and Performance Evaluation</description></item><item><title>Chen 2018 Review of VI SLAM</title><link>https://salehahr.github.io/zettelkasten/bibliography/chen-2018-review/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/chen-2018-review/</guid><description>Source: http://www.mdpi.com/2218-6581/7/3/45
Authors: Chen et. al
Abstract Survey on visual-inertial SLAM over the last 10 years Aspects: filtering vs optimisation based, camera type, sensor fusion type Explains core theory of SLAM, feature extraction, feature tracking, loop closure Experimental comparison of filtering-based and optimisation-based methods Research trends for VI-SLAM Recommended other works s. Works of possible interest Contents/Chapters SLAM SLAM: build a real-time map of the unknown environment based on sensor data, while the sensor (robot) itself is traversing the environment</description></item><item><title>Works of possible interest</title><link>https://salehahr.github.io/zettelkasten/bibliography/works-of-possible-interest/</link><pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/works-of-possible-interest/</guid><description>General SLAM Cadena 2016 &amp;ndash; Past, Present, and Future of SLAM durrant-whyte 2006 slam tutorial part i Prerequisites g2o paper - graph-based SLAM Existing SLAM algorithms MonoSLAM, works by Andrew Davison focusing on fusion instead of vision-only SLAM
Maplab (filtering-based) not looking at filtering-based algos
mentioned in the Chen 2018 Review of VI SLAM paper:
ORB-SLAM paper — ORB features VIORB implementation ORB-SLAM3 (improves on ORBSLAM, incl.</description></item><item><title>Key frames in loop closure detection</title><link>https://salehahr.github.io/zettelkasten/SLAM/key-frames-in-loop-closure-detection/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/key-frames-in-loop-closure-detection/</guid><description>Source: cometlabs Most common method to get candidate key frames: use a place recognition approach
approach based on vocab tree feature descriptors of candidate key frames are quantised one colour in image below corresponds to one feature descriptor/&amp;lsquo;vocabulary&amp;rsquo; each point is a &amp;lsquo;word&amp;rsquo; that belongs to a vocabulary the words can then be counted and put into a frequency histogram the histogram is used to compare similarity of images I think similar images then get filtered out, so we get key frames</description></item><item><title>Loop closure detection</title><link>https://salehahr.github.io/zettelkasten/SLAM/loop-closure-detection/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/loop-closure-detection/</guid><description>Source: cometlabs Loop closure Process of observing the same scene by non-adjacent frames and adding a constraint (relationship? association?) between them A long-term data association in the VSLAM Framework (part of front end) Sort of incorporates topological SLAM into metric SLAM Importance Final refinement step (in data association) Important for obtaining a globally consistent SLAM solution, especially when optimising over a long period of time Basic loop closure detection Match the current frame to all previous frames using feature matching</description></item><item><title>(Wu 2018) Image-based camera localization</title><link>https://salehahr.github.io/zettelkasten/bibliography/wu-2018/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/wu-2018/</guid><description>Authors: Wu, Tang, Li
Abstract/Contents overview (classification) of image-based camera localization classification of image-based camera localization approaches techniques, trends only considers 2D cameras focuses on points as features in images (not lines etc) Chapters Classification of image-based camera localization approaches Multisensor fusion — why use the visual-inertial sensor combination? Loose vs Tight coupling Filter localisation methods Some optimisation-based tightly-coupled multisensor SLAM algorithms Questions What&amp;rsquo;s a metric map &amp;ndash; normal map (with landmarks, normal distances) as opposed to a topological one Takeaway Learning SLAM is gaining in popularity, but geometric SLAM is often the chosen method for most applications, as it is more generalisable and at the same time reasonably accurate For reliability and low cost practical applications, multisensor vision-centred fusion is an effective method.</description></item><item><title>Cometlabs What You Need to Know About SLAM</title><link>https://salehahr.github.io/zettelkasten/bibliography/cometlabs/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/cometlabs/</guid><description>Source: http://blog.cometlabs.io/teaching-robots-presence-what-you-need-to-know-about-slam-9bf0ca037553
SLAM chicken and egg problem Position acquisition Multisensor fusion Sensors (absolute measurements) for measuring distance to landmarks Mapping representations in robotics Visual SLAM Implementation Framework Feature-based vs direct SLAM workflow Sparse/Feature-based VSLAM Dense/direct VSLAM</description></item><item><title>Monocular cameras</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/monocular-cameras/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/monocular-cameras/</guid><description>Source: Cometlabs + Simpler hardware implementation + Smaller and cheapter systems - need complexer algos and software because of lack of direct depth information from a 2D image How is the shape of the map generated? Integrating measurements in the chain of frames over time Use triangulation method As well as camera motion, if camera isn&amp;rsquo;t stationary Depths of points are not oberved directly (s.</description></item><item><title>Some optimisation-based tightly-coupled multisensor SLAM algorithms</title><link>https://salehahr.github.io/zettelkasten/SLAM/algos-optimisation-based/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/algos-optimisation-based/</guid><description>Source: Wu 2018 Uses nonlinear optimization may potentially achieve higslamher accuracy due to the capability to limit linearization errors through repeated linearization of the inherently nonlinear problem
[117] Forster: preintegration theory [118] OKVIS: a novel approach to tightly integrate visual measurements with IMU optimise a joint nonlinear cost function that integrates an IMU error term with the landmark reprojection error in a fully probabilistic manner real-time operation: old states are marginalized to maintain a bounded-sized optimization window Li et al.</description></item><item><title>Visual SLAM Implementation Framework</title><link>https://salehahr.github.io/zettelkasten/SLAM/vslam-framework/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/vslam-framework/</guid><description>Source: Cometlabs Basic principle: tracking a set of points through successive frames these tracks are used to triangulate the 3D positions of the points to create the map at the same time, using the the est point locations to calculate the pose of the camera, which could have observed them (i.e. calculate real time 3D structure of a scene from the estimated motion of the camera) Architecture Front-end Abstracts sensor data into models (which are good for estimation) / Processing Data association Short term (feature tracking); features in consecutive sensor measurements Either from sparse maps or dense-maps Long term ( loop closure ; associating new measurements to older landmarks Back-end Performs inference on the abstracted data produced by the front end</description></item><item><title>What is SLAM?</title><link>https://salehahr.github.io/zettelkasten/SLAM/what-is-slam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/what-is-slam/</guid><description>Source: Scaradozzi 2018 Process which allows a mobile robot to
construct a map of its environment (assumed to be unknown) compute its location using the map simultaneously Source: Lamarca 2020 Goal is to locate a sensor in an unknown map/environment, which is simultaneously being reconstructed. Typically used in exploratory trajectories (new or changing environments) Source: Wikipedia SLAM Simultaneous localization and mapping (SLAM)
computational problem construct/update a map of an unknown environment simultaneously keep track of an agent&amp;rsquo;s location within it Source: SLAM for Dummies Goal: is to use the environment to update robot position</description></item></channel></rss>