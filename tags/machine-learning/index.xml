<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>machine-learning on Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/tags/machine-learning/</link><description>Recent content in machine-learning on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><lastBuildDate>Sun, 28 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://salehahr.github.io/zettelkasten/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Backpropagation</title><link>https://salehahr.github.io/zettelkasten/ma/backpropagation/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/backpropagation/</guid><description>Source: google-ml-course Backpropagation Gradient descent for neural networks Idea Gradience/the need for differentiable functions: things need to be differentiable in order for learning to occur
Problems Vanishing gradients In too deep networks, the signal to noise ratio gets worse further in the model Thus the gradients for the initial layers can approach zero Learning becomes slow Strategies: Limit model depth Use ReLUs Exploding gradients Especially if learning rates are too high, weights too large &amp;ndash;&amp;gt; NaNs in model Gradients in initial layers explode, become too large to converge Strategies: Lower learning rate Batch normalisation ReLU layers can &amp;lsquo;die&amp;rsquo; Due to the cap at zero If values end up being below zero then the gradients can&amp;rsquo;t get backpropagated Strategies: Different initialisation Lower learning rate Tricks Scaling/normalising features If features are roughly on the same scale, this can make convergence faster Dropout</description></item><item><title>bias</title><link>https://salehahr.github.io/zettelkasten/ma/bias/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/bias/</guid><description>Parent: classification Source: google-ml-course Bias Unbiased: average of predictions is equal to the average of observations Zero bias does not mean everything is ok (e.g. could come from a model that was training on too little data) but can be a useful sanity check If there is bias in the model, there is a problem e.g. biased training sample?</description></item><item><title>classification</title><link>https://salehahr.github.io/zettelkasten/ma/classification/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/classification/</guid><description>Source: google-ml-course Classification Logistic regression returns a probability This probability can be converted to a binary value by making use of a classification threshold Classification/decision threshold: the value which splits between true and false
s. confusion matrix , which contains information used for the performance metrics Evaluation metrics One possibility: accuracy (fraction of correct predictions over total predictions) $\dfrac{\text{TP} + \text{TN}}{\text{all predictions}}$ Poor metric, especially when working with datasets with class imbalance (significant difference between number of positive and negative labels) e.</description></item><item><title>confusion-matrix</title><link>https://salehahr.github.io/zettelkasten/ma/confusion-matrix/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/confusion-matrix/</guid><description>Source: google-ml-course Confusion matrix $N \times N$ matrix which shows how successful the classification model&amp;rsquo;s predictions were. $N$: number of classes, e.g. $N=2$ for true/false classes Axes: predicted class/label, actual class/label Predicted True Predicted False Actual True TP FN Actual False FP TN In multi-class classification, the confusion matrix can help to identify mistake patternsâ€”does the model tend to mistakenly predict a certain class for another?</description></item><item><title>data-in-ml</title><link>https://salehahr.github.io/zettelkasten/ma/data-in-ml/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/data-in-ml/</guid><description>Source: google-ml-course Data in ML Sampling data Batching data Data representation One-hot encoding Scaling features Binning Feature crossing Choosing labels and features Use quantifiable, observable data.</description></item><item><title>dropout</title><link>https://salehahr.github.io/zettelkasten/ma/dropout/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/dropout/</guid><description>Parent: regularisation Source: google-ml-course Dropout A form of regularisation for neural networks Randomly not consider a node for a single gradient step Greater dropout &amp;ndash;&amp;gt; stronger regularisation</description></item><item><title>embedding-dimensions</title><link>https://salehahr.github.io/zettelkasten/ma/embedding-dimensions/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/embedding-dimensions/</guid><description>Parent: hyperparameters Source: google-ml-course Embedding dimensions Good starting value: $$D \approx \sqrt[4]{\text{total values}}$$
Higher dimensions
allow more accurate representation of the relationships between the input data can lead to overfitting, slower training</description></item><item><title>embeddings</title><link>https://salehahr.github.io/zettelkasten/ma/embeddings/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/embeddings/</guid><description>Source: google-ml-course Embeddings Translate a high dimensional vector into a lower dimensional space (embedding) e.g. representing words by sparse vectors Ideally, similar inputs are grouped close together in the embedding space (similarity metric) Learning embeddings from data Embedding layer: a hidden layer with $D$ nodes, where $D$ is the dimension of the embedding space Each of these dimensions is called a latent dimension due to it being inferred from the data This hidden layer learns how to cluster the data to optimise the metric we have chosen Weights between the the input and the embedding layer are the coordinate values of the input within the embedding space Hyperparameter: how many embedding dimensions $D$?</description></item><item><title>feature-crossing</title><link>https://salehahr.github.io/zettelkasten/ma/feature-crossing/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/feature-crossing/</guid><description>Parent: data-representation Source: google-ml-course Synthetic features (feature crossing) e.g. generate feature $x_3$ by combining $x_1$ and $x_2$
$$x_3 = x_1 x_2$$ crossing boolean features can result in a very sparse feature set a more sophisticated version of feature crossing is a neural network Advantage Enables learning with nonlinear features while making use of a linear model
&amp;ndash;&amp;gt; nonlinear features scale well with large scale data sets
Disadvantage Crossing sparse features may significantly increase the size of the feature space.</description></item><item><title>hyperparameters</title><link>https://salehahr.github.io/zettelkasten/ma/hyperparameters/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/hyperparameters/</guid><description>Source: Wikipedia Parameters to control the process of learning the model, as opposed to being parameters of the model, which are to be learned.
Subsets:
model hyperparameters (for model selection), e.g. size of the NN topology of the NN algorithm hyperparameters (affect the efficiency of the learning process), e.g. learning rate batch size Source: google-ml-course Learning rate $\alpha$ Regularisation rate $\lambda$ Embedding dimensions</description></item><item><title>l0-regularisation</title><link>https://salehahr.github.io/zettelkasten/ma/l0-regularisation/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/l0-regularisation/</guid><description>Parent: regularisation Source: google-ml-course L$_0$ regularisation Explicitly set some weights to zero Regularisation term: only $\mathbf{w}$ Disadvantages This would penalise non-zero weights Non-convex optimisation Solution &amp;ndash;&amp;gt; relax this to an L$_1$ regularisation</description></item><item><title>l1-regularisation</title><link>https://salehahr.github.io/zettelkasten/ma/l1-regularisation/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/l1-regularisation/</guid><description>Parent: regularisation Source: google-ml-course L$_1$ regularisation Relaxed version of an L$_0$ regularisation Penalises the sum of the absolute values of the weights Regularisation term: $$\left\lvert \mathbf{w} \right\rvert = \left| |w_1| + \dots + |w_n| \right|$$ Convex problem Encourages a sparse model: model will want to drive $w$ to zero c.f. L$_2$ regularisation , which aims to make the weights smaller</description></item><item><title>l2-regularisation</title><link>https://salehahr.github.io/zettelkasten/ma/l2-regularisation/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/l2-regularisation/</guid><description>Parent: regularisation Source: google-ml-course L$_2$ regularisation / Ridge regularisation Model complexity is given by the sum of the squares of the weights1
$$ \begin{align} &amp;amp;\min L(x, y, \text{model}) + \lambda \left\lVert \mathbf{w} \right\rVert_2^2 \end{align} $$
with the L$_2$ regularisation term $$ \left\lVert \mathbf{w} \right\rVert_2^2 = \left( w_1^2 + \dots + w_n^2 \right) $$
and the regularisation rate $\lambda$ determines whether the total loss is more dependent on the training loss or on the model complexity.</description></item><item><title>logistic-regression</title><link>https://salehahr.github.io/zettelkasten/ma/logistic-regression/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/logistic-regression/</guid><description>See also: Linear regression Source: google-ml-course Linear logistic regression For predicting probabilities (value between 0 and 1) As an alternative, it could be possible to cap the predicted data at 1, but this would be introducing bias to the model Therefore the necessity for a different loss function and data prediction model, which outputs the predicted probabilities Sigmoid function Gives a bounded value between zero and one. $$ y = \dfrac{1}{1 + e^{-z}} $$</description></item><item><title>multi-class-classification</title><link>https://salehahr.github.io/zettelkasten/ma/multi-class-classification/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/multi-class-classification/</guid><description>Source: google-ml-course Multi-class classification Building off on binary classification &amp;ndash;&amp;gt; do one-vs-all classification, e.g. for the classes red-blue-yellow: Red vs not red Blue vs not blue Yellow vs not yellow For unique classes: SoftMax can be used, where the sum of the outputs is one For non-unique classes: one-vs-all classification with multiple logistic regressions , the sum of the outputs need not be one</description></item><item><title>neural-networks</title><link>https://salehahr.github.io/zettelkasten/ma/neural-networks/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/neural-networks/</guid><description>Source: google-ml-course Neural networks Goal: we want the model to learn the nonlinear features itself without us having to manually define the nonlinear features More sophisticated version of feature crosses Automatic learning of the feature crosses Linear model Each input (feature) is assigned a weight and the weight combination of all these features result in the output.
Incorporate nonlinearity Add another layer of features?
not linear yet linear combination of linear functions is still linear solution: do a nonlinear transform between layers (activation function) Activation function Sigmoid function $$ F(x) = \dfrac{1}{1 + e^{-x}} $$</description></item><item><title>overfitting</title><link>https://salehahr.github.io/zettelkasten/ma/overfitting/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/overfitting/</guid><description>Parent: generalisation Source: google-ml-course Overfitting Model is overly specific
is only usable to predict a subset of input data (the training data that was used to train the model) model would have been fitted to the specific properties of the dataset used, e.g. if the dataset was noisy, and overfitted model takes into account the noise of this particular dataset not usable on new data &amp;ndash;&amp;gt; not generalisable</description></item><item><title>precision-and-recall</title><link>https://salehahr.github.io/zettelkasten/ma/precision-and-recall/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/precision-and-recall/</guid><description>Parent: classification Source: google-ml-course Precision and Recall Precision true positives / all positive predictions
$$\dfrac{\text{TP}}{\text{TP} + \text{FP}}$$
&amp;ldquo;What fraction of positive predictions were actually correct?&amp;rdquo; â€” focus on the correctness of the positive predictions or: &amp;ldquo;Did the model predict positive too often?&amp;rdquo; e.g. precision = 0.5, model is correct 50% of the time Recall / True Positive Rate (TPR) true positives / all actual positives
$$\dfrac{\text{TP}}{\text{TP} + \text{FN}}$$</description></item><item><title>regularisation-rate</title><link>https://salehahr.github.io/zettelkasten/ma/regularisation-rate/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/regularisation-rate/</guid><description>Parent: Hyperparameters Source: google-ml-course Regularisation rate $\lambda$ If $\lambda$ too high: simpler model, risk of underfitting (not enough training being done) If $\lambda$ too low: model more complex, risk of overfitting Ideal $\lambda$ depends on data &amp;ndash;&amp;gt; needs to be tuned Strong L$_2$ regularisation has similar effect to that of lower learning rate (smaller step size) High regularisation drives weights towards zero Lower learning rates result in lower step sizes, and the steps towards zero (in parameter space) are smaller than steps away Therefore tuning $\alpha$ and $\lambda$ simultaneously could be a bit confusing Ensure that there are a high enough number of iterations (so that effect of early stopping doesn&amp;rsquo;t affect the tuning of $\lambda$)</description></item><item><title>simple-regression</title><link>https://salehahr.github.io/zettelkasten/ma/simple-regression/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/simple-regression/</guid><description>Source: google-ml-course Simple linear regression model $$ \begin{align} f(\mathbf{x}) = y' &amp;amp;= b + w_1 x_1 + \dots + w_n x_n\
&amp;amp;= w_0 + w_1 x_1 + \dots + w_n x_n \end{align} $$</description></item><item><title>SoftMax</title><link>https://salehahr.github.io/zettelkasten/ma/softmax/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/softmax/</guid><description>Source: google-ml-course SoftMax A more general version of logistic regression For multi-class classification with unique labels Additionally constraints all output nodes to sum up to 1.0 Helps convergence Probabilistic interpretation of outputs Options Full SoftMax (brute force): a probability is calculated for every single class Cheap for small number of classes Candidate sampling: calculates probability for all positive labels but only for a random sample of negative labels e.</description></item><item><title>batch</title><link>https://salehahr.github.io/zettelkasten/definitions/batch/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/definitions/batch/</guid><description>See also: batching-in-ml ]
Source: google-ml-course Batch Number of data points used to calculate the gradient in a single iteration</description></item><item><title>batching-in-ml</title><link>https://salehahr.github.io/zettelkasten/ma/batching-in-ml/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/batching-in-ml/</guid><description>Parent: Data in ML See also: Batch Source: google-ml-course Large batches of data
result in a long computation time of a single iteration possibly contain redundant data Hence, this results in variations of the classic gradient descent , which performs calculation over the whole dataset per iteration.</description></item><item><title>binning</title><link>https://salehahr.github.io/zettelkasten/ma/binning/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/binning/</guid><description>Parent: data-representation Source: google-ml-course Binning using histograms &amp;ndash;&amp;gt; then use one-hot encoding ; this is a cheap way of mapping nonlinear data into the model</description></item><item><title>data-representation</title><link>https://salehahr.github.io/zettelkasten/ma/data-representation/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/data-representation/</guid><description>Parent: Data in ML Source: google-ml-course Good ML relies on good data.
Data representation Feature engineering: extracting features from raw data Features: data that is readable or workable on by the ML algorithm Good features Should appear with non-zero values sufficiently often enough within the dataset (i.e. is actually useful enough to classify a lot of data points in the dataset and not just overly specific to sevveral data points only) Have clear and obvious meaning (e.</description></item><item><title>generalisation</title><link>https://salehahr.github.io/zettelkasten/ma/generalisation/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/generalisation/</guid><description>Source: google-ml-course Generalisation Balancing a model between underfitting and overfitting.
Underfitting Model is not specific enough; fails to accurately predict outputs.
Overfitting Model is overly specific, s. overfitting .
Strategies to improve generalisation, avoid overfitting Split the dataset strategically Regularisation â€” penalise model complexity</description></item><item><title>gradient-descent</title><link>https://salehahr.github.io/zettelkasten/ma/gradient-descent/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/gradient-descent/</guid><description>Parent: Variations of gradient descent Source: google-ml-course Gradient descent An iterated approach
Labeled data arrives Gradient of the loss function is computed Now the direction for updating the model parameters $\mathbf{w}$ is known (negative gradient). A step is taken in this direction, in the parameter space. The step size is equivalent to the learning rate . Repeat This process tunes all model parameters simultaneously.
Notes Works well for convex problems (loss function w.</description></item><item><title>learning-rate</title><link>https://salehahr.github.io/zettelkasten/ma/learning-rate/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/learning-rate/</guid><description>Parent: Hyperparameters Source: google-ml-course Hyperparameter &amp;lsquo;learning rate&amp;rsquo; $\alpha$ Small learning rate: small steps, long time to reach the minimum Big learning rate: big steps, shorter computation time but with potential to overshoot the minimum Ideal learning rate In 1D: the inverse of the second derivative of the model $$ \begin{align} \alpha = \frac{1}{f(x)''} \end{align} $$
In $\geq$ 2D: inverse of the Hessian</description></item><item><title>losses</title><link>https://salehahr.github.io/zettelkasten/ma/losses/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/losses/</guid><description>Source: google-ml-course Losses Losses according to dataset Training loss Validation loss â€” this is the one that matters, as this is the measure of the model&amp;rsquo;s ability to predict on new and unseen data Losses according to definition Mean square error Average square loss of the whole dataset. The greater the prediction disparity, the higher the penalisation (squared amplification)
e.g. a disparity of 2 units results in a loss 4x bigger than a disparity of 1 unit Single datapoint L$_2$ loss, squared error $$\left(\mathbf{y} - \mathbf{f}(\mathbf{x})\right)^\text{T} \left(\mathbf{y} - \mathbf{f}(\mathbf{x})\right)$$</description></item><item><title>machine-learning</title><link>https://salehahr.github.io/zettelkasten/ma/what-is-machine-learning/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/what-is-machine-learning/</guid><description>See also: ml-terminology Source: google-ml-course What is machine learning (Supervised) Machine learning in a nutshell Learn the patterns that exist within a set of input-output data Apply the patterns to a set of new input data, so as to be able to predict the outputs</description></item><item><title>mini-batch-gradient-descent</title><link>https://salehahr.github.io/zettelkasten/ma/mini-batch-gradient-descent/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/mini-batch-gradient-descent/</guid><description>Parent: Variations of gradient descent Source: google-ml-course Mini-Batch Gradient Descent Performs gradient and loss calculation on 10 to 1000 data points Less noise compared to SGD Nevertheless more efficient than full batch gradient descent</description></item><item><title>ml-terminology</title><link>https://salehahr.github.io/zettelkasten/ma/ml-terminology/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/ml-terminology/</guid><description>Source: google-ml-course ML Terminology Term Symbol Description Output, label $\mathbf{y}$ Variable to be predicted Features $\left\lbrace \mathbf{x}_1, \dots, \mathbf{x}_n \right\rbrace$ Representation of the (input) data Model $f: \mathbf{x} \rightarrow \mathbf{y}'$ Mapping of the input to the predictions $\mathbf{y}'$ Weights $\mathbf{w}$ Parameters of the model Term Description Training Process of determining the model parameters (weights) that will allow an accurate mapping of the input to the resulting output (prediction) i.</description></item><item><title>one-hot-encoding</title><link>https://salehahr.github.io/zettelkasten/ma/one-hot-encoding/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/one-hot-encoding/</guid><description>Parent: data-representation Source: google-ml-course One-hot encoding We can assign each class to a unique coefficient, i.e. maps a category to a number. The problem with this (using integer coefficients) These coefficients get bundled in with the ML math and the weights learned are specific to the defined coefficient code.
$$ \text{model} = \sum \text{weight} * \text{feature}$$
The weight for the specified feature vector is multiplied with all values of the feature, in this case the coefficients&amp;hellip; what happens if we add a new class and/or change the coefficient code?</description></item><item><title>reducing-loss-ml</title><link>https://salehahr.github.io/zettelkasten/ma/reducing-loss-ml/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/reducing-loss-ml/</guid><description>Source: google-ml-course Reducing loss in ML Aim: compute model parameters (weights) such that loss is minimal
For which parameters does the loss converge? (Reach a minimum) How to get these optimal model parameters? &amp;ndash;&amp;gt; which direction do we go in the parameter space? One possibility: compute the gradient and steer the model parameters based on the gradient &amp;ndash;&amp;gt; gradient descent , variations of gradient descent In neural networks : backpropagation Gradient: derivative of the loss function with respect to the model parameters $$ \begin{align} \dfrac{\partial L}{\partial \mathbf{w}} \end{align} $$</description></item><item><title>redundancy-in-ml</title><link>https://salehahr.github.io/zettelkasten/ma/redundancy-in-ml/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/redundancy-in-ml/</guid><description>See also: Batching in ML Source: google-ml-course Redundancy in ML Redundancy tends to increase with batch size Some redundancy is useful to reduce noise in gradient calculations</description></item><item><title>regularisation</title><link>https://salehahr.github.io/zettelkasten/ma/regularisation/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/regularisation/</guid><description>Parent: Generalisation Source: google-ml-course Regularisation So far: penalisation of wrong predictions [empirical risk minimisation]
$$ \min L(x, y, \text{model})$$ Now: penalise model complexity [structural risk minimisation] to prevent overfitting $$ \min L(x, y, \text{model}) + \text{complexity}(\text{model})$$ Some metrics for model complexity Function of the weights Function of the total number of features with nonzero weights Types Early stopping: stop the training before convergence (while using the training data) L$_0$ regularisation L$_1$ regularisation L$_2$ regularisation Dropout L$_1$ vs.</description></item><item><title>sampling-data</title><link>https://salehahr.github.io/zettelkasten/ma/sampling-data/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/sampling-data/</guid><description>Parent: Data in ML Source: google-ml-course Sampling data in ML Three basic assumptions
Data points are drawn independently and identically (i.i.d.), and at random, from the data distribution, i.e. the data points don&amp;rsquo;t influence each other The data distribution doesn&amp;rsquo;t change over time (stationary) The data is pulled from the same distribution, for both training and validation sets Situations where these assumptions may be violated:
change in user perception, therefore resulting in different labelling of a dataset change in population which result in new demographics or target market</description></item><item><title>scaling-features</title><link>https://salehahr.github.io/zettelkasten/ma/scaling-features/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/scaling-features/</guid><description>Parent: data-representation Source: google-ml-course Scaling/normalising feature values No benefit if only one feature in the feature set Can be beneficial for multifeature set Helps the gradient descent to converge more quickly Helps avoid numbers becoming NaN Makes the model spend the same time learning for each feature vector. If scaling is not done, more learning is done for features which have a bigger range of data. Scaling methods Map [min, max] to [-1, +1] Use Z scores</description></item><item><title>splitting-dataset</title><link>https://salehahr.github.io/zettelkasten/ma/splitting-dataset/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/splitting-dataset/</guid><description>Parent: generalisation Source: google-ml-course Splitting the dataset Strategy 1 A good performance on the training set doesn&amp;rsquo;t necessarily correspond to good performance on another test set The training set must be large enough (to exclude possibility of underfitting) The training set isn&amp;rsquo;t reused for validation (to exclude possibility of overfitting ) Thus, the idea is, if we only have one large data set, to split it into subsets: The training set is used to learn the model weights.</description></item><item><title>stochastic-gradient-descent</title><link>https://salehahr.github.io/zettelkasten/ma/stochastic-gradient-descent/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/stochastic-gradient-descent/</guid><description>Parent: Variations of gradient descent Source: google-ml-course Stochastic Gradient Descent (SGD) Performs gradient calculation on only a single data point Gradient is very noisy &amp;lsquo;Stochastic&amp;rsquo;: random data point is chosen at every iteration</description></item><item><title>variations-gradient-descent</title><link>https://salehahr.github.io/zettelkasten/ma/variations-gradient-descent/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/ma/variations-gradient-descent/</guid><description>See also: Batching Source: google-ml-course Variations of gradient descent The two extremes: Gradient descent Batch = whole dataset Stochastic Gradient Descent (SGD) Batch = one data point The compromise: Mini-Batch Gradient Descent For neural networks : backpropagation</description></item></channel></rss>