<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>-resources/-bibliography/bib-read on Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/tags/resources/-bibliography/bib-read/</link><description>Recent content in -resources/-bibliography/bib-read on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><atom:link href="https://salehahr.github.io/zettelkasten/tags/resources/-bibliography/bib-read/index.xml" rel="self" type="application/rss+xml"/><item><title>Baumgarte stabilisation over the SO(3) rotation group for control</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/baumgarte-stabilisation-over-the-so-3-rotation-group-for-control/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/baumgarte-stabilisation-over-the-so-3-rotation-group-for-control/</guid><description>Author: Sebastien Gros</description></item><item><title>Schneider 2013 How to not make the EKF fail</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/schneider-2013-how-to-not-make-the-ekf-fail/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/schneider-2013-how-to-not-make-the-ekf-fail/</guid><description>Authors: Schneider, Georgakis URL: http://www.researchgate.net/publication/263942618_How_To_NOT_Make_the_Extended_Kalman_Filter_Fail/citations DOI 10.1021/ie300415d Measurement noise R, V (landmark) Kalman filter initial estimates Process noise Q and W (odometry) Kalman filter performance metric</description></item><item><title>Whampsey MEKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/whampsey-mekf/</link><pubDate>Sat, 14 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/whampsey-mekf/</guid><description>http://matthewhampsey.github.io/blog/2020/07/18/mekf
Motivation:
Working with noisy IMU measurements IMUs usually provide redundant information that can be used to improve dead-reckoning Uses: Hamilton quaternion convention Which orientation parametrisation to choose? Error-State Kalman Filter</description></item><item><title>Jeon 2009 Kinematic Kalman Filter for Robot End-Effector Sensing</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/jeon-2009-kinematic-kalman-filter-for-robot-end-effector-sensing/</link><pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/jeon-2009-kinematic-kalman-filter-for-robot-end-effector-sensing/</guid><description>Backlinks: Discussion 2021-05-25 Authors: Jeon and Tomizuka
Abstract
inaccuracies in estimation of EE motion can come from kinematic error (error in parameters in kinematic equations)
to overcome this: take direct measurements e.g. using vision, but vision has high latency IMUs are used to provide interframe data fuse camera and IMU in a kinematic Kalman filter (KKF) framework. Note: uses ESKF
effect of camera measurement delay, augmenting the KF states to also estimate the time delay</description></item><item><title>(Leiner) Digital Endoscope Design</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/leiner/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/leiner/</guid><description>Backlinks: Endoscopes URL: http://www.spiedigitallibrary.org/ebooks/SL/Digital-Endoscope-Design/1/Digital-Endoscope-Design/10.1117/3.2235283.ch1?SSO=1
Notes Insertion of an endoscope Types of endoscopes Endoscope system components Endoscope specification</description></item><item><title>(Science Focus) How can one eye alone provide depth perception</title><link>https://salehahr.github.io/zettelkasten/bibliography/science-focus/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/science-focus/</guid><description>Source: http://www.sciencefocus.com/the-human-body/how-can-one-eye-alone-provide-depth-perception/
Author: Hilary Guite
In humans with normal binocular vision, depth perception is obtained using the parallax in the two overlapping fields of vision (&amp;ldquo;binocular disparity&amp;rdquo;)
Each single field of vision has a slightly different view to the other If vision in one eye is impaired, depth perception is still obtainable even with only one eye. Some tricks that the brain uses:
We know the real size of things Using perspective, e.</description></item><item><title>Solà 2017 Quaternion kinematics for ESKF</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sol%C3%A0-2017-quaternion-kinematics-for-eskf/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sol%C3%A0-2017-quaternion-kinematics-for-eskf/</guid><description>Link: http://www.iri.upc.edu/people/jsola/JoanSola/objectes/notes/kinematics.pdf
Author: Joan Solà Abstract:
Primer on quaternion/rotation group math Math for error state Kalman filters using IMUs Contents/Chapters:
Quaternions Rotations, s. also SO(3) 3D rotation group Quaternion conventions Perturbations, derivatives, integrals Error-State Kalman Filter for IMU-driven systems Variables in ESKF IMU measurement model IMU motion model [The initial gravity vector/orientation for the IMU ESKF](the initial gravity-vector_orientation-for-the-imu-eskf.</description></item><item><title>MKok 2017 Using inertial sensors for position and orientation estimation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mkok-2017/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mkok-2017/</guid><description>Source: http://arxiv.org/abs/1704.06053 Authors: M Kok, JD Hol, TB Schön
Abstract
Contents/Chapters Quaternions Probabilistic models for IMU Orientation parametrisations Which orientation parametrisation to choose? Linearisation of an orientation in SO(3) IMU measurement model Modelling noise and bias for IMU IMU motion models IMU prior models</description></item><item><title>(Mur-Artal 2017) VI-ORB</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mur-artal-2017-vi-orb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mur-artal-2017-vi-orb/</guid><description>Backlinks: [keyframe-based tightly-coupled multisensor slam](keyframe-based tightly-coupled multisensor slam.md), todo , works of-possible-interest URL: http://ieeexplore.ieee.org/abstract/document/7817784, Authors: Mur-Artal, Tardós Code: http://paperswithcode.com/paper/visual-inertial-monocular-slam-with-map-reuse Results (video): http://www.youtube.com/watch?v=JXRCSovuxbA
Abstract
current VI odometry approaches: drift accumulates due to lack of loop closure therefore there is a need for tightly-coupled VI-SLAM with loop closure and map reuse here: focus on monocular case, but applicable to other camera configurations builds on ORB-SLAM (from same author) IMU initialisation method (initialises: scale, gravity direction, velocities, gyroscope bias, accelerometer bias) depends on visual monocular initialisation (coupled initialisation) Other works: recent tightly-coupled VIO (both filtering- and optimisation-based) lack loop closure, so drift accumulates</description></item><item><title>Chen 2018 Review of VI SLAM</title><link>https://salehahr.github.io/zettelkasten/bibliography/chen-2018-review/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/chen-2018-review/</guid><description>Source: http://www.mdpi.com/2218-6581/7/3/45
Authors: Chen et. al
Abstract Survey on visual-inertial SLAM over the last 10 years Aspects: filtering vs optimisation based, camera type, sensor fusion type Explains core theory of SLAM, feature extraction, feature tracking, loop closure Experimental comparison of filtering-based and optimisation-based methods Research trends for VI-SLAM Recommended other works s. Works of possible interest Contents/Chapters SLAM SLAM: build a real-time map of the unknown environment based on sensor data, while the sensor (robot) itself is traversing the environment</description></item><item><title>Song 2018 MIS-SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/song-2018-mis-slam/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/song-2018-mis-slam/</guid><description>Authors Song et al Backlinks: referred to in Lamarca 2019 as a stereovisual deformable SLAM, uses CPU and GPU, nonlinear optimisation Video: http://www.youtube.com/watch?v=2pXokldQBWM
Abstract
Uses CPU and GPU CPU for ORBSLAM (initial global position) GPU for deformable tracking and dense mapping Contents/Chapters
Poor localisation of scope in MIS, compared with open surgery Related works mentioned don&amp;rsquo;t provide a RT and robust solution for localisation while reconstructing dense deformable surfaces focus on the monocular scope, fail to solve the problem of missing scale Fast movement makes visual odometry unstable causes blurry images worsens registrations ORB-SLAM proven to be suitable for coupling with dense deformable SLAM Initial tracking: ORB-SLAM</description></item><item><title>Wikipedia Lokalisierung</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-lokalisierung/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-lokalisierung/</guid><description>Source: http://de.wikipedia.org/wiki/Lokalisierung_(Robotik Localisation Particle filters Categories of sensors for localisation</description></item><item><title>Qin 2019 General Optimization-based Framework (Multisensor)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/qin-2019-general-optimization-based-framework-multisensor/</link><pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/qin-2019-general-optimization-based-framework-multisensor/</guid><description>Authors: Qin et al Code: http://github.com/HKUST-Aerial-Robotics/VINS-Fusion (uses ROS)
Abstract:
odometry estimation with multiple sensors, general framework which is optimisation-based demonstrated combinations: stereo cameras monocular cam + IMU stereo cams + IMU sensor = factor in the framework comparison with other state-of-the-art algos Aim:
to create a general algo which supports different multisensor suites also for redundancy: in case of sensor failure, it can be switched out easily Related work:</description></item><item><title>(Scaradozzi 2018) SLAM application in surgery</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/scaradozzi-2018/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/scaradozzi-2018/</guid><description>Abstract:
SLAM&amp;rsquo;s potential in image-guided surgery assuming static environment Review of main techniques in general robotics SLAM Insight into visual SLAM SLAM in surgery Chapters What is SLAM? Filter-based vs optimisation-based SLAM General Kalman Filter General EKF Unscented Kalman Filter Information Filter &amp;hellip;.
Takeaway
EKF is popular in surgery SLAM techniques Deformable environment encumbers precise registration and data fusion</description></item><item><title>Lamarca 2020 DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/lamarca-2020/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/lamarca-2020/</guid><description>URL: http://arxiv.org/abs/1908.08918 Authors: Lamarca et al Code: http://github.com/UZ-SLAMLab/DefSLAM Results (video): http://www.youtube.com/watch?v=6mmhD2_t6Gs Summary
First monocular SLAM for deformable environments in real-time Most other SLAM implementations assume rigidity Main techniques used (techniques for monocular non-rigid scenes): isometric shape from template (SfT) non-rigid structure from motion (NRSfM) Main principle: computation in two parallel threads (s. DefSLAM framework) Deformation tracking [front end] Deformation mapping [back end] The map from the mapping thread defines the shape-at-rest template used by deformation tracking Validation: compare with ORBSLAM (rigid) Assumes isometric deformation Future work: relocalisation (s.</description></item><item><title>(Wu 2018) Image-based camera localization</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/wu-2018-image-based-camera-localization-an-overview/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/wu-2018-image-based-camera-localization-an-overview/</guid><description>Authors: Wu, Tang, Li
Abstract/Contents
overview (classification) of image-based camera localization classification of image-based camera localization approaches techniques, trends only considers 2D cameras focuses on points as features in images (not lines etc) Chapters Classification of image-based camera localization approaches [Multisensor fusion](multisensor fusion.md) — why use the visual-inertial sensor combination? Loose vs Tight coupling Filter localisation methods Some optimisation-based tightly-coupled multisensor SLAM algorithms Questions
What&amp;rsquo;s a metric map &amp;ndash; normal map (with landmarks, normal distances) as opposed to a topological one Takeaway</description></item><item><title>Cometlabs What You Need to Know About SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cometlabs/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cometlabs/</guid><description>Source: http://blog.cometlabs.io/teaching-robots-presence-what-you-need-to-know-about-slam-9bf0ca037553
SLAM chicken and egg problem Position acquisition Multisensor fusion [Sensors (absolute measurements) for measuring distance to landmarks](sensors (absolute measurements)-for-measuring-distance-to-landmarks.md) Mapping representations in robotics Visual SLAM Implementation Framework Feature-based vs direct SLAM workflow Sparse/Feature-based VSLAM Dense/direct VSLAM</description></item><item><title>Riisgaard SLAM for dummies</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/riisgaard-slam-for-dummies/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/riisgaard-slam-for-dummies/</guid><description>Authors: Søren Riisgaard and Morten Rufus Blas Parent: SLAM resources Abstract:
Tutorial introduction to SLAM, with minimal prerequisites for the understanding of SLAM as explained here Mostly explains a single approach to the steps involved in SLAM Complete solution for SLAM using EKF (extended Kalman filter) Only considers 2D motion, not 3D Chapters
What is SLAM? Overview of SLAM using EKF Hardware Robot Range measurement device SLAM process Step 1: Odometry update Step 2: Reobservation Step 3: Add new landmarks Laser data Odometry data Landmarks Landmark extraction 1.</description></item><item><title>Wikipedia Visual odometry</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-visual-odometry/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-visual-odometry/</guid><description>Source: http://en.wikipedia.org/wiki/Visual_odometry Odometry Visual sensors for localisation</description></item><item><title>SOFA extended documentation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sofa-extended-documentation/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sofa-extended-documentation/</guid><description>Source: http://hal.inria.fr/hal-00681539 Authors: Faure et al Backlinks: Scope of Studienarbeit Abstract
SOFA: open source C++ library mainly for interactive physical/medical simulation modular approach by decomposing simulators into its constituent components (DOF, differential equations, solvers etc), and organising them in a scenegraph data structure multimodel representation of objects (collision model, visual model etc) Chapters
Read
1: introduction 2: multimodel framework 3: data structures 3.1 scenegraph and visitors 3.</description></item></channel></rss>