<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>to-do/missing-tag on Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/tags/to-do/missing-tag/</link><description>Recent content in to-do/missing-tag on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><atom:link href="https://salehahr.github.io/zettelkasten/tags/to-do/missing-tag/index.xml" rel="self" type="application/rss+xml"/><item><title>Dynamic Bayesian Network formulation of SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/dynamic-bayesian-network-formulation-of-slam/</link><pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/dynamic-bayesian-network-formulation-of-slam/</guid><description>Source: Grisetti 2011 - Tutorial graph-based SLAM Dynamic Bayesian Network
Solution of full SLAM problem: Transition model: Observation model:  The observation model is usually multimodal: a single observation may result in multiple edges (in the spatial graph) Therefore, the Gaussian assumption does not hold</description></item><item><title>Tracking in VIORB</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-in-viorb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-in-viorb/</guid><description>Source: Mur-Artal 2017 VI-ORB Tracking in VIORB
Visual-inertial tracking at frame rate, instead of using an ad-hoc motion model as in the original ORB-SLAM Tracked states: [sensor pose (R, p), velocities v, biases b] Once the camera pose is predicted, map points are projected, then matches with existing features on the frame Then optimise the current frame j, depending on whether the map has just been updated the map is unchanged Here, the optimisation function for tracking (when map unchanged) is:</description></item><item><title>(Liu 2020) Learned Descriptor</title><link>https://salehahr.github.io/zettelkasten/bibliography/liu-2020/</link><pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/liu-2020/</guid><description>Note: I&amp;rsquo;m only reading this paper for the into to SLAM/SfM
Abstract Problem: 3D reconstuction has subpar performance when dealing with endoscopic videos, partly due to local descriptors &amp;hellip; Introduction Correspondence estimation: match between 2D points in image and corresponding 3D location (s. registration ) Correspondence estimation is needed by SfM, SLAM, &amp;hellip; SfM + SLAM combination has been shown to be effective for surgical navigation in endoscopy &amp;ndash; simultaneous estimation of sparse 3D structure of the observed scene camera trajectory Complementarity of SfM + SLAM Good camera tracking requires dense 3D reconstruction</description></item><item><title>Structure from Motion</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sfm/</link><pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sfm/</guid><description>Source: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Schonberger_Structure-From-Motion_Revisited_CVPR_2016_paper.pdf
Note:
This paper uses incremental SfM Corresponding paper for COLMAP Structure from motion Reconstruction of 3D structure from a sequence of 2D images of that structure, taken from different viewpoints.
Search for correspondence between images &amp;ndash;&amp;gt; output: scene graph (nodes: images, edges: verified pairs) Feature extraction Feature matching Output: set of image pairs and their associated feature correspondences Verification: do features map to the same scene point?</description></item></channel></rss>