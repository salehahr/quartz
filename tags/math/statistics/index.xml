<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>math/statistics on Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/tags/math/statistics/</link><description>Recent content in math/statistics on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><atom:link href="https://salehahr.github.io/zettelkasten/tags/math/statistics/index.xml" rel="self" type="application/rss+xml"/><item><title>Error ellipse/Confidence ellipse</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/error-ellipse-confidence-ellipse/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/error-ellipse-confidence-ellipse/</guid><description>Parent: Multivariate Gaussian distributions Source: rlabbe Kalman/Bayesian filters in Python Any slice through a multivariate Gaussian is an ellipse Plots show the slice for 3 standard deviations
Showing correlation using error ellipses</description></item><item><title>Showing correlation using error ellipses</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/showing-correlation-using-error-ellipses/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/showing-correlation-using-error-ellipses/</guid><description>Parent: Error ellipse/Confidence ellipse Source: rlabbe Kalman/Bayesian filters in Python A slanted ellipse implies correlation
The &amp;lsquo;thinner&amp;rsquo; side isn&amp;rsquo;t necessarily more accurate, it just means that the spread of data is reduced along this dimension (when viewing sensor data, for example) Example First epoch Yellow: prior (very uncertain about position) Green: evidence (more accurate in one of the dimensions than the other; more certainty compared to prior) Blue: posterior via multiplication Posterior retains the shape of the evidence (which has more certainty than the prior)</description></item><item><title>50.2. Bayes' Theorem</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.-bayes-theorem/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.-bayes-theorem/</guid><description>Parent: Bayesian Filter Update Step Source: rlabbe Kalman/Bayesian filters in Python How do we compute the probability of an event given previous information? (s. also Frequentist vs Bayesian statistics )
Formula to compute new information into existing information
Used in the update step of a Bayesian filter (valid for both probabilities as well as probability distributions) where || . || expresses normalisation
B Evidence (sensor measurements z) p(A) Prior p(B|A) Likelihood p(A|B) Posterior In filtering systems, computing p(x|z) is nearly impossible, but computing p(z|x) is fairly straightforward, which then facilitates the computation of p(x|z) via the Bayes' theorem formula.</description></item><item><title>Central Limit Theorem</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/central-limit-theorem/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/central-limit-theorem/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python If we make many measurements, the measurements will be normally distributed. (only applies under certain conditions)</description></item><item><title>Computational properties of Gaussian distributions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/computational-properties-of-gaussian-distributions/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/computational-properties-of-gaussian-distributions/</guid><description>Parent: Gaussian distribution Backlinks: Showing correlation using error ellipses Source: rlabbe Kalman/Bayesian filters in Python g1 + g2 = g3; all are Gaussians g1 * g2 = g3; g3 is not Gaussian, but proportional to a Gaussian Sum of two Gaussians Product of two Gaussians: Product of multidimensional Gaussians:</description></item><item><title>Correlation and independence</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/correlation-and-independence/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/correlation-and-independence/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python Independent variables are uncorrelated. But the reverse is not always true: uncorrelated variables may be dependent on one another e.g. y=x^2 has no [linear] correlation, but y depends on x nonetheless</description></item><item><title>Empirical rule 68/95/99.7</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/empirical-rule-68-95-99.7/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/empirical-rule-68-95-99.7/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python Emprical rule, a.k.a. 68–95–99.7 rule About 68% of all values lie within one standard deviation of the mean.</description></item><item><title>Frequentist vs Bayesian statistics</title><link>https://salehahr.github.io/zettelkasten/math/statistics/frequentist-vs-bayesian/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/statistics/frequentist-vs-bayesian/</guid><description>Frequentist vs Bayesian statistics Sources: rlabbe , blitzstein-hwang Frequentist Probability represents the frequency over an experiment repeated infinitely many times Probability of flipping a fair coin infinitely many times is 50% Bayesian Probability represents a belief about the event [ bh ] $\hat{=}$ hypothesis e.g. probability that someone is guilty Probability of flipping a fair coin one more time: which way do I believe it landed?</description></item><item><title>Gaussian distribution</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/gaussian-distribution/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/gaussian-distribution/</guid><description>Parent: Probability theory Backlinks: Limitations of the discrete Bayes filter Source: rlabbe a.k.a. Normal distribution Unimodal, continuous probability distribution function (pdf)
The probability of a range of measurements is the area under the graph of the probability distribution between the end values of the range &amp;ndash; cumulative distribution function (cdf)
Background statistics Variance, standard deviation, covariances Central Limit Theorem Correlation and independence Types Univariate Gaussian distribution Multivariate Gaussian distributions Computational properties of Gaussian distributions Pros and cons of Gaussian distributions</description></item><item><title>Multivariate Gaussian distributions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/multivariate-gaussian-distributions/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/multivariate-gaussian-distributions/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python See also: Probability distribution N means for N dimensions Variances are now also combined with covariances (to take into account correlation between different dimensions)
Variance: how does a population vary amongst themselves? Covariance: how much do two variables change relative to each other? The correlation helps prediction!
Here: only linear correlation considered; however nonlinear correlations also exist.
Error ellipse/Confidence ellipse</description></item><item><title>Pros and cons of Gaussian distributions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/pros-and-cons-of-gaussian-distributions/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/pros-and-cons-of-gaussian-distributions/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python Big advantage of using Gaussian distributions (as opposed to discrete ones w/ histogram bins): less data, b/c a Gaussian distribution is represented fully using only two values: the mean and the variance Limitations of using Gaussian distributions to model the world i.e. deviations from the central limit theorem
Not all situations are describable by Gaussian distributions e.g. sensors in the real world have fat tails (kurtosis) — don&amp;rsquo;t extend to infinity and skew Can&amp;rsquo;t depict any arbitrary probability distributions like in e.</description></item><item><title>Random variable</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/random-variable/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/random-variable/</guid><description>Random variable $X$ Source: Wiki Alternative name: stochastic variable Function that assigns each elementary outcome $\omega$ in the sample set $\Omega$ to another set $\mathcal{A}$ (often to $\mathbb{R}$) $$X: \Omega \rightarrow \mathcal{A}$$ $P(X = x)$ is the probability that the function $X$ attains the value $x \in \mathcal{A}$ $x$: specific realisation of a random variable $X$ Can be discrete (e.g. number of students) or continuous (e.g. height of students) Example Source Experiment: two dice are thrown Sample set: $\left{ (D_i, D_j) \right}$ $\forall, i,j$ $X$: random variable that gives the number of dice with sixes Example trials: Trial 1: (2, 3) &amp;ndash;&amp;gt; $X = 0$ Trial 2: (6, 1) &amp;ndash;&amp;gt; $X = 1$ Trial 3: (6, 6) &amp;ndash;&amp;gt; $X = 2$</description></item><item><title>Univariate Gaussian distribution</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/univariate-gaussian-distribution/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/univariate-gaussian-distribution/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python If normalised (area under the graph is 1): Gaussian distribution If not normalised: Gaussian function Notation: The random variable X has a Gaussian distribution with mean &amp;hellip; and variance &amp;hellip; .</description></item><item><title>Variance, standard deviation, covariances</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/variance-standard-deviation-covariances/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/variance-standard-deviation-covariances/</guid><description>Backlinks: Multivariate Gaussian distributions Source: rlabbe Kalman/Bayesian filters in Python See also: Empirical rule 68/95/99.7 How much do the values vary from the mean?
There are other ways of calculating variance (e.g. by using absolute values of error instead of error squared). The other methods may be better w.r.t. outliers (outliers get magnified in the square term) Process variance: error in the process model Sensor variance: error in each sensor measurement</description></item><item><title>Expected value</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/expected-value/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/expected-value/</guid><description>Parent: Probability theory Expectation Source: Bishop
Definition $$ \begin{alignat}{3} \mathbb{E}[X] &amp;amp;= \sum_{x \in X} x \cdot P(x) &amp;amp;&amp;amp;\qquad \text{discrete}\
&amp;amp;= \int_{X} x \cdot p(x) ~ \text{d}x &amp;amp;&amp;amp;\qquad \text{continuous} \end{alignat} $$
Approximation Given a finite number of $N$ points drawn from the probability distribution, $\mathbb{E}$ can be approximated by $$ \mathbb{E}[X] \approx \dfrac{1}{N} \sum_{n=1}^N X(\omega_n) $$
Source: rlabbe Example If we take a thousand sensor readings, the readings won&amp;rsquo;t always be the same (due to the inherent noise).</description></item></channel></rss>