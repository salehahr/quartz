<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>math/statistics on Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/tags/math/statistics/</link><description>Recent content in math/statistics on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><atom:link href="https://salehahr.github.io/zettelkasten/tags/math/statistics/index.xml" rel="self" type="application/rss+xml"/><item><title>Error ellipse/Confidence ellipse</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/error-ellipse-confidence-ellipse/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/error-ellipse-confidence-ellipse/</guid><description>Parent: Multivariate Gaussian distributions Source: rlabbe Kalman/Bayesian filters in Python Any slice through a multivariate Gaussian is an ellipse Plots show the slice for 3 standard deviations
Showing correlation using error ellipses</description></item><item><title>Showing correlation using error ellipses</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/showing-correlation-using-error-ellipses/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/showing-correlation-using-error-ellipses/</guid><description>Parent: Error ellipse/Confidence ellipse Source: rlabbe Kalman/Bayesian filters in Python A slanted ellipse implies correlation
The &amp;lsquo;thinner&amp;rsquo; side isn&amp;rsquo;t necessarily more accurate, it just means that the spread of data is reduced along this dimension (when viewing sensor data, for example) Example First epoch Yellow: prior (very uncertain about position) Green: evidence (more accurate in one of the dimensions than the other; more certainty compared to prior) Blue: posterior via multiplication Posterior retains the shape of the evidence (which has more certainty than the prior)</description></item><item><title>50.2. Bayes' Theorem</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.-bayes-theorem/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/50.2.-bayes-theorem/</guid><description>Parent: Bayesian Filter Update Step Source: rlabbe Kalman/Bayesian filters in Python How do we compute the probability of an event given previous information? (s. also Frequentist vs Bayesian statistics )
Formula to compute new information into existing information
Used in the update step of a Bayesian filter (valid for both probabilities as well as probability distributions) where || . || expresses normalisation
B Evidence (sensor measurements z) p(A) Prior p(B|A) Likelihood p(A|B) Posterior In filtering systems, computing p(x|z) is nearly impossible, but computing p(z|x) is fairly straightforward, which then facilitates the computation of p(x|z) via the Bayes' theorem formula.</description></item><item><title>Central Limit Theorem</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/central-limit-theorem/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/central-limit-theorem/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python If we make many measurements, the measurements will be normally distributed. (only applies under certain conditions)</description></item><item><title>Computational properties of Gaussian distributions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/computational-properties-of-gaussian-distributions/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/computational-properties-of-gaussian-distributions/</guid><description>Parent: Gaussian distribution Backlinks: Showing correlation using error ellipses Source: rlabbe Kalman/Bayesian filters in Python g1 + g2 = g3; all are Gaussians g1 * g2 = g3; g3 is not Gaussian, but proportional to a Gaussian Sum of two Gaussians Product of two Gaussians: Product of multidimensional Gaussians:</description></item><item><title>Correlation and independence</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/correlation-and-independence/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/correlation-and-independence/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python Independent variables are uncorrelated. But the reverse is not always true: uncorrelated variables may be dependent on one another e.g. y=x^2 has no [linear] correlation, but y depends on x nonetheless</description></item><item><title>Empirical rule 68/95/99.7</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/empirical-rule-68-95-99.7/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/empirical-rule-68-95-99.7/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python Emprical rule, a.k.a. 68–95–99.7 rule About 68% of all values lie within one standard deviation of the mean.</description></item><item><title>Frequentist vs Bayesian statistics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/frequentist-vs-bayesian-statistics/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/frequentist-vs-bayesian-statistics/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python Frequentist vs Bayesian statistics
Probability of flipping a fair coin infinitely many times is 50% - frequentist Probability of flipping a fair coin one more time (which way do I believe it landed?), single event - Bayesian Bayesian statistics takes past information (prior) into account If finding the prior is tricky, frequentist techniques are sometimes used e.</description></item><item><title>Gaussian distribution</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/gaussian-distribution.1/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/gaussian-distribution.1/</guid><description>Backlinks: Limitations of the discrete Bayes filter Source: rlabbe Kalman/Bayesian filters in Python a.k.a. Normal distribution Unimodal, continuous probability distribution function (pdf)
The probability of a range of measurements is the area under the graph of the probability distribution between the end values of the range &amp;ndash; cumulative distribution function (cdf)
Background statistics Variance, standard deviation, covariances Central Limit Theorem Correlation and independence Types Univariate Gaussian distribution Multivariate Gaussian distributions Computational properties of Gaussian distributions Pros and cons of Gaussian distributions</description></item><item><title>Multivariate Gaussian distributions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/multivariate-gaussian-distributions/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/multivariate-gaussian-distributions/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python See also: Probability distribution N means for N dimensions Variances are now also combined with covariances (to take into account correlation between different dimensions)
Variance: how does a population vary amongst themselves? Covariance: how much do two variables change relative to each other? The correlation helps prediction!
Here: only linear correlation considered; however nonlinear correlations also exist.
Error ellipse/Confidence ellipse</description></item><item><title>Pros and cons of Gaussian distributions</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/pros-and-cons-of-gaussian-distributions/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/pros-and-cons-of-gaussian-distributions/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python Big advantage of using Gaussian distributions (as opposed to discrete ones w/ histogram bins): less data, b/c a Gaussian distribution is represented fully using only two values: the mean and the variance Limitations of using Gaussian distributions to model the world i.e. deviations from the central limit theorem
Not all situations are describable by Gaussian distributions e.g. sensors in the real world have fat tails (kurtosis) — don&amp;rsquo;t extend to infinity and skew Can&amp;rsquo;t depict any arbitrary probability distributions like in e.</description></item><item><title>Random variable</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/random-variable/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/random-variable/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python Combination of values + associated probabilities. &amp;ldquo;Event&amp;rdquo; e.g. die toss, height of students e.g. in a fair die values = {1, 2, &amp;hellip;, 6} (range of values = sample space) probabilities = {1/6} * 6</description></item><item><title>Univariate Gaussian distribution</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/univariate-gaussian-distribution/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/univariate-gaussian-distribution/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python If normalised (area under the graph is 1): Gaussian distribution If not normalised: Gaussian function Notation: The random variable X has a Gaussian distribution with mean &amp;hellip; and variance &amp;hellip; .</description></item><item><title>Variance, standard deviation, covariances</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/variance-standard-deviation-covariances/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/variance-standard-deviation-covariances/</guid><description>Backlinks: Multivariate Gaussian distributions Source: rlabbe Kalman/Bayesian filters in Python See also: Empirical rule 68/95/99.7 How much do the values vary from the mean?
There are other ways of calculating variance (e.g. by using absolute values of error instead of error squared). The other methods may be better w.r.t. outliers (outliers get magnified in the square term) Process variance: error in the process model Sensor variance: error in each sensor measurement</description></item><item><title>Expected value</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/expected-value/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/expected-value/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python Example: if we take a thousand sensor readings, the readings won&amp;rsquo;t always be the same (due to the inherent noise).
The expected value &amp;lsquo;averages&amp;rsquo; all of the readings into a single value. This can be a mean (probabilities of all values assumed equal) Or if incorporating individual and different probabilities, the expectation isn&amp;rsquo;t the mean of the range of values Proven: the average of a large number of measurements will be very close to the actual weight</description></item></channel></rss>