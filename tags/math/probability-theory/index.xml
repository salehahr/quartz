<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>math/probability-theory on Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/tags/math/probability-theory/</link><description>Recent content in math/probability-theory on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><atom:link href="https://salehahr.github.io/zettelkasten/tags/math/probability-theory/index.xml" rel="self" type="application/rss+xml"/><item><title>bayes-rule</title><link>https://salehahr.github.io/zettelkasten/math/statistics/bayes-rule/</link><pubDate>Thu, 12 May 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/statistics/bayes-rule/</guid><description>Parent: probability-theory Bayes rule Source: blitzstein-hwang $$P(A|B) = \dfrac{P(B|A) P(A)}{P(B)}$$
Extra conditioning &amp;ldquo;Everything is conditioned on $C$.&amp;rdquo;
$$P(A|B, C) = \dfrac{P(B|A, C) P(A|C)}{P(B | C)}$$ given $P(A\cap E) &amp;gt; 0$ and $P(B\cap E) &amp;gt; 0$.
Alternative approaches for interpretation
$B, ~C$ as a single event $B\cap C$
$$P(A|B,C) = \dfrac{P(A, B, C)}{P(B, C)}$$ Swap roles of $B$ and $C$ $$P(A|B, C) = \dfrac{P(C|A, B) P(A|B)}{P(C | B)}$$</description></item><item><title>conditional-probability</title><link>https://salehahr.github.io/zettelkasten/math/statistics/conditional-probability/</link><pubDate>Thu, 12 May 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/statistics/conditional-probability/</guid><description>Parent: probability-theory Conditional probability Source: blitzstein-hwang $$P(A|B) = \dfrac{P(A~\cap~B)}{P(B)}$$
Notation Description $P(A)$ prior held belief $B$ evidence that is observed $P(A\lvert B)$ posterior (updated belief) Properties $0 \leq P(A|B) \leq 1$ $P(\Omega|E) = 1$, $P(\emptyset | E) = 0$ For disjoint events $A_i$, $P(\bigcup_i A_i | E) = \sum_i P(A_i|E)$ Complement: $P(A^c | E) = 1 - P(A|E)$ Union: $P(A\cup B|E) = P(A|E) + P(B|E) - P(A\cap B|E)$</description></item><item><title>Event</title><link>https://salehahr.github.io/zettelkasten/definitions/event/</link><pubDate>Thu, 12 May 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/definitions/event/</guid><description>Parent: probability-theory See also: random-variable Event $E$ Source: Wiki $$E = \left{ \omega_i \right} \subseteq \Omega$$
Group of outcomes Every event $E$ is assigned a probability of it happening Example event: $E = \left{ \omega \in \Omega \mid X(\omega) \leq x \right}$
&amp;ldquo;Set of all outcomes $\omega$ which satisfy the condition $X(\omega) \leq x$&amp;rdquo; $$P(E) = P(X \leq x) = p_E$$ Example Experiment: flip a coin twice $$\Omega = \left{ (H, H), (H, T), (T, H), (T, T) \right}$$</description></item><item><title>naive-probability</title><link>https://salehahr.github.io/zettelkasten/math/statistics/naive-probability/</link><pubDate>Thu, 12 May 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/statistics/naive-probability/</guid><description>Naive probability Source: blitzstein-hwang Let event $A \subseteq \Omega$ $$ P_\text{naive}(A) = \dfrac{|A|}{|\Omega|} $$ where $|A|$ is the number of elements ( outcomes in $A$).
Required assumptions The sample space $\Omega$ is finite. The outcomes $\omega \in \Omega$ have equal probability of happening each. Usage Problems with equally likely outcomes. Problems with symmetry $\rightarrow$ equally likely probabilities for all outcomes
e.g. flipping a fair coin As a null model (as a hypothesis)</description></item><item><title>Outcome</title><link>https://salehahr.github.io/zettelkasten/definitions/outcome/</link><pubDate>Thu, 12 May 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/definitions/outcome/</guid><description>Parent: probability-theory Outcome $\omega$ Source: Wiki Result of a single run of an experiment . Element of the sample space $\Omega$
$$\omega \in \Omega$$</description></item><item><title>probability-function</title><link>https://salehahr.github.io/zettelkasten/math/statistics/probability-function/</link><pubDate>Thu, 12 May 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/statistics/probability-function/</guid><description>Parent: probability-theory Probability function Source: blitzstein-hwang Maps an event $E$ to probability values.
The probability function $P$ must satisfy the axioms:
$P(\emptyset)=0$, $P(\Omega)=1$ $P\left(, \bigcup_i A_i ,\right) = \sum_i P(A_i)$ if the events $A_i$ are mutually exclusive. Properties $P(A^c) = 1 - P(A)$ If $A \subseteq B$, then $P(A) \leq P(B)$ $P(A~\cup~B) = P(A) + P(B) - P(A~\cap~B)$</description></item><item><title>sample-space</title><link>https://salehahr.github.io/zettelkasten/definitions/sample-space/</link><pubDate>Thu, 12 May 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/definitions/sample-space/</guid><description>Parent: probability-theory Sample space Source: blitzstein-hwang Sample space of an experiment :
Set of all possible outcomes $\omega$ of the experiment. Notation: $S$, $\Omega$
$$S = \left{ s_i \right} ~ \forall,i$$ $$\Omega = \left{ \omega_i \right} ~ \forall,i$$ Contains event subsets</description></item><item><title>Probability mass function</title><link>https://salehahr.github.io/zettelkasten/math/statistics/probability-mass-function/</link><pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/statistics/probability-mass-function/</guid><description>Backlinks: Experiments in probability theory Probability mass function Source: Wiki Each outcome $\omega$ in the sample space $\Omega$ is assigned a probability value via the probability mass function $p(\omega)$ with the properties $$ \begin{align} p(\omega) &amp;amp;\in \left[ 0, 1\right] \quad \forall, \omega \in \Omega\
\sum_{\omega \in \Omega} p(\omega) &amp;amp;= 1 \end{align} $$
Source: Yang
$X$ is distributed according to the distribution $P$, $$X \sim P(x) ~.$$
For a specific instance of $X$ (or outcome), evaluating the pmf gives the probability $$P(X = x_i) = P(x_i) = p_i \geq 0$$</description></item><item><title>probability-theory</title><link>https://salehahr.github.io/zettelkasten/math/statistics/probability-theory/</link><pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/statistics/probability-theory/</guid><description>Terminology/Definitions Probability experiment Outcome Sample space Event Probability function Naive probability Frequentist vs. Bayesian probablity interpretation Conditional probability Bayes' rule Random variable Expectation $\mathbb{E}$ Conditional independence Distributions Probability distribution Probability mass function Gaussian distribution Literature Blitzstein, Hwang</description></item><item><title>Untitled 1</title><link>https://salehahr.github.io/zettelkasten/definitions/probability-experiment/</link><pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/definitions/probability-experiment/</guid><description>Parent: probability-theory See also: Random variable Experiment/Probability space Source: Wiki procedure that can be infinitely repeated has a well-defined sample space constituents: $\Omega$: sample space $\mathcal{F}$ set of events $E$ $$\mathcal{F} = \left{ E_j \right} ~ \forall,j$$ $P$: probability measure function that maps events to probabilities (s. probability mass function ) $$P(E) = p_E$$</description></item><item><title>Frequentist vs Bayesian statistics</title><link>https://salehahr.github.io/zettelkasten/math/statistics/frequentist-vs-bayesian/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/math/statistics/frequentist-vs-bayesian/</guid><description>Frequentist vs Bayesian statistics Sources: rlabbe , blitzstein-hwang Frequentist Probability represents the frequency over an experiment repeated infinitely many times Probability of flipping a fair coin infinitely many times is 50% Bayesian Probability represents a belief about the event [ bh ] $\hat{=}$ hypothesis e.g. probability that someone is guilty Probability of flipping a fair coin one more time: which way do I believe it landed?</description></item><item><title>Probability distribution</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/probability-distribution/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/probability-distribution/</guid><description>Parents: Discrete Bayesian filter , Probability theory Source: rlabbe Probability distribution collection of all possible probabilities for an event the distribution lists all possible events and the probability of each sum up to 1 Prior probability distribution probability prior to incorporating any measurements or other information
Joint probability $P(x,y)$: probability of both events happening the multivariate Gaussian distribution is already a joint probability distribution Marginal probability $P(x)$: probability of an event happening, without considering the occurrence of other events &amp;lsquo;projecting&amp;rsquo; the variances onto the walls</description></item><item><title>Random variable</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/random-variable/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/random-variable/</guid><description>Random variable $X$ Source: Wiki Alternative name: stochastic variable Function that assigns each elementary outcome $\omega$ in the sample set $\Omega$ to another set $\mathcal{A}$ (often to $\mathbb{R}$) $$X: \Omega \rightarrow \mathcal{A}$$ $P(X = x)$ is the probability that the function $X$ attains the value $x \in \mathcal{A}$ $x$: specific realisation of a random variable $X$ Can be discrete (e.g. number of students) or continuous (e.g. height of students) Example Source Experiment: two dice are thrown Sample set: $\left{ (D_i, D_j) \right}$ $\forall, i,j$ $X$: random variable that gives the number of dice with sixes Example trials: Trial 1: (2, 3) &amp;ndash;&amp;gt; $X = 0$ Trial 2: (6, 1) &amp;ndash;&amp;gt; $X = 1$ Trial 3: (6, 6) &amp;ndash;&amp;gt; $X = 2$</description></item><item><title>Expected value</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/expected-value/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/expected-value/</guid><description>Parent: Probability theory Expectation Source: Bishop
Definition $$ \begin{alignat}{3} \mathbb{E}[X] &amp;amp;= \sum_{x \in X} x \cdot P(x) &amp;amp;&amp;amp;\qquad \text{discrete}\
&amp;amp;= \int_{X} x \cdot p(x) ~ \text{d}x &amp;amp;&amp;amp;\qquad \text{continuous} \end{alignat} $$
Approximation Given a finite number of $N$ points drawn from the probability distribution, $\mathbb{E}$ can be approximated by $$ \mathbb{E}[X] \approx \dfrac{1}{N} \sum_{n=1}^N X(\omega_n) $$
Source: rlabbe Example If we take a thousand sensor readings, the readings won&amp;rsquo;t always be the same (due to the inherent noise).</description></item><item><title>Conditional independence</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/conditional-independence/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/conditional-independence/</guid><description>Parent: Probability theory Source: http://en.wikipedia.org/wiki/Conditional_independence $A$ and $B$ are conditionally independent given $C$ $\Leftrightarrow$ given the knowledge that $C$ occurs, the knowledge of whether $A$ occurs provides no information whatsoever on the likelihood of $B$ occurring, and vice versa.
Examples Weather and delay Let the two events be the probabilities of persons $A$ and $B$ getting home in time for dinner The third event $C$ is the fact that a snow storm hit the city.</description></item></channel></rss>