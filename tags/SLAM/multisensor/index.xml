<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SLAM/multisensor on</title><link>https://salehahr.github.io/tags/SLAM/multisensor/</link><description>Recent content in SLAM/multisensor on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://salehahr.github.io/tags/SLAM/multisensor/index.xml" rel="self" type="application/rss+xml"/><item><title>Chen 2018 Review of VI SLAM</title><link>https://salehahr.github.io/studienarbeit/chen-2018-review-of-vi-slam/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/chen-2018-review-of-vi-slam/</guid><description>Source: http://www.mdpi.com/2218-6581/7/3/45 Backlinks
Authors: Chen et. al Abstract
Survey on visual-inertial SLAM over the last 10 years Aspects: filtering vs optimisation based, camera type, sensor fusion type Explains core theory of SLAM, feature extraction, feature tracking, loop closure Experimental comparison of filtering-based and optimisation-based methods Research trends for VI-SLAM Recommended other works s. Works of possible interest Contents/Chapters SLAM: build a real-time map of the unknown environment based on sensor data, while the sensor (robot) itself is traversing the environment</description></item></channel></rss>