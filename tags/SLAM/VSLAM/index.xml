<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SLAM/VSLAM on Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/tags/SLAM/VSLAM/</link><description>Recent content in SLAM/VSLAM on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><atom:link href="https://salehahr.github.io/zettelkasten/tags/SLAM/VSLAM/index.xml" rel="self" type="application/rss+xml"/><item><title>Initialisation of monocular SLAM</title><link>https://salehahr.github.io/zettelkasten/SLAM/initialisation-of-monocular-slam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/initialisation-of-monocular-slam/</guid><description>Source: Lamarca 2019 DefSLAM Depth information has to be generated before localisation can be performed — how?
Capture multiple images which have enough parallax &amp;laquo;&amp;laquo;&amp;laquo;&amp;lt; HEAD:content/studienarbeit/initialisation-of-monocular-slam.md These images with parallax allows depth information to be calculated (this uses motion parallax ) ======= These images with parallax allows depth information to be calculated (this uses motion parallax ) content:content/SLAM/initialisation-of-monocular-slam.md * From these images, the map can be generated * Localisation can then be carried out with respect to the map (as long as camera doesn&amp;rsquo;t move off to an unexplored region)</description></item><item><title>Programmatic implementations of MonoSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/programmatic-implementations-of-monoslam/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/programmatic-implementations-of-monoslam/</guid><description>Parent: SLAM resources Python http://github.com/agnivsen/Py-M-SLAM http://github.com/agnivsen/LibMonoSLAM
MATLAB http://perso.ensta-paris.fr/~filliat/Courses/2011_projets_C10-2/BRUNEAU_DUBRAY_MURGUET/monoSLAM_bruneau_dubray_murguet_en.html</description></item><item><title>Back-end optimisation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/back-end-optimisation/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/back-end-optimisation/</guid><description>Parent: Visual SLAM Implementation Framework Source: cometlabs (Camera pose optimisation)
To compensate for drift of pose estimation Traditionally using EKF ( filter-based ) simple implementation therefore good for small scale estimations Alternative: bundle adjustment (graph optimisation) joint optimisation of the camera pose and the 3D structure parameters combines numerical methods and graph theory increasingly favoured over filtering, due to the latter&amp;rsquo;s inherent inconsistency more efficient when combined with sub-mapping</description></item><item><title>(Wu 2018) Image-based camera localization</title><link>https://salehahr.github.io/zettelkasten/bibliography/wu-2018/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/bibliography/wu-2018/</guid><description>Authors: Wu, Tang, Li
Abstract/Contents overview (classification) of image-based camera localization classification of image-based camera localization approaches techniques, trends only considers 2D cameras focuses on points as features in images (not lines etc) Chapters Classification of image-based camera localization approaches Multisensor fusion — why use the visual-inertial sensor combination? Loose vs Tight coupling &amp;laquo;&amp;laquo;&amp;laquo;&amp;lt; HEAD:content/bibliography/wu-2018.md Filter localisation methods ======= Filter localisation methods content:content/bibliography/wu-2018.</description></item><item><title>Classification of image-based camera localization approaches</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/classification-of-image-based-camera-localization-approaches/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/classification-of-image-based-camera-localization-approaches/</guid><description>Parent: SLAM Index Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)
Unknown environment (must be reconstructed from image data) Online/real-time mapping (SLAM)
geometric metric SLAM (accurate computations, therefore still widely used in practice) monocular multiocular multi-kind sensor (active) loosely-coupled closely-coupled learning SLAM (active) needs a prior dataset to train NN &amp;ndash; dataset determines performanace of the SLAM low generalisation capability, therefore not as flexible as geom.</description></item><item><title>Visual SLAM Implementation Framework</title><link>https://salehahr.github.io/zettelkasten/SLAM/vslam-framework/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/SLAM/vslam-framework/</guid><description>Source: Cometlabs Basic principle: tracking a set of points through successive frames these tracks are used to triangulate the 3D positions of the points to create the map at the same time, using the the est point locations to calculate the pose of the camera, which could have observed them (i.e. calculate real time 3D structure of a scene from the estimated motion of the camera) Architecture Front-end Abstracts sensor data into models (which are good for estimation) / Processing &amp;laquo;&amp;laquo;&amp;laquo;&amp;lt; HEAD Data association ======= Data association content * Short term (feature tracking); features in consecutive sensor measurements * Either from sparse maps or dense-maps * Long term ( loop closure ; associating new measurements to older landmarks</description></item></channel></rss>