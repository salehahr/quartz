<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SLAM on Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/tags/SLAM/</link><description>Recent content in SLAM on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><lastBuildDate>Thu, 04 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://salehahr.github.io/zettelkasten/tags/SLAM/index.xml" rel="self" type="application/rss+xml"/><item><title>System::forceTrajectory</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/system-forcetrajectory/</link><pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/system-forcetrajectory/</guid><description>Parent: DefSLAM branch overview Reference: DefSLAMGT (stereo as ground truth) For testing: DefSLAMVI
Description Force update of DefSLAMVI&amp;rsquo;s current frame pose to that of DefSLAMGT&amp;rsquo;s for the frames 230 to 239
Without System::Reset Frame pose is &amp;lsquo;updated&amp;rsquo; during the interval, but after the interval, the optimisation (which uses frame pose as an estimate and also uses map node positions) makes the system resume it&amp;rsquo;s trajectory before the update
(below: with pure monocular trajectory, without any forced updates) With System::Reset The system is reset after every forced pose update (i.</description></item><item><title>DefSLAM branch overview</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-branch-overview/</link><pubDate>Fri, 19 Feb 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-branch-overview/</guid><description>Parent: SA TODO Repo http://github.com/feudalism/DefSLAM
Dormant
master sa Deprecated
windows - deprecated, changes made for building on Windows imu - deprecated, has Imu tracking functions but dependencies not resolved obs_tuple - initial attempt to incorporate Atlas, attempt to use OS3&amp;rsquo;s structure for MapPoint observations : &amp;lt;KeyFrame, tuple&amp;lt;int, int&amp;raquo; as opposed to &amp;lt;Keyframe, int&amp;gt; in DefSLAM+OS2 Temporary/Experimental
s. to do list
debugging the segfault that seemingly appears in Surface::getNormalSurfacePoint seems to happen after System reset</description></item><item><title>ORBSLAM2 unofficial documentation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam2-unofficial-documentation/</link><pubDate>Wed, 17 Feb 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam2-unofficial-documentation/</guid><description>Partially done, abandonned: http://github.com/raulmur/ORB_SLAM2/compare/master&amp;hellip;AlejandroSilvestri:master In Spanish: http://alejandrosilvestri.github.io/os1/doc/html/</description></item><item><title>NormalSurfacePoint Segfault</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/normalsurfacepoint-segfault/</link><pubDate>Tue, 09 Feb 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/normalsurfacepoint-segfault/</guid><description>Crash around frame 65 [New Thread 0x7fff7ffff700 (LWP 3854)] [Thread 0x7fff7ffff700 (LWP 3854) exited] [New Thread 0x7fff7ffff700 (LWP 3855)] [Thread 0x7fff7ffff700 (LWP 3855) exited] [New Thread 0x7fff7ffff700 (LWP 3856)]
Thread 5 &amp;ldquo;DefSLAM&amp;rdquo; received signal SIGSEGV, Segmentation fault. [Switching to Thread 0x7fffc215d700 (LWP 3571)] defSLAM::SurfacePoint::thereisNormal (this=0x6705) at /home/user3/slam/DefSLAM/Modules/Mapping/SurfacePoint.cc:54 54 bool SurfacePoint::thereisNormal() { return NormalOn; } (gdb) bt #0 0x00007ffff78798a0 in defSLAM::SurfacePoint::thereisNormal() (this=0x6705) at /home/user3/slam/DefSLAM/Modules/Mapping/SurfacePoint.cc:54 #1 0x00007ffff7878f95 in defSLAM::Surface::setNormalSurfacePoint(unsigned long, cv::Vec&amp;lt;float, 3&amp;gt;&amp;amp;) (this=0x555565b0e030, ind=ind@entry=939, N=&amp;hellip;) at /home/user3/slam/DefSLAM/Modules/Mapping/Surface.</description></item><item><title>Viewer segfault</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/viewer-segfault/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/viewer-segfault/</guid><description>Error [New Thread 0x7fff86ffd700 (LWP 1117)] NORMALS REESTIMATED : 277 - 277 [Thread 0x7fff86ffd700 (LWP 1117) exited] NORMAL ESTIMATOR OUTPoints potential : 293 70 New template requested Number Of normals 277 0x555566923da0 -0.79956 0.655022 -0.594482POINTS matched:167 Points Scale Error Keyframe : 1 stan dev 0.310974 chi 0.013115 0.01 201 SurfaceRegistration not sucessful (Not enough points to align or chi2 too big
Thread 6 &amp;ldquo;DefSLAM&amp;rdquo; received signal SIGSEGV, Segmentation fault. [Switching to Thread 0x7fffc1996700 (LWP 275)] __memmove_avx_unaligned_erms () at .</description></item><item><title>Segfault in DefTracking (imu branch)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/segfault-in-deftracking-imu-branch/</link><pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/segfault-in-deftracking-imu-branch/</guid><description>/home/user3/slam/datasets/mandala0/images/stereo_im_l_1560936003993.png i: 30 POINTS matched:10 Track lost soon after initialisation, reseting&amp;hellip; /home/user3/slam/datasets/mandala0/images/stereo_im_l_1560936004022.png i: 31 System Reseting NORMAL ESTIMATOR IN - NORMALS REESTIMATED : 0 - 0 NORMAL ESTIMATOR OUTPoints potential : 939 70 New template requested Number Of normals 0 0x5555636b1fb0 Not enough normals Reseting Local Mapper&amp;hellip; done Reseting Loop Closing&amp;hellip; done Reseting Database&amp;hellip; done
Thread 1 &amp;ldquo;DefSLAM&amp;rdquo; received signal SIGSEGV, Segmentation fault. 0x00007ffff78d9fae in cv::Mat::Mat (m=&amp;hellip;, this=0x7ffffffeaea0) at /usr/local/include/opencv4/opencv2/core/mat.inl.hpp:545 545 step[0] = m.</description></item><item><title>DefSLAM and discontinuous areas (classical datasets)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-and-discontinuous-areas-classical-datasets/</link><pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-and-discontinuous-areas-classical-datasets/</guid><description>Parent: Lamarca 2020 DefSLAM Source: http://github.com/UZ-SLAMLab/DefSLAM/issues/1
JoseLamarca: DefSLAM is suitable for rigid areas, proof of that is the abdominal sequence that is kind of rigid. The problem for these sequences is the discontinuous areas. For the monocular case, we are assuming that the surface is smooth that is not usually valid for the classical datasets. Apart from complexity issues that algorithms with RGB-D and stereo cameras could have in those scenes [1] and [2].</description></item><item><title>DefSLAM errors encountered</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-errors-encountered/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-errors-encountered/</guid><description>Rebuilding DefSLAM in Debug mode Error: &amp;ldquo;Virtual memory exhausted: Cannot allocate memory&amp;rdquo; Solution: reduce degree of make -j
Segmentation fault in Defslam debug mode Based on http://stackoverflow.com/questions/19615371/segmentation-fault-due-to-vectors Changed: surfacePoints_[ind] to surfacePoints_.at(ind)
New error surfacePoints_ appears to be NULL? Was it instantiated in another thread? http://stackoverflow.com/questions/11645857/debugging-with-gdb-why-this-0x0
Using core dumps with gdb http://jvns.ca/blog/2018/04/28/debugging-a-segfault-on-linux/</description></item><item><title>Pizarro 2016 Schwarps</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/pizarro-2016-schwarps/</link><pubDate>Sun, 20 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/pizarro-2016-schwarps/</guid><description>Author: Daniel Pizarro et al.
Abstract
Warp between two images of a deforming surface: a transformation that depict the geometric deformation between the two &amp;lsquo;maps points between images of a deforming surface&amp;rsquo; Current approach to enforce a warp&amp;rsquo;s smoothness: penalise its second order partial derivatives However this favours locally affine warps Does not capture the local projective component of the image deformation Propose: novel penalty to smooth the warp while capturing the deformation&amp;rsquo;s local projective structure Proposed penalty is based on equivalents to the Schwarzian derivatives Schwarzian derivatives: projective differential invariants exactly preserved by homographies Methodology to derive a set of PDEs with only homographies as the solutions Validation: Schwarps outperform existing warps in modeling and extrapolation power: perform better in deformable reconstruction methods Introduction/Related work</description></item><item><title>System in a VIN problem with IMU preintegration</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/system-in-a-vin-problem-with-imu-preintegration/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/system-in-a-vin-problem-with-imu-preintegration/</guid><description>Source: Forster 2017 IMU Preintegration State x_i of the system at time i with All keyframes up till time k State of all keyframes camera measurements IMU measurements between KFs i and j (consecutive) Set of measurements up till time k IMU pose: , maps a point in B to W</description></item><item><title>Non-Rigid Guided Matching (b/w KFs) in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/non-rigid-guided-matching-b-w-kfs-in-defslam/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/non-rigid-guided-matching-b-w-kfs-in-defslam/</guid><description>Source: Lamarca 2019 DefSLAM Backlinks: NRSfM in DefSLAM Matching between keyframes (used in deformation mapping in DefSLAM)
Use an estimated warp as a reference
To increase number of matches in the covisible keyframes Process
Matches are given by deformation tracking Estimate an initial warp between k and k* (covisible keyframes) how? Using this initial warp, estimate where a point would be seen in k* Define a search region around thesse estimated positions.</description></item><item><title>Surface alignment in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/surface-alignment-in-defslam/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/surface-alignment-in-defslam/</guid><description>Parent: Mapping step-by-step in DefSLAM Source: Lamarca 2019 DefSLAM Goal:
to scale the up-to-scale surface (output of NRSfM) to the proper dimensions get an idea of the proper dimensions from the already estimated map i.e. resulting surface must match the scale of the template T_(k-1) T_(k-1): deformed map generated by the tracker at the instance of KF=k insertion, with shape-at-rest of S_(k-1) generated from KF:(k-1) result: scale-corrected shape-at-rest Sk Method:</description></item><item><title>Template substitution in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/template-substitution-in-defslam/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/template-substitution-in-defslam/</guid><description>Parent: Mapping step-by-step in DefSLAM Source: Lamarca 2019 DefSLAM Tracking runs at frame-rate, and mapping at keyframe-rate Tracking processes Nm frames during a whole mapping run Process
New keyframe (k) is made. Now at time t=k At this point, the template in the tracking is still based on the old shape-at-rest, S_(k-1) Mapping thread starts creates surface S_k which is aligned to prev. template T_(k-1) k is set as the reference keyframe from S_k, create template T_k and from now on use this template instead of the old one T_(k-1) At time t=k+Nm, use data from the tracking thread image points at t=k+Nm deform the recently computed template T_k based on these images use SfT but neglecting the temporal term (to allow large deformation, &amp;ldquo;as a lot might have happened in the time span of Nm&amp;rdquo;) so now we get a T_k that is deformed (updated) to the most recent image points we do this extra step instead of passing T_k (from step 1) to the tracker immediately because, due to the new points occurring at t=k+Nm, using the original T_k might lead to data association errors mapper passes the new template T_k (t=k+Nm) to the tracker</description></item><item><title>Data association in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/data-association-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/data-association-in-defslam/</guid><description>Source: Lamarca 2019 DefSLAM See also: Data association Goal: match keypoints in current frame (newly extracted) with map points (already in map/system) Use the active matching strategy proposed in [Agudo 2015]: “Simultaneous pose and non-rigid shape with particle dynamics,” Steps  ORB points (keypoints) are detected in current frame
Camera pose Tcw is predicted
using camera motion model camera motion model: function of past camera poses Predict where map points (existing in map) would be imaged, based on last estimated template i.</description></item><item><title>DefSLAM framework</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-framework/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-framework/</guid><description>Source: Lamarca 2019 DefSLAM &amp;ldquo;Fusion of the methods available for processing non-rigid monocular scenes&amp;rdquo;
Deformation tracking [front end]
estimates/recovers/optimises: camera pose scene deformation / deformation of map points (observations) the map points are then embedded into the template Tk (to compute their position on the surface) operates at frame rate SFT-based (shape from template), requires prior geometry (template of scene at rest) for the currently being viewed map Map points are deformed (updated) by solving an optimisation problem min { reprojection error + deformation energy } per frame Deformation mapping [back end]</description></item><item><title>Initialisation of monocular SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/initialisation-of-monocular-slam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/initialisation-of-monocular-slam/</guid><description>Source: Lamarca 2019 DefSLAM Depth information has to be generated before localisation can be performed — how?
Capture multiple images which have enough parallax These images with parallax allows depth information to be calculated (this uses motion parallax ) From these images, the map can be generated Localisation can then be carried out with respect to the map (as long as camera doesn&amp;rsquo;t move off to an unexplored region)</description></item><item><title>Initialisation of monocular SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/initialisation-of-monocular-slam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/initialisation-of-monocular-slam/</guid><description>Source: Lamarca 2019 DefSLAM Depth information has to be generated before localisation can be performed — how?
Capture multiple images which have enough parallax These images with parallax allows depth information to be calculated (this uses motion parallax ) From these images, the map can be generated Localisation can then be carried out with respect to the map (as long as camera doesn&amp;rsquo;t move off to an unexplored region)</description></item><item><title>Mapping step-by-step in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-step-by-step-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-step-by-step-in-defslam/</guid><description>Source: Lamarca 2019 DefSLAM Parent: DefSLAM framework Steps
Recover warps between k and k* (s. [Non-Rigid Guided Matching (b/w KFs) in DefSLAM](non-rigid guided-matching-(b_w-kfs)-in-defslam.md)) with k: anchor keyframes, i.e. KFs where one of the observed map points was initialised with k*: set of best covisible keyframes warps: transformation between the images Ik to Ik* In DefSLAM, Schwarps (a family of warps using 2D Schwarzian equation regularisers) is used Schwarps has something to do with the infinitesimal planarity assumption of NRSfM [ NRSfM ] Process k* to get estimate of an up-to-scale surface  Input of NRSfM: warps [ Surface alignment ] Up-to-scale surface (\hat{S}_k) is aligned with the whole map in order to obtained the scaled surface Sk w.</description></item><item><title>Non-rigid monocular techniques in the literature</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/non-rigid-monocular-techniques-in-the-literature/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/non-rigid-monocular-techniques-in-the-literature/</guid><description>Source: Lamarca 2019 DefSLAM SfT methods
require: 1 monocular image 1 textured shape at rest (template) &amp;ldquo;geometry&amp;rdquo; as the deformation model different definitions of the deformation model analytic, e.g. isometric deformation-based: assumes preserved geodesic distance between surface points isometry for SfT has proven to be well-posed &amp;ndash;&amp;gt; led to stable, real-time solutions energy-based; jointly minimises {energy shape w.r.t. template [shape at rest] + reprojection error for image correspondences} classification according to [ http://www.</description></item><item><title>NRSfM and SfT</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm-and-sft/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm-and-sft/</guid><description>Source: Lamarca 2019 DefSLAM In literature, non-rigid monocular scenes are handled by NRSfM and SfT
NRSfM (non-rigid structure from motion)
batch processing of images to recover deformation computationally demanding — slower than SFT SFT (shape from template)
uses only a single image — faster than NRFfM lower computational cost must have a known 3D template (textured model)</description></item><item><title>NRSfM and SfT</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm-and-sft/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm-and-sft/</guid><description>Source: Lamarca 2019 DefSLAM In literature, non-rigid monocular scenes are handled by NRSfM and SfT
NRSfM (non-rigid structure from motion)
batch processing of images to recover deformation computationally demanding — slower than SFT SFT (shape from template)
uses only a single image — faster than NRFfM lower computational cost must have a known 3D template (textured model)</description></item><item><title>NRSfM in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/nrsfm-in-defslam/</guid><description>Parent: Mapping step-by-step in DefSLAM Source: Lamarca 2019 DefSLAM Assumptions
Isometric deformation Infinitesimal planarity [DEF]: any surface can be approximated as a plane at infinitesimal level, all the while maintaining its curvature at a global level The method used here is a local method &amp;ndash;&amp;gt; implies that it handles missing data and occlusions inherently
surface deformation is modelled locally for each point, under the above assumptions Embedding, phi_k of the scene surface</description></item><item><title>ORBSLAM2 mods</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam2-mods/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam2-mods/</guid><description>Patch to work with opencv4 http://github.com/Windfisch/ORB_SLAM2 ORBSLAM2 Python bindings http://github.com/jskinn/ORB_SLAM2-PythonBindings</description></item><item><title>Template in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/template-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/template-in-defslam/</guid><description>Source: Lamarca 2019 DefSLAM Template
2D triangular mesh floating in the 3D space consists of a set of 2D triangular facets F a facet has 3 nodes (set V) and 3 edges (set E) map points observed in keyframe k are embedded in the facets Map point coordinates in barycentric coordinates</description></item><item><title>Tracking optimisation in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-optimisation-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-optimisation-in-defslam/</guid><description>Source: Lamarca 2019 DefSLAM Backlinks: Template substitution in DefSLAM Optimisation function
Minimises reprojection error (in the image) deformation energy (of the template) boundary nodes of the local zone are fixed (i.e. not set as arguments to the optimisation function) this makes the absolute camera pose observable how? in order to constrain the gauge freedoms Initial guess: values from previous optimisation (i.e. previous frame: t-1) Reprojection error robust against outliers due to Huber robust kernel</description></item><item><title>DefSLAM dependency/inheritance diagram</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-dependency-inheritance-diagram/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-dependency-inheritance-diagram/</guid><description/></item><item><title>Handling the computational complexity of optimisation-based SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/handling-the-computational-complexity-of-optimisation-based-slam/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/handling-the-computational-complexity-of-optimisation-based-slam/</guid><description>Parent: SLAM Index Source: Forster 2017 IMU Preintegration Complexity of nonlinear batch optimisation
The trajectory and the map, which comprise the states, grow with time The larger the SLAM problem, the less feasible it is to perform the optimisation in real-time Solutions to improve computational efficiency
Keyframe-based methods: discard frames except for a few selected keyframes Run the optimisation parallelly (e.g. tracking and mapping threads) Fixed-lag smoothing: Use of a local map of fixed size, with marginalisation of the old states (summarise the old states into a prior term) Filtering is a special case of this: window of size 1, i.</description></item><item><title>DefTracking::MonocularInitialization</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-monocularinitialization/</link><pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-monocularinitialization/</guid><description>Parent: DefTracking::Track Initialises
surface points in the surface If num. features in current frame &amp;gt; 100
set frame pose to origin make new KF (GroundTruthKeyFrame) pKFini add the KF to the map mpMap iterate over the N features get feature kp convert kp to 3d point make new DefMapPoint(3dp, pKFini, mpMap) set pointers between DefMapPoint, GroundTruthKeyFrame, DefMap Save surface using bbs Set mLastFrame := mCurrentFrame Local window: Add KF to local KFs vector, add MapPoints to local MP vector, mpLocalMapper Calculate Tcr from Tcw Initialise SLAM: Set reference KF, reference MapPoints set mState to OK</description></item><item><title>ORBSLAM::Frame constructor (monocular)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam-frame-constructor-monocular/</link><pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/orbslam-frame-constructor-monocular/</guid><description>Source: Tracking::GrabImageMonocular Set scale level info from ORB extractor Extract ORB features mvKeys (vector of keypoints/features) Set N number of features Make mvpMapPoints (null, but with size N), mvbOutlier (all entries false, size N) If first frame or calibration change: ComputeImageBounds AssignFeaturesToGrid()</description></item><item><title>DefOptimizer::DefPoseOptimization</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defoptimizer-defposeoptimization/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defoptimizer-defposeoptimization/</guid><description>Parent: DefTracking::Track As far as I understand it:
Uses g2o library for the optimisation (graph-based SLAM) cost function terms are converted to edges and nodes each cost function term seems to correspond to an edge in the graph in g2o paper/tutorial: an edge is fully characterised by its error function and its information matrix int DefPoseOptimization(Frame *pFrame, Map *mMap, double RegLap, double RegInex, double RegTemp, uint NeighboursLayers) // define optimiser, set solver optimizer = new &amp;hellip; optimizer.</description></item><item><title>DefOptimizer::poseOptimization</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defoptimizer-poseoptimization/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defoptimizer-poseoptimization/</guid><description>Parent: DefTracking:TrackWithMotionModel() int DefOptimizer::poseOptimization(Frame *pFrame)
// Set estimate of solution to current camera pose g2o::VertexSE3Expmap *vSE3 = new g2o::VertexSE3Expmap(); vSE3-&amp;gt;setEstimate(Converter::toSE3Quat(pFrame-&amp;gt;mTcw)); vSE3-&amp;gt;setId(0); vSE3-&amp;gt;setFixed(false); optimizer.addVertex(vSE3);
// Set MapPoint vertices (num. nodes in opt. graph?) const int N = pFrame-&amp;gt;N;</description></item><item><title>defSLAM::System constructor</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-system-constructor/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-system-constructor/</guid><description>Parent: DefSLAM simple_camera defSLAM::System::System(const string &amp;amp;strVocFile, const string &amp;amp;strSettingsFile, const bool bUseViewer)  mSensor(MONOCULAR), mpLoopCloser(NULL), mpViewer(static_cast&amp;lt;Viewer *&amp;gt;(nullptr)), mbReset(false), mbActivateLocalizationMode(false), mbDeactivateLocalizationMode(false) Constructor // initialise mpVocabulary from file // create mpKeyFrameDatabase from mpVocabulary // create map DefMap() // create drawers for viewer DefFrameDrawer DefMapDrawer // initialise tracking, mapping, viewer threads; loop closing not implemented in DefSLAM mpTracker = new DefTracking(&amp;hellip;); mpLocalMapper = new DefLocalMapping(&amp;hellip;); mpViewer = new DefViewer(&amp;hellip;);
Attributes eSensor mSensor ORBVocabulary *mpVocabulary KeyFrameDatabase *mpKeyFrameDatabase Map *mpMap // stores pointers to all KFs, all MapPoints</description></item><item><title>defSLAM::System::TrackMonocular</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-system-trackmonocular/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-system-trackmonocular/</guid><description>Parent: DefSLAM simple_camera cv::Mat defSLAM::System::TrackMonocular cv::Mat Tcw = mpTracker-&amp;gt; GrabImageMonocular (im, timestamp);
// get information from mpTracker: // get states mTrackingState // get map points mTrackedMapPoints // get key points mTrackedKeyPointsUn
// return camera pose return Tcw;</description></item><item><title>DefTracking::Track</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-track/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-track/</guid><description>Parent: Tracking::GrabImageMonocular void DefTracking::Track // if: not initialised, do: monocular initialisation // elseif: already initialised, do: track frame { // if: tracking and mapping, do: bOK = TrackWithMotionModel ();
// if bOK (there exists camera pose estimate and matching), track local map // if template is updated (keyframe-rate update) set reference KF from new template do DefPoseOptimization (&amp;hellip;); bOK = TrackLocalMap();
// if: bOK, update motion model (update mVelocity); clean VO matches // check if we should insert a new KF, delete outliers for BA</description></item><item><title>DefTracking:TrackWithMotionModel()</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-trackwithmotionmodel-/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/deftracking-trackwithmotionmodel-/</guid><description>Parent: DefTracking::Track // Initial tracking to locate rigidly the camera and discard outliers. bool DefTracking::TrackWithMotionModel()
// Update last frame relative pose according to its reference keyframe UpdateLastFrame();
// Project points seen in prev frames int th = 15; int nmatches = Defmatcher.SearchByProjection(*mCurrentFrame, mLastFrame, th, mSensor == System::MONOCULAR);
// Optimise frame pose with all matches to initialise camera pose Optimizer:: poseOptimization (mCurrentFrame, myfile);
// Discard outliers
// return: sufficient number of matches?</description></item><item><title>Tracking_GrabImageMonocular</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-grabimagemonocular/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-grabimagemonocular/</guid><description>Parent: defSLAM::System::TrackMonocular cv::Mat ORB_SLAM2::Tracking::GrabImageMonocular
colour conversion make frame using image, timestamp, ORB stuff, calibration data, etc. mCurrentFrame = new Frame (mImGray, timestamp, mpORBextractorLeft, mpORBVocabulary, mK, mDistCoef, mbf, mThDepth, im)
perform tracking: Track (); return camera pose return mCurrentFrame-&amp;gt;mTcw.clone();</description></item><item><title>(Mur-Artal 2017) VI-ORB</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mur-artal-2017-vi-orb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mur-artal-2017-vi-orb/</guid><description>Backlinks: Chen 2018 Review of VI SLAM , [keyframe-based tightly-coupled multisensor slam](keyframe-based tightly-coupled multisensor slam.md), todo , works of-possible-interest URL: http://ieeexplore.ieee.org/abstract/document/7817784, Authors: Mur-Artal, Tardós Code: http://paperswithcode.com/paper/visual-inertial-monocular-slam-with-map-reuse Results (video): http://www.youtube.com/watch?v=JXRCSovuxbA
Abstract
current VI odometry approaches: drift accumulates due to lack of loop closure therefore there is a need for tightly-coupled VI-SLAM with loop closure and map reuse here: focus on monocular case, but applicable to other camera configurations builds on ORB-SLAM (from same author) IMU initialisation method (initialises: scale, gravity direction, velocities, gyroscope bias, accelerometer bias) depends on visual monocular initialisation (coupled initialisation) Other works: recent tightly-coupled VIO (both filtering- and optimisation-based) lack loop closure, so drift accumulates</description></item><item><title>Loop closing in VIORB</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/loop-closing-in-viorb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/loop-closing-in-viorb/</guid><description>Parent: SLAM Index Source: Mur-Artal 2017 VI-ORB Overview
To reduce drift accumulated during exploration (when returning to an already mapped location) Loop detection: of large loops using place recognition Loop correction: first do lightweight pose-graph optimisation (PGO), then do full BA in a separate thread (in order not to interfere with real-time operations) Implementation
After loop detection: do match validation (alignment of points between keyframes) Then pose-graph optimisation to reduce the accumulated error in trajectory (PGO: pose-only, ignores IMU info) IMU info ignored, but velocities are corrected by rotating them according to keyframe orientation &amp;ndash;&amp;gt; suboptimal, but should be accurate enough to allow IMU data to be used right after the PGO in ORBSLAM: PGO is 7-DoF optimisation (due to scale + 3 rot + 3 xyz) in VIORB, 6 DoF (scale is known from initialisation bzw.</description></item><item><title>Loop closing in VIORB</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/loop-closing-in-viorb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/loop-closing-in-viorb/</guid><description>Parent: SLAM Index Source: Mur-Artal 2017 VI-ORB Overview
To reduce drift accumulated during exploration (when returning to an already mapped location) Loop detection: of large loops using place recognition Loop correction: first do lightweight pose-graph optimisation (PGO), then do full BA in a separate thread (in order not to interfere with real-time operations) Implementation
After loop detection: do match validation (alignment of points between keyframes) Then pose-graph optimisation to reduce the accumulated error in trajectory (PGO: pose-only, ignores IMU info) IMU info ignored, but velocities are corrected by rotating them according to keyframe orientation &amp;ndash;&amp;gt; suboptimal, but should be accurate enough to allow IMU data to be used right after the PGO in ORBSLAM: PGO is 7-DoF optimisation (due to scale + 3 rot + 3 xyz) in VIORB, 6 DoF (scale is known from initialisation bzw.</description></item><item><title>Mapping in VIORB</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-in-viorb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-in-viorb/</guid><description>Source: Mur-Artal 2017 VI-ORB Mapping in VIORB Previously in ORBSLAM (only poses are optimised): Now in VIORB, more states to optimise: Increase in complexity
more states to optimise (v, b) IMU measurements creates constraints between keyframes Original ORBSLAM discards redundants KFs (poses a problem with IMU constraints!) Workaround: in local BA, only allow discarding of KF if, after discarding, the time between two consecutive KFs is short enough (&amp;lt;= 0.</description></item><item><title>Tracking in VIORB</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-in-viorb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/tracking-in-viorb/</guid><description>Source: Mur-Artal 2017 VI-ORB Tracking in VIORB
Visual-inertial tracking at frame rate, instead of using an ad-hoc motion model as in the original ORB-SLAM Tracked states: [sensor pose (R, p), velocities v, biases b] Once the camera pose is predicted, map points are projected, then matches with existing features on the frame Then optimise the current frame j, depending on whether the map has just been updated the map is unchanged Here, the optimisation function for tracking (when map unchanged) is:</description></item><item><title>Why use the visual-inertial sensor combination?</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/why-use-the-visual-inertial-sensor-combination-/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/why-use-the-visual-inertial-sensor-combination-/</guid><description>Parent: SLAM Index See also: Multisensor fusion Source: Mur-Artal 2017 VI-ORB Cheap but also with good potential Cameras provide rich information but are relatively cheap IMU provides self-motion info, helps recover scale in monocular applications enables estimation of the direction of gravity &amp;ndash;&amp;gt; renders pitch and roll observable Source: Forster 2017 IMU Preintegration Visual-inertial fusion for 3D structure and motion estimation Both cameras and IMUs are cheap, easy to find and complement each other well Camera exteroceptive sensor measures, up to a to-be-determined metric scale, appearance and geometrical structure of a 3D scene IMU interoceptive sensor makes metric scale of monocular cameras, as well as the direction of gravity, observable Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.</description></item><item><title>DefSLAM simple_camera</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-simple-camera/</link><pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/defslam-simple-camera/</guid><description>Without ground truth
App run // Create defSLAM::system, which initializes all threads (local mapping, loop closing, viewer) defSLAM::System SLAM(orbVocab, calibFile, bUseViewer);
// Timestamp uint timestamp := 0;
// Process frames from video capture while (capture.isOpened()) { // Get the capture as a matrix; // SLAM SLAM. TrackMonocular (img_matrix, timestamp); timestamp++; }</description></item><item><title>Cadena 2016 Past, Present, and Future of SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/cadena-2016/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/cadena-2016/</guid><description>Authors Cadena et al
Abstract
cited by 1.2k people &amp;ldquo;This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM&amp;rdquo; Recommended other works s. Works of possible interest Contents/Chapters Takeaway</description></item><item><title>Chen 2018 Review of VI SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/chen-2018-review/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/chen-2018-review/</guid><description>Source: http://www.mdpi.com/2218-6581/7/3/45 Backlinks: 10-monocular-depth-perception Authors: Chen et. al Abstract
Survey on visual-inertial SLAM over the last 10 years Aspects: filtering vs optimisation based, camera type, sensor fusion type Explains core theory of SLAM, feature extraction, feature tracking, loop closure Experimental comparison of filtering-based and optimisation-based methods Research trends for VI-SLAM Recommended other works s. Works of possible interest Contents/Chapters SLAM: build a real-time map of the unknown environment based on sensor data, while the sensor (robot) itself is traversing the environment</description></item><item><title>Song 2018 MIS-SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/song-2018-mis-slam/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/song-2018-mis-slam/</guid><description>Authors Song et al Backlinks: referred to in Lamarca 2019 as a stereovisual deformable SLAM, uses CPU and GPU, nonlinear optimisation Video: http://www.youtube.com/watch?v=2pXokldQBWM
Abstract
Uses CPU and GPU CPU for ORBSLAM (initial global position) GPU for deformable tracking and dense mapping Contents/Chapters
Poor localisation of scope in MIS, compared with open surgery Related works mentioned don&amp;rsquo;t provide a RT and robust solution for localisation while reconstructing dense deformable surfaces focus on the monocular scope, fail to solve the problem of missing scale Fast movement makes visual odometry unstable causes blurry images worsens registrations ORB-SLAM proven to be suitable for coupling with dense deformable SLAM Initial tracking: ORB-SLAM</description></item><item><title>Filter-based vs optimisation-based SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/filter-based-vs-optimisation-based-slam/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/filter-based-vs-optimisation-based-slam/</guid><description>Parent: SLAM Index Source: Scaradozzi 2018 SLAM application in surgery Main paradigms of SLAM
Filters — Kalman filters , Particle filters Graph-based SLAM Estimate the entire trajectory and the map from the full set of measurements (full SLAM) Which SLAM algorithm to use? Depends on application
map resolution update time (real time or not) type of environment type of sensors available</description></item><item><title>Particle filters</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/particle-filters/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/particle-filters/</guid><description>Parent: Filter localisation methods Source: Wikipedia Lokalisierung Particle filter / Monte Carlo localisation / sequential Monte Carlo methods
allow solution of all three localisation problems POSE represented by a particle cloud Each particle : possible POSE The filter checks the plausibility of each particle Increases and decreases the probabilities of each particle accordingly When a lower probability threshold is exceeded, the particle is not considered any longer Source: Scaradozzi 2018 SLAM application in surgery Particle filters (sequential Monte Carlo)</description></item><item><title>Filter localisation methods</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/filter-localisation-methods/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/filter-localisation-methods/</guid><description>Parent: SLAM Index Backlinks: [Back-end optimisation](back-end optimisation.md), [what is slam?](what is slam_.md), filter-based vs-optimisation-based-slam Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md) EKF to propagate and update motion states of visual-inertial sensors
Source: Scaradozzi 2018 SLAM application in surgery Filtering techniques in SLAM
Augment/refine the position estimates and map estimates by incorporating new measurements when they become available Generally online, due to their incremental nature Types Kalman filters Particle filters</description></item><item><title>State-of-the-art SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/state-of-the-art-slam/</link><pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/state-of-the-art-slam/</guid><description>Parent: SLAM Index Backlinks: Qin 2019 General Optimization-based Framework (Multisensor) Things that I&amp;rsquo;ve seen mentioned several times so far
ORBSLAM: monocular MonoSLAM: monocular (old?) — Andrew Davison OKVIS: visual inertial, stereovision PTAM: parallel tracking and mapping MSCKF: real-time EKF VINS-mono: visual inertial, monocular http://en.wikipedia.org/wiki/List_of_SLAM_Methods</description></item><item><title>Lamarca 2020 DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/lamarca-2020-defslam/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/lamarca-2020-defslam/</guid><description>URL: http://arxiv.org/abs/1908.08918 Authors: Lamarca et al Code: http://github.com/UZ-SLAMLab/DefSLAM Results (video): http://www.youtube.com/watch?v=6mmhD2_t6Gs Summary
First monocular SLAM for deformable environments in real-time Most other SLAM implementations assume rigidity Main techniques used (techniques for monocular non-rigid scenes): isometric shape from template (SfT) non-rigid structure from motion (NRSfM) Main principle: computation in two parallel threads (s. DefSLAM framework) Deformation tracking [front end] Deformation mapping [back end] The map from the mapping thread defines the shape-at-rest template used by deformation tracking Validation: compare with ORBSLAM (rigid) Assumes isometric deformation Future work: relocalisation (s.</description></item><item><title>Lamarca 2020 DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/lamarca-2020-defslam/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/lamarca-2020-defslam/</guid><description>URL: http://arxiv.org/abs/1908.08918 Authors: Lamarca et al Code: http://github.com/UZ-SLAMLab/DefSLAM Results (video): http://www.youtube.com/watch?v=6mmhD2_t6Gs Summary
First monocular SLAM for deformable environments in real-time Most other SLAM implementations assume rigidity Main techniques used (techniques for monocular non-rigid scenes): isometric shape from template (SfT) non-rigid structure from motion (NRSfM) Main principle: computation in two parallel threads (s. DefSLAM framework) Deformation tracking [front end] Deformation mapping [back end] The map from the mapping thread defines the shape-at-rest template used by deformation tracking Validation: compare with ORBSLAM (rigid) Assumes isometric deformation Future work: relocalisation (s.</description></item><item><title>SLAM Index</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/slam-index/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/slam-index/</guid><description>Definition Localisation What is SLAM? Main paradigms of SLAM Sensors for SLAM Position acquisition (relative vs. absolute) SLAM hardware Relative Odometry IMU Absolute [Sensors (absolute measurements) for measuring distance to landmarks](sensors (absolute measurements)-for-measuring-distance-to-landmarks.md) [Visual sensors for localisation](visual-sensors Monocular depth perception ular-depth-perception.md)
Pinhole camera model Camera calibration World to camera trafo Fusion Multisensor fusion Loose vs Tight coupling Why use the visual-inertial sensor combination? Visual SLAM Classification of image-based camera localization approaches Visual SLAM Implementation Framework Feature-based vs direct SLAM workflow Sparse Sparse/Feature-based VSLAM Dense Dense/direct VSLAM Landmarks/Feature extraction landmarks Bag of words Data association Data association Loop closure Loop closing in VIORB Loop closure detection Filter-based SLAM Filter localisation methods [Extended Kalman Filter](extended kalman filter.</description></item><item><title>Programmatic implementations of MonoSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/programmatic-implementations-of-monoslam/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/programmatic-implementations-of-monoslam/</guid><description>Parent: SLAM resources Python http://github.com/agnivsen/Py-M-SLAM http://github.com/agnivsen/LibMonoSLAM
MATLAB http://perso.ensta-paris.fr/~filliat/Courses/2011_projets_C10-2/BRUNEAU_DUBRAY_MURGUET/monoSLAM_bruneau_dubray_murguet_en.html</description></item><item><title>Back-end optimisation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/back-end-optimisation/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/back-end-optimisation/</guid><description>Parent: Visual SLAM Implementation Framework Source: cometlabs (Camera pose optimisation)
To compensate for drift of pose estimation Traditionally using EKF ( filter-based ) simple implementation therefore good for small scale estimations Alternative: bundle adjustment (graph optimisation) joint optimisation of the camera pose and the 3D structure parameters combines numerical methods and graph theory increasingly favoured over filtering, due to the latter&amp;rsquo;s inherent inconsistency more efficient when combined with sub-mapping</description></item><item><title>Feature-based vs direct SLAM workflow</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/feature-based-vs-direct-slam-workflow/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/feature-based-vs-direct-slam-workflow/</guid><description>Parent: SLAM Index Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Feature-based (aka sparse ) direct (aka dense ) Extraction of features required No abstraction necessary Aims to minimise error between point location estimate (from odometry) and location based on camera Tracks objects by minimising photometric error (intensity differences)</description></item><item><title>Loop closure detection</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/loop-closure-detection/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/loop-closure-detection/</guid><description>Parent: VSLAM Framework , slam-index Source: cometlabs Backlinks: Step 2: Re-observation Process of observing the same scene by non-adjacent frames and adding a constraint (relationship? association?) between them A long-term data association in the VSLAM Framework (part of front end) Sort of incorporates topological SLAM into metric SLAM Importance
Final refinement step (in data association) Important for obtaining a globally consistent SLAM solution, especially when optimising over a long period of time Basic loop closure detection Match the current frame to all previous frames using feature matching</description></item><item><title>Precision recall curve</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/precision-recall-curve/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/precision-recall-curve/</guid><description>Parent: Loop closure detection Source: cometlabs used to better quantify the performance (balance between false positives and false negatives in loop closure detection ) highlights tradeoff between precision and recall precision (absence of false positives) but may lead to the appearance of false negatives recall (prediction power) e.g. tweaking to improve recall increases sensitivity to similarities in the image thus increases possibility of false positives</description></item><item><title>(Wu 2018) Image-based camera localization</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/wu-2018-image-based-camera-localization-an-overview/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/wu-2018-image-based-camera-localization-an-overview/</guid><description>Authors: Wu, Tang, Li
Abstract/Contents
overview (classification) of image-based camera localization classification of image-based camera localization approaches techniques, trends only considers 2D cameras focuses on points as features in images (not lines etc) Chapters Classification of image-based camera localization approaches [Multisensor fusion](multisensor fusion.md) — why use the visual-inertial sensor combination? Loose vs Tight coupling Filter localisation methods Some optimisation-based tightly-coupled multisensor SLAM algorithms Questions
What&amp;rsquo;s a metric map &amp;ndash; normal map (with landmarks, normal distances) as opposed to a topological one Takeaway</description></item><item><title>Classification of image-based camera localization approaches</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/classification-of-image-based-camera-localization-approaches/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/classification-of-image-based-camera-localization-approaches/</guid><description>Parent: SLAM Index Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)
Unknown environment (must be reconstructed from image data) Online/real-time mapping (SLAM)
geometric metric SLAM (accurate computations, therefore still widely used in practice) monocular multiocular multi-kind sensor (active) loosely-coupled closely-coupled learning SLAM (active) needs a prior dataset to train NN &amp;ndash; dataset determines performanace of the SLAM low generalisation capability, therefore not as flexible as geom.</description></item><item><title>Dense/direct VSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/dense-direct-vslam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/dense-direct-vslam/</guid><description>Parent: Visual SLAM Implementation Framework , slam-index See also: Feature-based vs direct SLAM workflow Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Front-end part of the Visual SLAM Implementation Framework Use most or all of the pixels in each received frame Provide more information about the environment Many more pixels, require GPUs Feature-based vs direct SLAM workflow Disadvantages: Don&amp;rsquo;t handle outliers very well (outliers will be processed and implemented into the final map) Slower than feature-based variants Aims to minimise photometric error (intensity differences) Semi-dense</description></item><item><title>Feature maps</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/feature-maps/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/feature-maps/</guid><description>Parents: [Mapping representations in robotics](mapping representations-in-robotics.md), sparse/feature-based-vslam Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Uses a limited number of sparse objects to represent a map e.g. points, lines
Low computation cost because of the sparsity Map management solutions are good solutions for current applications What&amp;rsquo;s map management (probably storing maps in databases and recognising an existing map) [-] Sensitivity to false data association</description></item><item><title>Geometric metric SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/geometric-metric-slam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/geometric-metric-slam/</guid><description>Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md) Parent: Classification of image-based camera localization approaches Computes 3D maps with accurate mathematical equations
Classification according to sensors
monocular multiocular (most studies focus on binocular vision) multisensor fusion (e.g. vision and IMU &amp;ndash; vision and IMU fusion gaining in popularity) Classification according to techniques used
Filter-based SLAM Keyframe -based SLAM (active) Feature-based (keyframe-based feature SLAM) / sparse Direct / dense Grid-based SLAM (mainly deals with laser data, deals only a bit with image data) According to [76] keyframe-based can provide more accurate results compared to filter-based</description></item><item><title>Geometric metric SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/geometric-metric-slam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/geometric-metric-slam/</guid><description>Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md) Parent: Classification of image-based camera localization approaches Computes 3D maps with accurate mathematical equations
Classification according to sensors
monocular multiocular (most studies focus on binocular vision) multisensor fusion (e.g. vision and IMU &amp;ndash; vision and IMU fusion gaining in popularity) Classification according to techniques used
Filter-based SLAM Keyframe -based SLAM (active) Feature-based (keyframe-based feature SLAM) / sparse Direct / dense Grid-based SLAM (mainly deals with laser data, deals only a bit with image data) According to [76] keyframe-based can provide more accurate results compared to filter-based</description></item><item><title>Kidnapped robot problem</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kidnapped-robot-problem/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kidnapped-robot-problem/</guid><description>Source: Wikipedia Lokalisierung Backlinks: Lamarca 2020 DefSLAM Position initially known Then robot is repositioned without knowing it Robot has to be able to realise that the initial successful localisation isn&amp;rsquo;t valid any more &amp;ndash; a new global localisation must be carried out Realise this via unplausible sensor measurements (huge contradiction to prev. measurements) Has to do with the measure of robustness of the localisation method carried out</description></item><item><title>Loose vs Tight coupling</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/loose-vs-tight-coupling/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/loose-vs-tight-coupling/</guid><description>Parent: SLAM Index Backlinks: Multisensor fusion Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)
In loosely-coupled systems: all sensor states are independently estimated and optimized
easier to process frame and IMU data less accurate/robust compared to tight coupling e.g. Integrated IMU data can be incorporated as independent measurements in stereo vision optimization e.g. Vision-only pose estimates are used to update an EKF so that IMU propagation can be performed In tightly-coupled systems: all sensor states are jointly estimated and optimized</description></item><item><title>Multisensor fusion</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/multisensor-fusion/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/multisensor-fusion/</guid><description>Parent: SLAM Index , geometric-metric-slam Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Avoid limitations of using only one sensor Relative measurements: provide precise positioning information constantly At certain times absolute measurements are made to correct potential errors (correct drift) several approaches (for localisation), e.g. merge sensor feeds at the lowest level before being processed homogeneously hierarchical approaches (fuse state estimates derived independently from multiple sensors) s.</description></item><item><title>Some optimisation-based tightly-coupled multisensor SLAM algorithms</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/some-optimisation-based-tightly-coupled-multisensor-slam-algorithms/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/some-optimisation-based-tightly-coupled-multisensor-slam-algorithms/</guid><description>Parent: SLAM Index Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)
Uses nonlinear optimization may potentially achieve higher accuracy due to the capability to limit linearization errors through repeated linearization of the inherently nonlinear problem
[117] Forster: preintegration theory [118] OKVIS: a novel approach to tightly integrate visual measurements with IMU optimise a joint nonlinear cost function that integrates an IMU error term with the landmark reprojection error in a fully probabilistic manner real-time operation: old states are marginalized to maintain a bounded-sized optimization window Li et al.</description></item><item><title>Some optimisation-based tightly-coupled multisensor SLAM algorithms</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/some-optimisation-based-tightly-coupled-multisensor-slam-algorithms/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/some-optimisation-based-tightly-coupled-multisensor-slam-algorithms/</guid><description>Parent: SLAM Index Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)
Uses nonlinear optimization may potentially achieve higher accuracy due to the capability to limit linearization errors through repeated linearization of the inherently nonlinear problem
[117] Forster: preintegration theory [118] OKVIS: a novel approach to tightly integrate visual measurements with IMU optimise a joint nonlinear cost function that integrates an IMU error term with the landmark reprojection error in a fully probabilistic manner real-time operation: old states are marginalized to maintain a bounded-sized optimization window Li et al.</description></item><item><title>Sparse/Feature-based VSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/sparse-feature-based-vslam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/sparse-feature-based-vslam/</guid><description>Parent: Visual SLAM Implementation Framework , slam-index See also: Feature-based vs direct SLAM workflow Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Front-end part of the Visual SLAM Implementation Framework Use only a small selected subset of the pixels in an image frame Feature maps generated are point clouds &amp;ndash;&amp;gt; used to track the camera pose Requires feature extraction and matching To minimise: reprojection error (difference between a point&amp;rsquo;s tracked location and where it is expected to be given camera pose estimate) Pose estimation based on RANSAC A frame with most of its features concentrated in a small area: bad as the features are more likely to overlap Sparse</description></item><item><title>Topological SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/topological-slam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/topological-slam/</guid><description>Parent: Classification of image-based camera localization approaches Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)
does not need accurate computation of 3D maps represents the environment by connectivity or topology e.g. Kuipers [130] used a hierarchical description of the spatial environment
a topological network description mediates between a control and metrical level distinctive places and paths are defined by their properties at the control level serve as nodes and arcs of the topological model Decreasing in popularity</description></item><item><title>Visual SLAM Implementation Framework</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/visual-slam-implementation-framework/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/visual-slam-implementation-framework/</guid><description>Parent: SLAM Index Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Basic principle:
tracking a set of points through successive frames these tracks are used to triangulate the 3D positions of the points to create the map at the same time, using the the est point locations to calculate the pose of the camera, which could have observed them (i.e. calculate real time 3D structure of a scene from the estimated motion of the camera) Two main architecture components:</description></item><item><title>What is SLAM?</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/what-is-slam-/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/what-is-slam-/</guid><description>Parent: SLAM Index Source: Scaradozzi 2018 Process which allows a mobile robot to
construct a map of its environment (assumed to be unknown) compute its location using the map simultaneously Source: Lamarca 2019 DefSLAM Goal is to locate a sensor in an unknown map/environment, which is simultaneously being reconstructed. Typically used in exploratory trajectories (new or changing environments) Source: Wikipedia SLAM Simultaneous localization and mapping (SLAM)</description></item><item><title>Riisgaard SLAM for dummies</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/riisgaard-slam-for-dummies/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/riisgaard-slam-for-dummies/</guid><description>Authors: Søren Riisgaard and Morten Rufus Blas Parent: SLAM resources Abstract:
Tutorial introduction to SLAM, with minimal prerequisites for the understanding of SLAM as explained here Mostly explains a single approach to the steps involved in SLAM Complete solution for SLAM using EKF (extended Kalman filter) Only considers 2D motion, not 3D Chapters
What is SLAM? Overview of SLAM using EKF Hardware Robot Range measurement device SLAM process Step 1: Odometry update Step 2: Reobservation Step 3: Add new landmarks Laser data Odometry data Landmarks Landmark extraction 1.</description></item><item><title>SLAM resources</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/slam-resources/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/slam-resources/</guid><description>Parent: SLAM Index Theory
Wikipedia SLAM http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation Thrun - Probabilistic Robotics SLAM for dummies Andrew Davison research page at the Department of Computing , Imperial College London about SLAM using vision. Paper 2002 on monocular SLAM SLAM lectures on YouTube http://openslam-org.github.io / Tutorials SLAM summer school SS06: http://www.robots.ox.ac.uk/~SSS06/Website/
Programming Programmatic implementations of MonoSLAM</description></item><item><title>Wikipedia SLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-slam/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-slam/</guid><description>Source: http://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping Parents: SLAM Index , slam-resources Different types of sensors give rise to different SLAM algorithms whose assumptions are most appropriate to the sensors.
At one extreme, visual features provide details of many points within an area &amp;ndash;&amp;gt; rendering SLAM unnecessary shapes in these point clouds can be easily and unambiguously aligned at each step via image registration . At the opposite extreme, tactile sensors are extremely sparse they contain only information about points very close to the agent require strong prior models to compensate in purely tactile SLAM.</description></item></channel></rss>