<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>-sa/processed on</title><link>https://salehahr.github.io/tags/sa/processed/</link><description>Recent content in -sa/processed on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://salehahr.github.io/tags/sa/processed/index.xml" rel="self" type="application/rss+xml"/><item><title>Rigid cystoscope dimensions</title><link>https://salehahr.github.io/studienarbeit/rigid-cystoscope-dimensions/</link><pubDate>Fri, 20 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/rigid-cystoscope-dimensions/</guid><description> ca 20cm x 65 cm = 0.02m x 0.065 m
Source: http://en.wikipedia.org/wiki/Cystoscopy
The sizes of the sheath of the rigid cystoscope are 17 French gauge (5.7 mm diameter), 19 Fr gauge (6.3 mm diameter), and 22 Fr gauge (7.3 mm diameter).
Source: http://www.karlstorz.com/cps/rde/xbcr/karlstorz_assets/ASSETS/3405020.pdf Camera</description></item><item><title>50.2.3 Kalman filter initial estimates</title><link>https://salehahr.github.io/studienarbeit/50.2.3-kalman-filter-initial-estimates/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.3-kalman-filter-initial-estimates/</guid><description>Source: [Schneider 2013 How to not make the EKF fail](schneider 2013 how to-not-make-the-ekf-fail.md)
Initial state estimate x0, P0
Filter generally not badly affected by wrong initial state x0, but convergence will be slow if we are way off
If P0 too small whereas x0 is way off
the gain K becomes small filter relies on the model more than on the measurements Thus: important to have a consistent pair x0, P0</description></item><item><title>50.2.40 Kalman filter performance metric</title><link>https://salehahr.github.io/studienarbeit/50.2.40-kalman-filter-performance-metric/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.40-kalman-filter-performance-metric/</guid><description>Source: [Schneider 2013 How to not make the EKF fail](schneider 2013 how to-not-make-the-ekf-fail.md)
k: time / step j: how many EKF runs? in tutorial: EKF was ran 1000 times (non-deterministic system due to noise)</description></item><item><title>Schneider 2013 How to not make the EKF fail</title><link>https://salehahr.github.io/studienarbeit/schneider-2013-how-to-not-make-the-ekf-fail/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/schneider-2013-how-to-not-make-the-ekf-fail/</guid><description>Authors: Schneider, Georgakis URL: http://www.researchgate.net/publication/263942618_How_To_NOT_Make_the_Extended_Kalman_Filter_Fail/citations DOI 10.1021/ie300415d Measurement noise R, V (landmark) Kalman filter initial estimates Process noise Q and W (odometry) Kalman filter performance metric</description></item><item><title>(Markley 2014) Fundamentals of Spacecraft Attitude Determination and Control</title><link>https://salehahr.github.io/studienarbeit/markley-2014-fundamentals-of-spacecraft-attitude-determination-and-control/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/markley-2014-fundamentals-of-spacecraft-attitude-determination-and-control/</guid><description>Authors: FL Markley, John Crassidis DOI: 10.1007/978-1-4939-0802-8
Note/Nomenclature:
This book interpetes rotations/transformations in the passive/alias sense (I&amp;rsquo;m not a fan) Quaternions in JPL convention instead of Hamiltonian (not a fan of this either&amp;hellip;) Rotation matrix = attitude matrix Introduction
Attitude determination: memoryless approach without using statistics Attitude estimation: approaches with memory, uses statistical info from a series of measurements filter approaches uses a dynamic motion model of the object Quaternions Quaternion conventions Quaternion multiplication Rotations &amp;ldquo;Euler&amp;rsquo;s theorem: any rotation is a rotation about a fixed axis&amp;rdquo;</description></item><item><title>50.4.1 Additive quaternion filtering</title><link>https://salehahr.github.io/studienarbeit/50.4.1-additive-quaternion-filtering/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.4.1-additive-quaternion-filtering/</guid><description>Parents: [Quaternion index](quaternion index.md), which orientation parametrisation to-choose? Source: Markley Fundamentals of Spacecraft Attitude Determination Additive quaternion filtering Additive quaternion error Methods of enforcing the normalisation
Renormalise the estimate by brute force Modify KF update equations to enforce a norm constraint using a Lagrange multiplier
[1] and [2] yield biased estimates of the quaternion
Methods that don&amp;rsquo;t enforce normalisation
Define the rotation matrix to be guarantees orthogonality introduces unobservable DOF: the quaternion norm Use the above equation without the ||q||-2 factor &amp;ndash;&amp;gt; no orthogonality</description></item><item><title>50.4.2 Multiplicative quaternion filtering (MEKF)</title><link>https://salehahr.github.io/studienarbeit/50.4.2-multiplicative-quaternion-filtering-mekf/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.4.2-multiplicative-quaternion-filtering-mekf/</guid><description>See also: Which orientation parametrisation to choose? Source: Markley Fundamentals of Spacecraft Attitude Determination Main idea is to use
the quaternion as a global rotation representation
a three component state vector as the local representation of rotation errors each term (q_true, delta_q, q_est) is a normalised unit quaternion
Any of the rotation error representations can be used to calculate delta_theta, which is part of the error state of the MEKF.</description></item><item><title>50.7.2 Calculation of K and P in ESKF update</title><link>https://salehahr.github.io/studienarbeit/50.7.2-calculation-of-k-and-p-in-eskf-update/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.7.2-calculation-of-k-and-p-in-eskf-update/</guid><description>Parent: 50.3 Error-State Kalman Filter , eskf-update See also: Evaluation of the H Jacobian Source: Solà 2017 Quaternion kinematics for ESKF The filter correction equations are (yields a posteriori estimates)
Notes:
Here, the simplest form of the covariance update is used. This has poor numerical stability, however (no guarantee of symmetricity or positive definiteness) More stable forms are e.g. Joseph form (symmetric and positive) Error correction?</description></item><item><title>Euler axis/angle representation</title><link>https://salehahr.github.io/studienarbeit/euler-axis-angle-representation/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/euler-axis-angle-representation/</guid><description>Parents: [Rotations / SO(3) group index](rotations _ so(3)-group-index.md), orientation-parametrisations See also: Rotation vector representation Backlinks: [Gibbs / Rodrigues parameter representation for rotations](gibbs _-rodrigues-parameter-representation-for-rotations.md)
Source: Markley Fundamentals of Spacecraft Attitude Determination 3 parameters:
there appears to be 4: 1 angle, 3-component unit vector (Euler axis, Euler angle of the rotation) however, the vector e is a unit vector (constrained by ) To rotation matrix The matrix is periodic (period 2*pi) From rotation matrix If -1 &amp;lt; cos theta &amp;lt; 1: If cos theta = 1: axis undefined</description></item><item><title>Gibbs / Rodrigues parameter representation for rotations</title><link>https://salehahr.github.io/studienarbeit/gibbs-rodrigues-parameter-representation-for-rotations/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/gibbs-rodrigues-parameter-representation-for-rotations/</guid><description>Parent: Orientation parametrisations Backlinks: [50.5.1 Observation of the error state (filter correction)](50.5.1 observation of the error state (filter correction).md), 50.5.3-eskf-reset See also: Rotation error representation Source: Markley Fundamentals of Spacecraft Attitude Determination From unit quaternions : From Euler axis/angle : To unit quaternions :   Plane of the figure contains identity quaternion, origin The circle is a cross section of the quaternion sphere S^3 The upper horizontal axis is the 3D Gibbs vector hyperplane (tangent at the identity quaternion) [+] q and -q map to the same Gibbs vector, therefore there is a 1:1 mapping of rotations between quaternions and the Gibbs parameter [-] the Gibbs vector is infinite for 180 degree rotations (q.</description></item><item><title>Intrinsic vs extrinsic rotations</title><link>https://salehahr.github.io/studienarbeit/intrinsic-vs-extrinsic-rotations/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/intrinsic-vs-extrinsic-rotations/</guid><description>Parent: Rotations / SO(3) group index See also: Active/passive or Alibi/alias rotation transformations Source: http://rock-learning.github.io/pytransform3d/transformation_ambiguities.html We want to rotate first by R1, then by R2.
In global coordnates, extrinsic rotation: In local coordinates, intrinsic rotation: (R1 defines new coordinates in which R2 is applied) Specifying the convention is relevant when dealing with Euler angles!!!</description></item><item><title>Quaternion to rotation matrix</title><link>https://salehahr.github.io/studienarbeit/quaternion-to-rotation-matrix/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/quaternion-to-rotation-matrix/</guid><description>Parents: [Quaternion index](quaternion index.md), rotations / so(3) group-index Source: Markley Fundamentals of Spacecraft Attitude Determination s. Unit quaternions for a non-explanation on double cover (why the angle is halved)</description></item><item><title>Rotation error representation</title><link>https://salehahr.github.io/studienarbeit/rotation-error-representation/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/rotation-error-representation/</guid><description>Parents: [Rotations / SO(3) group index](rotations _ so(3)-group-index.md), quaternion-index Backlinks: [50.2.2.1 Variables in ESKF using IMUs](50.2.2.1 variables in eskf using imus.md), mekf-measurement-update See also: [Orientation parametrisations](orientation parametrisations.md), which orientation parametrisation to-choose? Source: Markley Fundamentals of Spacecraft Attitude Determination Note:
Only for small angle approximations! all these representations are equivalent through second order as In terms of&amp;hellip;
Rotation vector Quaternion Euler angles Use upper sign if {i,j,k} even permutation of {1,2,3}, lower sign otherwise</description></item><item><title>Rotation vector representation</title><link>https://salehahr.github.io/studienarbeit/rotation-vector-representation/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/rotation-vector-representation/</guid><description>Parents: [Rotations / SO(3) group index](rotations _ so(3)-group-index.md), orientation-parametrisations See also: Euler axis/angle representation Source: Markley Fundamentals of Spacecraft Attitude Determination Combine the Euler axis/angle into a three component rotation vector Convenient for analysis, but not for computation</description></item><item><title>Rotations / SO(3) group index</title><link>https://salehahr.github.io/studienarbeit/rotations-so-3-group-index/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/rotations-so-3-group-index/</guid><description>Group theory SE(3) Special Euclidian Group SO(3) 3D rotation group Lie group, Lie algebra Exponential map Logarithm map Ambiguities in rotation representations Active/passive or Alibi/alias rotation transformations Intrinsic vs extrinsic rotations Rotation representations Orientation parametrisations Which orientation parametrisation to choose? Linearisation of an orientation in SO(3) As Euler angles
Euler angles Rotations as xyz Bryan-Tait angles (Kardanwinkel) Axis/angle</description></item><item><title>20.4 Which orientation parametrisation to choose?</title><link>https://salehahr.github.io/studienarbeit/20.4-which-orientation-parametrisation-to-choose-/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/20.4-which-orientation-parametrisation-to-choose-/</guid><description>Parent: [Rotations / SO(3) group index](rotations _ so(3) group-index.md), quaternion-index , orientation-parametrisations See also: Rotation error representation Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
Estimation algorithms (filtering, smoothing) usually assume that the unknown states and parameters are represented in Euclidean space
Due to wrapping and gimbal lock, Euclidian addition and subtraction don&amp;rsquo;t work Also generally don&amp;rsquo;t work for rotation matrices and unit quaternions Constraints (unit quaternion norm, rotation matrix orthogonality) are usually hard to implement in estimation algorithms these concerns are led to the development of the MEKF To deal with this: Linearisation of an orientation in SO(3) Alternative method to estimate orientation:</description></item><item><title>Active/passive or Alibi/alias rotation transformations</title><link>https://salehahr.github.io/studienarbeit/active-passive-or-alibi-alias-rotation-transformations/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/active-passive-or-alibi-alias-rotation-transformations/</guid><description>Parent: Rotations / SO(3) group index See also: Intrinsic vs extrinsic rotations Backlinks: Markley Fundamentals of Spacecraft Attitude Determination Source: [http://en.wikipedia.org/wiki/Rotation_matrix
Ambiguities](http://en.wikipedia.org/wiki/Rotation_matrix Ambiguities) Alibi / Active Alias / Passive CS is fixed CS is rotated Point rotates within fixed CS Point remains stationary but is represented within a new CS Counterclockwise rotation by theta mathematics physics, robotics Source: http://rock-learning.</description></item><item><title>Euler angles</title><link>https://salehahr.github.io/studienarbeit/euler-angles/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/euler-angles/</guid><description>Parents: [Rotations / SO(3) group index](rotations _ so(3)-group-index.md), orientation-parametrisations Backlinks: Orientation parametrisations Source: http://en.wikipedia.org/wiki/Euler_angles Possible representations
Proper Euler angles (e.g. zxz) vs Tait-Bryan (e.g. xyz, zyx) Extrinsic rotations (around fixed CS xyz) vs intrinsic rotations (around body CS XYZ = x''' y''' z''') As a rotation matrix This means either: (s. Intrinsic vs extrinsic rotations )
extrinsic rotations about z -&amp;gt; y -&amp;gt; x / yaw pitch roll intrinsic rotations about x -&amp;gt; y' -&amp;gt; z'' = Z = z''' Note: Any extrinsic rotation is equivalent to an intrinsic rotation by the same angles but with inverted order of elemental rotations, and vice versa.</description></item><item><title>Whampsey MEKF</title><link>https://salehahr.github.io/studienarbeit/whampsey-mekf/</link><pubDate>Sat, 14 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/whampsey-mekf/</guid><description>http://matthewhampsey.github.io/blog/2020/07/18/mekf
Motivation:
Working with noisy IMU measurements IMUs usually provide redundant information that can be used to improve dead-reckoning Uses: Hamilton quaternion convention Which orientation parametrisation to choose? Error-State Kalman Filter</description></item><item><title>ESKF repos</title><link>https://salehahr.github.io/studienarbeit/eskf-repos/</link><pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/eskf-repos/</guid><description>C++ http://github.com/skrogh/msf_ekf http://github.com/je310/ESKF http://github.com/hobbeshunter/IMU_EKF (only IMU)
Python http://github.com/enginBozkurt/Error-State-Extended-Kalman-Filter http://github.com/uoip/stereo_vio_eskf (unsuccessful) &amp;ndash; uses average IMU readings http://github.com/aipiano/ESEKF_IMU</description></item><item><title>Converting IMU data to inertial frame</title><link>https://salehahr.github.io/studienarbeit/converting-imu-data-to-inertial-frame/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/converting-imu-data-to-inertial-frame/</guid><description>Parent: IMU index Source: http://redshiftlabs.com.au/wp-content/uploads/2018/02/an-1005_-_understanding_euler_angles.pdf IMU outputs are in the body frame of the sensor.
Convention used in the article: yaw/psi (z) - pitch/theta (y) - roll/phi (x) around momentary axes Momentary coordinate systems: W -&amp;gt; W' -&amp;gt; W'' -&amp;gt; B Body acceleration to inertial acceleration W_a = R_WB @ B_a
Body angular rate to inertial angular rate
Each angular rate must be converted to the corresponding frame p: gyro_z -&amp;gt; rotated into W: R_w_w' @ R_w'_w'' @ R_w''_B @ q q: gyro_y -&amp;gt; rotated into W': R_w'_w'' @ R_w''_B @ q r: gyro_x -&amp;gt; rotated into W'': R_w''_B @ r with Gimbal lock: pitch approaches +-90, terms divided by cos90</description></item><item><title>Exponential map</title><link>https://salehahr.github.io/studienarbeit/exponential-map/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/exponential-map/</guid><description>Parent: [Quaternion index](quaternion index.md), rotations / so(3) group-index Backlinks: Linearisation of an orientation in SO(3) Source: Forster 2017 IMU Preintegration [At the identity] maps an element of the lie-algebra (a skew symmetric matrix) to a rotation First order approximation Some properties of the exponential map
Perturbations, first order approximation with the right Jacobian of SO(3) Jr = I for (very small angles)
Following the Adjoint expression difference between Exp and exp?</description></item><item><title>Importing a fork in Python instead of installed package</title><link>https://salehahr.github.io/studienarbeit/importing-a-fork-in-python-instead-of-installed-package/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/importing-a-fork-in-python-instead-of-installed-package/</guid><description>http://stackoverflow.com/questions/23075397/python-how-to-edit-an-installed-package
Run this in repo that uses the fork (this installs the package as a submodule): python3 -m pip install -e git+[ssh://git@github-feudalism/feudalism/spatialmath-python.git
egg=f-spatialmath](ssh://git@github-feudalism/feudalism/spatialmath-python.git egg=f-spatialmath) &amp;ndash;upgrade Instructions:
Fork the package repo cd to own repo where you want to use the package Install the fork using the above pip install command. This creates ./src/submodule When making changes to fork: make changes in either the submodule folder (for immediate effect), or in the fork subdirectory + push + reinstall</description></item><item><title>Lie group, Lie algebra</title><link>https://salehahr.github.io/studienarbeit/lie-group-lie-algebra/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/lie-group-lie-algebra/</guid><description>Parent: Rotations / SO(3) group index Backlinks: SO(3) 3D rotation group Source: http://www.seas.upenn.edu/~meam620/slides/kinematicsI.pdf A group that is a differentiable (smooth) manifold is called a Lie group
Source: http://en.wikipedia.org/wiki/3D_rotation_group Lie algebra  Every Lie group has an associated Lie algebra Lie algebra: linear space with same dimension as the Lie group Consists of all skew-symmetric 3x3 matrices Elements of the Lie algebra are elements of the tangent space of the manifold SO(3)/Lie group at the identity element Source: Forster 2017 IMU Preintegration An Euler vector can be represented by a skew symmetric matrix in the Lie algebra Mappings: Exponential map , logarithm-map</description></item><item><title>Logarithm map</title><link>https://salehahr.github.io/studienarbeit/logarithm-map/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/logarithm-map/</guid><description>Parent: [Quaternion index](quaternion index.md), rotations / so(3) group-index Backlinks: Linearisation of an orientation in SO(3) Source: Forster 2017 IMU Preintegration Maps a rotation matrix R in SO(3) to a skew-symmetric-matrix-( lie-algebra ) Perturbations, first order approximation S. Forster [2015] suppplementary material for the inverse Jacobian
Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
Approximations for small perturbations</description></item><item><title>30.1.1 Endoscopy</title><link>https://salehahr.github.io/studienarbeit/30.1.1-endoscopy/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/30.1.1-endoscopy/</guid><description>Source: NHS Backlinks: Endoscopes Endoscopy is a procedure which enables inspection of organs inside the body. The prefix &amp;lsquo;endo&amp;rsquo; comes from the Greek language and means &amp;lsquo;within&amp;rsquo; or &amp;lsquo;inside&amp;rsquo;, cf. the prefix &amp;lsquo;exo&amp;rsquo;/&amp;lsquo;ecto&amp;rsquo; meaning &amp;lsquo;outside&amp;rsquo;. Endo- from Greek ἔνδον (within, inside), cf. exo-/ecto- from έκτός (outside) The instrument: endoscope</description></item><item><title>30.1.2 Endoscope</title><link>https://salehahr.github.io/studienarbeit/30.1.2-endoscope/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/30.1.2-endoscope/</guid><description>Source: NHS Backlinks: Endoscopes The general term for the medical instrument used to perform endoscopy is, correspondingly, the endoscope. It is a device that makes use of optical technology to relay images from one end of the scope to another.
Functions are  not only for looking inside, but also have additional functionalities such as removing small tissue samples (biopsy) Insertion either through
natural body orifices (e.g. mouth, urethra) small incision in case a keyhole surgery is being performed</description></item><item><title>30.1.2.2 Endoscope system components</title><link>https://salehahr.github.io/studienarbeit/30.1.2.2-endoscope-system-components/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/30.1.2.2-endoscope-system-components/</guid><description>Source: Leiner Backlinks: Endoscopes Imaging system (rod lens array for rigid endoscopes) within a tube/shaft
Illumination/Light source (it&amp;rsquo;s dark inside the body)
separate from the imaging system in order to reduce glare [low contrast of received image] surrounds the imaging system like a ring light Video camera
Coupling device (camera to imaging system)
Sheath and bridge which contain the telescope, fibre optics, and provides a channel for other stuff like irrigation, forceps, other surgical instruments</description></item><item><title>30.1.2.3 Endoscope specification</title><link>https://salehahr.github.io/studienarbeit/30.1.2.3-endoscope-specification/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/30.1.2.3-endoscope-specification/</guid><description>Source: Leiner Backlinks: Endoscopes Field of view: maximum angle that can be viewed Leiner Generally constructed so that the FOV is wide enough so that the &amp;lsquo;head on&amp;rsquo; view is visible even when the DOV is not zero. Leiner This is to reduce the chances of bumping the instrument into anatomy right in front of the shaft. Leiner Direction of view: angular offset of the optical axis from the longitudinal axis of the endoscope shaft</description></item><item><title>Distal and proximal ends</title><link>https://salehahr.github.io/studienarbeit/distal-and-proximal-ends/</link><pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/distal-and-proximal-ends/</guid><description>Source: Leiner distal: far from the surgeon proximal: near the surgeon</description></item><item><title>Smooth polymial trajectory generation</title><link>https://salehahr.github.io/studienarbeit/smooth-polymial-trajectory-generation/</link><pubDate>Mon, 19 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/smooth-polymial-trajectory-generation/</guid><description>Source: FLS handouts</description></item><item><title>Program outline</title><link>https://salehahr.github.io/studienarbeit/program-outline/</link><pubDate>Wed, 07 Jul 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/program-outline/</guid><description>Current assumptions (to take care of later!)
Probe is rigid — DOFs are either 0 or constant — switch to rotating scope later No gravity No bias/offset, no noise in IMU Note: Stuff marked with checkboxes are either to-dos or things I&amp;rsquo;m not sure that I implemented correctly
http://github.com/feudalism/dvi-ekf/tree/eskf; projects generate_data.py Data generation (is called from main.py)
Main objects
Generate camera data (from DefSLAM mono trajectory) Make RigidSimpleProbe (for now, all DOFs are 0 or constant) Make IMU object, generate first (om, acc) values from interpolated camera data ( - should generate it from stereo data instead) Variables</description></item><item><title>Differentiation in different coordinate systems</title><link>https://salehahr.github.io/studienarbeit/differentiation-in-different-coordinate-systems/</link><pubDate>Sun, 27 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/differentiation-in-different-coordinate-systems/</guid><description>Source: Woernle Mehrkörpersysteme Backlinks: Kinematics primer</description></item><item><title>Thesis restructure</title><link>https://salehahr.github.io/studienarbeit/thesis-restructure/</link><pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/thesis-restructure/</guid><description>Parent: Thesis Backlinks: Next update 2021-06 Introduction
General Bladder cancer surgery — cryosection The navigation/localisation problem GRK Problem statement Questions arising from the localisation task:
Which localisation algorithm do we use? We have settled on the camera-IMU combination. How do we take into account the non-constant calibration parameters between the camera and IMU (due to the surgeon&amp;rsquo;s manipulation of the cystoscope)? Also, modularity of setup: the calibration estimation allows the IMU-camera combination to be applied to different devices  Corresponding contributions Literature research deformable VI localisation Modelling the kinematic relations between the IMU and camera on the cystoscope Estimation of IMU calibration parameters for VI localisation on cystoscope Literature review (I)</description></item><item><title>Update 2021-07-19</title><link>https://salehahr.github.io/studienarbeit/update-2021-07-19/</link><pubDate>Sun, 20 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/update-2021-07-19/</guid><description>Agenda
Comparison prop only, prop + update Comparison P+U plots (Rp 1000, Rp .01, Rp 1e-6) Changes to equations (pc, qc, err_pc, err_theta_c) s. KF kinematics Currently: getting probe output as function of DOFs Open tasks s. also dvi-eskf project board debug update stage??? get probe outputs as symbols/functions of DOFs switch from rigid probe to rotating scope - at which point do I compensate for notch rotation?</description></item><item><title>Camera views as seen by SLAM at distal end of probe/scope</title><link>https://salehahr.github.io/studienarbeit/camera-views-as-seen-by-slam-at-distal-end-of-probe-scope/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/camera-views-as-seen-by-slam-at-distal-end-of-probe-scope/</guid><description/></item><item><title>Equations for obtaining omega (angular velocity) and acceleration of IMU from camera</title><link>https://salehahr.github.io/studienarbeit/equations-for-obtaining-omega-angular-velocity-and-acceleration-of-imu-from-camera/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/equations-for-obtaining-omega-angular-velocity-and-acceleration-of-imu-from-camera/</guid><description>Parent: Update 2021-06-11</description></item><item><title>Notch positions due to scope rotation</title><link>https://salehahr.github.io/studienarbeit/notch-positions-due-to-scope-rotation/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/notch-positions-due-to-scope-rotation/</guid><description>Backlinks: Update 2021-06-11</description></item><item><title>Obtaining IMU measurements from camera by forward kinematics</title><link>https://salehahr.github.io/studienarbeit/obtaining-imu-measurements-from-camera-by-forward-kinematics/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/obtaining-imu-measurements-from-camera-by-forward-kinematics/</guid><description>Parent: SA TODO Backlinks: Thesis restructure Done:
reverse-fwkin (scrapped) omega_B symbolic check links in BC and CB config — new diagrams (split up into &amp;gt;=2 bodies?) check om_B = om_C + om_CB (s. [http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters Kinematics](http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters Kinematics), Woernle ) save om_B to container obtain accel. (s. [http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters Kinematics](http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters Kinematics), Kinematics primer ) where (ang. vel of body j w.r.t. body i, expressed in CS k)  Chaining velocities and accelerations: [validation] reconstruct rot_B from om_B, compare with camera debug first reconstruction of IMU traj, if that doesn&amp;rsquo;t work debug the fake data generation - update readme update robot model with simplification around pivot point validate updated model Anhang</description></item><item><title>Update 2021-06-11</title><link>https://salehahr.github.io/studienarbeit/update-2021-06-11/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/update-2021-06-11/</guid><description>General model for probe (from Forward kinematics IMU to camera )
Using the standard DH convention  Note: I have since changed the axis configuration at the camera part—above diagram is no longer up to date; to be updated!
This has the IMU (B) as the base and the camera (C) as the end effector Using robotics-toolbox-python: http://github.com/petercorke/robotics-toolbox-python Simplified model (from Forward kinematics IMU to camera ) Currently using a simplified model with all degrees of freedom set to 0 or constant Modification to (above) existing robot model Probably need to modify the rotational joints around the pivot s.</description></item><item><title>Rigid cystoscope mechanism</title><link>https://salehahr.github.io/studienarbeit/rigid-cystoscope-mechanism/</link><pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/rigid-cystoscope-mechanism/</guid><description>Parent: Update 2021-06-11</description></item><item><title>Discussion 2021-06-01</title><link>https://salehahr.github.io/studienarbeit/discussion-2021-06-01/</link><pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/discussion-2021-06-01/</guid><description>Notes
IMU data to be generated using kinematic relations, not via numerical differentiation Reduce loss of data, model for prediction, noise propagation Forward kinematics B &amp;ndash;&amp;gt; C (everything in terms of SLAM coordinates), s. Probe forward kinematics For visualisation: IMU data in W coordinates Monday: real probe Python robotics toolboxes for generating of forward kinematics matrices, velocity expressions (symbolic differentiation) Predict step
Kinematics ( equations of motion IMU to camera ) = f(DOF) p_BC = f(l1, l2) v_BC = f(ang_vel) a_BC = f(acc)</description></item><item><title>Equations of motion IMU to camera</title><link>https://salehahr.github.io/studienarbeit/kinematics-equations-of-motion-imu-to-camera/</link><pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/kinematics-equations-of-motion-imu-to-camera/</guid><description>Backlinks: Discussion 2021-06-01 R_WB = R_WC * R_CB
Notes: ref http://docs.sympy.org/latest/modules/physics/vector/vectors.html for vector calculus (symbolic)</description></item><item><title>Forward kinematics IMU to camera</title><link>https://salehahr.github.io/studienarbeit/forward-kinematics-imu-to-camera/</link><pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/forward-kinematics-imu-to-camera/</guid><description>Backlinks: [Discussion 2021-05-21](discussion 2021-05-21.md), discussion-2021-06-01 , update-2021-06-11 Simplified model (rigid)</description></item><item><title>Inverse of a homogeneous transformation matrix</title><link>https://salehahr.github.io/studienarbeit/inverse-of-a-homogeneous-transformation-matrix/</link><pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/inverse-of-a-homogeneous-transformation-matrix/</guid><description>Parent: Kinematics primer Source: http://mathematica.stackexchange.com/questions/106257/how-do-i-get-the-inverse-of-a-homogeneous-transformation-matrix</description></item><item><title>Discussion 2021-05-25</title><link>https://salehahr.github.io/studienarbeit/discussion-2021-05-25/</link><pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/discussion-2021-05-25/</guid><description>Backlinks: Discussion 2021-05-10 , discussion-2021-05-21 Notes
IMU-rod transformation: rotation part (spherical joint), translation part predict and update equations? maybe change variables in states vector to local coordinates add gravity later Ausblick: Einfluss der IMU auf verbesserte Lokalisierung &amp;ndash;&amp;gt; evtl eine IMU Koordinate weglassen Next:
generate fake imu data (delegated, s. [obtaining imu measurements from camera by forward kinematics](obtaining imu measurements-from-camera-by-forward-kinematics.md)) look for existing literature on IMU fusion/EKF which uses kinematic relations Massenmatrix, Koriolisterme etc.</description></item><item><title>(Hibbeler) Dynamics</title><link>https://salehahr.github.io/studienarbeit/hibbeler-dynamics/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/hibbeler-dynamics/</guid><description>Author: Russell Hibbeler Contents
Kinematics, kinetics of particle [planar] rigid body [3D] rigid body Vibrations Kinematics primer</description></item><item><title>(Woernle) Mehrkoerpersysteme</title><link>https://salehahr.github.io/studienarbeit/woernle-mehrkoerpersysteme/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/woernle-mehrkoerpersysteme/</guid><description>Author: Christoph Woernle Contents:
Kinematics, kinetics (Dynamik) Some basics Converting velocity from CS1 to CS0 Chaining rotation matrices and angular velocities [Poisson equation for skew symmetric matrix of angular velocity](poisson equation for skew-symmetric-matrix-of-angular-velocity.md) Differentiation in different coordinate systems Kinematics primer Reversed kinematics relations Rotations as xyz Bryan-Tait angles (Kardanwinkel) Holonomic systems, non-holonomic systems Holonomic constraints</description></item><item><title>Chaining rotation matrices and angular velocities</title><link>https://salehahr.github.io/studienarbeit/chaining-rotation-matrices-and-angular-velocities/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/chaining-rotation-matrices-and-angular-velocities/</guid><description>Source: Woernle Mehrkörpersysteme Backlinks: [Kinematics primer](kinematics primer.md), [obtaining imu measurements from camera by forward kinematics](obtaining imu measurements-from-camera-by-forward-kinematics.md) See also: Rotations / SO(3) group index Chaining rotation matrices T_02 transforms a point in CS2 to CS0
compare: Chaining homogeneous transformation matrices Chaining angular velocities (in same CS) ang.vel. of 2 rel to 0 = ang.vel. of 1 rel. to 0 + ang.vel. of 2 rel. to 1</description></item><item><title>Converting velocity from CS1 to CS0</title><link>https://salehahr.github.io/studienarbeit/converting-velocity-from-cs1-to-cs0/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/converting-velocity-from-cs1-to-cs0/</guid><description>Source: Woernle Mehrkörpersysteme Backlinks: [Kinematics primer](kinematics primer.md), [poisson equation for skew symmetric matrix of angular velocity](poisson equation for skew-symmetric-matrix-of-angular-velocity.md)
Linear velocity This is the derivative of r relative to CS0, as depicted in CS0 coordinates
Where the expression in square brackets means: the derivative of r relative to CS0, as depicted in CS1 coordinates
Angular velocity with : ang.vel. of D relative to B, given in E coordinates
Note : </description></item><item><title>Kinematics primer</title><link>https://salehahr.github.io/studienarbeit/kinematics-primer/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/kinematics-primer/</guid><description>Source: Hibbeler Dynamics , woernle-mehrkörpersysteme See also: Reversed kinematics relations , denavit-hartenberg-convention Backlinks: [Obtaining IMU measurements from camera by forward kinematics](obtaining imu measurements from camera by forward kinematics.md), rotations / so(3) group-index Prereqs:
Chaining rotation matrices and angular velocities Converting velocity from CS1 to CS0 [Poisson equation for skew symmetric matrix of angular velocity](poisson equation for skew-symmetric-matrix-of-angular-velocity.md) Inverse of a homogeneous transformation matrix Differentiation in different coordinate systems Position (in world coordinates)velocity (in world coordinates)where the skew symmetric matrix is the angular velocity of cs1 relative to cs0,(~~given in cs1 coordinates?</description></item><item><title>Modified Denavit-Hartenberg convention</title><link>https://salehahr.github.io/studienarbeit/modified-denavit-hartenberg-convention/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/modified-denavit-hartenberg-convention/</guid><description>Source: Craig - Introduction to Robotics Backlinks: Kinematics primer Note: s. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1083.6428&amp;amp;rep=rep1&amp;amp;type=pdf for comparison (Lipkin)
Link-frame attachment
Identify joint axes
For joint axes i and i+1, identify the  common perpendicular + where it meets axis i, or point of intersection and let this be the link-frame origin Let Z_i point along the i-th joint axis
Let X_i
point along common perpendicular, or be normal to the plane containing the two axes Assign Y_i (right hand coordinate system)</description></item><item><title>Poisson equation for skew symmetric matrix of angular velocity</title><link>https://salehahr.github.io/studienarbeit/poisson-equation-for-skew-symmetric-matrix-of-angular-velocity/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/poisson-equation-for-skew-symmetric-matrix-of-angular-velocity/</guid><description>Source: Woernle Mehrkörpersysteme Backlinks: [Kinematics primer](kinematics primer.md), converting velocity from cs1 to cs0 Skew-symmetric angular velocity: Poisson equation</description></item><item><title>Reversed kinematics relations</title><link>https://salehahr.github.io/studienarbeit/reversed-kinematics-relations/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/reversed-kinematics-relations/</guid><description>Source: Woernle Mehrkörpersysteme See also: Kinematics primer PositionVelocity(given that omega_00 = 0)(given that v_00 = 0)Accelerationgiven , , </description></item><item><title>Rotations as xyz Bryan-Tait angles (Kardanwinkel)</title><link>https://salehahr.github.io/studienarbeit/rotations-as-xyz-bryan-tait-angles-kardanwinkel/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/rotations-as-xyz-bryan-tait-angles-kardanwinkel/</guid><description>Parent: Rotations / SO(3) group index Source: Woernle Mehrkörpersysteme Backlinks: Euler angles Rotation angle nomenclature Euler angles: ZXZ (mitgedrehte Achsen) Kardan-Winkel [de] / Bryan-Tait angles: ZYX (mitgedrehte Achsen)
xyz-Kardan-Winkel
K3: körperfestes KS K0: Welt-KS
Ausgangslage 1. Drehung um x0 2. Drehung um y1 3. Drehung um z2 Winkelgeschwindigkeit</description></item><item><title>30.1.2.1 Types of endoscopes</title><link>https://salehahr.github.io/studienarbeit/30.1.2.1-types-of-endoscopes/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/30.1.2.1-types-of-endoscopes/</guid><description>Source: Leiner Digital Endoscope Design Backlinks: Endoscopes Types of endoscopes
Endoscopes can be classified according to their flexibility, thus resulting in &amp;lsquo;rigid&amp;rsquo;, &amp;lsquo;flexible&amp;rsquo; and &amp;lsquo;semi-rigid&amp;rsquo; variants.
Video sensor at distal (&amp;ldquo;away from the surgeon&amp;rdquo;, opposite of proximal) end allows rigid endoscope to be converted to a flexible one solely by mechanical design Note: the term endoscope in hospital environments typically refers to the flexible variant In rigid endoscopes, an array of rod lenses (lenses which are much longer than their diameter) within a metal tube — also known as a telescope — are used to transmit the images from the [distal end to the proximal end](distal end to the proximal end.</description></item><item><title>Discussion 2021-05-21</title><link>https://salehahr.github.io/studienarbeit/discussion-2021-05-21/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/discussion-2021-05-21/</guid><description>Notes — Before
Got the ESKF implementation (Solà) to work with my fake IMU data [non-noisy IMU] results look ok for low process noise (trust the prediction more) with relatively high measurement noise [noisy IMU] ok? current assumptions/simplifications: fake data assumes IMU is sitting right on top of camera fake data, as of yet, does not take into account: biases, gravity simplified state vector (no scale estimate, no gravity estimate, no bias estimate etc) TBD: modify equations/states to fit the problem, i.</description></item><item><title>Endoscope/cystoscopy pics/videos</title><link>https://salehahr.github.io/studienarbeit/endoscope-cystoscopy-pics-videos/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/endoscope-cystoscopy-pics-videos/</guid><description>Backlinks: Discussion 2021-05-21 Rigid endoscope for cystoscopy
Source: http://www.ebay.com/itm/113780645426 Source: http://www.researchgate.net/figure/Intraoperative-image-of-the-rigid-cystoscope-entering-the-bladder-through-the-screw-tip_fig2_322897289 Source: http://www.youtube.com/watch?v=1gEpz9wijoY http://www.maestro-portal.eu/procedure/detail/4 Videos: Semi-Rigid Ureteroscopy and Laser Lithotripsy for Ureter Stones
Source: http://www.medicinenet.com/how_painful_is_a_cystoscopy/article.htm</description></item><item><title>IMU on cystoscope</title><link>https://salehahr.github.io/studienarbeit/diagram-imu-on-cystoscope/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/diagram-imu-on-cystoscope/</guid><description>Backlinks: Discussion 2021-05-21</description></item><item><title>Leiner Digital Endoscope Design</title><link>https://salehahr.github.io/studienarbeit/leiner-digital-endoscope-design/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/leiner-digital-endoscope-design/</guid><description>Backlinks: Endoscopes URL: http://www.spiedigitallibrary.org/ebooks/SL/Digital-Endoscope-Design/1/Digital-Endoscope-Design/10.1117/3.2235283.ch1?SSO=1
Notes Insertion of an endoscope Types of endoscopes Endoscope system components Endoscope specification</description></item><item><title>40.1 IMU measurement model</title><link>https://salehahr.github.io/studienarbeit/40.1-imu-measurement-model/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/40.1-imu-measurement-model/</guid><description>Parent: [IMU index](imu index.md), probabilistic models-for-imu Backlinks: [IMU motion model](imu motion model.md), imu kinematic model using euler integration An IMU measures, relative to an inertial frame, acceleration and rotation rate.
The measurements are corrupted by bias and noise (often assumed to be white Gaussian noise ). mkok-2017 Additionally, the acceleration measured is affected by gravity. Note the [assumptions in modelling the true angular velocity in IMUs](assumptions in modelling the-true-angular-velocity-in-imus.</description></item><item><title>40.1.1 Assumptions in modelling the true angular velocity in IMUs</title><link>https://salehahr.github.io/studienarbeit/40.1.1-assumptions-in-modelling-the-true-angular-velocity-in-imus/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/40.1.1-assumptions-in-modelling-the-true-angular-velocity-in-imus/</guid><description>Parent: [IMU index](imu index.md), imu-measurement-model Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
For angular velocity, the termshould really be with
negligible Earth rotation = 0 stationary navigation frame, = 0</description></item><item><title>50.3 IMU motion model in a Kalman filter</title><link>https://salehahr.github.io/studienarbeit/50.3-imu-motion-model-in-a-kalman-filter/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.3-imu-motion-model-in-a-kalman-filter/</guid><description>Parent: IMU index Source: Solà 2017 Quaternion kinematics for ESKF Which states do we use for the motion model? [Choice of states for the IMU motion/kinematics model](choice of states-for-the-imu-motion_kinematics-model.md)
How do we model the IMU motion? [Choice of model for the IMU motion model](choice of model-for-the-imu-motion-model.md)
The kinematics (true state) can be partitioned into a nominal part and an error part, s. variables in ESKF using IMUs . The corresponding [nominal state dynamics and error state dynamics](nominal state-dynamics-and-error-state-dynamics.</description></item><item><title>50.3.1 Choice of states for the IMU motion/kinematics model</title><link>https://salehahr.github.io/studienarbeit/50.3.1-choice-of-states-for-the-imu-motion-kinematics-model/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.3.1-choice-of-states-for-the-imu-motion-kinematics-model/</guid><description>Parent: IMU index See also: [Choice of model for the IMU motion model](choice of model-for-the-imu-motion-model.md)
According to MKok 2017 , we can either
Use the full state vector [+] knowledge about sensor motion is included in model [-] large state vector Or the partial state vector, where the inputs are the inertial measurements from the IMU [+] process noise intuitively represents IMU noise. This is useful when we have no knowledge about the motion model.</description></item><item><title>50.3.2 Choice of model for the KF using IMU readings</title><link>https://salehahr.github.io/studienarbeit/50.3.2-choice-of-model-for-the-kf-using-imu-readings/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.3.2-choice-of-model-for-the-kf-using-imu-readings/</guid><description>Parent: [IMU index](imu index.md), imu kinematic-equations/motion-model According to MKok 2017 , here are some models that assume either a constant acceleration or a constant angular velocity:
Constant acceleration model Constant angular velocity model (Notation: angular velocity of the body with respect to world (n), expressed in body CS)
If motion is unknown, there is also the option of modelling the states using random walk equations.</description></item><item><title>50.5.1 IMU nominal-state and error-state kinematics</title><link>https://salehahr.github.io/studienarbeit/50.5.1-imu-nominal-state-and-error-state-kinematics/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.5.1-imu-nominal-state-and-error-state-kinematics/</guid><description>Parents: [IMU index](imu index.md), 50.3 error-state-kalman-filter Note on discretisation Solà 2017 :
Convert the differential equations to difference equations (use integration) Integration methods may vary Closed form solutions Numerical integration Integration is done for: The nominal state The error state (deterministic part): error state dynamics and control The error state (stochastic part): noise and perturbations Nominal state Error state Model without noise and perturbations Continuous Discrete summary:with the jacobians defined in imu eskf-prediction-equations</description></item><item><title>50.5.1.1 States of the ESKF for estimating IMU pose</title><link>https://salehahr.github.io/studienarbeit/50.5.1.1-states-of-the-eskf-for-estimating-imu-pose/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.5.1.1-states-of-the-eskf-for-estimating-imu-pose/</guid><description>Parent: IMU index Backlinks: Fusing IMU with complementary sensory data Source: Solà 2017 Quaternion kinematics for ESKF Full state Vector with 19 elements The corresponding kinematics equations/motion model is given in IMU kinematic equations/motion model .
Notes:
The angular error in 3D space is given by the notation. In quaternion space, this angle is halved, with  (&amp;lsquo;double cover&amp;rsquo;, s. [Unit quaternions](unit quaternions.md), rotation-error-representation )
The angular error is defined locally w.</description></item><item><title>50.5.1.2 The initial gravity vector/orientation for the IMU ESKF</title><link>https://salehahr.github.io/studienarbeit/50.5.1.2-the-initial-gravity-vector-orientation-for-the-imu-eskf/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.5.1.2-the-initial-gravity-vector-orientation-for-the-imu-eskf/</guid><description>Parent: [IMU index](imu index.md), [choice of model for the imu motion model](choice of model-for-the-imu-motion-model.md)
Notes on the initial gravity vector/orientation for the IMU ESKF Solà 2017 For simplicity, it is assumed that  The gravity vector g is estimated in terms of frame q0
This puts the initial uncertainty on the gravity direction, rather than on the initial orientation.
Doing this improves linearity, because now the equation is linear in g and the initiial rotation R0 has no uncertainty</description></item><item><title>50.6 ESKF prediction equations</title><link>https://salehahr.github.io/studienarbeit/50.6-eskf-prediction-equations/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.6-eskf-prediction-equations/</guid><description>Parents: [IMU index](imu index.md), 50.3 error-state-kalman-filter Source: Solà 2017 Quaternion kinematics for ESKF Error state system equation becomes: where (s. IMU nominal-state and error-state kinematics for an overview of the nonlinear kinematics equations)
State propagation (without considering noise) — produces a state estimate (a priori) Note: this always returns zero as the mean of the error initialises to zero!
Covariance propagation (considers noise); a priori estimate with the Jacobians (transition matrix approximated using first order Euler, more precise methods are available)</description></item><item><title>50.7 ESKF update / Fusing IMU with complementary sensory data</title><link>https://salehahr.github.io/studienarbeit/50.7-eskf-update-fusing-imu-with-complementary-sensory-data/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.7-eskf-update-fusing-imu-with-complementary-sensory-data/</guid><description>Parent: [IMU index](imu index.md), 50.3 error-state-kalman-filter Source: Solà 2017 Quaternion kinematics for ESKF In the ESKF, the arrival of non-IMU sensor data triggers a correction stage. This correction makes the IMU biases observable , allows correct estimation of the biases The correction stage is three-fold:
observe the error state by way of filter correction &amp;lsquo;add&amp;rsquo; the observed errors to the nominal state to get the supposed &amp;lsquo;true&amp;rsquo; state according to the composition rules in variables in ESKF using IMUs reset the error state Source: Markley Fundamentals of Spacecraft Attitude Determination What if several measurements come in without IMU / propagation in between (i.</description></item><item><title>50.7.1 Observation of the error state (filter correction)</title><link>https://salehahr.github.io/studienarbeit/50.7.1-observation-of-the-error-state-filter-correction/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.7.1-observation-of-the-error-state-filter-correction/</guid><description>Parents: [50.3 Error-State Kalman Filter](50.3 error-state kalman filter.md), [50.5 eskf update / fusing imu with complementary sensory data](50.5 eskf update _ fusing-imu-with-complementary-sensory-data.md) Source: Solà 2017 Quaternion kinematics for ESKF Given is a non-IMU sensor with the measurement function [ Solà , markley ] where x_t is the true state and v is a white Gaussian noise Source: Markley If the measurements are given in quaternion form:
we can directly calculate the orientation error between measured orientation and estimated orientation this becomes our &amp;lsquo;measured&amp;rsquo; angular error which can be used to calculate the residual term [JPL convention] q_meas x (q_est).</description></item><item><title>50.7.1.1 H Jacobian matrix in the ESKF filter correction</title><link>https://salehahr.github.io/studienarbeit/50.7.1.1-h-jacobian-matrix-in-the-eskf-filter-correction/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.7.1.1-h-jacobian-matrix-in-the-eskf-filter-correction/</guid><description>Parent: Filter correction , eskf-update Source: Solà 2017 Quaternion kinematics for ESKF Evaluation of the H Jacobian
In the prediction stage, the filter estimates the error state. Therefore, the Jacobian H needs to be defined w.r.t. the error state , and evaluated at the true state estimate  However, as the error state mean is zero (not yet observed), the true state is approximated to the nominal state  Thus we can use the nominal state as the evaluation point The first Jacobian Depends on the sensor&amp;rsquo;s particular measurement function The second Jacobian with Source: Markley Fundamentals of Spacecraft Attitude Determination Measurement sensitivity matrix (Jacobian w.</description></item><item><title>50.7.3 ESKF reset</title><link>https://salehahr.github.io/studienarbeit/50.7.3-eskf-reset/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.7.3-eskf-reset/</guid><description>Parent: Fusing IMU with complementary sensory data Backlinks: 50.3 Error-State Kalman Filter Source: Markley moves the rotation error to the global rotation this keeps the rotation error small and far from any singularities To update the global state, the reset has to obey The reset has to preserve the quaternion norm, therefore an exact unit norm expression must be used, instead of an approximation. Using the Rodrigues parameter , the reset becomes which leads to a two step update (1.</description></item><item><title>(Science Focus) How can one eye alone provide depth perception</title><link>https://salehahr.github.io/studienarbeit/science-focus-how-can-one-eye-alone-provide-depth-perception-/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/science-focus-how-can-one-eye-alone-provide-depth-perception-/</guid><description>Source: http://www.sciencefocus.com/the-human-body/how-can-one-eye-alone-provide-depth-perception/ Author: Hilary Guite
In humans with normal binocular vision, depth perception is obtained using the parallax in the two overlapping fields of vision (&amp;ldquo;binocular disparity&amp;rdquo;)
Each single field of vision has a slightly different view to the other If vision in one eye is impaired, depth perception is still obtainable even with only one eye. Some tricks that the brain uses:
We know the real size of things Using perspective, e.</description></item><item><title>10. Monocular depth perception</title><link>https://salehahr.github.io/studienarbeit/10.-monocular-depth-perception/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/10.-monocular-depth-perception/</guid><description>Parent: SLAM Index Backlinks: Thesis Depth perception in real life
In nature, prey animals typically have eyes on either side of their head to maximise field of view, while most predators have forward-facing eyes with overlapping fields of vision (binocular vision) for maximum depth perception. Humans also have binocular vision. (Some exceptions: fruit bats, killer whales)
We perceive depth, or distance to the objects that we see, based on several visual cues.</description></item><item><title>50.5 Error-State Kalman Filter</title><link>https://salehahr.github.io/studienarbeit/50.5-error-state-kalman-filter/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.5-error-state-kalman-filter/</guid><description>Source: Markley An EKF propagates the expectation and covariance of the state The MEKF propagates the expectation and the covariance of the error state Source: Whampsey MEKF Previously: orientation is represented by one state Now: orientation is split up into  a large signal q_nom (nominal orientation) and a small signal (perturbation angle alpha) &amp;ndash; parametrises an error quaternion  This reformulates the error in terms of the group operation and so maintains the rotation invariance (rotation preserves the origin, length, angle between two vectors, orientation, etc.</description></item><item><title>Identity quaternion</title><link>https://salehahr.github.io/studienarbeit/identity-quaternion/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/identity-quaternion/</guid><description>Parent: Quaternion index Source: Solà 2017 Quaternion kinematics for ESKF</description></item><item><title>Inverse quaternion</title><link>https://salehahr.github.io/studienarbeit/inverse-quaternion/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/inverse-quaternion/</guid><description>Parent: Quaternion index Source: Solà 2017 Quaternion kinematics for ESKF The inverse is the conjugate in case of unit quaternions</description></item><item><title>Quaternion conventions</title><link>https://salehahr.github.io/studienarbeit/quaternion-conventions/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/quaternion-conventions/</guid><description>Parent: Quaternion index Source: Solà 2017 Quaternion kinematics for ESKF Backlinks: MEKF Source: [ Wikipedia ], markley For quaternion multiplication : change the order to transform between conventions Hamilton Shuster Transpose of the Hamiltonian version</description></item><item><title>Quaternion differentiation</title><link>https://salehahr.github.io/studienarbeit/quaternion-differentiation/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/quaternion-differentiation/</guid><description>Parent: Quaternion index Source: J. D. Hol — Sensor fusion and calibration of inertial sensors, vision, ultra-wideband and GPS
Using the identities: http://math.stackexchange.com/questions/189185/quaternion-differentiation Numerical differentiation (Euler)
http://math.stackexchange.com/questions/1896379/how-to-use-the-quaternion-derivative q(t+dt) = q(t)*dq
dq/dt = (1/2)*W*q with W = 0 + wx*i + wy*j + wz*k.
Integrating q(t) = q(t0)*exp((1/2)*W*(t-t0)) --&amp;gt; dq = exp((1/2)*W*dt).</description></item><item><title>Quaternion index</title><link>https://salehahr.github.io/studienarbeit/quaternion-index/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/quaternion-index/</guid><description>Notation Quaternion conventions Basic math Quaternion multiplication Identity quaternion Quaternion conjugate Quaternion norm Inverse quaternion Unit quaternions Calculus Quaternion differentiation As rotation Rotations / SO(3) group index Exponential map Logarithm map Orientation parametrisations Which orientation parametrisation to choose? Linearisation of an orientation in SO(3) Quaternion to rotation matrix Rotation error representation For filtering Additive quaternion filtering Literature Solà 2017 Quaternion kinematics for ESKF [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.</description></item><item><title>Quaternion multiplication</title><link>https://salehahr.github.io/studienarbeit/quaternion-multiplication/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/quaternion-multiplication/</guid><description>Parent: Quaternion index Source: Solà 2017 Quaternion kinematics for ESKF Here: Hamiltonian convention, s. Quaternion conventions Non-commutative  Associative  Distributive Multiplication as a matrix product With
the matrices the skew operator (skew symmetric matrix) s. also cross product Source: Markley Fundamentals of Spacecraft Attitude Determination with the matrices</description></item><item><title>Quaternion norm</title><link>https://salehahr.github.io/studienarbeit/quaternion-norm/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/quaternion-norm/</guid><description>Parent: Quaternion index Source: Solà 2017 Quaternion kinematics for ESKF With the property</description></item><item><title>Solà 2017 Quaternion kinematics for ESKF</title><link>https://salehahr.github.io/studienarbeit/sol%C3%A0-2017-quaternion-kinematics-for-eskf/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/sol%C3%A0-2017-quaternion-kinematics-for-eskf/</guid><description>Link: http://www.iri.upc.edu/people/jsola/JoanSola/objectes/notes/kinematics.pdf
Author: Joan Solà Abstract:
Primer on quaternion/rotation group math Math for error state Kalman filters using IMUs Contents/Chapters:
Quaternions Rotations, s. also SO(3) 3D rotation group Quaternion conventions Perturbations, derivatives, integrals Error-State Kalman Filter for IMU-driven systems Variables in ESKF IMU measurement model IMU motion model [The initial gravity vector/orientation for the IMU ESKF](the initial gravity-vector_orientation-for-the-imu-eskf.</description></item><item><title>Unit quaternions</title><link>https://salehahr.github.io/studienarbeit/unit-quaternions/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/unit-quaternions/</guid><description>Parent: Quaternion index , orientation-parametrisations Backlinks: Variables in ESKF , quaternion to rotation matrix , gibbs /-rodrigues-parameter Source: Solà 2017 Quaternion kinematics for ESKF Properties
Can be written in the form with u as a unit vector theta is the angle between q and the identity quaternion q_I = [1, 0, 0, 0]
Double cover Also: with phi as the angle rotated by q on objects in the 3D space</description></item><item><title>IMU data generation from camera/visual data</title><link>https://salehahr.github.io/studienarbeit/imu-data-generation-from-camera-visual-data/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/imu-data-generation-from-camera-visual-data/</guid><description>Parent: IMU index Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)</description></item><item><title>Solà 2014 SLAM with EKF</title><link>https://salehahr.github.io/studienarbeit/sol%C3%A0-2014-slam-with-ekf/</link><pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/sol%C3%A0-2014-slam-with-ekf/</guid><description> Notes on EKF-SLAM that uses landmarks MATLAB code Notes on partial landmark initialisation (convariance matrix) Notes on the linearity of the observation function in scale</description></item><item><title>Discussion 2021-05-10</title><link>https://salehahr.github.io/studienarbeit/discussion-2021-05-10/</link><pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/discussion-2021-05-10/</guid><description>Agenda
Change/Reduction of scope of SA (from fusing IMU with camera) to using sensor fusion to determine transformation parameters between IMU and camera Camera and IMU setup involves kinematic modelling (not fixed transformation as previously assumed!) Offline implementation in Python/MATLAB (scripting language) HiWi tasks can include DefSLAM bindings / interface C++ bindings of skrogh EKF implementation? HiWi prioritises Versuchsstand for now Tasks
Find an EKF implementation that works well and can be used with DefSLAM + IMU data implement kinematic model equations in the prediction-step, s.</description></item><item><title>Quaternion conjugate</title><link>https://salehahr.github.io/studienarbeit/quaternion-conjugate/</link><pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/quaternion-conjugate/</guid><description>Parent: Quaternion index Source: http://en.wikipedia.org/wiki/Quaternion
Flip signs of vector part
Source: Solà 2017 Quaternion kinematics for ESKF Multiplying with own conjugate (scalar!)
Conjugate operation on quaternion products</description></item><item><title>Transforming velocities to another frame</title><link>https://salehahr.github.io/studienarbeit/transforming-velocities-to-another-frame/</link><pubDate>Mon, 03 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/transforming-velocities-to-another-frame/</guid><description>http://physics.stackexchange.com/questions/197009/transform-velocities-from-one-frame-to-an-other-within-a-rigid-body
Transforming velocities to another frame Further reading: http://core.ac.uk/download/pdf/154240607.pdf</description></item><item><title>IMU states, dynamics equations</title><link>https://salehahr.github.io/studienarbeit/imu-states-dynamics-equations/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/imu-states-dynamics-equations/</guid><description>Parent: IMU index Source: Mur-Artal 2017 VI-ORB Evolution of IMU states (world frame to IMU: orientation R, position p, velocity v) between consecutive keyframes
Evolution of IMU states (world frame to IMU: orientation R, position p, velocity v) between consecutive frames
Using the preintegration terms Preintegration (delta) terms and the Jacobians can be computed iteratively as IMU measurements arrive (s. Forster&amp;rsquo;s paper on preintegration)</description></item><item><title>Modelling noise and bias for IMU</title><link>https://salehahr.github.io/studienarbeit/modelling-noise-and-bias-for-imu/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/modelling-noise-and-bias-for-imu/</guid><description>Parent: [IMU index](imu index.md), imu-measurement-model Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
Modelling the noise The noise not only represents measurement noise, but also model uncertainty.
With proper calibration, the three gyroscope axes are independent: Same for accelerometer — assume diagonal for a properly calibrated sensor
Modelling the biases — two approaches
treat bias as constant (due to short experiment times) pre-calibrate in a separate experiment, or make part of the parameters vector treat as slowly time-varying (due to long experiment times or shorter bias stability) make the bias part of the state vector model the bias as a random walk</description></item><item><title>OpenCV Kalman filter pre/post states</title><link>https://salehahr.github.io/studienarbeit/opencv-kalman-filter-pre-post-states/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/opencv-kalman-filter-pre-post-states/</guid><description/></item><item><title>Besprechung 2021-04-01</title><link>https://salehahr.github.io/studienarbeit/besprechung-2021-04-01/</link><pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/besprechung-2021-04-01/</guid><description>Agenda
Recap: last meeting (2021-03-15) Offline Kalman — after this works, do a &amp;lsquo;live&amp;rsquo; implementation on DefSLAM run
IMU measurements as [acc, gyro] readings
With noise, but without considering bias, IMU-cam transformation, gravity Two sets of measurements for filter: IMU measurements, DefSLAM (camera) measurements
KF prediction using random walk
Recap: SLAM +filtering terminology (loose coupling, tight coupling) Literature Original suggestion using random walk model Interface (DefSLAM, Python) read: OK write: to do Offline Kalman as a separate repo Good papers?</description></item><item><title>Markov assumption</title><link>https://salehahr.github.io/studienarbeit/markov-assumption/</link><pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/markov-assumption/</guid><description>Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md) Backlinks: [Grisetti 2011 - Tutorial graph-based SLAM](grisetti 2011 - tutorial graph-based slam.md), probabilistic models-for-imu Models with state x which have the Markov property:
all information up till time t is contained in xt enables marginalisation of state xt at time t+1</description></item><item><title>Linearisation of an orientation in SO(3)</title><link>https://salehahr.github.io/studienarbeit/linearisation-of-an-orientation-in-so-3/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/linearisation-of-an-orientation-in-so-3/</guid><description>Parents: [Rotations / SO(3) group index](rotations _ so(3) group-index.md), quaternion-index , orientation-parametrisations Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
Rotation of a vector in SO(3)
The SO(3) -group-is-a lie-group , so there exists an exponential map from a corresponding Lie algebra to the SO(3) group a reverse log map Possible to represent orientations using unit quaternions or rotation matrices in SO(3) — linearisation point orientation deviations using rotation vectors on R^3 (Lie algebra)</description></item><item><title>Orientation parametrisations</title><link>https://salehahr.github.io/studienarbeit/orientation-parametrisations/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/orientation-parametrisations/</guid><description>Parent: [Rotations / SO(3) group index](rotations _ so(3) group index.md), [quaternion index](quaternion index.md), probabilistic models-for-imu See also: Rotation error representation Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
Orientation parametrisations
Note: CCW rotation of a vector x_v to x_u corresponds to a CW rotation of the CS v to CS u. Rotations in R^3 are a member of the special orthogonal group SO(3) rotation matrix unique description of orientation Euler axis/angle rotation-vector not unique, due to wrapping Euler angles not unique, due to wrapping and gimbal lock Unit quaternions not unique, -q and q depict the same orientationProof: http://math.</description></item><item><title>Probabilistic models for IMU</title><link>https://salehahr.github.io/studienarbeit/probabilistic-models-for-imu/</link><pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/probabilistic-models-for-imu/</guid><description>Parent: IMU index Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
Three main components to the probabilistic models
IMU measurement model (infer knowledge about pose from measurements)  Prediction model (how sensor pose changes over time) Models of the initial pose (prior) Knowledge we are interested in: pose of the sensor
time-varying variables: states  constants: parameters  Knowledge available to us: sensor dynamics, available sensor measurements Conditional probability distribution</description></item><item><title>Besprechung 2021-03-15</title><link>https://salehahr.github.io/studienarbeit/besprechung-2021-03-15/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/besprechung-2021-03-15/</guid><description>Status
Last week: generated noisy IMU data (pose) from stereo trajectory, &amp;lsquo;offline&amp;rsquo; Kalman This week: Kalman + &amp;lsquo;live&amp;rsquo; DefSLAM Still to do: design KF (EKF, or other methods&amp;hellip;) &amp;lsquo;Offline&amp;rsquo; Kalman
To learn how to use openCV&amp;rsquo;s Kalman filter without having to rebuild DefSLAM every time Aim was to figure out the update/correction workflow and implement it in live DefSLAM run
Uses pre-extracted trajectories (mono and stereo)</description></item><item><title>vi.cc using kalman for xyz states (what goes on with the map?)</title><link>https://salehahr.github.io/studienarbeit/vi.cc-using-kalman-for-xyz-states/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/vi.cc-using-kalman-for-xyz-states/</guid><description>Offline Kalman Kalman and live DefSLAM</description></item><item><title>Besprechung 2021-03-08</title><link>https://salehahr.github.io/studienarbeit/besprechung-2021-03-08/</link><pubDate>Mon, 08 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/besprechung-2021-03-08/</guid><description>Agenda
DefSLAM + OS3 Up till DefTracking::MonocularInitialization() on hold, working on Kalman stuff for the time being DefSLAM + Kalman new plot (monocular trajectory without any pose updating) to do: use noisy stereo data, plug into update step while discarding images functions in System.cc: read data, update pose DefSLAM + sockets Meeting notes:
next step: implement the Kalman filter. When that is done, discuss next steps e.</description></item><item><title>System::forceTrajectory</title><link>https://salehahr.github.io/studienarbeit/system-forcetrajectory/</link><pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/system-forcetrajectory/</guid><description>Parent: DefSLAM branch overview Reference: DefSLAMGT (stereo as ground truth) For testing: DefSLAMVI
Description Force update of DefSLAMVI&amp;rsquo;s current frame pose to that of DefSLAMGT&amp;rsquo;s for the frames 230 to 239
Without System::Reset Frame pose is &amp;lsquo;updated&amp;rsquo; during the interval, but after the interval, the optimisation (which uses frame pose as an estimate and also uses map node positions) makes the system resume it&amp;rsquo;s trajectory before the update
(below: with pure monocular trajectory, without any forced updates) With System::Reset The system is reset after every forced pose update (i.</description></item><item><title>World to camera trafo</title><link>https://salehahr.github.io/studienarbeit/world-to-camera-trafo/</link><pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/world-to-camera-trafo/</guid><description>Parent: SLAM Index See also: [Pinhole camera model](pinhole camera model.md), pinhole camera-projection-function Source: http://www.cse.psu.edu/~rtc12/CSE486/lecture12.pdf
Camera coordinates (X, Y, Z) World coordinates (U, V, W) Image plane (x, y) / Pixel coordinates (u, v) Forward projection
Representing 2D point as a fictitious 3D point (x', y', z') [for matrix calculations] Convention: Given (x', y', z'), we can recover the 2D point (x, y) as World to camera trafo</description></item><item><title>DefSLAM branch overview</title><link>https://salehahr.github.io/studienarbeit/defslam-branch-overview/</link><pubDate>Fri, 19 Feb 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/defslam-branch-overview/</guid><description>Parent: SA TODO Repo http://github.com/feudalism/DefSLAM
Dormant
master sa Deprecated
windows - deprecated, changes made for building on Windows imu - deprecated, has Imu tracking functions but dependencies not resolved obs_tuple - initial attempt to incorporate Atlas, attempt to use OS3&amp;rsquo;s structure for MapPoint observations : &amp;lt;KeyFrame, tuple&amp;lt;int, int&amp;raquo; as opposed to &amp;lt;Keyframe, int&amp;gt; in DefSLAM+OS2 Temporary/Experimental
s. to do list
debugging the segfault that seemingly appears in Surface::getNormalSurfacePoint seems to happen after System reset</description></item><item><title>ORBSLAM2 unofficial documentation</title><link>https://salehahr.github.io/studienarbeit/orbslam2-unofficial-documentation/</link><pubDate>Wed, 17 Feb 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/orbslam2-unofficial-documentation/</guid><description>Partially done, abandonned: http://github.com/raulmur/ORB_SLAM2/compare/master&amp;hellip;AlejandroSilvestri:master In Spanish: http://alejandrosilvestri.github.io/os1/doc/html/</description></item><item><title>NormalSurfacePoint Segfault</title><link>https://salehahr.github.io/studienarbeit/normalsurfacepoint-segfault/</link><pubDate>Tue, 09 Feb 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/normalsurfacepoint-segfault/</guid><description>Crash around frame 65 [New Thread 0x7fff7ffff700 (LWP 3854)] [Thread 0x7fff7ffff700 (LWP 3854) exited] [New Thread 0x7fff7ffff700 (LWP 3855)] [Thread 0x7fff7ffff700 (LWP 3855) exited] [New Thread 0x7fff7ffff700 (LWP 3856)]
Thread 5 &amp;ldquo;DefSLAM&amp;rdquo; received signal SIGSEGV, Segmentation fault. [Switching to Thread 0x7fffc215d700 (LWP 3571)] defSLAM::SurfacePoint::thereisNormal (this=0x6705) at /home/user3/slam/DefSLAM/Modules/Mapping/SurfacePoint.cc:54 54 bool SurfacePoint::thereisNormal() { return NormalOn; } (gdb) bt #0 0x00007ffff78798a0 in defSLAM::SurfacePoint::thereisNormal() (this=0x6705) at /home/user3/slam/DefSLAM/Modules/Mapping/SurfacePoint.cc:54 #1 0x00007ffff7878f95 in defSLAM::Surface::setNormalSurfacePoint(unsigned long, cv::Vec&amp;lt;float, 3&amp;gt;&amp;amp;) (this=0x555565b0e030, ind=ind@entry=939, N=&amp;hellip;) at /home/user3/slam/DefSLAM/Modules/Mapping/Surface.</description></item><item><title>Viewer segfault</title><link>https://salehahr.github.io/studienarbeit/viewer-segfault/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/viewer-segfault/</guid><description>Error [New Thread 0x7fff86ffd700 (LWP 1117)] NORMALS REESTIMATED : 277 - 277 [Thread 0x7fff86ffd700 (LWP 1117) exited] NORMAL ESTIMATOR OUTPoints potential : 293 70 New template requested Number Of normals 277 0x555566923da0 -0.79956 0.655022 -0.594482POINTS matched:167 Points Scale Error Keyframe : 1 stan dev 0.310974 chi 0.013115 0.01 201 SurfaceRegistration not sucessful (Not enough points to align or chi2 too big
Thread 6 &amp;ldquo;DefSLAM&amp;rdquo; received signal SIGSEGV, Segmentation fault. [Switching to Thread 0x7fffc1996700 (LWP 275)] __memmove_avx_unaligned_erms () at .</description></item><item><title>Segfault in DefTracking (imu branch)</title><link>https://salehahr.github.io/studienarbeit/segfault-in-deftracking-imu-branch/</link><pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/segfault-in-deftracking-imu-branch/</guid><description>/home/user3/slam/datasets/mandala0/images/stereo_im_l_1560936003993.png i: 30 POINTS matched:10 Track lost soon after initialisation, reseting&amp;hellip; /home/user3/slam/datasets/mandala0/images/stereo_im_l_1560936004022.png i: 31 System Reseting NORMAL ESTIMATOR IN - NORMALS REESTIMATED : 0 - 0 NORMAL ESTIMATOR OUTPoints potential : 939 70 New template requested Number Of normals 0 0x5555636b1fb0 Not enough normals Reseting Local Mapper&amp;hellip; done Reseting Loop Closing&amp;hellip; done Reseting Database&amp;hellip; done
Thread 1 &amp;ldquo;DefSLAM&amp;rdquo; received signal SIGSEGV, Segmentation fault. 0x00007ffff78d9fae in cv::Mat::Mat (m=&amp;hellip;, this=0x7ffffffeaea0) at /usr/local/include/opencv4/opencv2/core/mat.inl.hpp:545 545 step[0] = m.</description></item><item><title>DefSLAM and discontinuous areas (classical datasets)</title><link>https://salehahr.github.io/studienarbeit/defslam-and-discontinuous-areas-classical-datasets/</link><pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/defslam-and-discontinuous-areas-classical-datasets/</guid><description>Parent: Lamarca 2020 DefSLAM Source: http://github.com/UZ-SLAMLab/DefSLAM/issues/1
JoseLamarca: DefSLAM is suitable for rigid areas, proof of that is the abdominal sequence that is kind of rigid. The problem for these sequences is the discontinuous areas. For the monocular case, we are assuming that the surface is smooth that is not usually valid for the classical datasets. Apart from complexity issues that algorithms with RGB-D and stereo cameras could have in those scenes [1] and [2].</description></item><item><title>DefSLAM errors encountered</title><link>https://salehahr.github.io/studienarbeit/defslam-errors-encountered/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/defslam-errors-encountered/</guid><description>Rebuilding DefSLAM in Debug mode Error: &amp;ldquo;Virtual memory exhausted: Cannot allocate memory&amp;rdquo; Solution: reduce degree of make -j
Segmentation fault in Defslam debug mode Based on http://stackoverflow.com/questions/19615371/segmentation-fault-due-to-vectors Changed: surfacePoints_[ind] to surfacePoints_.at(ind)
New error surfacePoints_ appears to be NULL? Was it instantiated in another thread? http://stackoverflow.com/questions/11645857/debugging-with-gdb-why-this-0x0
Using core dumps with gdb http://jvns.ca/blog/2018/04/28/debugging-a-segfault-on-linux/</description></item><item><title>extern c++</title><link>https://salehahr.github.io/studienarbeit/extern-c++/</link><pubDate>Mon, 11 Jan 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/extern-c++/</guid><description>http://en.cppreference.com/w/cpp/language/storage_duration is a storage class specifier that controls storage duration and its linkage.
extern - static or thread storage duration and external linkage
Storage duration
static: storage for the object is allocated when the program begins and deallocated when the program ends. only one instance of the object exists thread storage for the object is allocated when the thread begins and deallocated when the thread ends each thread has its own instance of the object Linkage</description></item><item><title>Fact checks</title><link>https://salehahr.github.io/studienarbeit/fact-checks/</link><pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/fact-checks/</guid><description>Parent: Thesis Duration of cryosection GRK stuff SLAM rigidity assumption camera-imu-complementarity</description></item><item><title>Pizarro 2016 Schwarps</title><link>https://salehahr.github.io/studienarbeit/pizarro-2016-schwarps/</link><pubDate>Sun, 20 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/pizarro-2016-schwarps/</guid><description>Author: Daniel Pizarro et al.
Abstract
Warp between two images of a deforming surface: a transformation that depict the geometric deformation between the two &amp;lsquo;maps points between images of a deforming surface&amp;rsquo; Current approach to enforce a warp&amp;rsquo;s smoothness: penalise its second order partial derivatives However this favours locally affine warps Does not capture the local projective component of the image deformation Propose: novel penalty to smooth the warp while capturing the deformation&amp;rsquo;s local projective structure Proposed penalty is based on equivalents to the Schwarzian derivatives Schwarzian derivatives: projective differential invariants exactly preserved by homographies Methodology to derive a set of PDEs with only homographies as the solutions Validation: Schwarps outperform existing warps in modeling and extrapolation power: perform better in deformable reconstruction methods Introduction/Related work</description></item><item><title>Notes on current thesis version</title><link>https://salehahr.github.io/studienarbeit/notes-on-current-thesis-version/</link><pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/notes-on-current-thesis-version/</guid><description>Parent: Thesis Variables table unterhalb der Gleichungen doppelt-gemoppelt? Wenn schon ein Symbolverzeichnis existiert Margin headers — not sure if I like them or not Too many links for repeated abbreviations? Must Prof. Sawodny&amp;rsquo;s name be in my cover page twice? Will write more about
Navigation subtheme of the B focus in the intro chapter ORB for deformable envs, other SLAM algos in the lit review chapter Filter-based SLAM in chapter 2 Optimisation-based SLAM in chapter 2 IMU preintegration om chapter 2 once I understand the math in the preintegration paper&amp;hellip; MAP estimation Data association (in chapter 2) OR DefSLAM-specific data association in chapter 3 ORB-SLAM3 in chapter 3 Will change</description></item><item><title>Tex stuff</title><link>https://salehahr.github.io/studienarbeit/tex-stuff/</link><pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/tex-stuff/</guid><description>Parent: Thesis tikzexternalize
vimtex custom compile; include makeglossary
vim UltiSnips doesn&amp;rsquo;t work in math contexts! (Solved: &amp;lt;http://github.com/SirVer/ultisnips/issues/1193
issuecomment-620455011&amp;gt;)
get symbols to work with glossaries or glossaries-extra
set up spellcheck
automated list of symbols</description></item><item><title>Descriptors in feature detection/extraction</title><link>https://salehahr.github.io/studienarbeit/descriptors-in-feature-detection-extraction/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/descriptors-in-feature-detection-extraction/</guid><description>Source: http://medium.com/data-breach/introduction-to-feature-detection-and-matching-65e27179885d Backlinks: Bag of words A description of the local appearance around each feature point (keypoint) The descriptor encodes &amp;lsquo;interesting&amp;rsquo; information from the image into numbers and act as an identifier (&amp;lsquo;fingerprint&amp;rsquo;) to differentiate between features The description should ideally be invariant to changes (such as illumination, translation, scale, in-plane rotation) so that the feature can be found again, even if the image is transformed Typically: for each feature point, there is a descriptor vector Classes of descriptors:</description></item><item><title>FAST keypoint detector</title><link>https://salehahr.github.io/studienarbeit/fast-keypoint-detector/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/fast-keypoint-detector/</guid><description>Source: http://medium.com/data-breach/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf Parent: ORB descriptor FAST (Features from Accelerated and Segments Test)
How it works
Given: pixel p, surrounded by other pixels in the image
Take the surrounding pixels that are in a small circle around p If more than half of the surrounding pixels are darker/brighter than p, p is selected as a keypoint
Good for edge detection
Drawbacks</description></item><item><title>Feature matching</title><link>https://salehahr.github.io/studienarbeit/feature-matching/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/feature-matching/</guid><description>Source: http://medium.com/data-breach/introduction-to-feature-detection-and-matching-65e27179885d Backlinks: Bag of words , sparse/feature-based-vslam For matching between images, i.e. to establish a relationship (&amp;lsquo;correspondence&amp;rsquo;) between two images of the same scene or object.
Basic algorithm
Find/detect a set of identifying (&amp;lsquo;distinctive&amp;rsquo;) keypoints from all images to be matched Define a search region around each keypoint Extract and normalise the region content Compute a local descriptor from the normalised region Match local descriptors between the images Performance of matching methods depend on</description></item><item><title>ORB descriptor</title><link>https://salehahr.github.io/studienarbeit/orb-descriptor/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/orb-descriptor/</guid><description>Source: http://medium.com/data-breach/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf Backlinks: Descriptors in feature detection/extraction Oriented FAST and Rotated BRIEF, developed 2011 Was developed as an alternative to SIFT and SURF, and ended up being better/faster than both Build on FAST keypoint detector BRIEF descriptor ORB using FAST, but with (partial) scale invariance
Use a multiscale image pyramid
Each level of the pyramid is the same image, but scaled at different resolutions (reduced size as you go higher up) Once the pyramid is computed, FAST is used to detect keypoints  ORB detection</description></item><item><title>Gauss-Newton Method on Manifold</title><link>https://salehahr.github.io/studienarbeit/gauss-newton-method-on-manifold/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/gauss-newton-method-on-manifold/</guid><description>Source: Forster 2017 IMU Preintegration Standard approach for optimization on manifold
define a retraction to reparametrise the problem (lifting) retraction bijective map map between an element of the tangent space at x and a neighbourhood of x on the manifold i.e. we work in the tangent space (locally like a Euclidian space) and apply standard optimisation techniques for Gauss-Newton specifically: [ts] squared cost around current estimate [ts] solve the quadratic approximation &amp;ndash;&amp;gt; we get vector in tangent space [m] update the current guess on the manifold  Consider: Reparametrised: Retraction for SE(3) The exponential map of SE(3) as a retraction is possible, but may not be convenient (computationally)</description></item><item><title>IMU kinematic model using Euler integration</title><link>https://salehahr.github.io/studienarbeit/imu-kinematic-model-using-euler-integration/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/imu-kinematic-model-using-euler-integration/</guid><description>Parent: IMU index Source: Forster 2017 IMU Preintegration Backlinks: [IMU preintegration on manifold](imu preintegration on-manifold.md), imu-measurement-model Kinematic model Using Euler integration assuming acc and angVel are constant in the time interval: Using the measurement equations:</description></item><item><title>IMU preintegration on manifold</title><link>https://salehahr.github.io/studienarbeit/imu-preintegration-on-manifold/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/imu-preintegration-on-manifold/</guid><description>Parent: IMU index Source: Forster 2017 IMU Preintegration Backlinks: IMU model Preintegration on manifold
Summarising all measurements between the keyframes i and j into a single measurement This preintegrated IMU measurement constrains the motion between two consecutive keyframes Assume IMU is synchronised with the camera The above equations already provide the summarised IMU measurements, however, the integration has to be repeated whenever the linearisation point at t=t_i changes i.</description></item><item><title>SE(3) Special Euclidian Group</title><link>https://salehahr.github.io/studienarbeit/se-3-special-euclidian-group/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/se-3-special-euclidian-group/</guid><description>Parent: Rotations / SO(3) group index Source: Forster 2017 IMU Preintegration Group of rigid motion in 3D</description></item><item><title>SO(3) 3D rotation group</title><link>https://salehahr.github.io/studienarbeit/so-3-3d-rotation-group/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/so-3-3d-rotation-group/</guid><description>Parent: Rotations / SO(3) group index See also: [Orientation parametrisations](orientation parametrisations.md), [linearisation of an orientation in so(3)](linearisation of an orientation in so(3).md), solà 2017 quaternion kinematics for eskf Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
All orthogonal matrices with dim 3x3 have the property  They are part of the orthogonal group O(3) If, additionally, det R = 1, then the matrix belongs to SO(3) and is a rotation matrix Source: http://en.</description></item><item><title>Spaces in mathematics</title><link>https://salehahr.github.io/studienarbeit/spaces-in-mathematics/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/spaces-in-mathematics/</guid><description>Source: http://upload.wikimedia.org/wikiversity/en/c/cd/Spaces_in_mathematics.pdf Types of spaces in mathematics
Euclidian spaces (3D space, 2D space/Euclidian plane) Linear spaces Topological spaces Hilbert spaces etc. What is a space?
No real definition Made of selected mathematical objects which are treated as points selected relationships between these points Points can be elements of a set functions subspaces Isomorphic spaces are considered identical Isomorphism between two spaces: one-to-one mapping between the points, that preserves the relationships between the points</description></item><item><title>System in a VIN problem with IMU preintegration</title><link>https://salehahr.github.io/studienarbeit/system-in-a-vin-problem-with-imu-preintegration/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/system-in-a-vin-problem-with-imu-preintegration/</guid><description>Source: Forster 2017 IMU Preintegration State x_i of the system at time i with All keyframes up till time k State of all keyframes camera measurements IMU measurements between KFs i and j (consecutive) Set of measurements up till time k IMU pose: , maps a point in B to W</description></item><item><title>Non-Rigid Guided Matching (b/w KFs) in DefSLAM</title><link>https://salehahr.github.io/studienarbeit/non-rigid-guided-matching-b-w-kfs-in-defslam/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/non-rigid-guided-matching-b-w-kfs-in-defslam/</guid><description>Source: Lamarca 2019 DefSLAM Backlinks: NRSfM in DefSLAM Matching between keyframes (used in deformation mapping in DefSLAM)
Use an estimated warp as a reference
To increase number of matches in the covisible keyframes Process
Matches are given by deformation tracking Estimate an initial warp between k and k* (covisible keyframes) how? Using this initial warp, estimate where a point would be seen in k* Define a search region around thesse estimated positions.</description></item><item><title>Surface alignment in DefSLAM</title><link>https://salehahr.github.io/studienarbeit/surface-alignment-in-defslam/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/surface-alignment-in-defslam/</guid><description>Parent: Mapping step-by-step in DefSLAM Source: Lamarca 2019 DefSLAM Goal:
to scale the up-to-scale surface (output of NRSfM) to the proper dimensions get an idea of the proper dimensions from the already estimated map i.e. resulting surface must match the scale of the template T_(k-1) T_(k-1): deformed map generated by the tracker at the instance of KF=k insertion, with shape-at-rest of S_(k-1) generated from KF:(k-1) result: scale-corrected shape-at-rest Sk Method:</description></item><item><title>Template substitution in DefSLAM</title><link>https://salehahr.github.io/studienarbeit/template-substitution-in-defslam/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/template-substitution-in-defslam/</guid><description>Parent: Mapping step-by-step in DefSLAM Source: Lamarca 2019 DefSLAM Tracking runs at frame-rate, and mapping at keyframe-rate Tracking processes Nm frames during a whole mapping run Process
New keyframe (k) is made. Now at time t=k At this point, the template in the tracking is still based on the old shape-at-rest, S_(k-1) Mapping thread starts creates surface S_k which is aligned to prev. template T_(k-1) k is set as the reference keyframe from S_k, create template T_k and from now on use this template instead of the old one T_(k-1) At time t=k+Nm, use data from the tracking thread image points at t=k+Nm deform the recently computed template T_k based on these images use SfT but neglecting the temporal term (to allow large deformation, &amp;ldquo;as a lot might have happened in the time span of Nm&amp;rdquo;) so now we get a T_k that is deformed (updated) to the most recent image points we do this extra step instead of passing T_k (from step 1) to the tracker immediately because, due to the new points occurring at t=k+Nm, using the original T_k might lead to data association errors mapper passes the new template T_k (t=k+Nm) to the tracker</description></item><item><title>The making of EndoSLAM dataset</title><link>https://salehahr.github.io/studienarbeit/the-making-of-endoslam-dataset/</link><pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/the-making-of-endoslam-dataset/</guid><description>http://www.youtube.com/watch?v=G_LCe0aWWdQ Github: http://github.com/CapsuleEndoscope/EndoSLAM</description></item><item><title>Camera calibration</title><link>https://salehahr.github.io/studienarbeit/camera-calibration/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/camera-calibration/</guid><description>Parent: SLAM Index Source: http://de.mathworks.com/help/vision/ug/camera-calibration.html
estimates lens/sensor parameters e.g. to correct lens distortion, determine position, measurement etc there are several camera models, e.g. fisheye, pinhole Camera parameters
intrinsic extrinsic distortion coefficients How to solve for camera parameters?
Need to have 3D world points and the corresponding 2D image points Take multiple images of a calibration pattern to obtain these correspondences With the mapping 3Dp -&amp;gt; 2Dp, solve for camera parameters Evaluate accuracy of estimated camera parameters:</description></item><item><title>Data association in DefSLAM</title><link>https://salehahr.github.io/studienarbeit/data-association-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/data-association-in-defslam/</guid><description>Source: Lamarca 2019 DefSLAM See also: Data association Goal: match keypoints in current frame (newly extracted) with map points (already in map/system) Use the active matching strategy proposed in [Agudo 2015]: “Simultaneous pose and non-rigid shape with particle dynamics,” Steps  ORB points (keypoints) are detected in current frame
Camera pose Tcw is predicted
using camera motion model camera motion model: function of past camera poses Predict where map points (existing in map) would be imaged, based on last estimated template i.</description></item><item><title>DefSLAM framework</title><link>https://salehahr.github.io/studienarbeit/defslam-framework/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/defslam-framework/</guid><description>Source: Lamarca 2019 DefSLAM &amp;ldquo;Fusion of the methods available for processing non-rigid monocular scenes&amp;rdquo;
Deformation tracking [front end]
estimates/recovers/optimises: camera pose scene deformation / deformation of map points (observations) the map points are then embedded into the template Tk (to compute their position on the surface) operates at frame rate SFT-based (shape from template), requires prior geometry (template of scene at rest) for the currently being viewed map Map points are deformed (updated) by solving an optimisation problem min { reprojection error + deformation energy } per frame Deformation mapping [back end]</description></item><item><title>Initialisation of monocular SLAM</title><link>https://salehahr.github.io/studienarbeit/initialisation-of-monocular-slam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/initialisation-of-monocular-slam/</guid><description>Source: Lamarca 2019 DefSLAM Depth information has to be generated before localisation can be performed — how?
Capture multiple images which have enough parallax These images with parallax allows depth information to be calculated (this uses motion parallax ) From these images, the map can be generated Localisation can then be carried out with respect to the map (as long as camera doesn&amp;rsquo;t move off to an unexplored region)</description></item><item><title>Mapping step-by-step in DefSLAM</title><link>https://salehahr.github.io/studienarbeit/mapping-step-by-step-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/mapping-step-by-step-in-defslam/</guid><description>Source: Lamarca 2019 DefSLAM Parent: DefSLAM framework Steps
Recover warps between k and k* (s. [Non-Rigid Guided Matching (b/w KFs) in DefSLAM](non-rigid guided-matching-(b_w-kfs)-in-defslam.md)) with k: anchor keyframes, i.e. KFs where one of the observed map points was initialised with k*: set of best covisible keyframes warps: transformation between the images Ik to Ik* In DefSLAM, Schwarps (a family of warps using 2D Schwarzian equation regularisers) is used Schwarps has something to do with the infinitesimal planarity assumption of NRSfM [ NRSfM ] Process k* to get estimate of an up-to-scale surface  Input of NRSfM: warps [ Surface alignment ] Up-to-scale surface (\hat{S}_k) is aligned with the whole map in order to obtained the scaled surface Sk w.</description></item><item><title>NRSfM and SfT</title><link>https://salehahr.github.io/studienarbeit/nrsfm-and-sft/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/nrsfm-and-sft/</guid><description>Source: Lamarca 2019 DefSLAM In literature, non-rigid monocular scenes are handled by NRSfM and SfT
NRSfM (non-rigid structure from motion)
batch processing of images to recover deformation computationally demanding — slower than SFT SFT (shape from template)
uses only a single image — faster than NRFfM lower computational cost must have a known 3D template (textured model)</description></item><item><title>NRSfM in DefSLAM</title><link>https://salehahr.github.io/studienarbeit/nrsfm-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/nrsfm-in-defslam/</guid><description>Parent: Mapping step-by-step in DefSLAM Source: Lamarca 2019 DefSLAM Assumptions
Isometric deformation Infinitesimal planarity [DEF]: any surface can be approximated as a plane at infinitesimal level, all the while maintaining its curvature at a global level The method used here is a local method &amp;ndash;&amp;gt; implies that it handles missing data and occlusions inherently
surface deformation is modelled locally for each point, under the above assumptions Embedding, phi_k of the scene surface</description></item><item><title>ORBSLAM2 mods</title><link>https://salehahr.github.io/studienarbeit/orbslam2-mods/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/orbslam2-mods/</guid><description>Patch to work with opencv4 http://github.com/Windfisch/ORB_SLAM2 ORBSLAM2 Python bindings http://github.com/jskinn/ORB_SLAM2-PythonBindings</description></item><item><title>Pinhole camera projection function</title><link>https://salehahr.github.io/studienarbeit/pinhole-camera-projection-function/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/pinhole-camera-projection-function/</guid><description>Backlinks: Pinhole camera model See also: World to camera trafo Source: Mur-Artal 2017 VI-ORB 3D points Projection function  Transforms 3D points into 2D points on image plane Focal length:  Principal point:  The projection does not consider the distortion due to the lens
therefore when extracting image features, first undistort their coordinates only then match to projected points (existing features which have undergone projection from 3D to 2D) Source: Lamarca 2019 DefSLAM 3D point: Projection function maps</description></item><item><title>Template in DefSLAM</title><link>https://salehahr.github.io/studienarbeit/template-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/template-in-defslam/</guid><description>Source: Lamarca 2019 DefSLAM Template
2D triangular mesh floating in the 3D space consists of a set of 2D triangular facets F a facet has 3 nodes (set V) and 3 edges (set E) map points observed in keyframe k are embedded in the facets Map point coordinates in barycentric coordinates</description></item><item><title>Tracking optimisation in DefSLAM</title><link>https://salehahr.github.io/studienarbeit/tracking-optimisation-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/tracking-optimisation-in-defslam/</guid><description>Source: Lamarca 2019 DefSLAM Backlinks: Template substitution in DefSLAM Optimisation function
Minimises reprojection error (in the image) deformation energy (of the template) boundary nodes of the local zone are fixed (i.e. not set as arguments to the optimisation function) this makes the absolute camera pose observable how? in order to constrain the gauge freedoms Initial guess: values from previous optimisation (i.e. previous frame: t-1) Reprojection error robust against outliers due to Huber robust kernel</description></item><item><title>Bag of words</title><link>https://salehahr.github.io/studienarbeit/bag-of-words/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/bag-of-words/</guid><description>Parent: SLAM Index Source: http://towardsdatascience.com/bag-of-visual-words-in-a-nutshell-9ceea97ce0fb
Has its origins in natural language processing (NLP), information retrieval
A text can be seen as a bag of words, with each word having different frequencies from one another This can be used to compare and classify texts (similar histograms) In vision
Instead of words we have features (identifying pattern in an image) An image is represented as a set of features Features consist of Keypoints: points that are invariant to transformation Descriptors : description of the keypoint, for feature representation Construct a frequency histogram of features in the image Workflow Feature detection/extraction &amp;ndash;&amp;gt; build vocabulary/codewords &amp;ndash;&amp;gt; make histogram = BoW</description></item><item><title>DefSLAM dependency/inheritance diagram</title><link>https://salehahr.github.io/studienarbeit/defslam-dependency-inheritance-diagram/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/defslam-dependency-inheritance-diagram/</guid><description/></item><item><title>Forster 2017 IMU Preintegration</title><link>https://salehahr.github.io/studienarbeit/forster-2017-imu-preintegration/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/forster-2017-imu-preintegration/</guid><description>Authors: Forster et al
Abstract:
First contribution: preintegration theory (building up on Lupton&amp;rsquo;s work) what&amp;rsquo;s different from Lupton&amp;rsquo;s: addresses manifold structure of the rotation group, analytic derivation of all Jacobians Lupton&amp;rsquo;s work uses Euler angles Using Euler angles and techniques of Euclidian spaces for state propagation/covariance estimation is not properly invariant under rigid transformations uncertainty propagation, a-posteriori bias correction same as Lupton: integration performed in local frame, eliminating need for reintegrating when linearisation point changes Second contribution: integration of the preintegrated IMU model into a visual-inertial pipeline The system presented uses incremental smoothing for fast computation of the optimal MAP estimate Uses structureless model (3D landmarks are not part of the variables to be estimated) for visual measurements &amp;ndash;&amp;gt; allows eliminating large numbers of variables Motivation:</description></item><item><title>Handling the computational complexity of optimisation-based SLAM</title><link>https://salehahr.github.io/studienarbeit/handling-the-computational-complexity-of-optimisation-based-slam/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/handling-the-computational-complexity-of-optimisation-based-slam/</guid><description>Parent: SLAM Index Source: Forster 2017 IMU Preintegration Complexity of nonlinear batch optimisation
The trajectory and the map, which comprise the states, grow with time The larger the SLAM problem, the less feasible it is to perform the optimisation in real-time Solutions to improve computational efficiency
Keyframe-based methods: discard frames except for a few selected keyframes Run the optimisation parallelly (e.g. tracking and mapping threads) Fixed-lag smoothing: Use of a local map of fixed size, with marginalisation of the old states (summarise the old states into a prior term) Filtering is a special case of this: window of size 1, i.</description></item><item><title>Visual-inertial datasets</title><link>https://salehahr.github.io/studienarbeit/visual-inertial-datasets/</link><pubDate>Fri, 06 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/visual-inertial-datasets/</guid><description>http://sites.google.com/view/awesome-slam-datasets/home
http://fpv.ifi.uzh.ch/ Aggressive drone racing http://www.lirmm.fr/aqualoc/ Underwater Monochromatic http://vision.in.tum.de/data/datasets/visual-inertial-dataset TUM indoor/urban, slides fisheye cameraUsed in ORBSLAM3 http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets EUROC MAV stereo, monochrUsed in ORBSLAM3</description></item><item><title>Discussion 2020-11-03</title><link>https://salehahr.github.io/studienarbeit/discussion-2020-11-03/</link><pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/discussion-2020-11-03/</guid><description>Current progress
Find out how g2o works, how the DefSLAM implementation of the tracking optimisation works Incorporate IMU data (s. ORBSLAM3 implementation) IMU initialisation IMU preintegration IMU terms in cost function (which function?) Topics
How g2o works IMU preintegration DefSLAM + IMU cost function Implementation in DefSLAM using g2o [tbd] ORBSLAM3&amp;rsquo;s implementation the IMU cost function terms initialisation preintegration Kalman idea for IMU integration Compile + run</description></item><item><title>DefTracking::MonocularInitialization</title><link>https://salehahr.github.io/studienarbeit/deftracking-monocularinitialization/</link><pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/deftracking-monocularinitialization/</guid><description>Parent: DefTracking::Track Initialises
surface points in the surface If num. features in current frame &amp;gt; 100
set frame pose to origin make new KF (GroundTruthKeyFrame) pKFini add the KF to the map mpMap iterate over the N features get feature kp convert kp to 3d point make new DefMapPoint(3dp, pKFini, mpMap) set pointers between DefMapPoint, GroundTruthKeyFrame, DefMap Save surface using bbs Set mLastFrame := mCurrentFrame Local window: Add KF to local KFs vector, add MapPoints to local MP vector, mpLocalMapper Calculate Tcr from Tcw Initialise SLAM: Set reference KF, reference MapPoints set mState to OK</description></item><item><title>ORBSLAM::Frame constructor (monocular)</title><link>https://salehahr.github.io/studienarbeit/orbslam-frame-constructor-monocular/</link><pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/orbslam-frame-constructor-monocular/</guid><description>Source: Tracking::GrabImageMonocular Set scale level info from ORB extractor Extract ORB features mvKeys (vector of keypoints/features) Set N number of features Make mvpMapPoints (null, but with size N), mvbOutlier (all entries false, size N) If first frame or calibration change: ComputeImageBounds AssignFeaturesToGrid()</description></item><item><title>Dynamic Bayesian Network formulation of SLAM</title><link>https://salehahr.github.io/studienarbeit/dynamic-bayesian-network-formulation-of-slam/</link><pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/dynamic-bayesian-network-formulation-of-slam/</guid><description>Source: Grisetti 2011 - Tutorial graph-based SLAM Dynamic Bayesian Network
Solution of full SLAM problem: Transition model: Observation model:  The observation model is usually multimodal: a single observation may result in multiple edges (in the spatial graph) Therefore, the Gaussian assumption does not hold</description></item><item><title>DefOptimizer::DefPoseOptimization</title><link>https://salehahr.github.io/studienarbeit/defoptimizer-defposeoptimization/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/defoptimizer-defposeoptimization/</guid><description>Parent: DefTracking::Track As far as I understand it:
Uses g2o library for the optimisation (graph-based SLAM) cost function terms are converted to edges and nodes each cost function term seems to correspond to an edge in the graph in g2o paper/tutorial: an edge is fully characterised by its error function and its information matrix int DefPoseOptimization(Frame *pFrame, Map *mMap, double RegLap, double RegInex, double RegTemp, uint NeighboursLayers) // define optimiser, set solver optimizer = new &amp;hellip; optimizer.</description></item><item><title>Edges in g2o</title><link>https://salehahr.github.io/studienarbeit/edges-in-g2o/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/edges-in-g2o/</guid><description/></item><item><title>DefOptimizer::poseOptimization</title><link>https://salehahr.github.io/studienarbeit/defoptimizer-poseoptimization/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/defoptimizer-poseoptimization/</guid><description>Parent: DefTracking:TrackWithMotionModel() int DefOptimizer::poseOptimization(Frame *pFrame)
// Set estimate of solution to current camera pose g2o::VertexSE3Expmap *vSE3 = new g2o::VertexSE3Expmap(); vSE3-&amp;gt;setEstimate(Converter::toSE3Quat(pFrame-&amp;gt;mTcw)); vSE3-&amp;gt;setId(0); vSE3-&amp;gt;setFixed(false); optimizer.addVertex(vSE3);
// Set MapPoint vertices (num. nodes in opt. graph?) const int N = pFrame-&amp;gt;N;</description></item><item><title>defSLAM::System constructor</title><link>https://salehahr.github.io/studienarbeit/defslam-system-constructor/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/defslam-system-constructor/</guid><description>Parent: DefSLAM simple_camera defSLAM::System::System(const string &amp;amp;strVocFile, const string &amp;amp;strSettingsFile, const bool bUseViewer)  mSensor(MONOCULAR), mpLoopCloser(NULL), mpViewer(static_cast&amp;lt;Viewer *&amp;gt;(nullptr)), mbReset(false), mbActivateLocalizationMode(false), mbDeactivateLocalizationMode(false) Constructor // initialise mpVocabulary from file // create mpKeyFrameDatabase from mpVocabulary // create map DefMap() // create drawers for viewer DefFrameDrawer DefMapDrawer // initialise tracking, mapping, viewer threads; loop closing not implemented in DefSLAM mpTracker = new DefTracking(&amp;hellip;); mpLocalMapper = new DefLocalMapping(&amp;hellip;); mpViewer = new DefViewer(&amp;hellip;);
Attributes eSensor mSensor ORBVocabulary *mpVocabulary KeyFrameDatabase *mpKeyFrameDatabase Map *mpMap // stores pointers to all KFs, all MapPoints</description></item><item><title>defSLAM::System::TrackMonocular</title><link>https://salehahr.github.io/studienarbeit/defslam-system-trackmonocular/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/defslam-system-trackmonocular/</guid><description>Parent: DefSLAM simple_camera cv::Mat defSLAM::System::TrackMonocular cv::Mat Tcw = mpTracker-&amp;gt; GrabImageMonocular (im, timestamp);
// get information from mpTracker: // get states mTrackingState // get map points mTrackedMapPoints // get key points mTrackedKeyPointsUn
// return camera pose return Tcw;</description></item><item><title>DefTracking::Track</title><link>https://salehahr.github.io/studienarbeit/deftracking-track/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/deftracking-track/</guid><description>Parent: Tracking::GrabImageMonocular void DefTracking::Track // if: not initialised, do: monocular initialisation // elseif: already initialised, do: track frame { // if: tracking and mapping, do: bOK = TrackWithMotionModel ();
// if bOK (there exists camera pose estimate and matching), track local map // if template is updated (keyframe-rate update) set reference KF from new template do DefPoseOptimization (&amp;hellip;); bOK = TrackLocalMap();
// if: bOK, update motion model (update mVelocity); clean VO matches // check if we should insert a new KF, delete outliers for BA</description></item><item><title>DefTracking:TrackWithMotionModel()</title><link>https://salehahr.github.io/studienarbeit/deftracking-trackwithmotionmodel-/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/deftracking-trackwithmotionmodel-/</guid><description>Parent: DefTracking::Track // Initial tracking to locate rigidly the camera and discard outliers. bool DefTracking::TrackWithMotionModel()
// Update last frame relative pose according to its reference keyframe UpdateLastFrame();
// Project points seen in prev frames int th = 15; int nmatches = Defmatcher.SearchByProjection(*mCurrentFrame, mLastFrame, th, mSensor == System::MONOCULAR);
// Optimise frame pose with all matches to initialise camera pose Optimizer:: poseOptimization (mCurrentFrame, myfile);
// Discard outliers
// return: sufficient number of matches?</description></item><item><title>Tracking_GrabImageMonocular</title><link>https://salehahr.github.io/studienarbeit/tracking-grabimagemonocular/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/tracking-grabimagemonocular/</guid><description>Parent: defSLAM::System::TrackMonocular cv::Mat ORB_SLAM2::Tracking::GrabImageMonocular
colour conversion make frame using image, timestamp, ORB stuff, calibration data, etc. mCurrentFrame = new Frame (mImGray, timestamp, mpORBextractorLeft, mpORBVocabulary, mK, mDistCoef, mbf, mThDepth, im)
perform tracking: Track (); return camera pose return mCurrentFrame-&amp;gt;mTcw.clone();</description></item><item><title>(Mur-Artal 2017) VI-ORB</title><link>https://salehahr.github.io/studienarbeit/mur-artal-2017-vi-orb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/mur-artal-2017-vi-orb/</guid><description>Backlinks: [Chen 2018 Review of VI SLAM](chen 2018 review of vi slam.md), [keyframe-based tightly-coupled multisensor slam](keyframe-based tightly-coupled multisensor slam.md), todo , works of-possible-interest URL: http://ieeexplore.ieee.org/abstract/document/7817784, Authors: Mur-Artal, Tardós Code: http://paperswithcode.com/paper/visual-inertial-monocular-slam-with-map-reuse Results (video): http://www.youtube.com/watch?v=JXRCSovuxbA
Abstract
current VI odometry approaches: drift accumulates due to lack of loop closure therefore there is a need for tightly-coupled VI-SLAM with loop closure and map reuse here: focus on monocular case, but applicable to other camera configurations builds on ORB-SLAM (from same author) IMU initialisation method (initialises: scale, gravity direction, velocities, gyroscope bias, accelerometer bias) depends on visual monocular initialisation (coupled initialisation) Other works: recent tightly-coupled VIO (both filtering- and optimisation-based) lack loop closure, so drift accumulates</description></item><item><title>IMU index</title><link>https://salehahr.github.io/studienarbeit/imu-index/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/imu-index/</guid><description>Parent: SLAM Index Backlinks: SA TODO General IMU Odometry Why use the visual-inertial sensor combination? IMU to camera coordinate transformations Practical Converting IMU data to inertial frame Modelling Probabilistic models for IMU [Choice of model for the IMU motion model](choice of model-for-the-imu-motion-model.md) [Choice of states for the IMU motion/kinematics model](choice of states-for-the-imu-motion_kinematics-model.md) Variables in ESKF using IMUs [IMU states, dynamics equations](imu states, dynamics equations.md) (kfs and preintegration) / [(kok) imu motion model (discrete)]((kok) imu motion model (discrete).</description></item><item><title>Mapping in VIORB</title><link>https://salehahr.github.io/studienarbeit/mapping-in-viorb/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/mapping-in-viorb/</guid><description>Source: Mur-Artal 2017 VI-ORB Mapping in VIORB Previously in ORBSLAM (only poses are optimised): Now in VIORB, more states to optimise: Increase in complexity
more states to optimise (v, b) IMU measurements creates constraints between keyframes Original ORBSLAM discards redundants KFs (poses a problem with IMU constraints!) Workaround: in local BA, only allow discarding of KF if, after discarding, the time between two consecutive KFs is short enough (&amp;lt;= 0.</description></item><item><title>Pinhole camera model</title><link>https://salehahr.github.io/studienarbeit/pinhole-camera-model/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/pinhole-camera-model/</guid><description>Parent: SLAM Index Backlinks: [Camera calibration](camera calibration.md), [pinhole camera projection function](pinhole camera projection function.md), [weiss thesis vision based navigation for micro helicopters](weiss thesis vision-based-navigation-for-micro-helicopters.md) See also: World to camera trafo Source: http://de.mathworks.com/help/vision/ug/camera-calibration.html Does not account for lens distortion (ideal pinhole camera doesn&amp;rsquo;t have a lens) To represent a real camera, the full camera model to be used should include (radial and tangential) lens distortion, (such as the one used in the MATLAB computer vision toolbox)</description></item><item><title>Why use the visual-inertial sensor combination?</title><link>https://salehahr.github.io/studienarbeit/why-use-the-visual-inertial-sensor-combination-/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/why-use-the-visual-inertial-sensor-combination-/</guid><description>Parent: SLAM Index See also: Multisensor fusion Source: Mur-Artal 2017 VI-ORB Cheap but also with good potential Cameras provide rich information but are relatively cheap IMU provides self-motion info, helps recover scale in monocular applications enables estimation of the direction of gravity &amp;ndash;&amp;gt; renders pitch and roll observable Source: Forster 2017 IMU Preintegration Visual-inertial fusion for 3D structure and motion estimation Both cameras and IMUs are cheap, easy to find and complement each other well Camera exteroceptive sensor measures, up to a to-be-determined metric scale, appearance and geometrical structure of a 3D scene IMU interoceptive sensor makes metric scale of monocular cameras, as well as the direction of gravity, observable Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.</description></item><item><title>DefSLAM simple_camera</title><link>https://salehahr.github.io/studienarbeit/defslam-simple-camera/</link><pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/defslam-simple-camera/</guid><description>Without ground truth
App run // Create defSLAM::system, which initializes all threads (local mapping, loop closing, viewer) defSLAM::System SLAM(orbVocab, calibFile, bUseViewer);
// Timestamp uint timestamp := 0;
// Process frames from video capture while (capture.isOpened()) { // Get the capture as a matrix; // SLAM SLAM. TrackMonocular (img_matrix, timestamp); timestamp++; }</description></item><item><title>Initialising graph in SP3</title><link>https://salehahr.github.io/studienarbeit/initialising-graph-in-sp3/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/initialising-graph-in-sp3/</guid><description>Parent: SofaPython Index http://github.com/SofaDefrost/plugin.SofaPython3.deprecated/pull/110
&amp;ldquo;Can you share an example of a scene and a component you have in mind ? Because currently to summary the discussion during sofa-meeting the problem with init/bdwInit/reinit is that it that this is severely ill defined and we are considering to totally remove that from Sofa and use alternatives pattern among which:
have an onSimulationStart / onSimulationStop event to detect when the simulation is on or not avoid using getContext() to fetch other components unless you store them in SingleLink.</description></item><item><title>Covisible keyframes</title><link>https://salehahr.github.io/studienarbeit/covisible-keyframes/</link><pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/covisible-keyframes/</guid><description>Source: Palafox 2019 ( thesis ) Backlinks: Lamarca 2019 DefSLAM Two keyframes are covisible if they share several common landmarks.</description></item><item><title>Pipenv venv in project folder</title><link>https://salehahr.github.io/studienarbeit/pipenv-venv-in-project-folder/</link><pubDate>Sun, 06 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/pipenv-venv-in-project-folder/</guid><description>PIPENV_VENV_IN_PROJECT=1
Powershell: $Env:PIPENV_VENV_IN_PROJECT=&amp;ldquo;1&amp;rdquo;</description></item><item><title>Error ellipse/Confidence ellipse</title><link>https://salehahr.github.io/studienarbeit/error-ellipse-confidence-ellipse/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/error-ellipse-confidence-ellipse/</guid><description>Parent: Multivariate Gaussian distributions Source: rlabbe Kalman/Bayesian filters in Python Any slice through a multivariate Gaussian is an ellipse Plots show the slice for 3 standard deviations
Showing correlation using error ellipses</description></item><item><title>Showing correlation using error ellipses</title><link>https://salehahr.github.io/studienarbeit/showing-correlation-using-error-ellipses/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/showing-correlation-using-error-ellipses/</guid><description>Parent: Error ellipse/Confidence ellipse Source: rlabbe Kalman/Bayesian filters in Python A slanted ellipse implies correlation
The &amp;lsquo;thinner&amp;rsquo; side isn&amp;rsquo;t necessarily more accurate, it just means that the spread of data is reduced along this dimension (when viewing sensor data, for example) Example First epoch Yellow: prior (very uncertain about position) Green: evidence (more accurate in one of the dimensions than the other; more certainty compared to prior) Blue: posterior via multiplication Posterior retains the shape of the evidence (which has more certainty than the prior)</description></item><item><title>50.2. Bayes' Theorem</title><link>https://salehahr.github.io/studienarbeit/50.2.-bayes-theorem/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.-bayes-theorem/</guid><description>Parent: Bayesian Filter Update Step Source: rlabbe Kalman/Bayesian filters in Python How do we compute the probability of an event given previous information? (s. also Frequentist vs Bayesian statistics )
Formula to compute new information into existing information
Used in the update step of a Bayesian filter (valid for both probabilities as well as probability distributions) where || . || expresses normalisation
B Evidence (sensor measurements z) p(A) Prior p(B|A) Likelihood p(A|B) Posterior In filtering systems, computing p(x|z) is nearly impossible, but computing p(z|x) is fairly straightforward, which then facilitates the computation of p(x|z) via the Bayes' theorem formula.</description></item><item><title>50.2.10 Discrete Bayesian filter</title><link>https://salehahr.github.io/studienarbeit/50.2.10-discrete-bayesian-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.10-discrete-bayesian-filter/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python The Kalman filter is a subset of Bayesian filters
Predict and update steps like in the g-h filter Here: error percentages are used to implicitly compute the g and h parameters Steps
[Initialise our belief in the state] The predict step always degrades our knowledge (belief/prior) However, in the update step , we add another measurement. This, will always improve our knowledge regardless of noise, enabling convergence Limitations of the discrete Bayes filter</description></item><item><title>50.2.10.1 Discrete Bayesian Filter Predict Step</title><link>https://salehahr.github.io/studienarbeit/50.2.10.1-discrete-bayesian-filter-predict-step/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.10.1-discrete-bayesian-filter-predict-step/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python The predict step uses the total probability theorem.
Computes total probability of multiple possible events Uses the system model (propagates the states from prev. time step [posterior] to the next one); prediction Accounts for the uncertainty (kernel) in the prediction: produces a prior Generalise the uncertainty using a kernel (distributes the uncertainty over a range around the prediction) Integrate the kernel into the calculations by using convolution * Convolving the &amp;ldquo;current probabilistic estimate&amp;rdquo; with the &amp;ldquo;probabilistic estimate of how much we think the position has changed&amp;rdquo; (from system model) The prior is a &amp;lsquo;degraded&amp;rsquo; version of the belief i.</description></item><item><title>50.2.10.2 Bayesian Filter Update Step</title><link>https://salehahr.github.io/studienarbeit/50.2.10.2-bayesian-filter-update-step/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.10.2-bayesian-filter-update-step/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python The update step uses Bayes' Theorem Produces the posterior by using the likelihood and the prior Also incorporates sensor data (measurements), as the measurements go into the likelihood calculation Update algorithm
Get a measurement, and associated belief about its accuracy Compute likelihood from the measurement and the measurement accuracy assumption Update the posterior using the likelihood and the prior</description></item><item><title>Central Limit Theorem</title><link>https://salehahr.github.io/studienarbeit/central-limit-theorem/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/central-limit-theorem/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python If we make many measurements, the measurements will be normally distributed. (only applies under certain conditions)</description></item><item><title>Computational properties of Gaussian distributions</title><link>https://salehahr.github.io/studienarbeit/computational-properties-of-gaussian-distributions/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/computational-properties-of-gaussian-distributions/</guid><description>Parent: Gaussian distribution Backlinks: Showing correlation using error ellipses Source: rlabbe Kalman/Bayesian filters in Python g1 + g2 = g3; all are Gaussians g1 * g2 = g3; g3 is not Gaussian, but proportional to a Gaussian Sum of two Gaussians Product of two Gaussians: Product of multidimensional Gaussians:</description></item><item><title>Correlation and independence</title><link>https://salehahr.github.io/studienarbeit/correlation-and-independence/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/correlation-and-independence/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python Independent variables are uncorrelated. But the reverse is not always true: uncorrelated variables may be dependent on one another e.g. y=x^2 has no [linear] correlation, but y depends on x nonetheless</description></item><item><title>Deriving Kalman filter from Discrete Bayes using Gaussians</title><link>https://salehahr.github.io/studienarbeit/deriving-kalman-filter-from-discrete-bayes-using-gaussians/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/deriving-kalman-filter-from-discrete-bayes-using-gaussians/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Replacing discrete Bayes with Gaussian distributions where the operators in the circles are as of yet undetermined</description></item><item><title>Empirical rule 68/95/99.7</title><link>https://salehahr.github.io/studienarbeit/empirical-rule-68-95-99.7/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/empirical-rule-68-95-99.7/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python Emprical rule, a.k.a. 68–95–99.7 rule About 68% of all values lie within one standard deviation of the mean.</description></item><item><title>Factors affecting Kalman filter performance</title><link>https://salehahr.github.io/studienarbeit/factors-affecting-kalman-filter-performance/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/factors-affecting-kalman-filter-performance/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python Difficulties of creating a well-performing Kalman filter: Includes modeling the sensor performance (what variance most accurately represents the reality? Which probability distribution?)
Factors affecting the performance of the Kalman filter
On modelling the process noise/variance Bad initial estimate Filter can recover from this, because we have a certain belief in the sensor measurements Typically the initial value is set to the first sensor measurement Nonlinearity of the system</description></item><item><title>Frequentist vs Bayesian statistics</title><link>https://salehahr.github.io/studienarbeit/frequentist-vs-bayesian-statistics/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/frequentist-vs-bayesian-statistics/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python Frequentist vs Bayesian statistics
Probability of flipping a fair coin infinitely many times is 50% - frequentist Probability of flipping a fair coin one more time (which way do I believe it landed?), single event - Bayesian Bayesian statistics takes past information (prior) into account If finding the prior is tricky, frequentist techniques are sometimes used e.</description></item><item><title>Kalman vs. nonlinear systems</title><link>https://salehahr.github.io/studienarbeit/kalman-vs.-nonlinear-systems/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/kalman-vs.-nonlinear-systems/</guid><description>Parent: Factors affecting Kalman filter performance Source: rlabbe Kalman/Bayesian filters in Python Kalman filter equations are linear Example: approximating a sine-wave signal Explanation:
Back to the basic g-h filter structure: the filter output chooses a value on the residual line The process model in the underlying filter assumes constant velocity (0 acceleration), whereas in the sine example above, the signal is always accelerating</description></item><item><title>Limitations of the discrete Bayes filter</title><link>https://salehahr.github.io/studienarbeit/limitations-of-the-discrete-bayes-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/limitations-of-the-discrete-bayes-filter/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python Limitations of the discrete Bayes filter
Scaling Dog tracking example is one-dimensional, but in real life we often want to track more things (e.g. 2D coordinates, velocities) Multidimensional case: store probabilities in a grid 4 tracked variables: O(n^4) per time step High computational cost with high dimensionality Filter is discrete and therefore gives discrete output But a lot of applications require continuous output Discretising a solution space can lead to lots of data (depending on accuracy required) &amp;ndash;&amp;gt; calculations for lots of different probabilities!</description></item><item><title>Probability distribution</title><link>https://salehahr.github.io/studienarbeit/probability-distribution/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/probability-distribution/</guid><description>Parent: Discrete Bayesian filter Source: rlabbe Kalman/Bayesian filters in Python Probability distribution:
collection of all possible probabilities for an event the distribution lists all possible events and the probability of each sum up to 1 Prior probability distribution: probability prior to incorporating any measurements or other information
Joint probability P(x,y):
probability of both events happening the multivariate Gaussian distribution is already already a joint probability distribution Marginal probability:</description></item><item><title>Pros and cons of Gaussian distributions</title><link>https://salehahr.github.io/studienarbeit/pros-and-cons-of-gaussian-distributions/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/pros-and-cons-of-gaussian-distributions/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python Big advantage of using Gaussian distributions (as opposed to discrete ones w/ histogram bins): less data, b/c a Gaussian distribution is represented fully using only two values: the mean and the variance Limitations of using Gaussian distributions to model the world i.e. deviations from the central limit theorem
Not all situations are describable by Gaussian distributions e.g. sensors in the real world have fat tails (kurtosis) — don&amp;rsquo;t extend to infinity and skew Can&amp;rsquo;t depict any arbitrary probability distributions like in e.</description></item><item><title>Random variable</title><link>https://salehahr.github.io/studienarbeit/random-variable/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/random-variable/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python Combination of values + associated probabilities. &amp;ldquo;Event&amp;rdquo; e.g. die toss, height of students e.g. in a fair die values = {1, 2, &amp;hellip;, 6} (range of values = sample space) probabilities = {1/6} * 6</description></item><item><title>Smug filter</title><link>https://salehahr.github.io/studienarbeit/smug-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/smug-filter/</guid><description>Parent: 1D Kalman filter algorithm Source: rlabbe Kalman/Bayesian filters in Python A filter that, once enough measurements are made, becomes very confident in its prediction (P gets smaller with time while the filter becomes more inaccurate!). From then on it will ignore measurements
To avoid this: add a bit of error to the prediction step, e.g. using the process variance</description></item><item><title>Univariate Gaussian distribution</title><link>https://salehahr.github.io/studienarbeit/univariate-gaussian-distribution/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/univariate-gaussian-distribution/</guid><description>Parent: Gaussian distribution Source: rlabbe Kalman/Bayesian filters in Python If normalised (area under the graph is 1): Gaussian distribution If not normalised: Gaussian function Notation: The random variable X has a Gaussian distribution with mean &amp;hellip; and variance &amp;hellip; .</description></item><item><title>Variance of the 1D Kalman filter</title><link>https://salehahr.github.io/studienarbeit/variance-of-the-1d-kalman-filter/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/variance-of-the-1d-kalman-filter/</guid><description>Parent: 1D Kalman filters Source: rlabbe Kalman/Bayesian filters in Python i.e., what variance is show by the estimated output/posterior?)
Always converges to a fixed value if the sensor and process variances are constant We can run simulations to determine the value to which the filter variance converges Then hard code this value into the filter (+ with first sensor measurement as initial value, the filter should have good performance) Alternative: instead of using the variance value, use the calculated Kalman gain Example implementation using the Kalman gain However, using the Kalman gain obscures the Bayesian approach</description></item><item><title>Variance, standard deviation, covariances</title><link>https://salehahr.github.io/studienarbeit/variance-standard-deviation-covariances/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/variance-standard-deviation-covariances/</guid><description>Backlinks: Multivariate Gaussian distributions Source: rlabbe Kalman/Bayesian filters in Python See also: Empirical rule 68/95/99.7 How much do the values vary from the mean?
There are other ways of calculating variance (e.g. by using absolute values of error instead of error squared). The other methods may be better w.r.t. outliers (outliers get magnified in the square term) Process variance: error in the process model Sensor variance: error in each sensor measurement</description></item><item><title>(AtsushiSakai) PythonRobotics</title><link>https://salehahr.github.io/studienarbeit/atsushisakai-pythonrobotics/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/atsushisakai-pythonrobotics/</guid><description>http://nbviewer.jupyter.org/github/AtsushiSakai/PythonRobotics/</description></item><item><title>(rlabbe) Kalman/Bayesian filters in Python</title><link>https://salehahr.github.io/studienarbeit/rlabbe-kalman-bayesian-filters-in-python/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/rlabbe-kalman-bayesian-filters-in-python/</guid><description>URL: http://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python nbviewer link: http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb Abstract:
Introductory text with Python code Caveat: most of the code is written for didactic purposes, may not be the most efficient solution (nor numerically stable) Recommended other works s. Works of possible interest Chapters
Preface Why Kalman filters? [Aim and main principle of Kalman filters](aim and-main-principle-of-kalman-filters.md) Expected value g-h filter or α-β filter Discrete Bayesian filter Gaussian distribution 1D Kalman filters Multivariate Kalman filters</description></item><item><title>50.1 Why Kalman filters?</title><link>https://salehahr.github.io/studienarbeit/50.1-why-kalman-filters-/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.1-why-kalman-filters-/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python We work with 2 sources of data:
Sensor measurements Our own predictions (based on knowledge of system behaviour) Sensors are noisy, don&amp;rsquo;t give perfect information
Simple solution: to average readings However, this doesn&amp;rsquo;t work when the sensor is too noisy data collection not possible The prediction, however, is also susceptible to noise (the world is noisy, outside/unaccounted for influences)</description></item><item><title>50.1.1 Aim and main principle of Kalman filters</title><link>https://salehahr.github.io/studienarbeit/50.1.1-aim-and-main-principle-of-kalman-filters/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.1.1-aim-and-main-principle-of-kalman-filters/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python Aim of the Kalman/Bayesian filters: to accumulate (or to somehow blend)
our noisy and limited knowledge (of system behaviour) noisy and limited sensor readings and with these, make the best possible prediction (estimate) of the system state.
Main principles:
use past information to make predictions for the future never throw away information predict/propagation step: calculate prediction based on process model and using previous state data (previous estimate) update step: calculate the estimates based on prediction and measurement Prediction step a.</description></item><item><title>Calculating the estimated state in the GH-filter</title><link>https://salehahr.github.io/studienarbeit/calculating-the-estimated-state-in-the-gh-filter/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/calculating-the-estimated-state-in-the-gh-filter/</guid><description>Parent: g-h filter or α-β filter Source: rlabbe Kalman/Bayesian filters in Python Using a gain , the estimate therefore always falls between the measurements (circles) and the predictions (in red). The prediction is dependent on the previous filter output (i.e. last estimate). Here it is modelled to increase by 1 from the previous estimate.
The estimates are not a straight line, but definitely closer in shape to the ground truth than the measurements alone.</description></item><item><title>Effects of varying g</title><link>https://salehahr.github.io/studienarbeit/effects-of-varying-g/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/effects-of-varying-g/</guid><description>Parent: [Calculating the estimated state in the GH-filter](calculating the-estimated-state-in-the-gh-filter.md) Source: rlabbe Kalman/Bayesian filters in Python The greater the g value, the more we follow the measurements rather than rely on our [model-based] predictions.</description></item><item><title>Effects of varying h</title><link>https://salehahr.github.io/studienarbeit/effects-of-varying-h/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/effects-of-varying-h/</guid><description>Parent Improving the gh-filter by using h Source: rlabbe Kalman/Bayesian filters in Python The greater the h value, the more we trust the rate of change that we can derive from the measurement data.
a larger h enables us to react to transient (initial condition dependent) changes more rapidly. Because if we have a large difference between our chosen IC and the measurement, this results in a huge residual velocity</description></item><item><title>Expected value</title><link>https://salehahr.github.io/studienarbeit/expected-value/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/expected-value/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python Example: if we take a thousand sensor readings, the readings won&amp;rsquo;t always be the same (due to the inherent noise).
The expected value &amp;lsquo;averages&amp;rsquo; all of the readings into a single value. This can be a mean (probabilities of all values assumed equal) Or if incorporating individual and different probabilities, the expectation isn&amp;rsquo;t the mean of the range of values Proven: the average of a large number of measurements will be very close to the actual weight</description></item><item><title>g-h filter or α-β filter</title><link>https://salehahr.github.io/studienarbeit/g-h-filter-or-%CE%B1-%CE%B2-filter/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/g-h-filter-or-%CE%B1-%CE%B2-filter/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python A filter that uses two scaling factors:
g or \alpha for the measurement h or \beta for the rate of change of measurement GH filter algorithm [Calculating the estimated state in the GH-filter](calculating the-estimated-state-in-the-gh-filter.md) Improving the gh-filter by using h Several unwanted effects using gh filters Basis for many other filters, e.g.
Kalman filter Least squares filter Benedict-Bordner filter etc.</description></item><item><title>Gain g of the gh-filter</title><link>https://salehahr.github.io/studienarbeit/gain-g-of-the-gh-filter/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/gain-g-of-the-gh-filter/</guid><description>Parent: [Calculating the estimated state based on measurements and predictions](calculating the estimated state-based-on-measurements-and-predictions.md) Backlinks: GH filter algorithm Source: rlabbe Kalman/Bayesian filters in Python Which one do we trust more, the meaasurement z or the prediction x? Applying corresponding weights to both, we obtain the estimate x_est
The prediction is nothing other than a propagated state estimate.
[Me] The prediction is basesd on the model (a priori knowledge) If the model also depends on previous states (which are themselves an output of the filter, i.</description></item><item><title>GH filter algorithm</title><link>https://salehahr.github.io/studienarbeit/gh-filter-algorithm/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/gh-filter-algorithm/</guid><description>Parent: g-h filter or α-β filter Source: rlabbe Kalman/Bayesian filters in Python Initialisation
Initialise the state of the filter Initialise our belief in the state Prediction
Use system model to propagate the state to the next time step Adjust our belief to account for uncertainty in the prediction Update
Get a measurement and an associated belief about its accuracy Calculate residual = measurement - estimated state Using a certain gain , our updated state estimate is somewhere on the residual line</description></item><item><title>Improving the gh-filter by using h</title><link>https://salehahr.github.io/studienarbeit/improving-the-gh-filter-by-using-h/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/improving-the-gh-filter-by-using-h/</guid><description>Parent: g-h filter or α-β filter Source: rlabbe Kalman/Bayesian filters in Python Implementing the g value without h We improve the estimation, previously by only predicting the state, by now predicting the rate of change of state. i.e. Also predict the weight gain per day instead of setting it at a constant value. We use the sensor information for this! Even if it&amp;rsquo;s noisy, there&amp;rsquo;s information in there somewhere, and data is always better than a guess.</description></item><item><title>Principle of never throwing away information</title><link>https://salehahr.github.io/studienarbeit/principle-of-never-throwing-away-information/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/principle-of-never-throwing-away-information/</guid><description>Source: rlabbe Kalman/Bayesian filters in Python &amp;ldquo;Two sensors are better than one, even if one is less accurate than the other.&amp;rdquo;
Example 1 Given:
A: a more accurate sensor B: a less accurate sensor Should we therefore choose A as the estimate and discard B? No, because B can improve our knowledge when combined with A.
Using the measurements from B further narrows the range of estimates (overlap between the error bars).</description></item><item><title>Several unwanted effects using gh filters</title><link>https://salehahr.github.io/studienarbeit/several-unwanted-effects-using-gh-filters/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/several-unwanted-effects-using-gh-filters/</guid><description>Parent: g-h filter or α-β filter Source: rlabbe Kalman/Bayesian filters in Python Effect of bad initial conditions: ringing (sinusoidal over- and undershooting before finally settling onto a trajectory) Effect of very noisy data Effect of acceleration (in data) Filter lags behind because it uses a model that assumes constant velocity in each propagation step. Hence, a filter is only as good as the mathematical model used to describe the phenomenon.</description></item><item><title>Conditional independence</title><link>https://salehahr.github.io/studienarbeit/conditional-independence/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/conditional-independence/</guid><description>Source: http://en.wikipedia.org/wiki/Conditional_independence A and B are conditionally independent given C If and only if, given the knowledge that C occurs, the knowledge of whether A occurs provides no information whatsoever on the likelihood of B occurring, and vice versa
Example Weather and delay
Let the two events be the probabilities of persons A and B getting home in time for dinner
The third event C is the fact that a snow storm hit the city.</description></item><item><title>Egomotion (vs odometry)</title><link>https://salehahr.github.io/studienarbeit/egomotion-vs-odometry/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/egomotion-vs-odometry/</guid><description>See also: Odometry Source: http://en.wiktionary.org/wiki/egomotion The three-dimensional movement of a camera relative to its environment
Source: http://answers.ros.org/question/296686/what-is-the-differences-between-ego-motion-and-odometry/
Generally used interchangeably with odometry Possible difference: Egomotion is more about the estimation of twist (lin, rotational velocities) Odometry is more about the estimation of path Examples
Wheel odometry: path estimation via time-integration of an estimated twist Visual odometry/Scan matching: direct estimation of pose without time-integration</description></item><item><title>Perceptual aliasing</title><link>https://salehahr.github.io/studienarbeit/perceptual-aliasing/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/perceptual-aliasing/</guid><description>Source: http://en.wikipedia.org/wiki/Robotic_mapping Two different places are perceived as the same
Source: http://arxiv.org/abs/1810.11692 Modeling Perceptual Aliasing in SLAM via Discrete-Continuous Graphical Models [Lajoie 2018] (from the abstract)
Phenomenon where different places generate a similar visual footprint Leads to spurious measurements being fed into the SLAM estimator Result: incorrect localisation and map</description></item><item><title>In vivo</title><link>https://salehahr.github.io/studienarbeit/in-vivo/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/in-vivo/</guid><description> Latin for &amp;ldquo;within the living&amp;rdquo; studies in which the effects of various biological entities are tested on whole, living organisms or cells, as opposed to a tissue extract/dead organism</description></item><item><title>Filter-based vs optimisation-based SLAM</title><link>https://salehahr.github.io/studienarbeit/filter-based-vs-optimisation-based-slam/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/filter-based-vs-optimisation-based-slam/</guid><description>Parent: SLAM Index Source: Scaradozzi 2018 SLAM application in surgery Main paradigms of SLAM
Filters — Kalman filters , Particle filters Graph-based SLAM Estimate the entire trajectory and the map from the full set of measurements (full SLAM) Which SLAM algorithm to use? Depends on application
map resolution update time (real time or not) type of environment type of sensors available</description></item><item><title>General Kalman Filter</title><link>https://salehahr.github.io/studienarbeit/general-kalman-filter/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/general-kalman-filter/</guid><description>Backlinks: Main paradigms of SLAM Source: Scaradozzi 2018 SLAM application in surgery KF original algorithm assumes linearity (rarely ever the case)
Variations of the Kalman filter: Extended Kalman Filter (EKF) Unscented Kalman Filter (UKF) Information filtering (IF) — dual to KF Combination of EKF and IF: CF-SLAM, with the goal to be more efficient w.r.t. computational complexity</description></item><item><title>Information Filter</title><link>https://salehahr.github.io/studienarbeit/information-filter/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/information-filter/</guid><description>Parent: General Kalman Filter Source: Scaradozzi 2018 SLAM application in surgery also same assumptions as the EKF main difference: how the Gaussian belief is represented est. cov. — replaced by information matrix (IM) est. state — replaced by information vector (IV) superior to KF in the following ways data is filtered by summing up the IMs and IVs often numerically more stable Dual character of KF and IF</description></item><item><title>Particle filters</title><link>https://salehahr.github.io/studienarbeit/particle-filters/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/particle-filters/</guid><description>Parent: Filter localisation methods Source: Wikipedia Lokalisierung Particle filter / Monte Carlo localisation / sequential Monte Carlo methods
allow solution of all three localisation problems POSE represented by a particle cloud Each particle : possible POSE The filter checks the plausibility of each particle Increases and decreases the probabilities of each particle accordingly When a lower probability threshold is exceeded, the particle is not considered any longer Source: Scaradozzi 2018 SLAM application in surgery Particle filters (sequential Monte Carlo)</description></item><item><title>Sensors (absolute measurements) for measuring absolute POSE</title><link>https://salehahr.github.io/studienarbeit/sensors-absolute-measurements-for-measuring-absolute-pose/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/sensors-absolute-measurements-for-measuring-absolute-pose/</guid><description>Parent: SLAM hardware Source: Wikipedia Lokalisierung GPS (only for outdoors) Innenraumsensorik Lidar, Ultra Wide Band (UWB), Wireless Fidelity, etc [ Wu ] Compared to these, cameras are flexible and low-cost [ Wu ] (are also passive sensors) [ comet ] Radiobaken</description></item><item><title>Unscented Kalman Filter</title><link>https://salehahr.github.io/studienarbeit/unscented-kalman-filter/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/unscented-kalman-filter/</guid><description>Parent: General Kalman Filter Source: Scaradozzi 2018 SLAM application in surgery developed to overcome main problems of the EKF like EKF, approximates the state distribution with a Gaussian Random Variable only the representation is different—using alpha points (a minimal set of sample points) capture posterior mean and covariance accurately for any nonlinearity, up to3rd order Taylor</description></item><item><title>Visual sensors for localisation</title><link>https://salehahr.github.io/studienarbeit/visual-sensors-for-localisation/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/visual-sensors-for-localisation/</guid><description>Parents: [SLAM Index](slam index.md), [sensors (absolute measurements) for measuring distance to landmarks](sensors (absolute measurements)-for-measuring-distance-to-landmarks.md)
Source: Wikipedia Visual odometry Process of determining robot POSE by analysing the associated camera images Use sequential camera image to estimate the distance travelled Applications: robotics, computer vision
Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md) Types
Monocular cameras Stereo cameras RGB-D cameras Provide rich visual information, but for that, higher computational cost</description></item><item><title>Wikipedia Lokalisierung</title><link>https://salehahr.github.io/studienarbeit/wikipedia-lokalisierung/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/wikipedia-lokalisierung/</guid><description>Source: http://de.wikipedia.org/wiki/Lokalisierung_(Robotik Localisation Particle filters Categories of sensors for localisation</description></item><item><title>Collision detection and response</title><link>https://salehahr.github.io/studienarbeit/collision-detection-and-response/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/collision-detection-and-response/</guid><description>Source: SOFA extended documentation Parent: Simulation algorithms in SOFA split into several phases each phase is scheduled by a CollisionPipeline component an object which can potentially collide is associated with a collision geometry returns pairs of colliding bounding volumes (broad phase component) returns pairs of geometric primitives + contact points (narrow phase component) the returned pairs are passed to the contact manager the contact manager creates contact interactions &amp;hellip; (skimmed)</description></item><item><title>Constraint solvers</title><link>https://salehahr.github.io/studienarbeit/constraint-solvers/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/constraint-solvers/</guid><description>Source: SOFA extended documentation Parent: Simulation algorithms in SOFA Backlinks: Scene graph in SOFA Lagrange multipliers to handle complex constraints (which aren&amp;rsquo;t handle-able using projection matrices ) May be combined with explicit or implicit integration phi: bilateral interaction laws (attachments, sliding joints, &amp;hellip;) psi: unilateral interaction laws (contact, friction, &amp;hellip;)
The Lagrange multipliers add force terms to the equation A*dv = b The H matrices are stored in the MechanicalState of each node.</description></item><item><title>Filter localisation methods</title><link>https://salehahr.github.io/studienarbeit/filter-localisation-methods/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/filter-localisation-methods/</guid><description>Parent: SLAM Index Backlinks: [Back-end optimisation](back-end optimisation.md), [what is slam?](what is slam_.md), filter-based vs-optimisation-based-slam Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md) EKF to propagate and update motion states of visual-inertial sensors
Source: Scaradozzi 2018 SLAM application in surgery Filtering techniques in SLAM
Augment/refine the position estimates and map estimates by incorporating new measurements when they become available Generally online, due to their incremental nature Types Kalman filters Particle filters</description></item><item><title>General EKF implementation (non-SLAM)</title><link>https://salehahr.github.io/studienarbeit/general-ekf-implementation-non-slam/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/general-ekf-implementation-non-slam/</guid><description>Parent: Extended Kalman Filter Source: SLAM for Dummies General (non-SLAM) implementation of EKF:
only state estimation robot is given a perfect map no map update necessary SLAM implementations of EKF requires map update and therefore the matrices are changed.
Source: Scaradozzi 2018 SLAM application in surgery EKF vs KF circumvents linearity assumption uses nonlinear functions to describe the next state probability measurement probability approximates the state distribution with a Gaussian Random Variable</description></item><item><title>Laser scanners</title><link>https://salehahr.github.io/studienarbeit/laser-scanners/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/laser-scanners/</guid><description>Source: SLAM for Dummies Backlinks: [Sensors (absolute measurements) for measuring distance to landmarks](sensors (absolute measurements)-for-measuring-distance-to-landmarks.md)
Commonly used [+] Precise, efficient, not much processing work necessary [-] Expensive, bad readings with certain surfaces, bad for underwater applications</description></item><item><title>Linear solvers</title><link>https://salehahr.github.io/studienarbeit/linear-solvers/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/linear-solvers/</guid><description>Source: SOFA extended documentation Parent: Simulation algorithms in SOFA Conjugate gradient J: first-order mapping of a node to its parent path(i): list of mappings from the independent DOFs to the node the force applies to
Computation using a visitor: Top down visitor: propagates the given displacement, clears force vector Bottom up visitor: accumulates forces, maps them up to the independent DOFs Direct solvers
can be used as preconditioners of the conjugate gradient algorithm can be used to solve the equation system A*dv=b implementations are based external libraries</description></item><item><title>Mesh topology</title><link>https://salehahr.github.io/studienarbeit/mesh-topology/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/mesh-topology/</guid><description>Source: SOFA extended documentation Parent: Data structure in SOFA See also: Mesh geometry Mesh topology: how the vertices are connected to each other (using what element?)
Hierarrchy of mesh topology: Topology objects consist of four functional members which creates/modifies/gets topology arrays/geometrical information:
Container Modifier Algorithms Geometry Topological mapping:
Define a new mesh topology from an existing one, using the same DOFs e.g. for subsetting a set of nodes, edges, or to split quads into 2 triangles each these topologies are therefore assigned to the same MechanicalState</description></item><item><title>ODE solvers</title><link>https://salehahr.github.io/studienarbeit/ode-solvers/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/ode-solvers/</guid><description>Source: SOFA extended documentation Parent: Simulation algorithms in SOFA Backlinks: Linear solvers , constraint-solvers implement animation algorithms at each time step integrate and compute positions and velocities one time step ahead uses state vectors (e.g. for position or force), denoted by symbolic identificators called VecId s this allows the solver to be implemented completely independently of the physical model Each statement in the example above is implemented using a visitor</description></item><item><title>Simulation algorithms in SOFA</title><link>https://salehahr.github.io/studienarbeit/simulation-algorithms-in-sofa/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/simulation-algorithms-in-sofa/</guid><description>Source: SOFA extended documentation ODE integration ( ODE solvers ) Linear equation solution ( linear solvers ) Complex constraints ( constraint solvers ) Collision detection and response GPU support</description></item><item><title>SofaPython Index</title><link>https://salehahr.github.io/studienarbeit/sofapython-index/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/sofapython-index/</guid><description>Building/setup Building SOFA on Windows Someone&amp;rsquo;s SP3 setup Running Running SOFA with Python Using python with existing scene Basic python script in Sofa Initialising graph in SP3 Plugins Possible plugins Install ROSConnector in SOFA STLIB (Sofa Template Library) Registration Communication Sending data using sockets Sockets Errno 10054 External data in SOFA Documentation SofaPython API/Documentation links Cheatsheet</description></item><item><title>B1 Modelling of tissue and sensor, navigation of multimodal sensors</title><link>https://salehahr.github.io/studienarbeit/b1-modelling-of-tissue-and-sensor-navigation-of-multimodal-sensors/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/b1-modelling-of-tissue-and-sensor-navigation-of-multimodal-sensors/</guid><description>Source: http://www.isys.uni-stuttgart.de/forschung/medizintechnik/intraoperative-multisensorische-gewebedifferenzierung/ Backlinks: [GRK 2543: Intraoperative Multi-sensor Tissue Differentiation in Oncology](grk 2543_ intraoperative-multi-sensor-tissue-differentiation-in-oncology.md)
cross-domain modelling (tissue and sensor) tissue parameters change depending on status (benign/malignant), obtained or derived from sensor signals sensor signal must be synchronised with the current position of the sensor probe on the tissue</description></item><item><title>Barycentric coordinates</title><link>https://salehahr.github.io/studienarbeit/barycentric-coordinates/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/barycentric-coordinates/</guid><description>Source: SOFA extended documentation Baclinks: [Top-down mapping (master to slave)](top-down mapping (master to slave).md), lamarca-2019-defslam Barycentre: centre of mass A coordinate system, in which the location of point of a simplex (line, triangle, tetrahedron, etc) is specified as the centre of mass of the masses placed at its vertices x_i vertices of a simplex p a point in space The a_i coefficients are the barycentric coordinates of p w.</description></item><item><title>Bottom-up mapping (slave to master)</title><link>https://salehahr.github.io/studienarbeit/bottom-up-mapping-slave-to-master/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/bottom-up-mapping-slave-to-master/</guid><description>Source: SOFA extended documentation Parent: Mappings Mapping of a slave forces to the master forces Newton&amp;rsquo;s law f=Ma applies
Equivalence of power using the kinematic relation using the principle of virtual work</description></item><item><title>Components of the internal model</title><link>https://salehahr.github.io/studienarbeit/components-of-the-internal-model/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/components-of-the-internal-model/</guid><description>Source: SOFA extended documentation Parent: [Internal model as a scene graph in SOFA](internal model as-a-scene-graph-in-sofa.md)
MeshLoader: topology, geometry
MechanicalState TetrahedronSetTopologyContainer
tetrahedral connectivity passed on to other components Forces Mass
DiagonalMass UniformMass: less accurate, but allows faster computation FixedConstraint: P (cancels displacements)
EulerSolver: integration scheme</description></item><item><title>Force classes in SOFA</title><link>https://salehahr.github.io/studienarbeit/force-classes-in-sofa/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/force-classes-in-sofa/</guid><description>Source: SOFA extended documentation Parent: Components of the internal model More than 30 classes available in SOFA
FEM
for deformable volumes/surfaces
volume: tetrahedron/hexahedron surface: shell/membrane TetrahedralCorotationalFEMForceField: forces based on FEM
corotational/hyperelastic formulations
wire/tubular objects
Springs
SpringForceField: forces generated by the surface (alternative: TriangleFEMFroceField) ConstantForceField: external forces</description></item><item><title>Internal model as a scene graph in SOFA</title><link>https://salehahr.github.io/studienarbeit/internal-model-as-a-scene-graph-in-sofa/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/internal-model-as-a-scene-graph-in-sofa/</guid><description>Source: SOFA extended documentation Parent: Internal model Scene graph of the internal model
Consists of components which are connected to a common scenegraph node (root of the internal model)
Each component is responsible for a set of tasks Examples: solver, mass, constraints, &amp;hellip;
Each component can query its parent node to get access to the its sibling components such as MechanicalState , topology
Components are independent of one another — modularity</description></item><item><title>Mathematical model of the internal model in SOFA</title><link>https://salehahr.github.io/studienarbeit/mathematical-model-of-the-internal-model-in-sofa/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/mathematical-model-of-the-internal-model-in-sofa/</guid><description>Source: SOFA extended documentation Parent: Internal model Backlinks: ODE solvers , constraint-solvers Dynamic/quasi-static system of particles (nodes) Independent DOFs: node coordinates, governed by  f: different force functions, e.g. volume, surface and external forces) M: mass matrix P: constraints (projection matrix) each operator corresponds to a simulation component</description></item><item><title>Related types of surgery</title><link>https://salehahr.github.io/studienarbeit/related-types-of-surgery/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/related-types-of-surgery/</guid><description>Source: http://en.wikipedia.org/wiki/Resection_(surgery) Backlinks: [GRK 2543: Intraoperative Multi-sensor Tissue Differentiation in Oncology](grk 2543_ intraoperative-multi-sensor-tissue-differentiation-in-oncology.md), cryosection By procedure
Resection: remove all parts or a key part of an internal organ s. also: resection margin Excision: cut out only a part of an organ/tissue By degree of invasiveness
minimally-invasive surgery (-scopy) laparoscopy</description></item><item><title>Resection margin</title><link>https://salehahr.github.io/studienarbeit/resection-margin/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/resection-margin/</guid><description>Source: http://en.wikipedia.org/wiki/Resection_margin Backlinks: Cryosection , related types-of-surgery Margin on non-cancerous tissue around a tumour that has been removed.
Negative margin: no tumour Microscopic positive: tumour identified microscopically Macroscopic positive: tumour significantly present</description></item><item><title>Scene graph (general)</title><link>https://salehahr.github.io/studienarbeit/scene-graph-general/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/scene-graph-general/</guid><description>Source: http://en.wikipedia.org/wiki/Scene_graph See also: Scene graph in SOFA A general data structure Collection of nodes in a graph/tree</description></item><item><title>Top-down mapping (master to slave)</title><link>https://salehahr.github.io/studienarbeit/top-down-mapping-master-to-slave/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/top-down-mapping-master-to-slave/</guid><description>Source: SOFA extended documentation Parent: Mappings Mapping of a master states to the slave states with the Jacobian (kinematic relation) Linear/nonlinear mappings
In linear mappings, J and J are the same In nonlinear mappings, J is nonlinear w.r.t. x_m, i.e. not a matrix Surfaces
Surfaces embedded in deformable cells: J contains barycentric coordinates Surfaces attached to rigid bodies: each row of J encodes for each vertex</description></item><item><title>Visual model</title><link>https://salehahr.github.io/studienarbeit/visual-model/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/visual-model/</guid><description>Source: SOFA extended documentation Parent: Models in SOFA More detailed geometry than that of the internal model , hence uses different meshes Mappings are used to update the visual model with the deformations taking place Contains rendering parameters Libraries for rendering graphics
OGRE (external) Open Scene Graph (external) SOFA&amp;rsquo;s own library based on openGL</description></item><item><title>Cancer biopsy</title><link>https://salehahr.github.io/studienarbeit/cancer-biopsy/</link><pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/cancer-biopsy/</guid><description>Source: http://en.wikipedia.org/wiki/Biopsy Backlinks: [GRK 2543: Intraoperative Multi-sensor Tissue Differentiation in Oncology](grk 2543_ intraoperative-multi-sensor-tissue-differentiation-in-oncology.md)
Biopsy: type of medical test in which a cell/tissue sample is extracted in order to detect disease Types of biopsy: excisional biopsy: removal of entire suspicious area to be diagnosed incisional biopsy: removal of only samples of the abnormal tissue needle aspiration biopsy: removal of cells via needle Diagnosing / Pathological examination to determine whether the abnormality is benign or malignant (classification of the cancer) to determine how far it has spread negative margins: no disease found at the edge of specimen positive margins: disease was found at edge, further excision may be in order In bladders: usually done using a cystocopy</description></item><item><title>State-of-the-art SLAM</title><link>https://salehahr.github.io/studienarbeit/state-of-the-art-slam/</link><pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/state-of-the-art-slam/</guid><description>Parent: SLAM Index Backlinks: Qin 2019 General Optimization-based Framework (Multisensor) Things that I&amp;rsquo;ve seen mentioned several times so far
ORBSLAM: monocular MonoSLAM: monocular (old?) — Andrew Davison OKVIS: visual inertial, stereovision PTAM: parallel tracking and mapping MSCKF: real-time EKF VINS-mono: visual inertial, monocular http://en.wikipedia.org/wiki/List_of_SLAM_Methods</description></item><item><title>Works of possible interest</title><link>https://salehahr.github.io/studienarbeit/works-of-possible-interest/</link><pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/works-of-possible-interest/</guid><description>General SLAM
cadena 2016 past, present, and future of slam durrant-whyte 2006 slam tutorial part i Prerequisites
g2o paper - graph-based SLAM Existing SLAM algorithms
MonoSLAM, works by Andrew Davison focusing on fusion instead of vision-only SLAM Maplab (filtering-based) not looking at filtering-based algos mentioned in the Chen 2018 Review of VI SLAM paper ORB-SLAM paper — ORB features VIORB implementation ORB-SLAM3 (improves on ORBSLAM, incl.</description></item><item><title>(Scaradozzi 2018) SLAM application in surgery</title><link>https://salehahr.github.io/studienarbeit/scaradozzi-2018-slam-application-in-surgery/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/scaradozzi-2018-slam-application-in-surgery/</guid><description>Abstract:
SLAM&amp;rsquo;s potential in image-guided surgery assuming static environment Review of main techniques in general robotics SLAM Insight into visual SLAM SLAM in surgery Chapters What is SLAM? Filter-based vs optimisation-based SLAM General Kalman Filter General EKF Unscented Kalman Filter Information Filter &amp;hellip;.
Takeaway
EKF is popular in surgery SLAM techniques Deformable environment encumbers precise registration and data fusion</description></item><item><title>Distance between landmarks</title><link>https://salehahr.github.io/studienarbeit/distance-between-landmarks/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/distance-between-landmarks/</guid><description>Source: SLAM for Dummies Backlinks: Nearest Neighbour Methods:
Euclidean distance (suitable for far distances) Mahalanobis distance (better, but more complex)</description></item><item><title>EKF System State</title><link>https://salehahr.github.io/studienarbeit/ekf-system-state/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/ekf-system-state/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices , step-2:-re-observation Contains robot POSE and landmark position POSE: (x y theta)_r LM: (x, y)_l1 &amp;hellip; (x,y)_ln; n = num. of landmarks Size: 3+2n rows</description></item><item><title>Kalman gain for EKF</title><link>https://salehahr.github.io/studienarbeit/kalman-gain-for-ekf/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/kalman-gain-for-ekf/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices How much we will trust the observed landmarks
compromise between odometry and landmark correction uses uncertainty of observed landmarks measure of quality of the range measurement device odometry performance Gains for range and brearing (3+2n x 2)</description></item><item><title>Lamarca 2020 DefSLAM</title><link>https://salehahr.github.io/studienarbeit/lamarca-2020-defslam/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/lamarca-2020-defslam/</guid><description>URL: http://arxiv.org/abs/1908.08918 Authors: Lamarca et al Code: http://github.com/UZ-SLAMLab/DefSLAM Results (video): http://www.youtube.com/watch?v=6mmhD2_t6Gs Summary
First monocular SLAM for deformable environments in real-time Most other SLAM implementations assume rigidity Main techniques used (techniques for monocular non-rigid scenes): isometric shape from template (SfT) non-rigid structure from motion (NRSfM) Main principle: computation in two parallel threads (s. DefSLAM framework) Deformation tracking [front end] Deformation mapping [back end] The map from the mapping thread defines the shape-at-rest template used by deformation tracking Validation: compare with ORBSLAM (rigid) Assumes isometric deformation Future work: relocalisation (s.</description></item><item><title>Landmark extraction</title><link>https://salehahr.github.io/studienarbeit/landmark-extraction/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/landmark-extraction/</guid><description>Source: SLAM for Dummies Backlinks: Basic EKF for SLAM , nearest-neighbour Basic landmark extraction using a laser scanner
Spike algorithm RANSAC ( ekf handles points) Expansion of RANSAC so that EKF handles lines Scan-matching: two successive laser scans are matched Spike and RANSAC are good for indoor environments</description></item><item><title>Nearest Neighbour</title><link>https://salehahr.github.io/studienarbeit/nearest-neighbour/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/nearest-neighbour/</guid><description>Source: SLAM for Dummies Backlinks: Data association Nearest neighbour approach
Get a new laser scan &amp;ndash;&amp;gt; ( landmark extraction ) extract all visible landmarks Associate each extracted LM to the closest LM we have seen more than N times Pass each pairs of association (extracted LM, LM in database) through a validation gate If pair passes &amp;ndash;&amp;gt; n = n + 1 (num. times seen) If pair fails &amp;ndash;&amp;gt; add new LM to database, with n := 1</description></item><item><title>SLAM Index</title><link>https://salehahr.github.io/studienarbeit/slam-index/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/slam-index/</guid><description>Definition Localisation What is SLAM? Main paradigms of SLAM Sensors for SLAM Position acquisition (relative vs. absolute) SLAM hardware Relative Odometry IMU Absolute [Sensors (absolute measurements) for measuring distance to landmarks](sensors (absolute measurements)-for-measuring-distance-to-landmarks.md) Visual sensors for localisation Monocular depth perception Pinhole camera model Camera calibration World to camera trafo Fusion Multisensor fusion Loose vs Tight coupling Why use the visual-inertial sensor combination? Visual SLAM Classification of image-based camera localization approaches Visual SLAM Implementation Framework Feature-based vs direct SLAM workflow Sparse Sparse/Feature-based VSLAM Dense Dense/direct VSLAM Landmarks/Feature extraction landmarks Bag of words Data association Data association Loop closure Loop closing in VIORB Loop closure detection Filter-based SLAM Filter localisation methods [Extended Kalman Filter](extended kalman filter.</description></item><item><title>Validation gate</title><link>https://salehahr.github.io/studienarbeit/validation-gate/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/validation-gate/</guid><description>Source: SLAM for Dummies Backlinks: Nearest Neighbour An observed landmark is associated to a landmark if the following holds v innovation S innovation covariance The Validation gate makes use of the fact that the EKF implementation gives a bound on the uncertainty of an observation of a LM
Is an observed LM a LM in the database?</description></item><item><title>Literature management</title><link>https://salehahr.github.io/studienarbeit/literature-management/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/literature-management/</guid><description> Only focus on one paper per topic at a time Skim through and take notes on only the important chapters Link and backlink Note which topics were skimmed Come back later for further literature review</description></item><item><title>Programmatic implementations of MonoSLAM</title><link>https://salehahr.github.io/studienarbeit/programmatic-implementations-of-monoslam/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/programmatic-implementations-of-monoslam/</guid><description>Parent: SLAM resources Python http://github.com/agnivsen/Py-M-SLAM http://github.com/agnivsen/LibMonoSLAM
MATLAB http://perso.ensta-paris.fr/~filliat/Courses/2011_projets_C10-2/BRUNEAU_DUBRAY_MURGUET/monoSLAM_bruneau_dubray_murguet_en.html</description></item><item><title>Template for a bibliography entry</title><link>https://salehahr.github.io/studienarbeit/template-for-a-bibliography-entry/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/template-for-a-bibliography-entry/</guid><description>Source Backlinks
Authors Abstract Contents/Chapters Takeaway</description></item><item><title>Back-end optimisation</title><link>https://salehahr.github.io/studienarbeit/back-end-optimisation/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/back-end-optimisation/</guid><description>Parent: Visual SLAM Implementation Framework Source: cometlabs (Camera pose optimisation)
To compensate for drift of pose estimation Traditionally using EKF ( filter-based ) simple implementation therefore good for small scale estimations Alternative: bundle adjustment (graph optimisation) joint optimisation of the camera pose and the 3D structure parameters combines numerical methods and graph theory increasingly favoured over filtering, due to the latter&amp;rsquo;s inherent inconsistency more efficient when combined with sub-mapping</description></item><item><title>Feature-based vs direct SLAM workflow</title><link>https://salehahr.github.io/studienarbeit/feature-based-vs-direct-slam-workflow/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/feature-based-vs-direct-slam-workflow/</guid><description>Parent: SLAM Index Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Feature-based (aka sparse ) direct (aka dense ) Extraction of features required No abstraction necessary Aims to minimise error between point location estimate (from odometry) and location based on camera Tracks objects by minimising photometric error (intensity differences)</description></item><item><title>Key frames in loop closure detection</title><link>https://salehahr.github.io/studienarbeit/key-frames-in-loop-closure-detection/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/key-frames-in-loop-closure-detection/</guid><description>Source: cometlabs Backlinks: Loop closure detection Most common method to get candidate key frames: use a place recognition approach
approach based on vocab tree feature descriptors of candidate key frames are quantised one colour in image below corresponds to one feature descriptor/&amp;lsquo;vocabulary&amp;rsquo; each point is a &amp;lsquo;word&amp;rsquo; that belongs to a vocabulary the words can then be counted and put into a frequency histogram the histogram is used to compare similarity of images I think similar images then get filtered out, so we get key frames</description></item><item><title>Loop closure detection</title><link>https://salehahr.github.io/studienarbeit/loop-closure-detection/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/loop-closure-detection/</guid><description>Parent: VSLAM Framework , slam-index Source: cometlabs Backlinks: Step 2: Re-observation Process of observing the same scene by non-adjacent frames and adding a constraint (relationship? association?) between them A long-term data association in the VSLAM Framework (part of front end) Sort of incorporates topological SLAM into metric SLAM Importance
Final refinement step (in data association) Important for obtaining a globally consistent SLAM solution, especially when optimising over a long period of time Basic loop closure detection Match the current frame to all previous frames using feature matching</description></item><item><title>Precision recall curve</title><link>https://salehahr.github.io/studienarbeit/precision-recall-curve/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/precision-recall-curve/</guid><description>Parent: Loop closure detection Source: cometlabs used to better quantify the performance (balance between false positives and false negatives in loop closure detection ) highlights tradeoff between precision and recall precision (absence of false positives) but may lead to the appearance of false negatives recall (prediction power) e.g. tweaking to improve recall increases sensitivity to similarities in the image thus increases possibility of false positives</description></item><item><title>(Wu 2018) Image-based camera localization</title><link>https://salehahr.github.io/studienarbeit/wu-2018-image-based-camera-localization-an-overview/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/wu-2018-image-based-camera-localization-an-overview/</guid><description>Authors: Wu, Tang, Li
Abstract/Contents
overview (classification) of image-based camera localization classification of image-based camera localization approaches techniques, trends only considers 2D cameras focuses on points as features in images (not lines etc) Chapters Classification of image-based camera localization approaches [Multisensor fusion](multisensor fusion.md) — why use the visual-inertial sensor combination? Loose vs Tight coupling Filter localisation methods Some optimisation-based tightly-coupled multisensor SLAM algorithms Questions
What&amp;rsquo;s a metric map &amp;ndash; normal map (with landmarks, normal distances) as opposed to a topological one Takeaway</description></item><item><title>Classification of image-based camera localization approaches</title><link>https://salehahr.github.io/studienarbeit/classification-of-image-based-camera-localization-approaches/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/classification-of-image-based-camera-localization-approaches/</guid><description>Parent: SLAM Index Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)
Unknown environment (must be reconstructed from image data) Online/real-time mapping (SLAM)
geometric metric SLAM (accurate computations, therefore still widely used in practice) monocular multiocular multi-kind sensor (active) loosely-coupled closely-coupled learning SLAM (active) needs a prior dataset to train NN &amp;ndash; dataset determines performanace of the SLAM low generalisation capability, therefore not as flexible as geom.</description></item><item><title>Cometlabs What You Need to Know About SLAM</title><link>https://salehahr.github.io/studienarbeit/cometlabs-what-you-need-to-know-about-slam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/cometlabs-what-you-need-to-know-about-slam/</guid><description>Source: http://blog.cometlabs.io/teaching-robots-presence-what-you-need-to-know-about-slam-9bf0ca037553
SLAM chicken and egg problem Position acquisition Multisensor fusion [Sensors (absolute measurements) for measuring distance to landmarks](sensors (absolute measurements)-for-measuring-distance-to-landmarks.md) Mapping representations in robotics Visual SLAM Implementation Framework Feature-based vs direct SLAM workflow Sparse/Feature-based VSLAM Dense/direct VSLAM</description></item><item><title>Dense/direct VSLAM</title><link>https://salehahr.github.io/studienarbeit/dense-direct-vslam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/dense-direct-vslam/</guid><description>Parent: Visual SLAM Implementation Framework , slam-index See also: Feature-based vs direct SLAM workflow Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Front-end part of the Visual SLAM Implementation Framework Use most or all of the pixels in each received frame Provide more information about the environment Many more pixels, require GPUs Feature-based vs direct SLAM workflow Disadvantages: Don&amp;rsquo;t handle outliers very well (outliers will be processed and implemented into the final map) Slower than feature-based variants Aims to minimise photometric error (intensity differences) Semi-dense</description></item><item><title>Feature maps</title><link>https://salehahr.github.io/studienarbeit/feature-maps/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/feature-maps/</guid><description>Parents: [Mapping representations in robotics](mapping representations-in-robotics.md), sparse/feature-based-vslam Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Uses a limited number of sparse objects to represent a map e.g. points, lines
Low computation cost because of the sparsity Map management solutions are good solutions for current applications What&amp;rsquo;s map management (probably storing maps in databases and recognising an existing map) [-] Sensitivity to false data association</description></item><item><title>Geometric metric SLAM</title><link>https://salehahr.github.io/studienarbeit/geometric-metric-slam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/geometric-metric-slam/</guid><description>Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md) Parent: Classification of image-based camera localization approaches Computes 3D maps with accurate mathematical equations
Classification according to sensors
monocular multiocular (most studies focus on binocular vision) multisensor fusion (e.g. vision and IMU &amp;ndash; vision and IMU fusion gaining in popularity) Classification according to techniques used
Filter-based SLAM Keyframe -based SLAM (active) Feature-based (keyframe-based feature SLAM) / sparse Direct / dense Grid-based SLAM (mainly deals with laser data, deals only a bit with image data) According to [76] keyframe-based can provide more accurate results compared to filter-based</description></item><item><title>Kidnapped robot problem</title><link>https://salehahr.github.io/studienarbeit/kidnapped-robot-problem/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/kidnapped-robot-problem/</guid><description>Source: Wikipedia Lokalisierung Backlinks: Lamarca 2020 DefSLAM Position initially known Then robot is repositioned without knowing it Robot has to be able to realise that the initial successful localisation isn&amp;rsquo;t valid any more &amp;ndash; a new global localisation must be carried out Realise this via unplausible sensor measurements (huge contradiction to prev. measurements) Has to do with the measure of robustness of the localisation method carried out</description></item><item><title>Loose vs Tight coupling</title><link>https://salehahr.github.io/studienarbeit/loose-vs-tight-coupling/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/loose-vs-tight-coupling/</guid><description>Parent: SLAM Index Backlinks: Multisensor fusion Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)
In loosely-coupled systems: all sensor states are independently estimated and optimized
easier to process frame and IMU data less accurate/robust compared to tight coupling e.g. Integrated IMU data can be incorporated as independent measurements in stereo vision optimization e.g. Vision-only pose estimates are used to update an EKF so that IMU propagation can be performed In tightly-coupled systems: all sensor states are jointly estimated and optimized</description></item><item><title>Mapping representations in robotics</title><link>https://salehahr.github.io/studienarbeit/mapping-representations-in-robotics/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/mapping-representations-in-robotics/</guid><description>Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Feature maps Occupancy grids Grids containing occupancy probability information Useful for path planning, exploration Drawback: computational complexity</description></item><item><title>Monocular cameras</title><link>https://salehahr.github.io/studienarbeit/monocular-cameras/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/monocular-cameras/</guid><description>Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md) Backlinks: Visual sensors for localisation + Simpler hardware implementation + Smaller and cheapter systems - need complexer algos and software because of lack of direct depth information from a 2D image How is the shape of the map generated?
Integrating measurements in the chain of frames over time Use triangulation method As well as camera motion, if camera isn&amp;rsquo;t stationary Depths of points are not observed directly (s.</description></item><item><title>Multisensor fusion</title><link>https://salehahr.github.io/studienarbeit/multisensor-fusion/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/multisensor-fusion/</guid><description>Parent: SLAM Index , geometric-metric-slam Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Avoid limitations of using only one sensor Relative measurements: provide precise positioning information constantly At certain times absolute measurements are made to correct potential errors (correct drift) several approaches (for localisation), e.g. merge sensor feeds at the lowest level before being processed homogeneously hierarchical approaches (fuse state estimates derived independently from multiple sensors) s.</description></item><item><title>Position acquisition (relative vs. absolute)</title><link>https://salehahr.github.io/studienarbeit/position-acquisition-relative-vs.-absolute/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/position-acquisition-relative-vs.-absolute/</guid><description>Parent: SLAM Index See also: SLAM hardware Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md) relative (interoceptive sensors) odometry absolute (exteroceptive sensors) can be used alongside relative measurement sensors in order to correct odometry drift s. [major sensor types in SLAM (absolute measurements)](major sensor types in-slam-(absolute-measurements).md) incl. visual-sensors Beacons direct measurement instead of integrating, therefore error in position does not grow unbounded e.</description></item><item><title>RGB-D cameras</title><link>https://salehahr.github.io/studienarbeit/rgb-d-cameras/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/rgb-d-cameras/</guid><description>Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md) Backlinks: [Visual sensors for localisation](visual sensors-for-localisation.md), stereo-cameras Provide depth information directly Employed by most of the SLAM systems Generate 3D images through structured light or time of flight technology Structured light camera projects a known pattern onto objects Perceives deformation of pattern by an infrared camera This lets depth and surface information of the objects be calculated Time of flight ToF of a light signal between camera and objects is measured &amp;ndash;&amp;gt; from this, depth is obtained Structured light sensors are sensitive to illumination &amp;ndash; not applicable in direct sunlight Limitations of RGB-D cameras</description></item><item><title>SA TODO</title><link>https://salehahr.github.io/studienarbeit/sa-todo/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/sa-todo/</guid><description>Parent: Scope of Studienarbeit See also: [HiWi to-do](hiwi to-do.md), defslam +-orbslam3-integration Studienarbeit Camera-based localisation
Find a classification of approaches/techniques Briefly describe each See if it applies to the project Look into the most promising approach &amp;ndash; how to implement (DefSLAM) DefSLAM Install DefSLAM library Skim through an existing VI-SLAM (rigid) implementation to see how sensor fusion is done (as an overview for the coming sensor fusion task) VINS-Mono, VIORB paper Prepare dummy data for testing VI-SLAM (eventually VI-DefSLAM) — interpolate data between frames and add noise/bias Go through code Get the executables working VideoCapture OpenCV problem &amp;ndash; reinstall with all FFMMPEG options Figure out g2o Go through the rest of DefSLAM Go through the rest of ORBSLAM3 IMU term in cost function IMU preintegration IMU initialisation implement imu term in optimisation (either using ekf (s.</description></item><item><title>Sensors (absolute measurements) for measuring distance to landmarks</title><link>https://salehahr.github.io/studienarbeit/sensors-absolute-measurements-for-measuring-distance-to-landmarks/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/sensors-absolute-measurements-for-measuring-distance-to-landmarks/</guid><description>Parents: SLAM Index , slam-hardware Backlinks: Position acquisition Source:: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Acoustic (Time of Flight) ToF technique Surfaces need to have good acoustic reflection Lack the ability to use surface properties for localisation examples Sonar Ultrasonic, ultrasound Laser rangerfinders ToF and phase-shift techniques Lack the ability to use surface properties for localisation e.g. Lidar Visual sensors for localisation Source: SLAM for Dummies Laser scanners Sonar, usually polaroid sonar [+] cheaper, good for underwater (s.</description></item><item><title>Some optimisation-based tightly-coupled multisensor SLAM algorithms</title><link>https://salehahr.github.io/studienarbeit/some-optimisation-based-tightly-coupled-multisensor-slam-algorithms/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/some-optimisation-based-tightly-coupled-multisensor-slam-algorithms/</guid><description>Parent: SLAM Index Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)
Uses nonlinear optimization may potentially achieve higher accuracy due to the capability to limit linearization errors through repeated linearization of the inherently nonlinear problem
[117] Forster: preintegration theory [118] OKVIS: a novel approach to tightly integrate visual measurements with IMU optimise a joint nonlinear cost function that integrates an IMU error term with the landmark reprojection error in a fully probabilistic manner real-time operation: old states are marginalized to maintain a bounded-sized optimization window Li et al.</description></item><item><title>Sparse/Feature-based VSLAM</title><link>https://salehahr.github.io/studienarbeit/sparse-feature-based-vslam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/sparse-feature-based-vslam/</guid><description>Parent: Visual SLAM Implementation Framework , slam-index See also: Feature-based vs direct SLAM workflow Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Front-end part of the Visual SLAM Implementation Framework Use only a small selected subset of the pixels in an image frame Feature maps generated are point clouds &amp;ndash;&amp;gt; used to track the camera pose Requires feature extraction and matching To minimise: reprojection error (difference between a point&amp;rsquo;s tracked location and where it is expected to be given camera pose estimate) Pose estimation based on RANSAC A frame with most of its features concentrated in a small area: bad as the features are more likely to overlap Sparse</description></item><item><title>Stereo cameras</title><link>https://salehahr.github.io/studienarbeit/stereo-cameras/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/stereo-cameras/</guid><description>Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md) Backlinks: Visual sensors for localisation two cameras separated by a fixed distance (baseline) observations of the position of the same 3D point in both cameras allows depth to be calculated through triangulation (like humans do) depth measurement limited by baseline and resolution generally, wider baseline &amp;ndash;&amp;gt; better depth estimate (but occupies more physical space) s.</description></item><item><title>Topological SLAM</title><link>https://salehahr.github.io/studienarbeit/topological-slam/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/topological-slam/</guid><description>Parent: Classification of image-based camera localization approaches Source: [Wu 2018 Image-based camera localization: an overview](wu 2018-image-based-camera-localization_-an-overview.md)
does not need accurate computation of 3D maps represents the environment by connectivity or topology e.g. Kuipers [130] used a hierarchical description of the spatial environment
a topological network description mediates between a control and metrical level distinctive places and paths are defined by their properties at the control level serve as nodes and arcs of the topological model Decreasing in popularity</description></item><item><title>Visual SLAM Implementation Framework</title><link>https://salehahr.github.io/studienarbeit/visual-slam-implementation-framework/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/visual-slam-implementation-framework/</guid><description>Parent: SLAM Index Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Basic principle:
tracking a set of points through successive frames these tracks are used to triangulate the 3D positions of the points to create the map at the same time, using the the est point locations to calculate the pose of the camera, which could have observed them (i.e. calculate real time 3D structure of a scene from the estimated motion of the camera) Two main architecture components:</description></item><item><title>What is SLAM?</title><link>https://salehahr.github.io/studienarbeit/what-is-slam-/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/what-is-slam-/</guid><description>Parent: SLAM Index Source: Scaradozzi 2018 Process which allows a mobile robot to
construct a map of its environment (assumed to be unknown) compute its location using the map simultaneously Source: Lamarca 2019 DefSLAM Goal is to locate a sensor in an unknown map/environment, which is simultaneously being reconstructed. Typically used in exploratory trajectories (new or changing environments) Source: Wikipedia SLAM Simultaneous localization and mapping (SLAM)</description></item><item><title>50.2.1 Process noise Q and W (odometry)</title><link>https://salehahr.github.io/studienarbeit/50.2.1-process-noise-q-and-w-odometry/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.1-process-noise-q-and-w-odometry/</guid><description>Parent: Factors affecting Kalman filter performance Source: Tereshkov 2015 http://www.researchgate.net/publication/271532640_A_Simple_Observer_for_Gyro_and_Accelerometer_Biases_in_Land_Navigation_Systems Process noise covariance matrix has no clear physical meaning, cannot be deduced from sensor characteristics Leads to non-intuitive, iterative procedures to tune KFs Which means that KF optimality is rarely achieved in practice Alternative to KF tuning: the use of geometric observers
estimates are expresssed only in terms of quantities with clear geometrical meaning Source: [Schneider 2013 How to not make the EKF fail](schneider 2013 how to-not-make-the-ekf-fail.</description></item><item><title>50.2.2 Measurement noise R, V (landmark)</title><link>https://salehahr.github.io/studienarbeit/50.2.2-measurement-noise-r-v-landmark/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/50.2.2-measurement-noise-r-v-landmark/</guid><description>Parent: Multivariate Kalman filter algorithm Source: rlabbe Kalman/Bayesian filters in Python R models the noise in the sensors as a covariance matrix dim(R) = m x m (m: number of sensors) Possible complications in multisensor systems, the correlation between the sensors might not be clear sensor noise might not be pure Gaussian Source: http://www.linkedin.com/pulse/tuning-extended-kalman-filter-process-noise-training-alex-thompson Ways to obtain R
Using the variances given in the sensor specifications Comopare the measurements against a strong ground truth and derive the variance variable by variable Record the steady state measurements over a long period of time and measure the variance (look at the histogram) Source: [Schneider 2013 How to not make the EKF fail](schneider 2013 how to-not-make-the-ekf-fail.</description></item><item><title>Covariance matrix P</title><link>https://salehahr.github.io/studienarbeit/covariance-matrix-p/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/covariance-matrix-p/</guid><description>Source: SLAM for Dummies s. also EKF matrices Covariance matrix P
Covariance: measure of correlation of two variables Correlation: measure of degree of linear dependence A covariance of the robote POSEupdated in step 1 (odometry update) 3x3 B .. C covariance on the first .. nth landmarkStep 3: New landmarks 2x2 D covariance between POSE and first LMupdated in step 1 (odometry update) 2x3 E, etc E = D^T, etcupdated in step 1 (odometry update) 3x2 F=G^T Step 3: New landmarks Initially P = A (robot has not seen any LMs)</description></item><item><title>Measurement model</title><link>https://salehahr.github.io/studienarbeit/measurement-model/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/measurement-model/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices/vectors Estimate of the range and bearing (from landmark) in Step 2: Re-observation x, y, theta - current position estimate lambdax, y - landmark position
Jacobian H w.r.t. x, y, theta (here for regular EKF, not for extended) In SLAM we need additional values for the landmarks here for landmark number two in extended EKF Upper row is for information, not part of matrix First three columns are regular H Landmarks don&amp;rsquo;t have any rotation</description></item><item><title>Prediction model</title><link>https://salehahr.github.io/studienarbeit/prediction-model/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/prediction-model/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices/vectors Used in the prediction step How to compute an expected position of the robot given the old position and the control input (so basically based on odometry ) Control terms are deltax, deltay, deltatheta) deltat - change in thrust q - error term
Jacobian (assuming linearised version) Not extended for landmarks because only used for robot position prediction</description></item><item><title>SLAM-specific jacobians</title><link>https://salehahr.github.io/studienarbeit/slam-specific-jacobians/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/slam-specific-jacobians/</guid><description>Source: SLAM for Dummies Backlinks: EKF matrices/vectors Jxr
Jacobian of the prediction of landmarks, which does not include prediction of theta, w.r.t. robot POSE same as J_prediction model, except without rotation term Jz Jacobian of prediction of landmarks, but w.r.t. [range, bearing]</description></item><item><title>Step 1 Odometry update (Prediction step)</title><link>https://salehahr.github.io/studienarbeit/step-1-odometry-update-prediction-step/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/step-1-odometry-update-prediction-step/</guid><description>Source: SLAM for Dummies Backlinks: Basic EKF for SLAM First step in the three-step EKF
Update current state using odometry data Based on the controls given to the robot Calculate estimate of new POSE Update equation: prediction model (x = x + deltax) Or in a simple model, neglect the error term q
State vector gets updated via the prediction model
Jacobian of the prediction model also needs to be updated every iteration (with the controls deltax, &amp;hellip;)</description></item><item><title>Step 2 Re-observation</title><link>https://salehahr.github.io/studienarbeit/step-2-re-observation/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/step-2-re-observation/</guid><description>Source: SLAM for Dummies Backlinks: Basic EKF for SLAM Second step in the three-step EKF — overview
In this step we update the robot position that we got in step 1 Compensate for errors due to odometry pos_est (odometry-based) - pos_actual (LM-based) = Innovation, (based on the LM that the robot can see)
Use this to update robot position
Update the uncertainty of each observed LM to reflect recent changes e.</description></item><item><title>Step 3 New landmarks</title><link>https://salehahr.github.io/studienarbeit/step-3-new-landmarks/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/step-3-new-landmarks/</guid><description>Source: SLAM for Dummies Backlinks: Basic EKF for SLAM Overview
Landmarks that are new are not dealt with until step 3. Delaying the incorporation of new landmarks until the will decrease the computation cost needed for this step the covariance matrix, P, and the system state, X, are smaller by then. Update state vector x and covariance matrix P with new landmarks Add new landmark to state vector X Add new row and column to covariance matrix Covariance for new landmark Robot-landmark covariance</description></item><item><title>Basic EKF for SLAM</title><link>https://salehahr.github.io/studienarbeit/basic-ekf-for-slam/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/basic-ekf-for-slam/</guid><description>Parent: Extended Kalman Filter , slam-index Backlinks: RANSAC See also: What is SLAM? Source: SLAM for Dummies A basic EKF implementation of SLAM consists of multiple parts:
Landmark extraction Data association After odometry change (due to robot moving), state estimation from odometry Update of the estimated state using re-observed landmark data Update landmark database with new landmarks Note: at any point in the three steps on the left, the EKF will have an estimate of the robots current position</description></item><item><title>Data association</title><link>https://salehahr.github.io/studienarbeit/data-association/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/data-association/</guid><description>Parents: SLAM Index , basic-ekf-for-slam Source: SLAM for Dummies Backlinks: [What is SLAM?](what is slam_.md), [data association in defslam](data association in defslam.md), visual slam-implementation-framework Matching observed landmarks from different scans (different time steps) with each other. Also called &amp;rsquo;re-observing' landmarks.
Problems that can arise:
The landmark(s) might not be observed every time step (bad landmark) Something might be observed as a landmark, but it never appears again (bad landmark) Wrong association of a landmark to a previously seen landmark Goal: define a suitable data-association policy to minimise the first two problems Given: database that stores previously seen landmarks (initially empty) As a rule: a landmark is only considered worthwhile to be used in SLAM once it is seen N times</description></item><item><title>EKF matrices/vectors</title><link>https://salehahr.github.io/studienarbeit/ekf-matrices-vectors/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/ekf-matrices-vectors/</guid><description>Source: SLAM for Dummies Backlinks: Extended Kalman Filter System state X Estimate of POSE Jacobian of prediction model Landmark range and bearing Jacobian of measurement model Covariance matrix P Kalman gain K SLAM-specific jacobians</description></item><item><title>Extended Kalman Filter</title><link>https://salehahr.github.io/studienarbeit/extended-kalman-filter/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/extended-kalman-filter/</guid><description>Parent: SLAM Index Backlinks: RANSAC , [nearest neighbour](nearest neighbour.md), filter-localisation-methods Source: SLAM for Dummies keeps track of an estimate of the position uncertainty keeps track of the uncertainty in the features/landmarks seen General EKF implementation (non-SLAM) Basic EKF for SLAM Diagram: Triangle Robot Stars Landmarks Dashed triangle Robot&amp;rsquo;s position based on odometry alone (where it thinks it is) Dotted triangle Robot&amp;rsquo;s position estimate based on EKF Solid line triangle Robot&amp;rsquo;s actual position in real life!</description></item><item><title>Riisgaard SLAM for dummies</title><link>https://salehahr.github.io/studienarbeit/riisgaard-slam-for-dummies/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/riisgaard-slam-for-dummies/</guid><description>Authors: Søren Riisgaard and Morten Rufus Blas Parent: SLAM resources Abstract:
Tutorial introduction to SLAM, with minimal prerequisites for the understanding of SLAM as explained here Mostly explains a single approach to the steps involved in SLAM Complete solution for SLAM using EKF (extended Kalman filter) Only considers 2D motion, not 3D Chapters
What is SLAM? Overview of SLAM using EKF Hardware Robot Range measurement device SLAM process Step 1: Odometry update Step 2: Reobservation Step 3: Add new landmarks Laser data Odometry data Landmarks Landmark extraction 1.</description></item><item><title>SLAM hardware</title><link>https://salehahr.github.io/studienarbeit/slam-hardware/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/slam-hardware/</guid><description>Parent: SLAM Index See also: Position acquisition (relative vs. absolute) Source: SLAM for Dummies Robot parameters to consider Ease of use Odometry performance: how well the robot can estimate its own position, just from the rotation of the wheels Max errors: 2cm per meter moved, 2deg per 45deg turned Bad odometry &amp;ndash;&amp;gt; bad estimation of current position &amp;ndash;&amp;gt; hard to implement SLAM Range measurement device options Source: Wikipedia Lokalisierung Categories of sensors for localisation</description></item><item><title>SLAM resources</title><link>https://salehahr.github.io/studienarbeit/slam-resources/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/slam-resources/</guid><description>Parent: SLAM Index Theory
Wikipedia SLAM http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation Thrun - Probabilistic Robotics SLAM for dummies Andrew Davison research page at the Department of Computing , Imperial College London about SLAM using vision. Paper 2002 on monocular SLAM SLAM lectures on YouTube http://openslam-org.github.io / Tutorials SLAM summer school SS06: http://www.robots.ox.ac.uk/~SSS06/Website/
Programming Programmatic implementations of MonoSLAM</description></item><item><title>Spike landmarks</title><link>https://salehahr.github.io/studienarbeit/spike-landmarks/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/spike-landmarks/</guid><description>Source: SLAM for Dummies Backlinks: Landmark extraction Uses extrema to find landmarks Find values in the range of a laser scan, where two values differ by more than a certain amount (e.g. 0.5 m) This finds big changes in the laser scan Alternatively, using three values next to each other: A, B, C (A - B) + (C - B) yields a value Better for finding spikes as it finds actual spikes Rely on the landscape changing a lot between two laser beams Algo will fail in smooth environments Suitable for indoor environments, however is not robust against envs w/ people people are picked up as spikes as theoretically they are good landmarks (just not stationary!</description></item><item><title>Wikipedia SLAM</title><link>https://salehahr.github.io/studienarbeit/wikipedia-slam/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/wikipedia-slam/</guid><description>Source: http://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping Parents: SLAM Index , slam-resources Different types of sensors give rise to different SLAM algorithms whose assumptions are most appropriate to the sensors.
At one extreme, visual features provide details of many points within an area &amp;ndash;&amp;gt; rendering SLAM unnecessary shapes in these point clouds can be easily and unambiguously aligned at each step via image registration . At the opposite extreme, tactile sensors are extremely sparse they contain only information about points very close to the agent require strong prior models to compensate in purely tactile SLAM.</description></item><item><title>Wikipedia Visual odometry</title><link>https://salehahr.github.io/studienarbeit/wikipedia-visual-odometry/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/wikipedia-visual-odometry/</guid><description>Source: http://en.wikipedia.org/wiki/Visual_odometry Odometry Visual sensors for localisation</description></item><item><title>External data in SOFA</title><link>https://salehahr.github.io/studienarbeit/external-data-in-sofa/</link><pubDate>Fri, 24 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/external-data-in-sofa/</guid><description>Parent: SofaPython Index http://www.sofa-framework.org/community/forum/topic/how-to-use-external-data-in-sofa/ http://www.sofa-framework.org/community/forum/topic/how-to-send-data-to-sofa-through-socket/ http://www.sofa-framework.org/community/forum/topic/connecting-sofa-to-an-external-data-com-port/</description></item><item><title>http://www.sofa-framework.org/applications/gallery/percutaneous-liver-surgery/</title><link>https://salehahr.github.io/studienarbeit/https-www.sofa-framework.org-applications-gallery-percutaneous-liver-surgery-/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/https-www.sofa-framework.org-applications-gallery-percutaneous-liver-surgery-/</guid><description>http://www.sofa-framework.org/applications/gallery/percutaneous-liver-surgery/ Constraint-based haptic rendering</description></item><item><title>Topological changes during elastic registration</title><link>https://salehahr.github.io/studienarbeit/topological-changes-during-elastic-registration/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/topological-changes-during-elastic-registration/</guid><description>http://www.sofa-framework.org/applications/gallery/augmented-reality-in-nephrology/
http://www.youtube.com/watch?v=3rfdL3-wWE0</description></item><item><title>Sockets Errno 10054</title><link>https://salehahr.github.io/studienarbeit/sockets-errno-10054/</link><pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/sockets-errno-10054/</guid><description>Parent: SofaPython Index WSAECONNRESET 10054 Connection reset by peer. An existing connection was forcibly closed by the remote host. This normally results if the peer application on the remote host is suddenly stopped, the host is rebooted, the host or remote network interface is disabled, or the remote host uses a hard close (see setsockopt for more information on the SO_LINGER option on the remote socket). This error may also result if a connection was broken due to keep-alive activity detecting a failure while one or more operations are in progress.</description></item><item><title>Building SOFA on Windows</title><link>https://salehahr.github.io/studienarbeit/building-sofa-on-windows/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/building-sofa-on-windows/</guid><description>Parent: SofaPython Index http://www.sofa-framework.org/community/doc/getting-started/build/windows/</description></item><item><title>Install ROSConnector in SOFA</title><link>https://salehahr.github.io/studienarbeit/install-rosconnector-in-sofa/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/install-rosconnector-in-sofa/</guid><description>Parent: SofaPython Index http://github.com/sofa-framework/SofaROSConnector Documentation (outdated for current SOFA version 20.06.00)
http://www.sofa-framework.org/community/forum/topic/error-configuring-cmake-sofarosconnector/ Pending answer. Last reply 10th July 2020.
Alternative using SoftRobots: &amp;lt;http://www.sofa-framework.org/community/forum/topic/error-with-plugins-with-sofarosconnector/
post-15665&amp;gt; http://project.inria.fr/softrobot/</description></item><item><title>Sending data using sockets</title><link>https://salehahr.github.io/studienarbeit/sending-data-using-sockets/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/sending-data-using-sockets/</guid><description>Parent: SofaPython Index http://github.com/psomers3/PyDataSocket
http://docs.python.org/3/howto/sockets.html Sockets: form of IPC (inter-process communication), for cross-platform communication (Alternatives, for fast IPC: pipes, shared memory)
“client” socket - an endpoint of a conversation e.g. browser, other client applications “server” socket, which is more like a switchboard operator. The client application (your browser, for example) uses e.g. web server (uses both server and client sockets) Roughly, how a socket works (ex: clicking a link on the browser) Client socket (browser) / Receive</description></item><item><title>SofaPython API/Documentation links</title><link>https://salehahr.github.io/studienarbeit/sofapython-api-documentation-links/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/sofapython-api-documentation-links/</guid><description>Parent: SofaPython Index SP2
SofaPython pdf http://www.sofa-framework.org/api/master/plugins/SofaPython/html/index.html http://sofacomponents.readthedocs.io/en/latest/index.html SP3
http://sofapython3.readthedocs.io/en/latest/menu/SofaPlugin.html</description></item><item><title>STLIB (Sofa Template Library)</title><link>https://salehahr.github.io/studienarbeit/stlib-sofa-template-library/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/stlib-sofa-template-library/</guid><description>Parent: SofaPython Index http://github.com/SofaDefrost/STLIB
API doc: http://stlib.readthedocs.io/en/latest/index.html
contains sofa scene template common scene template used regularly templates should be compatible with .pyscn and PSL scenes</description></item><item><title>Possible plugins</title><link>https://salehahr.github.io/studienarbeit/possible-plugins/</link><pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/possible-plugins/</guid><description>Parent: Scope of Studienarbeit , sofapython-index Communication
ZMQCommunication someone&amp;rsquo;s own plugin Optical system
OptiTrackNatNet Mesh geometry/topology
CGALPlugin (computational geometry algorithms) Haptic
Haptics with Geomagic &amp;ndash; requires Geomagic probe, but code/intro may be useful SofaHaptics Haption Flexible - for deformations Sensable Robot arm
SoftRobots ROS Connector  too complicated Scenes
STLIB (Sofa Template Library) Registration</description></item><item><title>SOFA Cataract surgery</title><link>https://salehahr.github.io/studienarbeit/sofa-cataract-surgery/</link><pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/sofa-cataract-surgery/</guid><description>http://www.sofa-framework.org/applications/gallery/eye-surgery-simulator-insimo/
SOFA – Cataract Surgery – InSimo www.sofa-framework.orgThe SOFA technology is at the core of a advanced eye surgery simulator developed in the context of the HelpMeSee project. HelpMeSee is an American foundation with a singular mission:… read more →</description></item><item><title>Basic python script in Sofa</title><link>https://salehahr.github.io/studienarbeit/basic-python-script-in-sofa/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/basic-python-script-in-sofa/</guid><description>Parent: SofaPython Index Imports import Sofa
General functions
createGraph(self, root) reset() onKeyPressed() &amp;hellip; Required in every script: createScene(rootNode)
Create a child from a node node.createChild(&amp;lsquo;Name&amp;rsquo;)
Add an object component to the node: node.createObject(type in string, kwargs**)</description></item><item><title>Collision model</title><link>https://salehahr.github.io/studienarbeit/collision-model/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/collision-model/</guid><description>Source: SOFA extended documentation Parent: Models in SOFA Primitives coming into contact — we need
collision detection collision response Collision detection approaches:
Distances between pairs of geometric primitives Points in distance fields Distances between colliding meshes using ray-tracing Intersection volume using images Collision model
Similar to internal model
Topology/geometry is different (geometry specially for contact model), can be stored in a data structure dedicated to collision detection e.</description></item><item><title>Data structure in SOFA</title><link>https://salehahr.github.io/studienarbeit/data-structure-in-sofa/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/data-structure-in-sofa/</guid><description>Source: SOFA extended documentation Three different solutions for three relevant levels [of organisation of simulation data].
Scenegraph ( directed acyclic graphs) s. also: Visitors Component attributes
Component params stored using Data&amp;lt;&amp;hellip;&amp;gt; containers e.g. list of particle indices Data&amp;lt;vector&amp;gt; Engines Create connections between Data instances, for synchronisation of values Compute a value from several others (input/output processor) Are only used to apply straightforward relations between model parameters State update algorithms are implemented using visitors Mesh geometry and mesh-topology</description></item><item><title>Haptic rendering</title><link>https://salehahr.github.io/studienarbeit/haptic-rendering/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/haptic-rendering/</guid><description>Source: SOFA extended documentation The main interest of interactive simulation is that
the user can modify the course of the computations in real-time when a virtual medical instrument comes into contact with some models of a soft-tissue, instantaneous deformations must be computed This visual feedback of the contact can be enhanced by haptic rendering so that the surgeon can really feel the contact.&amp;quot; Two main issues in SOFA for providing haptics</description></item><item><title>Internal model</title><link>https://salehahr.github.io/studienarbeit/internal-model/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/internal-model/</guid><description>Source: SOFA extended documentation Parent: Models in SOFA For the internal deformable mechanics
Contains the independent DOFs, mass and physical laws Mechanical behaviour modelled e.g. by FEM Geometry of this model is optimised for the computation of internal forces usually by using a reduced number of well-shaped tetrahedra this increases speed and stability however not accurate enough for collision detection nor is it smooth enough for visuals * Boxes: fixed nodes* Arrows: external forces [Mathematical model of the internal model in SOFA](mathematical model of-the-internal-model-in-sofa.</description></item><item><title>Mesh geometry</title><link>https://salehahr.github.io/studienarbeit/mesh-geometry/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/mesh-geometry/</guid><description>Source: SOFA extended documentation Parent: Data structure in SOFA See also: Mesh topology Mesh geometry: location of vertices in space
Meshes
k-simplices (triangles) k-cubes (quads) --&amp;gt; decomposition into k-cells
1-cell: edges 2-cells: triangles, quads 3-cells: tetrahedron, hexahedron Mesh data:
containers, similar to STL std::vector classes there are as many data structures for mesh data as topological elements , e.g. vertices, edges, triangles, quads, tetras, hexas e.</description></item><item><title>Models in SOFA</title><link>https://salehahr.github.io/studienarbeit/models-in-sofa/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/models-in-sofa/</guid><description>Source: SOFA extended documentation Backlinks: SOFA Introduction A simulation object can have several models
Each model is &amp;lsquo;predestined&amp;rsquo; for a certain task Each model is independent of the other Synchronisation of models: via a mapping mechanism Three typical models for a physical object
Internal mechanical model Collision model Visual model One of the models acts as the master
typically the internal model imposes its displacements to slaves using mappings (synchronisation of the models)</description></item><item><title>Running SOFA with Python</title><link>https://salehahr.github.io/studienarbeit/running-sofa-with-python/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/running-sofa-with-python/</guid><description>Parent: SofaPython Index From command line Add to path environment and then execute runSofa via command line
With a python script runSofa -l SofaPython ./script_name.py
How to make SofaPython loaded by default? In bin/plugin_list.conf? Yes sofa-launcher might be useful With pipenv pipenv run runsofa</description></item><item><title>Scene graph in SOFA</title><link>https://salehahr.github.io/studienarbeit/scene-graph-in-sofa/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/scene-graph-in-sofa/</guid><description>Source: SOFA extended documentation Parent: Data structure in SOFA Backlinks: SOFA Introduction See also: Scene graph (general) Pool of simulated objects and algorithms in a hierarchical data structure Scenes can be built procedurally or read from XML files Root node represents whole simulation Graph is processed by using visitors A scene graph node
Gathers components associated with the same DOFs/topology Connections between non-sibling components require explicit references Example: The collision spheres of the rigid object are in a child contact node of their own, because they are not independent DOFs (separate from independent DOFs in MechanicalState ) they are of a different data type Interactions between the rigid and deformable objects are handled by a shared component (ContactSpring) defined as a sibling node to both (coupling) Soft coupling using penalty forces Can be modelled by a constant interaction force (assumption) during each time step Compatible with all explicit time intergration schemes Hard coupling using penalty forces / constraint-based interaction via Lagrange multipliers Stiff interaction forces Implicit integration necessary, for large time steps without any instabilities Generally more efficient to process independent interaction groups using separate solvers</description></item><item><title>SOFA extended documentation</title><link>https://salehahr.github.io/studienarbeit/sofa-extended-documentation/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/sofa-extended-documentation/</guid><description>Source: http://hal.inria.fr/hal-00681539 Authors: Faure et al Backlinks: Scope of Studienarbeit Abstract
SOFA: open source C++ library mainly for interactive physical/medical simulation modular approach by decomposing simulators into its constituent components (DOF, differential equations, solvers etc), and organising them in a scenegraph data structure multimodel representation of objects (collision model, visual model etc) Chapters
Read
1: introduction 2: multimodel framework 3: data structures 3.1 scenegraph and visitors 3.</description></item><item><title>SOFA Introduction</title><link>https://salehahr.github.io/studienarbeit/sofa-introduction/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/sofa-introduction/</guid><description>Source: SOFA extended documentation Goal of SOFA: To provide a highly modular framework for interactive medical simulation, enabling collaboration across different disciplines Concept: scene-graph -based multimodel representatios
How it works:
Simulators are broken down into independent components Component: an aspect of the simulation e.g. DOF, forces, constraints, ODEs/PDEs, solvers, algorithms Components are organised in a scene graph data structure Simulated objects represented via several models</description></item><item><title>Using python with existing scene</title><link>https://salehahr.github.io/studienarbeit/using-python-with-existing-scene/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/using-python-with-existing-scene/</guid><description>Parent: SofaPython Index In scene graph
Add plugin in the scene using RequiredPlugin Define a PythonScriptController in the scene graph</description></item><item><title>(GRK 2543) Intraoperative Multi-sensor Tissue Differentiation in Oncology</title><link>https://salehahr.github.io/studienarbeit/grk-2543-intraoperative-multi-sensor-tissue-differentiation-in-oncology/</link><pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/grk-2543-intraoperative-multi-sensor-tissue-differentiation-in-oncology/</guid><description>Sources:
Deutsche Forschungsgemeinschaft project page ISYS project page Background:  Cooperation between Uni Tübingen and Uni Stuttgart Gynelogical and urological application scenarios Aims:
Minimise invasiveness and duration of surgical cancer treatment , while at the same time maximising effectiveness of the treatment Minimise damage to surrounding tissue during tumour resection Aid decision-making during surgery (intraoperative) Reliable differentiation between cancerous tissue and the surrounding healthy tissue Decide whether to preserve tissue or continue with surgery Complement existing techniques Histological Imaging Optical (IR, Raman spectroscopy) Current standard of intraoperative tissue identification: frozen section diagnostics (takes about 30 minutes)</description></item><item><title>Difference between haptic feedback and vibration alerting</title><link>https://salehahr.github.io/studienarbeit/difference-between-haptic-feedback-and-vibration-alerting/</link><pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/difference-between-haptic-feedback-and-vibration-alerting/</guid><description>Source: http://www.precisionmicrodrives.com/haptic-feedback/introduction-to-haptic-feedback/ Parent: Scope of Studienarbeit</description></item></channel></rss>