<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>vision on</title><link>https://salehahr.github.io/tags/vision/</link><description>Recent content in vision on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 17 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://salehahr.github.io/tags/vision/index.xml" rel="self" type="application/rss+xml"/><item><title>(Science Focus) How can one eye alone provide depth perception</title><link>https://salehahr.github.io/studienarbeit/science-focus-how-can-one-eye-alone-provide-depth-perception-/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/science-focus-how-can-one-eye-alone-provide-depth-perception-/</guid><description>Source: http://www.sciencefocus.com/the-human-body/how-can-one-eye-alone-provide-depth-perception/ Author: Hilary Guite
In humans with normal binocular vision, depth perception is obtained using the parallax in the two overlapping fields of vision (&amp;ldquo;binocular disparity&amp;rdquo;)
Each single field of vision has a slightly different view to the other If vision in one eye is impaired, depth perception is still obtainable even with only one eye. Some tricks that the brain uses:
We know the real size of things Using perspective, e.</description></item><item><title>10. Monocular depth perception</title><link>https://salehahr.github.io/studienarbeit/10.-monocular-depth-perception/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/10.-monocular-depth-perception/</guid><description>Parent: SLAM Index Backlinks: Thesis Depth perception in real life
In nature, prey animals typically have eyes on either side of their head to maximise field of view, while most predators have forward-facing eyes with overlapping fields of vision (binocular vision) for maximum depth perception. Humans also have binocular vision. (Some exceptions: fruit bats, killer whales)
We perceive depth, or distance to the objects that we see, based on several visual cues.</description></item><item><title>DBoW2 weighing and scoring</title><link>https://salehahr.github.io/studienarbeit/dbow2-weighing-and-scoring/</link><pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/dbow2-weighing-and-scoring/</guid><description>Source: http://github.com/dorian3d/DBow</description></item><item><title>Descriptors in feature detection/extraction</title><link>https://salehahr.github.io/studienarbeit/descriptors-in-feature-detection-extraction/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/descriptors-in-feature-detection-extraction/</guid><description>Source: http://medium.com/data-breach/introduction-to-feature-detection-and-matching-65e27179885d Backlinks: Bag of words A description of the local appearance around each feature point (keypoint) The descriptor encodes &amp;lsquo;interesting&amp;rsquo; information from the image into numbers and act as an identifier (&amp;lsquo;fingerprint&amp;rsquo;) to differentiate between features The description should ideally be invariant to changes (such as illumination, translation, scale, in-plane rotation) so that the feature can be found again, even if the image is transformed Typically: for each feature point, there is a descriptor vector Classes of descriptors:</description></item><item><title>FAST keypoint detector</title><link>https://salehahr.github.io/studienarbeit/fast-keypoint-detector/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/fast-keypoint-detector/</guid><description>Source: http://medium.com/data-breach/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf Parent: ORB descriptor FAST (Features from Accelerated and Segments Test)
How it works
Given: pixel p, surrounded by other pixels in the image
Take the surrounding pixels that are in a small circle around p If more than half of the surrounding pixels are darker/brighter than p, p is selected as a keypoint
Good for edge detection
Drawbacks</description></item><item><title>Feature matching</title><link>https://salehahr.github.io/studienarbeit/feature-matching/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/feature-matching/</guid><description>Source: http://medium.com/data-breach/introduction-to-feature-detection-and-matching-65e27179885d Backlinks: Bag of words , sparse/feature-based-vslam For matching between images, i.e. to establish a relationship (&amp;lsquo;correspondence&amp;rsquo;) between two images of the same scene or object.
Basic algorithm
Find/detect a set of identifying (&amp;lsquo;distinctive&amp;rsquo;) keypoints from all images to be matched Define a search region around each keypoint Extract and normalise the region content Compute a local descriptor from the normalised region Match local descriptors between the images Performance of matching methods depend on</description></item><item><title>ORB descriptor</title><link>https://salehahr.github.io/studienarbeit/orb-descriptor/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/orb-descriptor/</guid><description>Source: http://medium.com/data-breach/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf Backlinks: Descriptors in feature detection/extraction Oriented FAST and Rotated BRIEF, developed 2011 Was developed as an alternative to SIFT and SURF, and ended up being better/faster than both Build on FAST keypoint detector BRIEF descriptor ORB using FAST, but with (partial) scale invariance
Use a multiscale image pyramid
Each level of the pyramid is the same image, but scaled at different resolutions (reduced size as you go higher up) Once the pyramid is computed, FAST is used to detect keypoints  ORB detection</description></item><item><title>Bag of words</title><link>https://salehahr.github.io/studienarbeit/bag-of-words/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/studienarbeit/bag-of-words/</guid><description>Parent: SLAM Index Source: http://towardsdatascience.com/bag-of-visual-words-in-a-nutshell-9ceea97ce0fb
Has its origins in natural language processing (NLP), information retrieval
A text can be seen as a bag of words, with each word having different frequencies from one another This can be used to compare and classify texts (similar histograms) In vision
Instead of words we have features (identifying pattern in an image) An image is represented as a set of features Features consist of Keypoints: points that are invariant to transformation Descriptors : description of the keypoint, for feature representation Construct a frequency histogram of features in the image Workflow Feature detection/extraction &amp;ndash;&amp;gt; build vocabulary/codewords &amp;ndash;&amp;gt; make histogram = BoW</description></item></channel></rss>