<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>localisation on</title><link>https://salehahr.github.io/zettelkasten/tags/localisation/</link><description>Recent content in localisation on</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><lastBuildDate>Fri, 20 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://salehahr.github.io/zettelkasten/tags/localisation/index.xml" rel="self" type="application/rss+xml"/><item><title>Data association in DefSLAM</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/data-association-in-defslam/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/data-association-in-defslam/</guid><description>Source: Lamarca 2019 DefSLAM See also: Data association Goal: match keypoints in current frame (newly extracted) with map points (already in map/system) Use the active matching strategy proposed in [Agudo 2015]: “Simultaneous pose and non-rigid shape with particle dynamics,” Steps  ORB points (keypoints) are detected in current frame
Camera pose Tcw is predicted
using camera motion model camera motion model: function of past camera poses Predict where map points (existing in map) would be imaged, based on last estimated template i.</description></item><item><title>Covisible keyframes</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/covisible-keyframes/</link><pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/covisible-keyframes/</guid><description>Source: Palafox 2019 ( thesis ) Backlinks: Lamarca 2019 DefSLAM Two keyframes are covisible if they share several common landmarks.</description></item><item><title>Egomotion (vs odometry)</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/egomotion-vs-odometry/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/egomotion-vs-odometry/</guid><description>See also: Odometry Source: http://en.wiktionary.org/wiki/egomotion The three-dimensional movement of a camera relative to its environment
Source: http://answers.ros.org/question/296686/what-is-the-differences-between-ego-motion-and-odometry/
Generally used interchangeably with odometry Possible difference: Egomotion is more about the estimation of twist (lin, rotational velocities) Odometry is more about the estimation of path Examples
Wheel odometry: path estimation via time-integration of an estimated twist Visual odometry/Scan matching: direct estimation of pose without time-integration</description></item><item><title>Wikipedia Lokalisierung</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-lokalisierung/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-lokalisierung/</guid><description>Source: http://de.wikipedia.org/wiki/Lokalisierung_(Robotik Localisation Particle filters Categories of sensors for localisation</description></item><item><title>Odometry</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/odometry/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/odometry/</guid><description>Parent: [SLAM Index](SLAM Index.md), [IMU index](IMU index.md) Baclinks: [Position acquisition](Position acquisition.md), [SLAM hardware](SLAM hardware.md), [Prediction model](Prediction model.md) See also: [Egomotion (vs odometry)](Egomotion (vs odometry).md)
Source: [Wikipedia Visual odometry](Wikipedia Visual odometry.md)
Data can be generated from actuator movements, e.g. rotary encoders that measure motor shaft rotations This data can be used to estimate changes in position over time Usually has precision problems, e.g. due to wheels slipping and sliding, bumpy surfaces The errors are integrated over time and therefore get worse Source: cometlabs Acceleration is obtained: integrate to get velocity, displacement [estimates] However, as the estimates drift over time and get integrated, this leads to increased errors Subject to non-systematic errors e.</description></item><item><title>Distance between landmarks</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/distance-between-landmarks/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/distance-between-landmarks/</guid><description>Source: SLAM for Dummies Backlinks: Nearest Neighbour Methods:
Euclidean distance (suitable for far distances) Mahalanobis distance (better, but more complex)</description></item><item><title>Landmark extraction</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/landmark-extraction/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/landmark-extraction/</guid><description>Source: SLAM for Dummies Backlinks: Basic EKF for SLAM , nearest-neighbour Basic landmark extraction using a laser scanner
Spike algorithm RANSAC ( ekf handles points) Expansion of RANSAC so that EKF handles lines Scan-matching: two successive laser scans are matched Spike and RANSAC are good for indoor environments</description></item><item><title>Nearest Neighbour</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/nearest-neighbour/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/nearest-neighbour/</guid><description>Source: SLAM for Dummies Backlinks: Data association Nearest neighbour approach
Get a new laser scan &amp;ndash;&amp;gt; ( landmark extraction ) extract all visible landmarks Associate each extracted LM to the closest LM we have seen more than N times Pass each pairs of association (extracted LM, LM in database) through a validation gate If pair passes &amp;ndash;&amp;gt; n = n + 1 (num. times seen) If pair fails &amp;ndash;&amp;gt; add new LM to database, with n := 1</description></item><item><title>Validation gate</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/validation-gate/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/validation-gate/</guid><description>Source: SLAM for Dummies Backlinks: Nearest Neighbour An observed landmark is associated to a landmark if the following holds v innovation S innovation covariance The Validation gate makes use of the fact that the EKF implementation gives a bound on the uncertainty of an observation of a LM
Is an observed LM a LM in the database?</description></item><item><title>Key frames in loop closure detection</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/key-frames-in-loop-closure-detection/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/key-frames-in-loop-closure-detection/</guid><description>Source: cometlabs Backlinks: Loop closure detection Most common method to get candidate key frames: use a place recognition approach
approach based on vocab tree feature descriptors of candidate key frames are quantised one colour in image below corresponds to one feature descriptor/&amp;lsquo;vocabulary&amp;rsquo; each point is a &amp;lsquo;word&amp;rsquo; that belongs to a vocabulary the words can then be counted and put into a frequency histogram the histogram is used to compare similarity of images I think similar images then get filtered out, so we get key frames</description></item><item><title>Dead-reckoning</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/dead-reckoning/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/dead-reckoning/</guid><description>Source: https://en.wikipedia.org/wiki/Dead_reckoning Backlinks: [What is SLAM?](What is SLAM_.md)
In navigation, dead reckoning is the process of calculating one&amp;rsquo;s current position by using a previously determined position, by using estimations of speed and course over elapsed time
s. Brian Douglas video on sensor fusion</description></item><item><title>Kidnapped robot problem</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/kidnapped-robot-problem/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/kidnapped-robot-problem/</guid><description>Source: Wikipedia Lokalisierung Backlinks: Lamarca 2020 DefSLAM Position initially known Then robot is repositioned without knowing it Robot has to be able to realise that the initial successful localisation isn&amp;rsquo;t valid any more &amp;ndash; a new global localisation must be carried out Realise this via unplausible sensor measurements (huge contradiction to prev. measurements) Has to do with the measure of robustness of the localisation method carried out</description></item><item><title>Localisation</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/localisation/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/localisation/</guid><description>Parent: [SLAM Index](SLAM Index.md)
Source: [Wikipedia Lokalisierung](Wikipedia Lokalisierung.md) The positioning of an autonomous mobile robot relative to its environment
The position of a mobile robot is seldom known exactly An unknown initial position / measurement uncertainties while moving Becomes a SLAM problem when neither the position nor the map is known Goal/Output: POSE
Due to uncertainties etc, it&amp;rsquo;s good to have a POSE representation that also shows these uncertainties e.</description></item><item><title>Mapping representations in robotics</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-representations-in-robotics/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/mapping-representations-in-robotics/</guid><description>Source: [Cometlabs What You Need to Know About SLAM](cometlabs what you-need-to-know-about-slam.md)
Feature maps Occupancy grids Grids containing occupancy probability information Useful for path planning, exploration Drawback: computational complexity</description></item><item><title>Step 3 New landmarks</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/step-3-new-landmarks/</link><pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/step-3-new-landmarks/</guid><description>Source: SLAM for Dummies Backlinks: Basic EKF for SLAM Overview
Landmarks that are new are not dealt with until step 3. Delaying the incorporation of new landmarks until the will decrease the computation cost needed for this step the covariance matrix, P, and the system state, X, are smaller by then. Update state vector x and covariance matrix P with new landmarks Add new landmark to state vector X Add new row and column to covariance matrix Covariance for new landmark Robot-landmark covariance</description></item><item><title>Data association</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/data-association/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/data-association/</guid><description>Parents: SLAM Index , basic-ekf-for-slam Source: SLAM for Dummies Backlinks: [What is SLAM?](what is slam_.md), [data association in defslam](data association in defslam.md), visual slam-implementation-framework Matching observed landmarks from different scans (different time steps) with each other. Also called &amp;rsquo;re-observing' landmarks.
Problems that can arise:
The landmark(s) might not be observed every time step (bad landmark) Something might be observed as a landmark, but it never appears again (bad landmark) Wrong association of a landmark to a previously seen landmark Goal: define a suitable data-association policy to minimise the first two problems Given: database that stores previously seen landmarks (initially empty) As a rule: a landmark is only considered worthwhile to be used in SLAM once it is seen N times</description></item><item><title>Landmarks</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/landmarks/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/landmarks/</guid><description>Source: [SLAM for Dummies](SLAM for Dummies.md) Parent: [SLAM Index](SLAM Index.md)
Features which can easily be re-observed and distinguished from the environment
Characteristics:
Re-observable from different positions and angles Unique (i.e. no mix-up with other landmarks) Plentiful &amp;ndash; should not be so few that robot gets lost (robot spends extended time w/o enough visible landmarks) Stationary Basic landmark extraction</description></item><item><title>RANSAC</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/ransac/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/ransac/</guid><description>Source: [SLAM for Dummies](SLAM for Dummies.md) Parent: [Landmark extraction](Landmark extraction.md)
Random Sampling Consensus
to extract lines from a laser scan lines then used as landmarks indoors: straight lines from walls line landmarks are found by randomly taking a sample of laser readings (e.g. sample readings from 12deg to 22deg from within a range of 0 to 180deg) least squares approximation for line of best fit RANSAC then checks how many laser readings lie close to the best fit line initially, all readings are assumed to be unassociated to any lines if the num.</description></item><item><title>Spike landmarks</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/spike-landmarks/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/spike-landmarks/</guid><description>Source: SLAM for Dummies Backlinks: Landmark extraction Uses extrema to find landmarks Find values in the range of a laser scan, where two values differ by more than a certain amount (e.g. 0.5 m) This finds big changes in the laser scan Alternatively, using three values next to each other: A, B, C (A - B) + (C - B) yields a value Better for finding spikes as it finds actual spikes Rely on the landscape changing a lot between two laser beams Algo will fail in smooth environments Suitable for indoor environments, however is not robust against envs w/ people people are picked up as spikes as theoretically they are good landmarks (just not stationary!</description></item><item><title>Wikipedia Visual odometry</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-visual-odometry/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-visual-odometry/</guid><description>Source: http://en.wikipedia.org/wiki/Visual_odometry Odometry Visual sensors for localisation</description></item><item><title>Wikipedia Visual odometry</title><link>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-visual-odometry/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/studienarbeit/wikipedia-visual-odometry/</guid><description>Source: http://en.wikipedia.org/wiki/Visual_odometry Odometry Visual sensors for localisation</description></item></channel></rss>