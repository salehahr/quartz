<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Unlisteds on Zettelkasten</title><link>https://salehahr.github.io/zettelkasten/unlisted/</link><description>Recent content in Unlisteds on Zettelkasten</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><lastBuildDate>Tue, 05 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://salehahr.github.io/zettelkasten/unlisted/index.xml" rel="self" type="application/rss+xml"/><item><title>2022-04-05</title><link>https://salehahr.github.io/zettelkasten/unlisted/2022-04-05/</link><pubDate>Tue, 05 Apr 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/2022-04-05/</guid><description>Agenda Thesis To start on results part soon Hyperparameter optimisation normalised epochs probably need to get all runs to same epoch (50) before comparing several runs stopped early because of early stopping/crashed for some reason Found a bug optimiser was saved as a model attribute but wasn&amp;rsquo;t actually used in model.compile! Notes sweeps with best_val_precision instead of val_precision zu untersuchen: effect of batch size tensorboard profiler to figure out what&amp;rsquo;s causing the runs to slow down around epoch 20 https://www.</description></item><item><title>2022-03-22</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-03/2022-03-22/</link><pubDate>Fri, 18 Mar 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-03/2022-03-22/</guid><description>Agenda Literaturverzeichnis EdgeNN Hyperparameter optimisation with precision as metric Optimising the decision threshold on test set after training/tuning &amp;ndash; goal: reduce false positives Adjacency matrix prediction Auswertung &amp;ndash; MSE$(A_{pred} - A_{true})$ Optimised adj_matr prediction times, s. corresponding run script Only the very first run seems problematic (maybe due to first call of model.__call__?) &amp;ndash; load CUDA message appears The following prediction runs are much quicker by comparison Preview of the predictions in the loop: Starting with 3 neighbours</description></item><item><title>2022-03-15</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-03/2022-03-15/</link><pubDate>Sun, 13 Mar 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-03/2022-03-15/</guid><description>Agenda Fallunterscheidung when updating the adjacencies Starting with a large number of neighbours (seems to be better); batch size = num of neighbours To do?: increase threshold for adjacency probabilities Outcomes Note: probability output of a neural network used for classfication, s. https://stats.stackexchange.com/questions/256420/neural-networks-output-probability-estimates https://arxiv.org/pdf/1706.04599.pdf Softmax of state-of-art deep learning models is more of a score than probability estimates.
High prio tasks Thesis Adjust validation DG (ratio of edges to non-edges) resume training on 16.</description></item><item><title>Thesis outline</title><link>https://salehahr.github.io/zettelkasten/unlisted/thesis-outline/</link><pubDate>Fri, 11 Mar 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/thesis-outline/</guid><description>Outline Introduction Motivation Problem to be solved (overall) BCS takes too long Especially tissue identification (cryosection) Bypass time-consuming cryosection, incorporate sensors Why do we need sensor localisation for that Sensor data is only useful with positional information &amp;ndash;&amp;gt; need for localisation of the sensor measurements Problem statement functions take too long Deformable &amp;ndash; bad for typical point features (examples ORB, etc.</description></item><item><title>2022-03-08</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-03/2022-03-08/</link><pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-03/2022-03-08/</guid><description>Agenda Data pipeline Batching/threading logic Outcomes Use KNN instead of extracting image sections KNN for all nodes at once Adjust validation DG to reflect real world data Notes Save and restore in wandb https://docs.wandb.ai/guides/track/advanced/save-restore#how-can-i-sync-files-before-the-run-ends https://wandb.ai/lavanyashukla/save_and_restore/reports/Saving-and-Restoring-Models-with-W-B--Vmlldzo3MDQ3Mw https://github.com/wandb/client/issues/1372#issuecomment-907463675 The timestamps will currently always show the first timestamp that the file was created.
To-do (from last week 2022-03-01 ) 3rd network — batching method (CombinatorialAdjNN) For later/report Adjust validation DG (ratio of edges to non-edges) Thesis outline Hyperparameter optimisation of EdgeNN 3rd network Adj_vec method (BruteForceAdjNN) Get prediction times on model deployment Comparison on test dataset Low prio Extraction of edge pixels for data generator Hyperparameter optimisation of NodesNN Train first network (transfer learning)</description></item><item><title>2022-03-01</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-03/2022-03-01/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-03/2022-03-01/</guid><description>Outcomes/Discussed In progress: testing updated data generator over big dataset &amp;ndash;&amp;gt; removed empty graphs s. project dashboard for current runs Modification to EdgeNN network to prevent loss of information due to convolution.
[VGG] Convolving over the element-wise sum of the input tensors [VGG] Concatenating node_pair image with the convolution outputs in the first block [UNet] Use edge pixels, use contracting path for binary classification To-do (from last week 2022-02-15 ) 3rd network: Batching method (CombinatorialAdjNN) Train EdgeNN Implement batching/threading logic For later/report Hyperparameter optimisation of EdgeNN 3rd network Adj_vec method (BruteForceAdjNN) Get prediction times on model deployment Comparison on test dataset Low prio Extraction of edge pixels for data generator Programmatic implementation of early stopping Hyperparameter optimisation of second network Train first network (transfer learning)</description></item><item><title>2022-02-22</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-02/2022-02-22/</link><pubDate>Wed, 16 Feb 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-02/2022-02-22/</guid><description>Agenda Edge extraction Sample prediction table from test run Codifying of the node combinations in the input to the VGG network Currently modifying data generator for EdgeNN Use randomised node combinations across all images Equal parts adjacent nodes and non-adjacent nodes
To change: 3 layer input image to 2 layer move image batch size to run_config instead of in ds_config (or overwrite) Immediate to-dos: Debug empty Dataset error in the middle of training iterations (end of epoch 9) Check input matrix (both channels) Gesamter Lauf - test_train on whole dataset To-do (from last week 2022-02-15 ) 3rd network: Updated DataGenerator Batching method (CombinatorialAdjNN) Train EdgeNN Hyperparameter optimisation of EdgeNN Implement batching/threading logic Adj_vec method (BruteForceAdjNN) Extraction of edge pixels For later/report Get prediction times on model deployment Comparison on test dataset Low prio Programmatic implementation of early stopping Hyperparameter optimisation of second network Train first network (transfer learning) Misc Immediate to-dos: Run through all unit tests, CI &amp;ndash; no go because filepaths are all in Tuebingen wandb artifacts waiting on this issue: https://github.</description></item><item><title>2022-02-15</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-02/2022-02-15/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-02/2022-02-15/</guid><description>Agenda Currently working on EdgeNN model: VGG16 architecture — training and predicting OK Modification of existing skeletonised images to remove edges of length 1? s. previous meeting notes .
orig skel img -&amp;gt; extract nodes -&amp;gt; extract edges, find pixels to remove -&amp;gt; modify skel img -&amp;gt; modify list of nodes/node types New graph extraction w/ new edge attribute matrix? To-do (from last week 2022-02-02 ) 3rd network: Updated DataGenerator Batching method (CombinatorialAdjNN) Train EdgeNN Hyperparameter optimisation of EdgeNN Implement batching/threading logic Adj_vec method (BruteForceAdjNN) Extraction of edge pixels For later/report Get prediction times on model deployment Comparison on test dataset Low prio Programmatic implementation of early stopping Hyperparameter optimisation of second network Train first network (transfer learning)</description></item><item><title>2022-02-08</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-02/2022-02-08/</link><pubDate>Thu, 03 Feb 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-02/2022-02-08/</guid><description>Agenda Visualisation of the dataset for EdgeNN As discussed on Fri, train EdgeNN without the paths as labels for now Refactored edge_extraction function in graph-training repo &amp;ndash; it also detects edges of length one, removes them, slates the corresponding nodes for removal Comparison
Original Pixels to remove To do: new graph extraction?</description></item><item><title>2022-01-25</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-01/2022-01-25/</link><pubDate>Tue, 18 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-01/2022-01-25/</guid><description>Agenda For next week: unit tests demo To-do (from last week 2022-01-18 ) 3rd network: Batching method Adj_vec method Programmatic implementation of early stopping Hyperparameter optimisation &amp;ndash; wanb or similar Get prediction times on model deployment Outcomes New edge attributes (extracted using two local coordinates) &amp;ndash;&amp;gt; resulting edge attribute matrix isn&amp;rsquo;t symmetric anymore.</description></item><item><title>2022-02-02</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-02/2022-02-02/</link><pubDate>Tue, 18 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-02/2022-02-02/</guid><description>Agenda Modified DataGenerator &amp;ndash; makes use of tf.data.Dataset.map functionality Implemented data augmentation for adjacency matrix in graph execution mode
Checks show that it&amp;rsquo;s not correct yet. Done ! Currently working on EdgeExtractionDG to produce data for training EdgeNN Unit tests demo test_file_functions https://docs.python.org/3/library/unittest.html Aufbau: test cases Asserts test_graph CI, ML ops branch with failed test https://mattsegal.dev/devops-academic-research.html Dev Ops: from SW dev &amp;ndash; for shortening development times DVC channel Intro to CI for ML Outcomes For training EdgeNN &amp;ndash; two options: Only output 0 and 1, path extraction in a separate network Output both adjacency and path simultaneously; the idea is that the path extraction helps with the backpropagation Next week: incorporate new graph extraction method and regenerate graph objects [Fri 4.</description></item><item><title>2022-01-18</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-01/2022-01-18/</link><pubDate>Fri, 14 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-01/2022-01-18/</guid><description>Agenda Progress to do: data augmentation for $A$ data pipeline Unit tests demo &amp;ndash;&amp;gt; 2022-02-02 To-do (from last week 2022-01-11 ) Modify U-Net model to be more general 3rd network: Batching method Adj_vec method Programmatic implementation of early stopping Hyperparameter optimisation &amp;ndash; wanb or similar Get prediction times on model deployment</description></item><item><title>2022-01-11</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-01/2022-01-11/</link><pubDate>Tue, 04 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-01/2022-01-11/</guid><description>Agenda Node Extraction NN Unit tests demo &amp;ndash;&amp;gt; next week W&amp;amp;B demo Binary Termine naechste Woche To-do (from last week 2022-01-04 ) Modify U-Net model to be more general 3rd network: NLP zero padding tutorial with RaggedTensor Programmatic implementation of early stopping Hyperparameter optimisation &amp;ndash; wanb or similar Get prediction times on model deployment Notes Cross-validation is not implemented $A$ &amp;ndash;&amp;gt; binary number Network: skel &amp;ndash;&amp;gt; decimal number &amp;ndash;&amp;gt; $A$ Length of binary number:</description></item><item><title>2021-12-14-notes</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-12/2021-12-14-notes/</link><pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-12/2021-12-14-notes/</guid><description>Some test runs with no pretrained weights First run: starts with quite a low loss and high accuracy. Probably due to the zeros of the matrices being matched?
Problem with rotating as a data augmentation method &amp;ndash;&amp;gt; only do flips
Problem with degrees &amp;ndash;&amp;gt; fixed!</description></item><item><title>2022-01-04</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-01/2022-01-04/</link><pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2022-01/2022-01-04/</guid><description>Notes MA Data Filter parameters for individual videos Preview of processed videos Number of data Total for training + validation: 4416 (real) + 4487 (synth) 1 2 ls */[!t]*/graphs/* | wc -l ls [!t]*/graphs/* | wc -l For testing: 488 (real) + 0 (synth) 1 2 ls **/test_*/graphs/* | wc -l ls test_*/graphs/* | wc -l 1 NN — next steps First network</description></item><item><title>2021-12-14</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-12/2021-12-14/</link><pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-12/2021-12-14/</guid><description>Agenda MA Besprochen letzte Woche: scrap rotations as a data augmentation method modify network architecture skel &amp;ndash;&amp;gt; node attributes, e.g. fewer convolution filters filters between UNet final layer and the degrees and node types outputs architecture that prioritises degrees and node types (because the node positions can be inferred from both) for raw image &amp;ndash;&amp;gt; filt/skel network, use pretrained weights for medical image processing tasks, modifications after the final unet layer First run of model 20211210-102252\train with 64 filters Second run with reduced num.</description></item><item><title>2021-12-07</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-12/2021-12-07/</link><pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-12/2021-12-07/</guid><description>Agenda MA Model now trains till end of epoch with model.fit().
Fix: use dataset.as_numpy_iterator().tolist() instead of dataset Current problem: losses are NaN, resulting predicted matrices are NaN fixed Still looking into it, but according to this thread , the cross entropy loss might be [one of] the cause[s]. https://stackoverflow.com/questions/40192728/cross-entropy-is-nan fix: use softmax activation HiWi Kugel ist fertig
GUI mit Beispielberechnung im separaten Thread ist fertig</description></item><item><title>2021-11-30</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-30/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-30/</guid><description>Agenda HiWi New: light position = camera centre MA Currently: must debug DataGenerator: training stops 1-3 batches before end! To do (from last week) 2021-11-23 HiWi Make light follow camera rotation Enable camera movement parallel to an ongoing rendering calculation (s. example with the sphere to cow rendering) Weihnachtskugel malen MA Train model skeletonised -&amp;gt; node attributes using existing data Debug DataGenerator Preliminaries Data augmentation - debug Data generation Watch more videos and trim away bad sections Adjust filter parameters so that more structures are visible Training data generation up to 3k w/o data augmentation Outcomes Possible outcomes for later: Comparing brute force method (input-output method) to function-/theory-based method (evtl mit PyTorch Funktionen, da muessen die Funktionen ableitbar sein) Try ReLU activation instead of sigmoid Notes World and camera coordinate systems in PyTorch3D Blasenmuster Fake Real</description></item><item><title>2021-11-16</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-16-static/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-16-static/</guid><description>See also: 2021-11-16-outcomes Agenda Notes MA Scrapped translations as a data augmentation option Implemented function to sort nodes (left to right, up to down) before graph creation, thus ensuring that the adjacency matrix uses sorted node positions Created test suites which use a random photo from the existing dataset [training data] Testing suite in TestGraph (check if node positions are sorted, check if adj matrix matches photo) [tfgraph] Testing suite TestDataGeneration for testing data augmentation, s.</description></item><item><title>2021-11-23</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-23/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-23/</guid><description>Agenda Notes MA Function for classifying between end nodes and border (invalid) nodes
Checks if node coordinates are in border zone
Border: 2px wide Helper node (green) -&amp;gt; reclassified to crossing node (blue) Up till now, saved border nodes in graph with the attribute NodeType.BORDER = 0 &amp;ndash;&amp;gt; change to another value to avoid mixup with the black pixels! Sample training data Data augmentation not yet working as desired, s.</description></item><item><title>2021-11-16-sample-augmented-data</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-16-sample-augmented-data/</link><pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-16-sample-augmented-data/</guid><description>Parent: 2021-11-16-static Sample input Augmented Input Filtered Skeletonised Node positions</description></item><item><title>2021-11-09</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-09/</link><pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-09/</guid><description>Agenda To-dos Ongoing Training data generation (up to 3k), including adjacency matrix Figure out model checkpoints &amp;ndash; still don&amp;rsquo;t understand Adjust model &amp;ndash; add more output channels Start HiWi (Pytorch 3D + PyQT GUI) by the end of the week New Model summary &amp;ndash; ca. 30M parameters Make new training data for 256px images (two folders: 256px, 512px) Implement data augmentation Rotations, translation, flip ok Schwierig oder nicht gut machbar: blurring, stretching, zoom U-Net DataGenerator, so far without augmentation Based on the class UNet in 03_CNN &amp;ndash; modifications: Branched outputs (to apply different losses/activation functions) Filtered output: linear activation, MSE loss Skeletonised output: sigmoid activation, binary crossentropy loss Alternative: final conv2D layer with 4 filters?</description></item><item><title>2021-11-03</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-03/</link><pubDate>Wed, 03 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-03/</guid><description>To-do Old Generate up to 3k training photos [Hiwi] combine Peter&amp;rsquo;s existing PyQT GUI with PyTorch 3D to enable changing the camera view using mouse read Jupyter notebook tutorial go through PyQT stuff New Resize images to be smaller, squared, a square of two, e.g. 512 x 512 px To avoid unnecessary padding in the model Avoids the need for resizing within the model Note: if do end up using tf.</description></item><item><title>2021-11-02</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-02/</link><pubDate>Tue, 02 Nov 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-02/</guid><description>Agenda To clarify Training data Zugang zum Rechner Tuebingen Hiwi Training data Did a parameter study for B-COSFIRE filter parameters Automated some functions: naming convention, trimming according to video_data.py Some training data GRK021, GRK008 Goal: ca. 3,000 photos, apply transformations to these to get more training data ~10,000 Questions Which videos are already &amp;lsquo;done&amp;rsquo;, which are more important, in what order to process? &amp;ndash; doesn&amp;rsquo;t matter which, as long as I know which videos I took them from Do the images have to be squares?</description></item><item><title>2021-10-15</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-10/2021-10-15/</link><pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-10/2021-10-15/</guid><description>2021-10-15 Vorstellung Overall localisation project
Part that deals with localisation and mapping (graph stuff) Part that deals with deformation &amp;ndash; using Pytorch 3D Pytorch 3D 3D ground truth models are expensive Idea: generate 3D models (mesh) from 2D data (graph as a texture) Localisation using graphs Deformation problem: end nodes can appear to change position based on light conditions, bifurcation nodes are not affected Use neural network to extract adjacency matrix in real time One problem: Extraction of adjacency matrix is not robust to changes in matrix dimension, therefore do zero padding (as a partial solution to the problem) Idea First NN to extract node positions, node attributes such as polynomial degree, coefficients, etc number of connections Second NN to extract adjacency matrix between two nodes only (prerequisite: node positions already known, number of connections already known) Threading &amp;ndash; a thread can be terminated once the number of adjacencies has been found corresponding to the number of connections</description></item><item><title>2021-10-26</title><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-10/2021-10-26/</link><pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-10/2021-10-26/</guid><description>2021-10-26 Kick-off meeting Tasks Read Regine&amp;rsquo;s documentation, SA thesis Generate training data with ground truth (labels) using Regine&amp;rsquo;s code Trim videos to show only relevant sections (where blood vessels are visible) &amp;mdash; (differentiating the situations to be implemented in other work) &amp;mdash; creating function in Python to trim Framework that runs all the functions in one go Notes Meetings Tuesday 10 a.m. Previous work by Regine: not real time &amp;ndash;&amp;gt; achieve real time capability by using Unet Existing code does: Filter colour photo to black and white Skeletonises the photo Extracts node position from the photo Extracts node attributes NN input: 256x256x3 photo NN output: 256x256xn, with n channels Channel 1: filtered photo Channel 2: skeletonised photo Channel 3: node position Channel 4: polynomial degree Maybe skip Channel 1 (filtered photo) and go to output 2 (skeletonished photo) directly Channel 3 (node pos) to go to polynomial degree directly, because once a nonzero number is in the matrix, it indicates that a node exists at that position!</description></item><item><title>ma-minutes</title><link>https://salehahr.github.io/zettelkasten/unlisted/ma-minutes/</link><pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/ma-minutes/</guid><description> 2021-10-15 2021-10-26 2021-11-02 2021-11-03 2021-11-09 2021-11-16-static 2021-11-16-outcomes 2021-11-23 2021-11-30 2021-12-07 2021-12-14 node-detection-model-first-run 2022-01-04 2022-01-11 2022-01-18 2022-01-25 2022-02-02 2022-02-08 2022-02-15 2022-02-22 2022-03-01 2022-03-08 2022-03-15 2022-03-22 2022-03-28 — brief meeting 2022-04-05</description></item><item><title/><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-23-data-augmentation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-11/2021-11-23-data-augmentation/</guid><description>Data Augmentation To try: try a coordinate transformation of the node positions instead.
The current rotation transformation:
Rotation of an integer matrix results in floats between 0 and 1.</description></item><item><title/><link>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-12/2021-12-14-node-detection-model-first-run/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/zettelkasten/unlisted/minutes/2021-12/2021-12-14-node-detection-model-first-run/</guid><description>First run of model 20211210-102252\train 64 filters in first layer model architecture
Validation results Pred | Truth
Pred | Truth
Pred | Truth
Pred | Truth
Total loss
Partial losses
Node positions converge the fastest Node types converge the slowest</description></item></channel></rss>