<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Permanent notes index on</title><link>https://salehahr.github.io/quartz/permanent/</link><description>Recent content in Permanent notes index on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 15 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://salehahr.github.io/quartz/permanent/index.xml" rel="self" type="application/rss+xml"/><item><title>20.4 Which orientation parametrisation to choose?</title><link>https://salehahr.github.io/quartz/permanent/which-orientation-parametrisation-to-choose/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/quartz/permanent/which-orientation-parametrisation-to-choose/</guid><description>Parents: Permanent notes , Rotations/SO(3) group index , quaternion-index , orientation-parametrisations See also: Rotation error representation Source: [MKok 2017 Using inertial sensors for position and orientation estimation](mkok 2017 using inertial sensors-for-position-and-orientation-estimation.md)
Estimation algorithms (filtering, smoothing) usually assume that the unknown states and parameters are represented in Euclidean space
Due to wrapping and gimbal lock, Euclidian addition and subtraction don&amp;rsquo;t work Also generally don&amp;rsquo;t work for rotation matrices and unit quaternions Constraints (unit quaternion norm, rotation matrix orthogonality) are usually hard to implement in estimation algorithms these concerns are led to the development of the MEKF To deal with this: Linearisation of an orientation in SO(3) Alternative method to estimate orientation:</description></item><item><title>10. Monocular depth perception</title><link>https://salehahr.github.io/quartz/permanent/monocular-depth-perception/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://salehahr.github.io/quartz/permanent/monocular-depth-perception/</guid><description>Parent: Permanent notes , SLAM Index Backlinks: Thesis Depth perception in real life
In nature, prey animals typically have eyes on either side of their head to maximise field of view, while most predators have forward-facing eyes with overlapping fields of vision (binocular vision) for maximum depth perception. Humans also have binocular vision. (Some exceptions: fruit bats, killer whales)
We perceive depth, or distance to the objects that we see, based on several visual cues.</description></item></channel></rss>